{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyM33XngpXXwev9UkJHFOGw2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Experiment 1"],"metadata":{"id":"hQuTRF-dQxaO"}},{"cell_type":"markdown","source":["## Merging results"],"metadata":{"id":"i7ppNLU8Q1t0"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1205856,"status":"ok","timestamp":1731863268442,"user":{"displayName":"John Alejandro Mendoza Ortiz","userId":"11091031515932154769"},"user_tz":-60},"id":"eT9l9SLcenyh","outputId":"913ef327-e33a-4ee5-9799-71439aca9726"},"outputs":[{"name":"stdout","output_type":"stream","text":["🚀 Processing horizon 1 with prefix 'None'...\n","✅ Loaded Data/Test/horizon_1/bitcn_model0_1_horizon_1.csv\n","✅ Loaded Data/Test/horizon_1/tide_model0_1_horizon_1.csv\n","✅ Loaded Data/Test/horizon_1/tft_model0_1_horizon_1.csv\n","✅ Loaded Data/Test/horizon_1/nhits_model0_1_horizon_1.csv\n","🔗 Merging data for horizon 1 with prefix 'None'...\n","✅ Merging completed.\n","💾 Saved merged data to Data/Test/horizon_1/bitcn_tide_tft_nhits_model0_1_full_horizon_1.csv\n","💾 Saved subset data to Data/Test/horizon_1/bitcn_tide_tft_nhits_model0_1_sub_horizon_1.csv\n","\n","✅ Finished processing horizon 1 for prefix 'None'\n","\n","🚀 Processing horizon 5 with prefix 'None'...\n","✅ Loaded Data/Test/horizon_5/bitcn_model0_1_horizon_5.csv\n","✅ Loaded Data/Test/horizon_5/tide_model0_1_horizon_5.csv\n","✅ Loaded Data/Test/horizon_5/tft_model0_1_horizon_5.csv\n","✅ Loaded Data/Test/horizon_5/nhits_model0_1_horizon_5.csv\n","🔗 Merging data for horizon 5 with prefix 'None'...\n","✅ Merging completed.\n","💾 Saved merged data to Data/Test/horizon_5/bitcn_tide_tft_nhits_model0_1_full_horizon_5.csv\n","💾 Saved subset data to Data/Test/horizon_5/bitcn_tide_tft_nhits_model0_1_sub_horizon_5.csv\n","\n","✅ Finished processing horizon 5 for prefix 'None'\n","\n","🚀 Processing horizon 10 with prefix 'None'...\n","✅ Loaded Data/Test/horizon_10/bitcn_model0_1_horizon_10.csv\n","✅ Loaded Data/Test/horizon_10/tide_model0_1_horizon_10.csv\n","✅ Loaded Data/Test/horizon_10/tft_model0_1_horizon_10.csv\n","✅ Loaded Data/Test/horizon_10/nhits_model0_1_horizon_10.csv\n","🔗 Merging data for horizon 10 with prefix 'None'...\n","✅ Merging completed.\n","💾 Saved merged data to Data/Test/horizon_10/bitcn_tide_tft_nhits_model0_1_full_horizon_10.csv\n","💾 Saved subset data to Data/Test/horizon_10/bitcn_tide_tft_nhits_model0_1_sub_horizon_10.csv\n","\n","✅ Finished processing horizon 10 for prefix 'None'\n","\n","🚀 Processing horizon 20 with prefix 'None'...\n","✅ Loaded Data/Test/horizon_20/bitcn_model0_1_horizon_20.csv\n","✅ Loaded Data/Test/horizon_20/tide_model0_1_horizon_20.csv\n","✅ Loaded Data/Test/horizon_20/tft_model0_1_horizon_20.csv\n","✅ Loaded Data/Test/horizon_20/nhits_model0_1_horizon_20.csv\n","🔗 Merging data for horizon 20 with prefix 'None'...\n","✅ Merging completed.\n","💾 Saved merged data to Data/Test/horizon_20/bitcn_tide_tft_nhits_model0_1_full_horizon_20.csv\n","💾 Saved subset data to Data/Test/horizon_20/bitcn_tide_tft_nhits_model0_1_sub_horizon_20.csv\n","\n","✅ Finished processing horizon 20 for prefix 'None'\n","\n","🚀 Processing horizon 1 with prefix '8TY'...\n","✅ Loaded Data/Test/horizon_1/8TYbitcn_model0_1_horizon_1.csv\n","✅ Loaded Data/Test/horizon_1/8TYtide_model0_1_horizon_1.csv\n","✅ Loaded Data/Test/horizon_1/8TYtft_model0_1_horizon_1.csv\n","✅ Loaded Data/Test/horizon_1/8TYnhits_model0_1_horizon_1.csv\n","🔗 Merging data for horizon 1 with prefix '8TY'...\n","✅ Merging completed.\n","💾 Saved merged data to Data/Test/horizon_1/8TYbitcn_tide_tft_nhits_model0_1_full_horizon_1.csv\n","💾 Saved subset data to Data/Test/horizon_1/8TYbitcn_tide_tft_nhits_model0_1_sub_horizon_1.csv\n","\n","✅ Finished processing horizon 1 for prefix '8TY'\n","\n","🚀 Processing horizon 5 with prefix '8TY'...\n","✅ Loaded Data/Test/horizon_5/8TYbitcn_model0_1_horizon_5.csv\n","✅ Loaded Data/Test/horizon_5/8TYtide_model0_1_horizon_5.csv\n","✅ Loaded Data/Test/horizon_5/8TYtft_model0_1_horizon_5.csv\n","✅ Loaded Data/Test/horizon_5/8TYnhits_model0_1_horizon_5.csv\n","🔗 Merging data for horizon 5 with prefix '8TY'...\n","✅ Merging completed.\n","💾 Saved merged data to Data/Test/horizon_5/8TYbitcn_tide_tft_nhits_model0_1_full_horizon_5.csv\n","💾 Saved subset data to Data/Test/horizon_5/8TYbitcn_tide_tft_nhits_model0_1_sub_horizon_5.csv\n","\n","✅ Finished processing horizon 5 for prefix '8TY'\n","\n","🚀 Processing horizon 10 with prefix '8TY'...\n","✅ Loaded Data/Test/horizon_10/8TYbitcn_model0_1_horizon_10.csv\n","✅ Loaded Data/Test/horizon_10/8TYtide_model0_1_horizon_10.csv\n","✅ Loaded Data/Test/horizon_10/8TYtft_model0_1_horizon_10.csv\n","✅ Loaded Data/Test/horizon_10/8TYnhits_model0_1_horizon_10.csv\n","🔗 Merging data for horizon 10 with prefix '8TY'...\n","✅ Merging completed.\n","💾 Saved merged data to Data/Test/horizon_10/8TYbitcn_tide_tft_nhits_model0_1_full_horizon_10.csv\n","💾 Saved subset data to Data/Test/horizon_10/8TYbitcn_tide_tft_nhits_model0_1_sub_horizon_10.csv\n","\n","✅ Finished processing horizon 10 for prefix '8TY'\n","\n","🚀 Processing horizon 20 with prefix '8TY'...\n","✅ Loaded Data/Test/horizon_20/8TYbitcn_model0_1_horizon_20.csv\n","✅ Loaded Data/Test/horizon_20/8TYtide_model0_1_horizon_20.csv\n","✅ Loaded Data/Test/horizon_20/8TYtft_model0_1_horizon_20.csv\n","✅ Loaded Data/Test/horizon_20/8TYnhits_model0_1_horizon_20.csv\n","🔗 Merging data for horizon 20 with prefix '8TY'...\n","✅ Merging completed.\n","💾 Saved merged data to Data/Test/horizon_20/8TYbitcn_tide_tft_nhits_model0_1_full_horizon_20.csv\n","💾 Saved subset data to Data/Test/horizon_20/8TYbitcn_tide_tft_nhits_model0_1_sub_horizon_20.csv\n","\n","✅ Finished processing horizon 20 for prefix '8TY'\n","\n","✅ Completed all processing\n"]}],"source":["import pandas as pd\n","import os\n","\n","class HorizonMerger:\n","    def __init__(self, base_dir='Data/Test', models=['bitcn', 'tide', 'tft', 'nhits'],\n","                 model_version='model0_1', horizons=[1], prefixes=[None]):\n","        \"\"\"\n","        Initializes the HorizonMerger with specified parameters.\n","\n","        Parameters:\n","        - base_dir (str): Base directory where data is stored.\n","        - models (list): List of model names to process.\n","        - model_version (str): Version identifier for the models.\n","        - horizons (list): List of horizon values to process.\n","        - prefixes (list): List of prefixes to process, e.g., ['', '8TY'].\n","        \"\"\"\n","        self.base_dir = base_dir\n","        self.models = models\n","        self.model_version = model_version\n","        self.horizons = horizons\n","        self.prefixes = prefixes  # List of prefixes\n","        # Mapping from model names to their corresponding column prefixes\n","        self.model_column_mapping = {\n","            'bitcn': 'AutoBiTCN',\n","            'tide': 'AutoTiDE',\n","            'tft': 'AutoTFT',\n","            'nhits': 'AutoNHITS'\n","        }\n","\n","    def merge_models_for_horizon_and_prefix(self, horizon, prefix):\n","        \"\"\"\n","        Merges CSV files for a specific horizon and prefix, then saves the full and subset data.\n","\n","        Parameters:\n","        - horizon (int): The horizon value to process.\n","        - prefix (str): The prefix to add to the filename.\n","        \"\"\"\n","        # List to store individual DataFrames\n","        df_list = []\n","\n","        # Path to the current horizon's directory\n","        horizon_dir = os.path.join(self.base_dir, f'horizon_{horizon}')\n","\n","        # Ensure the horizon directory exists\n","        if not os.path.isdir(horizon_dir):\n","            print(f\"⚠️ Horizon directory does not exist: {horizon_dir}. Creating it...\")\n","            os.makedirs(horizon_dir, exist_ok=True)\n","\n","        # Read each model's CSV for the current horizon\n","        for model in self.models:\n","            # Update file name to include the prefix\n","            file_name = f'{prefix or \"\"}{model}_{self.model_version}_horizon_{horizon}.csv'\n","            file_path = os.path.join(horizon_dir, file_name)\n","            if not os.path.exists(file_path):\n","                print(f\"❌ File not found: {file_path}\")\n","                return  # Skip processing this horizon if any file is missing\n","            df = pd.read_csv(file_path)\n","            df_list.append(df)\n","            print(f\"✅ Loaded {file_path}\")\n","\n","        # Merge all DataFrames on the specified keys\n","        print(f\"🔗 Merging data for horizon {horizon} with prefix '{prefix}'...\")\n","        merged_df = df_list[0]\n","        for df in df_list[1:]:\n","            merged_df = pd.merge(merged_df, df, on=['unique_id', 'ds', 'cutoff', 'y'], how='inner')\n","        print(\"✅ Merging completed.\")\n","\n","        # Define the output filename and path for the full merged data\n","        full_output_filename = f'{prefix or \"\"}{\"_\".join(self.models)}_{self.model_version}_full_horizon_{horizon}.csv'\n","        full_output_path = os.path.join(horizon_dir, full_output_filename)\n","\n","        # Save the merged DataFrame\n","        merged_df.to_csv(full_output_path, index=False)\n","        print(f\"💾 Saved merged data to {full_output_path}\")\n","\n","        # Prepare subset columns based on the mapping\n","        subset_columns = ['unique_id', 'ds', 'y']\n","        for model in self.models:\n","            col_prefix = self.model_column_mapping.get(model)\n","            if col_prefix:\n","                subset_columns.extend([col_prefix, f'{col_prefix}1'])\n","            else:\n","                print(f\"⚠️ No column mapping found for model '{model}'. Skipping its subset columns.\")\n","\n","        # Check for missing columns and handle them\n","        missing_cols = [col for col in subset_columns if col not in merged_df.columns]\n","        if missing_cols:\n","            print(f\"⚠️ Missing columns for subset: {missing_cols}\")\n","            for col in missing_cols:\n","                merged_df[col] = pd.NA\n","\n","        # Create the subset DataFrame\n","        df_subset = merged_df[subset_columns].copy()\n","\n","        # Define the output filename and path for the subset data\n","        subset_output_filename = f'{prefix or \"\"}{\"_\".join(self.models)}_{self.model_version}_sub_horizon_{horizon}.csv'\n","        subset_output_path = os.path.join(horizon_dir, subset_output_filename)\n","\n","        # Save the subset DataFrame\n","        df_subset.to_csv(subset_output_path, index=False)\n","        print(f\"💾 Saved subset data to {subset_output_path}\\n\")\n","\n","    def process_all_horizons_and_prefixes(self):\n","        \"\"\"\n","        Processes all specified horizons and prefixes by merging and saving their respective data.\n","        \"\"\"\n","        for prefix in self.prefixes:\n","            for horizon in self.horizons:\n","                print(f\"🚀 Processing horizon {horizon} with prefix '{prefix}'...\")\n","                self.merge_models_for_horizon_and_prefix(horizon, prefix)\n","                print(f\"✅ Finished processing horizon {horizon} for prefix '{prefix}'\\n\")\n","\n","if __name__ == \"__main__\":\n","    # Initialize the HorizonMerger with horizons and prefixes\n","    merger = HorizonMerger(\n","        horizons=[1, 5, 10, 20],\n","        prefixes=[None, '8TY']\n","    )\n","\n","    # Start processing all specified horizons and prefixes\n","    merger.process_all_horizons_and_prefixes()\n","    print(\"✅ Completed all processing\")"]},{"cell_type":"markdown","source":["## QLIKE evaluation and DM test\n","\n","Following the evaluation metrics as well as MCS and DM test from Suoto and Moradi 2024\n","\n","https://www.emerald.com/insight/content/doi/10.1108/cfri-01-2024-0032/full/html\n","\n","https://github.com/hugogobato/Can-Transformers-Transform-Financial-Forecasting-"],"metadata":{"id":"5fyQqd02Q9xN"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"cuso9yHfQXoI","outputId":"67d907c8-5676-4bba-dccd-ee8a97f644ab"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing Prefix: '', Horizon: 1\n","Completed processing for Prefix: '', Horizon: 1\n","\n","Processing Prefix: '', Horizon: 5\n","Completed processing for Prefix: '', Horizon: 5\n","\n","Processing Prefix: '', Horizon: 10\n","Completed processing for Prefix: '', Horizon: 10\n","\n","Processing Prefix: '', Horizon: 20\n","Completed processing for Prefix: '', Horizon: 20\n","\n","Processing Prefix: '8TY', Horizon: 1\n","Completed processing for Prefix: '8TY', Horizon: 1\n","\n","Processing Prefix: '8TY', Horizon: 5\n","Completed processing for Prefix: '8TY', Horizon: 5\n","\n","Processing Prefix: '8TY', Horizon: 10\n","Completed processing for Prefix: '8TY', Horizon: 10\n","\n","Processing Prefix: '8TY', Horizon: 20\n","Completed processing for Prefix: '8TY', Horizon: 20\n","\n"]}],"source":["import os\n","import pandas as pd\n","import numpy as np\n","from itertools import combinations\n","from scipy.stats import t\n","from arch.bootstrap import MCS\n","\n","# Define the horizons and prefixes\n","horizons = [1, 5, 10, 20]\n","prefixes = ['', '8TY']\n","\n","# Loop over each prefix and horizon\n","for prefix in prefixes:\n","    for horizon in horizons:\n","        print(f\"Processing Prefix: '{prefix}', Horizon: {horizon}\")\n","\n","        # Define file paths based on prefix and horizon\n","        prefix_str = f\"{prefix}_\" if prefix else ''\n","        file_name = f\"{prefix}bitcn_tide_tft_nhits_model0_1_sub_horizon_{horizon}.csv\"\n","        data_path = f\"Data/Test/horizon_{horizon}/{file_name}\"\n","        output_dir = f\"Data/Evaluation/horizon_{horizon}\"\n","        os.makedirs(output_dir, exist_ok=True)\n","\n","        # Read the CSV file\n","        df = pd.read_csv(data_path)\n","        df = df.sort_values(by=['unique_id', 'ds']).drop_duplicates(subset=['ds', 'unique_id'])\n","        df['ds'] = pd.to_datetime(df['ds'])\n","\n","        # Extract model columns\n","        model_columns = ['AutoTiDE', 'AutoTiDE1', 'AutoTFT', 'AutoTFT1',\n","                         'AutoBiTCN', 'AutoBiTCN1', 'AutoNHITS', 'AutoNHITS1']\n","\n","        # Create a dictionary to hold pivoted DataFrames for each model\n","        model_dfs = {}\n","        for model in model_columns:\n","            model_df = df.pivot(index='ds', columns='unique_id', values=model)\n","            model_dfs[model] = model_df\n","\n","        # Pivot actual values\n","        Actuals = df.pivot(index='ds', columns='unique_id', values='y')\n","\n","        # Calculate error metrics for each model\n","        metrics = ['RMSE', 'MAE', 'MAPE', 'QLIKE']\n","        error_metrics = {metric: {} for metric in metrics}\n","\n","        for model_name, predictions in model_dfs.items():\n","            # Align the predictions and actuals\n","            predictions, actuals = predictions.align(Actuals, join='inner', axis=0)\n","            errors = predictions - actuals\n","\n","            # RMSE Calculation\n","            mse = (errors ** 2).mean().mean()\n","            error_metrics['RMSE'][model_name] = np.sqrt(mse)\n","\n","            # MAE Calculation\n","            mae = errors.abs().mean().mean()\n","            error_metrics['MAE'][model_name] = mae\n","\n","            # MAPE Calculation\n","            with np.errstate(divide='ignore', invalid='ignore'):\n","                mape = (errors.abs() / actuals).replace([np.inf, -np.inf], np.nan).mean().mean()\n","            error_metrics['MAPE'][model_name] = mape\n","\n","            # QLIKE Calculation\n","            with np.errstate(divide='ignore', invalid='ignore'):\n","                ratio = actuals / predictions\n","                qlike = (ratio - np.log(ratio) - 1).replace([np.inf, -np.inf], np.nan).mean().mean()\n","            error_metrics['QLIKE'][model_name] = qlike\n","\n","        # Create a DataFrame with error metrics\n","        error_metrics_df = pd.DataFrame(error_metrics)\n","        metrics_file = f\"{output_dir}/{prefix_str}metrics_horizon_{horizon}.csv\"\n","        error_metrics_df.to_csv(metrics_file)\n","\n","        # Function to calculate losses for MCS\n","        def calculate_losses(metric, model_dfs, actuals):\n","            residuals_dict = {}\n","            for model_name, predictions in model_dfs.items():\n","                # Align predictions and actuals\n","                predictions_aligned, actuals_aligned = predictions.align(actuals, join='inner', axis=0)\n","                if metric == 'RMSE':\n","                    residuals = (predictions_aligned - actuals_aligned) ** 2\n","                elif metric == 'MAE':\n","                    residuals = (predictions_aligned - actuals_aligned).abs()\n","                elif metric == 'MAPE':\n","                    with np.errstate(divide='ignore', invalid='ignore'):\n","                        residuals = (predictions_aligned - actuals_aligned).abs() / actuals_aligned\n","                elif metric == 'QLIKE':\n","                    with np.errstate(divide='ignore', invalid='ignore'):\n","                        ratio = actuals_aligned / predictions_aligned\n","                        residuals = ratio - np.log(ratio) - 1\n","                else:\n","                    raise ValueError(\"Unsupported metric\")\n","\n","                residuals = residuals.replace([np.inf, -np.inf], np.nan)\n","                residuals_dict[model_name] = residuals.mean(axis=1)\n","\n","            return pd.DataFrame(residuals_dict).dropna()\n","\n","        # Run MCS procedure for each metric\n","        pvalues_list = []\n","\n","        for metric in metrics:\n","            losses = calculate_losses(metric, model_dfs, Actuals)\n","            mcs = MCS(losses, size=0.05, method=\"R\", block_size=1000)\n","            mcs.compute()\n","            pvalues = mcs.pvalues.reset_index().rename(columns={'Pvalue': f'Pvalue_{metric}'})\n","            pvalues_list.append(pvalues)\n","\n","        # Merge p-values and save\n","        merged_pvalues = pvalues_list[0]\n","        for pvalues in pvalues_list[1:]:\n","            merged_pvalues = merged_pvalues.merge(pvalues, on='Model name', how='outer')\n","\n","        mcs_file = f\"{output_dir}/{prefix_str}MCS_horizon_{horizon}.csv\"\n","        merged_pvalues.to_csv(mcs_file, index=False)\n","\n","        # Function for Diebold-Mariano test\n","        def dm_test(actual, pred1, pred2, h=1, crit=\"RMSE\"):\n","            actual, pred1, pred2 = map(np.array, [actual, pred1, pred2])\n","            T = len(actual)\n","\n","            # Ensure predictions and actuals are of the same length\n","            if len(actual) != len(pred1) or len(actual) != len(pred2):\n","                return np.nan\n","\n","            # Remove NaN values\n","            mask = ~np.isnan(actual) & ~np.isnan(pred1) & ~np.isnan(pred2)\n","            actual = actual[mask]\n","            pred1 = pred1[mask]\n","            pred2 = pred2[mask]\n","            T = len(actual)\n","            if T == 0:\n","                return np.nan\n","\n","            if crit == \"RMSE\":\n","                loss_diff = (actual - pred1) ** 2 - (actual - pred2) ** 2\n","            elif crit == \"MAE\":\n","                loss_diff = np.abs(actual - pred1) - np.abs(actual - pred2)\n","            elif crit == \"MAPE\":\n","                with np.errstate(divide='ignore', invalid='ignore'):\n","                    loss_diff = (np.abs(actual - pred1) / actual) - (np.abs(actual - pred2) / actual)\n","            elif crit == \"QLIKE\":\n","                with np.errstate(divide='ignore', invalid='ignore'):\n","                    loss1 = actual / np.abs(pred1) - np.log(actual / np.abs(pred1)) - 1\n","                    loss2 = actual / np.abs(pred2) - np.log(actual / np.abs(pred2)) - 1\n","                    loss_diff = loss1 - loss2\n","            else:\n","                raise ValueError(\"Unsupported criterion\")\n","\n","            loss_diff = loss_diff[~np.isnan(loss_diff)]\n","            if len(loss_diff) == 0:\n","                return np.nan\n","\n","            d_mean = np.mean(loss_diff)\n","            gamma = [np.correlate(loss_diff - d_mean, loss_diff - d_mean, 'full')[len(loss_diff)-1:] / len(loss_diff)][0]\n","            V_d = gamma[0] + 2 * sum(gamma[1:h])\n","            DM_stat = d_mean / np.sqrt(V_d / len(loss_diff))\n","            adj = ((len(loss_diff) + 1 - 2 * h + h * (h - 1) / len(loss_diff)) / len(loss_diff)) ** 0.5\n","            return DM_stat * adj\n","\n","        # Run DM tests and save results\n","        models = model_columns\n","        stocks = Actuals.columns\n","\n","        for metric in metrics:\n","            better_count_matrix = pd.DataFrame(0, index=models, columns=models)\n","\n","            for model_a, model_b in combinations(models, 2):\n","                counts = {'a_better': 0, 'b_better': 0}\n","                for stock in stocks:\n","                    actual = Actuals[stock]\n","                    pred1 = model_dfs[model_a][stock]\n","                    pred2 = model_dfs[model_b][stock]\n","                    combined = pd.concat([actual, pred1, pred2], axis=1).dropna()\n","\n","                    if combined.empty:\n","                        continue\n","\n","                    dm_stat = dm_test(combined.iloc[:, 0], combined.iloc[:, 1], combined.iloc[:, 2], h=1, crit=metric)\n","                    if np.isnan(dm_stat):\n","                        continue\n","                    if dm_stat > 1.96:\n","                        counts['a_better'] += 1\n","                    elif dm_stat < -1.96:\n","                        counts['b_better'] += 1\n","\n","                better_count_matrix.loc[model_a, model_b] = counts['a_better']\n","                better_count_matrix.loc[model_b, model_a] = counts['b_better']\n","\n","            # Summarize results\n","            better_count_matrix['Outperform Count'] = better_count_matrix.sum(axis=1)\n","            better_count_matrix.loc['Outperformed Count'] = better_count_matrix.sum(axis=0)\n","            better_count_matrix.loc['Outperformed Count', 'Outperform Count'] = np.nan\n","            dm_file = f\"{output_dir}/{prefix_str}DM_{metric}_horizon_{horizon}.csv\"\n","            better_count_matrix.to_csv(dm_file)\n","\n","        print(f\"Completed processing for Prefix: '{prefix}', Horizon: {horizon}\\n\")"]},{"cell_type":"markdown","source":["## Evaluation agregation\n","\n","Following the evaluation metrics as well as MCS and DM test from Suoto and Moradi 2024\n","\n","https://www.emerald.com/insight/content/doi/10.1108/cfri-01-2024-0032/full/html\n","\n","https://github.com/hugogobato/Can-Transformers-Transform-Financial-Forecasting-"],"metadata":{"id":"m2QG9D_ARK-B"}},{"cell_type":"markdown","source":["### Metrics"],"metadata":{"id":"-VcRzpyHRV8t"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"aXkAGMVaOLz1","outputId":"38597deb-ef85-48c8-816d-0849c14cde7b"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"Excel file with metrics across horizons and prefixes has been created in 'Data/Evaluation/Final/'.\""]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","# Load the data for each horizon and merge by metric\n","horizons = [1, 5, 10, 20]\n","metrics = ['RMSE', 'MAE', 'MAPE', 'QLIKE']\n","prefixes = ['', '8TY_']\n","\n","metric_data = {metric: [] for metric in metrics}\n","\n","# Process each horizon, prefix, and add to the corresponding metric dictionary\n","for prefix in prefixes:\n","    for horizon in horizons:\n","        # Adjust file path with prefix\n","        file_path = f'Data/Evaluation/horizon_{horizon}/{prefix}metrics_horizon_{horizon}.csv'\n","        try:\n","            df = pd.read_csv(file_path)\n","            df = df.rename(columns={'Unnamed: 0': 'model'})\n","            df['model'] = df['model'].str.replace('Auto', '', regex=False)\n","            for metric in metrics:\n","                metric_df = df[['model', metric]].copy()\n","                metric_df = metric_df.rename(columns={metric: f'{metric}_{prefix}horizon_{horizon}'})\n","                metric_data[metric].append(metric_df)\n","        except FileNotFoundError:\n","            print(f\"File not found: {file_path}. Skipping this file.\")\n","\n","# Create the output directory if it doesn't exist\n","output_dir = 'Data/Evaluation/Final'\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Export each metric DataFrame to an Excel file\n","with pd.ExcelWriter(f'{output_dir}/metrics_across_horizons_and_prefixes.xlsx') as writer:\n","    for metric in metrics:\n","        # Concatenate the metric dataframes for each metric across horizons\n","        if metric_data[metric]:  # Check if data exists for the metric\n","            result_df = pd.concat(metric_data[metric], axis=1)\n","            result_df = result_df.loc[:,~result_df.columns.duplicated()]  # remove duplicate 'model' columns\n","            # Write each metric to a separate sheet in the Excel file\n","            result_df.to_excel(writer, sheet_name=metric, index=False)\n","\n","\"Excel file with metrics across horizons and prefixes has been created in 'Data/Evaluation/Final/'.\""]},{"cell_type":"markdown","source":["### MCS"],"metadata":{"id":"LWaN_DsXRT-9"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"K4t1cdtlOZ07","outputId":"2275b33b-bc92-4cca-d0c5-1d86c7a54aba"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"Excel file with MCS across horizons and prefixes has been created in 'Data/Evaluation/Final/'.\""]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","# Load the data for each horizon and merge by metric\n","horizons = [1, 5, 10, 20]\n","metrics = ['Pvalue_RMSE', 'Pvalue_MAE',\t'Pvalue_MAPE',\t'Pvalue_QLIKE']\n","prefixes = ['', '8TY_']\n","\n","metric_data = {metric: [] for metric in metrics}\n","\n","# Process each horizon, prefix, and add to the corresponding metric dictionary\n","for prefix in prefixes:\n","    for horizon in horizons:\n","        # Adjust file path with prefix\n","        file_path = f'Data/Evaluation/horizon_{horizon}/{prefix}MCS_horizon_{horizon}.csv'\n","        try:\n","            df = pd.read_csv(file_path)\n","            df = df.rename(columns={'Model name': 'model'})\n","            df['model'] = df['model'].str.replace('Auto', '', regex=False)\n","            for metric in metrics:\n","                metric_df = df[['model', metric]].copy()\n","                metric_df = metric_df.rename(columns={metric: f'{metric}_{prefix}horizon_{horizon}'})\n","                metric_data[metric].append(metric_df)\n","        except FileNotFoundError:\n","            print(f\"File not found: {file_path}. Skipping this file.\")\n","\n","output_dir = 'Data/Evaluation/Final'\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Export each metric DataFrame to an Excel file\n","with pd.ExcelWriter(f'{output_dir}/MCS_across_horizons_and_prefixes.xlsx') as writer:\n","    for metric in metrics:\n","        # Concatenate the metric dataframes for each metric across horizons\n","        if metric_data[metric]:  # Check if data exists for the metric\n","            result_df = pd.concat(metric_data[metric], axis=1)\n","            result_df = result_df.loc[:,~result_df.columns.duplicated()]  # remove duplicate 'model' columns\n","            # Write each metric to a separate sheet in the Excel file\n","            result_df.to_excel(writer, sheet_name=metric, index=False)\n","\"Excel file with MCS across horizons and prefixes has been created in 'Data/Evaluation/Final/'.\""]},{"cell_type":"markdown","source":["### DM"],"metadata":{"id":"v_w_uhohRi2a"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-w8nhIGgOaGE","outputId":"851dcef1-01e8-40d6-8e77-03267cfb8eb1"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Excel file created with summary of all metrics.'"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import os\n","\n","# Initialize dictionaries to store data for each metric and horizon\n","metrics = ['RMSE', 'MAE', 'MAPE', 'QLIKE']\n","prefixes = ['', '8TY_']\n","horizons = [1, 5, 10, 20]\n","metric_data = {metric: [] for metric in metrics}\n","\n","# Process each metric and compile data across all horizons and prefixes\n","for metric in metrics:\n","    for prefix in prefixes:\n","        for horizon in horizons:\n","            # Define file path and check if it exists\n","            file_path = f\"Data/Evaluation/horizon_{horizon}/{prefix}DM_{metric}_horizon_{horizon}.csv\"\n","            if os.path.exists(file_path):\n","                # Read the data, rename columns, and filter as specified\n","                df = pd.read_csv(file_path)\n","                df = df.rename(columns={'Unnamed: 0': 'model', 'Outperform Count': f'{metric}_{prefix.strip(\"_\")}_h{horizon}_Outperform'})\n","                df = df[df['model'] != 'Outperformed Count']\n","                df = df[['model', f'{metric}_{prefix.strip(\"_\")}_h{horizon}_Outperform']]\n","                df['model'] = df['model'].str.replace('Auto', '', regex=False)\n","                metric_data[metric].append(df)\n","\n","# Save data for each metric into an Excel file with each horizon's data in one sheet per metric\n","with pd.ExcelWriter('Data/Evaluation/Final/DM_across_horizons_and_prefixes.xlsx') as writer:\n","    for metric, data_frames in metric_data.items():\n","        # Concatenate data frames for the specific metric along columns by 'model' as the index\n","        result_df = pd.concat(data_frames, axis=1).loc[:,~pd.concat(data_frames, axis=1).columns.duplicated()]\n","        result_df.to_excel(writer, sheet_name=metric, index=False)\n","\n","'Excel file created with summary of all metrics.'"]}]}