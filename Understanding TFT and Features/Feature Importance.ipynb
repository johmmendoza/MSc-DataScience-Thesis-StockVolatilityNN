{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNUwaq/5YMbJe7zleteTgkN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# None vs Full vs Technical vs Firm"],"metadata":{"id":"K4HlKpHc2alz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"OTFOMnww1ekK"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)\n","import os\n","os.chdir('/content/drive/My Drive/Volatility_forecasting/')"]},{"cell_type":"code","source":["!pip install neuralforecast ruptures dask[dataframe] scikit-learn\n","import os\n","os.environ['NIXTLA_ID_AS_COL'] = '1'"],"metadata":{"id":"N3y88S0a2g1L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## All models results merging"],"metadata":{"id":"ruogcaO02eeW"}},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import gc  # Garbage collection module to free up memory\n","\n","def process_model(base_path, horizon, base_df_path, model_name, firm_model, tech_model, output_filename):\n","    try:\n","        # Load the base DataFrame\n","        base_df = pd.read_csv(base_df_path)\n","        base_df = base_df[['unique_id', 'ds', 'y', model_name, f'{model_name}1']].drop_duplicates(subset=['ds', 'unique_id'])\n","\n","        # Process FIRM model\n","        firm_path = f'{base_path}/horizon_{horizon}/{firm_model}_model1_horizon_{horizon}.csv'\n","        firm_df = pd.read_csv(firm_path)\n","        firm_df = firm_df.drop(['cutoff', f'{model_name}-median', f'{model_name}-lo-90', f'{model_name}-hi-90'], axis=1, errors='ignore')\n","        firm_df = firm_df.rename(columns={model_name: f'AutoFirm{model_name}'}).drop_duplicates(subset=['ds', 'unique_id'])\n","\n","        # Process TECH model\n","        tech_path = f'{base_path}/horizon_{horizon}/{tech_model}_model1_horizon_{horizon}.csv'\n","        tech_df = pd.read_csv(tech_path)\n","        tech_df = tech_df.drop(['cutoff', f'{model_name}-median', f'{model_name}-lo-90', f'{model_name}-hi-90'], axis=1, errors='ignore')\n","        tech_df = tech_df.rename(columns={model_name: f'AutoTech{model_name}'}).drop_duplicates(subset=['ds', 'unique_id'])\n","\n","        # Merge DataFrames\n","        merged_df = pd.merge(base_df, firm_df, on=['unique_id', 'ds', 'y'], how='left')\n","        merged_df = pd.merge(merged_df, tech_df, on=['unique_id', 'ds', 'y'], how='left')\n","\n","        # Save the merged DataFrame\n","        output_path = os.path.join(base_path, f'horizon_{horizon}/{output_filename}')\n","        merged_df.to_csv(output_path, index=False)\n","        print(f\"Saved merged DataFrame to {output_path}\")\n","    finally:\n","        # Free memory by deleting DataFrames\n","        del base_df, firm_df, tech_df, merged_df\n","        gc.collect()  # Force garbage collection to free memory\n","\n","# Parameters\n","base_path = 'Data/Test'\n","horizons = [1, 5, 10, 20]  # Process these horizons\n","output_filename_template = 'Tech_Firm_{model_name}_model0_1_sub_horizon_{horizon}.csv'\n","\n","# Define models and corresponding FIRM/TECH file prefixes\n","models = {\n","    'AutoTFT': ('FIRMtft', 'TECHtft'),\n","    'AutoBiTCN': ('FIRMbitcn', 'TECHbitcn'),\n","    'AutoTiDE': ('FIRMtide', 'TECHtide'),\n","    'AutoNHITS': ('FIRMnhits', 'TECHnhits')\n","}\n","\n","# Process each model for each horizon\n","for horizon in horizons:\n","    base_df_path = f'{base_path}/horizon_{horizon}/bitcn_tide_tft_nhits_model0_1_sub_horizon_{horizon}.csv'\n","\n","    for model_name, (firm_model, tech_model) in models.items():\n","        output_filename = output_filename_template.replace('{model_name}', model_name).replace('{horizon}', str(horizon))\n","        process_model(base_path, horizon, base_df_path, model_name, firm_model, tech_model, output_filename)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4PboUbxuy9G3","executionInfo":{"status":"ok","timestamp":1731947431889,"user_tz":-60,"elapsed":331066,"user":{"displayName":"John Alejandro Mendoza Ortiz","userId":"11091031515932154769"}},"outputId":"e9f2fe68-1671-47ce-ec06-1bea2a3a4370"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved merged DataFrame to Data/Test/horizon_1/Tech_Firm_AutoTFT_model0_1_sub_horizon_1.csv\n","Saved merged DataFrame to Data/Test/horizon_1/Tech_Firm_AutoBiTCN_model0_1_sub_horizon_1.csv\n","Saved merged DataFrame to Data/Test/horizon_1/Tech_Firm_AutoTiDE_model0_1_sub_horizon_1.csv\n","Saved merged DataFrame to Data/Test/horizon_1/Tech_Firm_AutoNHITS_model0_1_sub_horizon_1.csv\n","Saved merged DataFrame to Data/Test/horizon_5/Tech_Firm_AutoTFT_model0_1_sub_horizon_5.csv\n","Saved merged DataFrame to Data/Test/horizon_5/Tech_Firm_AutoBiTCN_model0_1_sub_horizon_5.csv\n","Saved merged DataFrame to Data/Test/horizon_5/Tech_Firm_AutoTiDE_model0_1_sub_horizon_5.csv\n","Saved merged DataFrame to Data/Test/horizon_5/Tech_Firm_AutoNHITS_model0_1_sub_horizon_5.csv\n","Saved merged DataFrame to Data/Test/horizon_10/Tech_Firm_AutoTFT_model0_1_sub_horizon_10.csv\n","Saved merged DataFrame to Data/Test/horizon_10/Tech_Firm_AutoBiTCN_model0_1_sub_horizon_10.csv\n","Saved merged DataFrame to Data/Test/horizon_10/Tech_Firm_AutoTiDE_model0_1_sub_horizon_10.csv\n","Saved merged DataFrame to Data/Test/horizon_10/Tech_Firm_AutoNHITS_model0_1_sub_horizon_10.csv\n","Saved merged DataFrame to Data/Test/horizon_20/Tech_Firm_AutoTFT_model0_1_sub_horizon_20.csv\n","Saved merged DataFrame to Data/Test/horizon_20/Tech_Firm_AutoBiTCN_model0_1_sub_horizon_20.csv\n","Saved merged DataFrame to Data/Test/horizon_20/Tech_Firm_AutoTiDE_model0_1_sub_horizon_20.csv\n","Saved merged DataFrame to Data/Test/horizon_20/Tech_Firm_AutoNHITS_model0_1_sub_horizon_20.csv\n"]}]},{"cell_type":"markdown","source":["## Evaluation\n","\n","Following the evaluation metrics as well as MCS and DM test from Suoto and Moradi 2024\n","\n","https://www.emerald.com/insight/content/doi/10.1108/cfri-01-2024-0032/full/html\n","\n","https://github.com/hugogobato/Can-Transformers-Transform-Financial-Forecasting-"],"metadata":{"id":"i6a6HPBQ5E-C"}},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","from itertools import combinations\n","from arch.bootstrap import MCS\n","\n","# Define the horizons and prefix\n","horizons = [1, 5, 10 , 20]\n","prefix = 'Tech_Firm'\n","\n","# Target models\n","target_models = ['AutoTFT', 'AutoBiTCN', 'AutoTiDE', 'AutoNHITS']\n","\n","# Function for Diebold-Mariano Test\n","def dm_test(actual, pred1, pred2, h=1, crit=\"RMSE\"):\n","    actual, pred1, pred2 = map(np.array, [actual, pred1, pred2])\n","    T = len(actual)\n","\n","    # Ensure predictions and actuals are of the same length\n","    if len(actual) != len(pred1) or len(actual) != len(pred2):\n","        return np.nan\n","\n","    # Remove NaN values\n","    mask = ~np.isnan(actual) & ~np.isnan(pred1) & ~np.isnan(pred2)\n","    actual, pred1, pred2 = actual[mask], pred1[mask], pred2[mask]\n","    T = len(actual)\n","    if T == 0:\n","        return np.nan\n","\n","    if crit == \"RMSE\":\n","        loss_diff = (actual - pred1) ** 2 - (actual - pred2) ** 2\n","    elif crit == \"MAE\":\n","        loss_diff = np.abs(actual - pred1) - np.abs(actual - pred2)\n","    elif crit == \"MAPE\":\n","        with np.errstate(divide='ignore', invalid='ignore'):\n","            loss_diff = (np.abs(actual - pred1) / actual) - (np.abs(actual - pred2) / actual)\n","    elif crit == \"QLIKE\":\n","        with np.errstate(divide='ignore', invalid='ignore'):\n","            loss1 = actual / np.abs(pred1) - np.log(actual / np.abs(pred1)) - 1\n","            loss2 = actual / np.abs(pred2) - np.log(actual / np.abs(pred2)) - 1\n","            loss_diff = loss1 - loss2\n","    else:\n","        raise ValueError(\"Unsupported criterion\")\n","\n","    loss_diff = loss_diff[~np.isnan(loss_diff)]\n","    if len(loss_diff) == 0:\n","        return np.nan\n","\n","    d_mean = np.mean(loss_diff)\n","    gamma = [np.correlate(loss_diff - d_mean, loss_diff - d_mean, 'full')[len(loss_diff) - 1:] / len(loss_diff)][0]\n","    V_d = gamma[0] + 2 * sum(gamma[1:h])\n","    DM_stat = d_mean / np.sqrt(V_d / len(loss_diff))\n","    adj = ((len(loss_diff) + 1 - 2 * h + h * (h - 1) / len(loss_diff)) / len(loss_diff)) ** 0.5\n","    return DM_stat * adj\n","\n","\n","# Loop over each horizon and model\n","for horizon in horizons:\n","    for model in target_models:\n","        print(f\"Processing Model: '{model}', Horizon: {horizon}\")\n","\n","        # Define file paths based on model, prefix, and horizon\n","        file_name = f\"{prefix}_{model}_model0_1_sub_horizon_{horizon}.csv\"\n","        data_path = f\"Data/Test/horizon_{horizon}/{file_name}\"\n","        output_dir = f\"Data/Evaluation/horizon_{horizon}\"\n","        os.makedirs(output_dir, exist_ok=True)\n","\n","        # Check if file exists\n","        if not os.path.exists(data_path):\n","            print(f\"File not found: {data_path}\")\n","            continue\n","\n","        # Read the CSV file\n","        df = pd.read_csv(data_path)\n","        df = df.sort_values(by=['unique_id', 'ds']).drop_duplicates(subset=['ds', 'unique_id'])\n","        df['ds'] = pd.to_datetime(df['ds'])\n","\n","        # Extract model columns dynamically (target model and variants, e.g., AutoBiTCN1)\n","        model_columns = [col for col in df.columns if model in col]\n","\n","        # Create a dictionary to hold pivoted DataFrames for each variant of the current model\n","        model_dfs = {}\n","        for variant in model_columns:\n","            model_df = df.pivot(index='ds', columns='unique_id', values=variant)\n","            model_dfs[variant] = model_df\n","\n","        # Pivot actual values\n","        Actuals = df.pivot(index='ds', columns='unique_id', values='y')\n","\n","        # Calculate error metrics for each model variant\n","        metrics = ['RMSE', 'MAE', 'MAPE', 'QLIKE']\n","        error_metrics = {metric: {} for metric in metrics}\n","\n","        for model_name, predictions in model_dfs.items():\n","            # Align the predictions and actuals\n","            predictions, actuals = predictions.align(Actuals, join='inner', axis=0)\n","            errors = predictions - actuals\n","\n","            # RMSE Calculation\n","            mse = (errors ** 2).mean().mean()\n","            error_metrics['RMSE'][model_name] = np.sqrt(mse)\n","\n","            # MAE Calculation\n","            mae = errors.abs().mean().mean()\n","            error_metrics['MAE'][model_name] = mae\n","\n","            # MAPE Calculation\n","            with np.errstate(divide='ignore', invalid='ignore'):\n","                mape = (errors.abs() / actuals).replace([np.inf, -np.inf], np.nan).mean().mean()\n","            error_metrics['MAPE'][model_name] = mape\n","\n","            # QLIKE Calculation\n","            with np.errstate(divide='ignore', invalid='ignore'):\n","                ratio = actuals / predictions\n","                qlike = (ratio - np.log(ratio) - 1).replace([np.inf, -np.inf], np.nan).mean().mean()\n","            error_metrics['QLIKE'][model_name] = qlike\n","\n","        # Create a DataFrame with error metrics\n","        error_metrics_df = pd.DataFrame(error_metrics)\n","        metrics_file = f\"{output_dir}/{prefix}_{model}_metrics_horizon_{horizon}.csv\"\n","        error_metrics_df.to_csv(metrics_file)\n","\n","        # Function to calculate losses for MCS\n","        def calculate_losses(metric, model_dfs, actuals):\n","            residuals_dict = {}\n","            for model_name, predictions in model_dfs.items():\n","                # Align predictions and actuals\n","                predictions_aligned, actuals_aligned = predictions.align(actuals, join='inner', axis=0)\n","                if metric == 'RMSE':\n","                    residuals = (predictions_aligned - actuals_aligned) ** 2\n","                elif metric == 'MAE':\n","                    residuals = (predictions_aligned - actuals_aligned).abs()\n","                elif metric == 'MAPE':\n","                    with np.errstate(divide='ignore', invalid='ignore'):\n","                        residuals = (predictions_aligned - actuals_aligned).abs() / actuals_aligned\n","                elif metric == 'QLIKE':\n","                    with np.errstate(divide='ignore', invalid='ignore'):\n","                        ratio = actuals_aligned / predictions_aligned\n","                        residuals = ratio - np.log(ratio) - 1\n","                else:\n","                    raise ValueError(\"Unsupported metric\")\n","\n","                residuals = residuals.replace([np.inf, -np.inf], np.nan)\n","                residuals_dict[model_name] = residuals.mean(axis=1)\n","\n","            return pd.DataFrame(residuals_dict).dropna()\n","\n","        # Run MCS procedure for each metric\n","        pvalues_list = []\n","\n","        for metric in metrics:\n","            losses = calculate_losses(metric, model_dfs, Actuals)\n","            mcs = MCS(losses, size=0.05, method=\"R\", block_size=1000)\n","            mcs.compute()\n","            pvalues = mcs.pvalues.reset_index().rename(columns={'Pvalue': f'Pvalue_{metric}'})\n","            pvalues_list.append(pvalues)\n","\n","        # Merge p-values and save\n","        merged_pvalues = pvalues_list[0]\n","        for pvalues in pvalues_list[1:]:\n","            merged_pvalues = merged_pvalues.merge(pvalues, on='Model name', how='outer')\n","\n","        mcs_file = f\"{output_dir}/{prefix}_{model}_MCS_horizon_{horizon}.csv\"\n","        merged_pvalues.to_csv(mcs_file, index=False)\n","\n","        # Run DM tests and save results\n","        stocks = Actuals.columns\n","\n","        for metric in metrics:\n","            better_count_matrix = pd.DataFrame(0, index=model_columns, columns=model_columns)\n","\n","            for model_a, model_b in combinations(model_columns, 2):\n","                counts = {'a_better': 0, 'b_better': 0}\n","                for stock in stocks:\n","                    actual = Actuals[stock]\n","                    pred1 = model_dfs[model_a][stock]\n","                    pred2 = model_dfs[model_b][stock]\n","                    combined = pd.concat([actual, pred1, pred2], axis=1).dropna()\n","\n","                    if combined.empty:\n","                        continue\n","\n","                    dm_stat = dm_test(combined.iloc[:, 0], combined.iloc[:, 1], combined.iloc[:, 2], h=1, crit=metric)\n","                    if np.isnan(dm_stat):\n","                        continue\n","                    if dm_stat > 1.96:\n","                        counts['a_better'] += 1\n","                    elif dm_stat < -1.96:\n","                        counts['b_better'] += 1\n","\n","                better_count_matrix.loc[model_a, model_b] = counts['a_better']\n","                better_count_matrix.loc[model_b, model_a] = counts['b_better']\n","\n","            # Summarize results\n","            better_count_matrix['Outperform Count'] = better_count_matrix.sum(axis=1)\n","            better_count_matrix.loc['Outperformed Count'] = better_count_matrix.sum(axis=0)\n","            better_count_matrix.loc['Outperformed Count', 'Outperform Count'] = np.nan\n","            dm_file = f\"{output_dir}/{prefix}_{model}_DM_{metric}_horizon_{horizon}.csv\"\n","            better_count_matrix.to_csv(dm_file)\n","\n","        print(f\"Completed processing for Model: '{model}', Horizon: {horizon}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mPCNzd9MCoRC","executionInfo":{"status":"ok","timestamp":1731947910250,"user_tz":-60,"elapsed":347601,"user":{"displayName":"John Alejandro Mendoza Ortiz","userId":"11091031515932154769"}},"outputId":"e7bfadc8-b9cc-4540-cb61-63b614b7b76a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing Model: 'AutoTFT', Horizon: 1\n","Completed processing for Model: 'AutoTFT', Horizon: 1\n","\n","Processing Model: 'AutoBiTCN', Horizon: 1\n","Completed processing for Model: 'AutoBiTCN', Horizon: 1\n","\n","Processing Model: 'AutoTiDE', Horizon: 1\n","Completed processing for Model: 'AutoTiDE', Horizon: 1\n","\n","Processing Model: 'AutoNHITS', Horizon: 1\n","Completed processing for Model: 'AutoNHITS', Horizon: 1\n","\n","Processing Model: 'AutoTFT', Horizon: 5\n","Completed processing for Model: 'AutoTFT', Horizon: 5\n","\n","Processing Model: 'AutoBiTCN', Horizon: 5\n","Completed processing for Model: 'AutoBiTCN', Horizon: 5\n","\n","Processing Model: 'AutoTiDE', Horizon: 5\n","Completed processing for Model: 'AutoTiDE', Horizon: 5\n","\n","Processing Model: 'AutoNHITS', Horizon: 5\n","Completed processing for Model: 'AutoNHITS', Horizon: 5\n","\n","Processing Model: 'AutoTFT', Horizon: 10\n","Completed processing for Model: 'AutoTFT', Horizon: 10\n","\n","Processing Model: 'AutoBiTCN', Horizon: 10\n","Completed processing for Model: 'AutoBiTCN', Horizon: 10\n","\n","Processing Model: 'AutoTiDE', Horizon: 10\n","Completed processing for Model: 'AutoTiDE', Horizon: 10\n","\n","Processing Model: 'AutoNHITS', Horizon: 10\n","Completed processing for Model: 'AutoNHITS', Horizon: 10\n","\n","Processing Model: 'AutoTFT', Horizon: 20\n","Completed processing for Model: 'AutoTFT', Horizon: 20\n","\n","Processing Model: 'AutoBiTCN', Horizon: 20\n","Completed processing for Model: 'AutoBiTCN', Horizon: 20\n","\n","Processing Model: 'AutoTiDE', Horizon: 20\n","Completed processing for Model: 'AutoTiDE', Horizon: 20\n","\n","Processing Model: 'AutoNHITS', Horizon: 20\n","Completed processing for Model: 'AutoNHITS', Horizon: 20\n","\n"]}]},{"cell_type":"markdown","source":["### Metrics"],"metadata":{"id":"p_F5Quzk57VX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vkkFtdGWpQOp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731953180651,"user_tz":-60,"elapsed":357,"user":{"displayName":"John Alejandro Mendoza Ortiz","userId":"11091031515932154769"}},"outputId":"6ceb0e00-7de9-4ada-d528-4f181360e232"},"outputs":[{"output_type":"stream","name":"stdout","text":["Excel file for AutoTFT metrics has been created: Data/Evaluation/Final/Tech_Firm_AutoTFT_metrics_across_horizons_and_prefixes.xlsx\n","Excel file for AutoTiDE metrics has been created: Data/Evaluation/Final/Tech_Firm_AutoTiDE_metrics_across_horizons_and_prefixes.xlsx\n","Excel file for AutoNHITS metrics has been created: Data/Evaluation/Final/Tech_Firm_AutoNHITS_metrics_across_horizons_and_prefixes.xlsx\n","Excel file for AutoBiTCN metrics has been created: Data/Evaluation/Final/Tech_Firm_AutoBiTCN_metrics_across_horizons_and_prefixes.xlsx\n"]}],"source":["import os\n","import pandas as pd\n","\n","# Define the horizons, metrics, prefixes, and models to treat\n","horizons = [1, 5, 10, 20]\n","metrics = ['RMSE', 'MAE', 'MAPE', 'QLIKE']\n","prefixes = ['Tech_Firm']\n","models_to_treat = ['AutoTFT', 'AutoTiDE', 'AutoNHITS', 'AutoBiTCN']\n","\n","# Iterate over each model to treat\n","for treated_model in models_to_treat:\n","    metric_data = {metric: [] for metric in metrics}  # Reset for each model\n","\n","    # Process each horizon and prefix\n","    for prefix in prefixes:\n","        for horizon in horizons:\n","            # Adjust file path with prefix and treated model\n","            file_path = f'Data/Evaluation/horizon_{horizon}/{prefix}_{treated_model}_metrics_horizon_{horizon}.csv'\n","            try:\n","                df = pd.read_csv(file_path)\n","                df = df.rename(columns={'Unnamed: 0': 'model'})  # Handle unnamed index column\n","                df['model'] = df['model'].str.replace('Auto', '', regex=False)  # Clean up model names\n","\n","                # Extract and rename columns for each metric\n","                for metric in metrics:\n","                    metric_df = df[['model', metric]].copy()\n","                    metric_df = metric_df.rename(columns={metric: f'{metric}_{prefix}horizon_{horizon}'})\n","                    metric_data[metric].append(metric_df)\n","            except FileNotFoundError:\n","                print(f\"File not found: {file_path}. Skipping this file.\")\n","\n","    # Create the output directory if it doesn't exist\n","    output_dir = 'Data/Evaluation/Final'\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    # Export each metric DataFrame to an Excel file\n","    excel_file = f'{output_dir}/{prefix}_{treated_model}_metrics_across_horizons_and_prefixes.xlsx'\n","    with pd.ExcelWriter(excel_file) as writer:\n","        for metric in metrics:\n","            # Concatenate the metric DataFrames for each metric across horizons\n","            if metric_data[metric]:  # Check if data exists for the metric\n","                result_df = pd.concat(metric_data[metric], axis=1)\n","                result_df = result_df.loc[:, ~result_df.columns.duplicated()]  # Remove duplicate 'model' columns\n","                # Write each metric to a separate sheet in the Excel file\n","                result_df.to_excel(writer, sheet_name=metric, index=False)\n","\n","    print(f\"Excel file for {treated_model} metrics has been created: {excel_file}\")"]},{"cell_type":"markdown","source":["### DM"],"metadata":{"id":"SXAUrpsv5-6t"}},{"cell_type":"code","source":["import pandas as pd\n","import os\n","\n","# Define the horizons, metrics, prefixes, and models to treat\n","horizons = [1, 5, 10, 20]\n","metrics = ['RMSE', 'MAE', 'MAPE', 'QLIKE']\n","prefixes = ['Tech_Firm']\n","models_to_treat = ['AutoTFT', 'AutoTiDE', 'AutoNHITS', 'AutoBiTCN']\n","\n","# Process each model to treat\n","for treated_model in models_to_treat:\n","    metric_data = {metric: [] for metric in metrics}  # Reset data storage for each model\n","\n","    # Iterate over prefixes, horizons, and metrics\n","    for prefix in prefixes:\n","        for horizon in horizons:\n","            for metric in metrics:\n","                # Construct the file path for the DM file\n","                file_path = f\"Data/Evaluation/horizon_{horizon}/{prefix}_{treated_model}_DM_{metric}_horizon_{horizon}.csv\"\n","\n","                if os.path.exists(file_path):\n","                    try:\n","                        # Read the DM file\n","                        df = pd.read_csv(file_path)\n","                        # Rename and filter columns as required\n","                        df = df.rename(columns={\n","                            'Unnamed: 0': 'model',\n","                            'Outperform Count': f'{metric}_{prefix.strip(\"_\")}_h{horizon}_Outperform'\n","                        })\n","                        df = df[df['model'] != 'Outperformed Count']  # Remove unwanted rows\n","                        df = df[['model', f'{metric}_{prefix.strip(\"_\")}_h{horizon}_Outperform']]\n","                        # Clean the 'model' column\n","                        df['model'] = df['model'].str.replace('Auto', '', regex=False)\n","                        # Append the processed DataFrame to the metric's list\n","                        metric_data[metric].append(df)\n","                    except Exception as e:\n","                        print(f\"Error processing file {file_path}: {e}\")\n","                else:\n","                    print(f\"File not found: {file_path}. Skipping.\")\n","\n","    # Create the output directory if it doesn't exist\n","    output_dir = 'Data/Evaluation/Final'\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    # Create an Excel file for the current model\n","    excel_file = f\"{output_dir}/{prefix}_{treated_model}_DM_across_horizons.xlsx\"\n","    with pd.ExcelWriter(excel_file) as writer:\n","        for metric, data_frames in metric_data.items():\n","            if data_frames:  # Only process if data exists for the metric\n","                # Concatenate data frames for the metric\n","                result_df = pd.concat(data_frames, axis=1)\n","                # Remove duplicate columns for 'model'\n","                result_df = result_df.loc[:, ~result_df.columns.duplicated()]\n","                # Write to a sheet named after the metric\n","                result_df.to_excel(writer, sheet_name=metric, index=False)\n","\n","    print(f\"Excel file created for {treated_model}: {excel_file}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rZVFxR6mSiqd","executionInfo":{"status":"ok","timestamp":1731953294433,"user_tz":-60,"elapsed":822,"user":{"displayName":"John Alejandro Mendoza Ortiz","userId":"11091031515932154769"}},"outputId":"c8090484-6353-468f-c1f3-8479c468c884"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Excel file created for AutoTFT: Data/Evaluation/Final/Tech_Firm_AutoTFT_DM_across_horizons.xlsx\n","Excel file created for AutoTiDE: Data/Evaluation/Final/Tech_Firm_AutoTiDE_DM_across_horizons.xlsx\n","Excel file created for AutoNHITS: Data/Evaluation/Final/Tech_Firm_AutoNHITS_DM_across_horizons.xlsx\n","Excel file created for AutoBiTCN: Data/Evaluation/Final/Tech_Firm_AutoBiTCN_DM_across_horizons.xlsx\n"]}]}]}