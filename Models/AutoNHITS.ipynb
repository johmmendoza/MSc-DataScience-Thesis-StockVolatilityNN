{"cells":[{"cell_type":"markdown","metadata":{"id":"UUtl51XS9Alu"},"source":["# Libraries and Drive"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19423,"status":"ok","timestamp":1731934973901,"user":{"displayName":"John Alejandro Mendoza Ortiz","userId":"11091031515932154769"},"user_tz":-60},"id":"ZsR36oob8ny0","outputId":"9bbb997d-9866-4152-e665-a5ef432af2c8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)\n","import os\n","os.chdir('/content/drive/My Drive/Volatility_forecasting/')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12664,"status":"ok","timestamp":1731934986562,"user":{"displayName":"John Alejandro Mendoza Ortiz","userId":"11091031515932154769"},"user_tz":-60},"id":"jjHzAYwB8tHL","outputId":"806b57db-92d3-4d93-d1d5-e3898bd5f16f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting neuralforecast\n","  Downloading neuralforecast-1.7.5-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: dask[dataframe] in /usr/local/lib/python3.10/dist-packages (2024.10.0)\n","Collecting coreforecast\u003e=0.0.6 (from neuralforecast)\n","  Downloading coreforecast-0.0.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from neuralforecast) (2024.10.0)\n","Requirement already satisfied: numpy\u003e=1.21.6 in /usr/local/lib/python3.10/dist-packages (from neuralforecast) (1.26.4)\n","Requirement already satisfied: pandas\u003e=1.3.5 in /usr/local/lib/python3.10/dist-packages (from neuralforecast) (2.2.2)\n","Requirement already satisfied: torch\u003e=2.0.0 in /usr/local/lib/python3.10/dist-packages (from neuralforecast) (2.5.1+cu121)\n","Collecting pytorch-lightning\u003e=2.0.0 (from neuralforecast)\n","  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n","Collecting ray\u003e=2.2.0 (from ray[tune]\u003e=2.2.0-\u003eneuralforecast)\n","  Downloading ray-2.39.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (17 kB)\n","Collecting optuna (from neuralforecast)\n","  Downloading optuna-4.1.0-py3-none-any.whl.metadata (16 kB)\n","Collecting utilsforecast\u003e=0.0.25 (from neuralforecast)\n","  Downloading utilsforecast-0.2.8-py3-none-any.whl.metadata (7.4 kB)\n","Requirement already satisfied: click\u003e=8.1 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (8.1.7)\n","Requirement already satisfied: cloudpickle\u003e=3.0.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (3.1.0)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (24.2)\n","Requirement already satisfied: partd\u003e=1.4.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (1.4.2)\n","Requirement already satisfied: pyyaml\u003e=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (6.0.2)\n","Requirement already satisfied: toolz\u003e=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (0.12.1)\n","Requirement already satisfied: importlib-metadata\u003e=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (8.5.0)\n","Collecting dask-expr\u003c1.2,\u003e=1.1 (from dask[dataframe])\n","  Downloading dask_expr-1.1.19-py3-none-any.whl.metadata (2.6 kB)\n","INFO: pip is looking at multiple versions of dask-expr to determine which version is compatible with other requirements. This could take a while.\n","  Downloading dask_expr-1.1.18-py3-none-any.whl.metadata (2.6 kB)\n","  Downloading dask_expr-1.1.16-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: pyarrow\u003e=14.0.1 in /usr/local/lib/python3.10/dist-packages (from dask-expr\u003c1.2,\u003e=1.1-\u003edask[dataframe]) (17.0.0)\n","Requirement already satisfied: zipp\u003e=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata\u003e=4.13.0-\u003edask[dataframe]) (3.21.0)\n","Requirement already satisfied: python-dateutil\u003e=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas\u003e=1.3.5-\u003eneuralforecast) (2.8.2)\n","Requirement already satisfied: pytz\u003e=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas\u003e=1.3.5-\u003eneuralforecast) (2024.2)\n","Requirement already satisfied: tzdata\u003e=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas\u003e=1.3.5-\u003eneuralforecast) (2024.2)\n","Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd\u003e=1.4.0-\u003edask[dataframe]) (1.0.0)\n","Requirement already satisfied: tqdm\u003e=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning\u003e=2.0.0-\u003eneuralforecast) (4.66.6)\n","Collecting torchmetrics\u003e=0.7.0 (from pytorch-lightning\u003e=2.0.0-\u003eneuralforecast)\n","  Downloading torchmetrics-1.6.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: typing-extensions\u003e=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning\u003e=2.0.0-\u003eneuralforecast) (4.12.2)\n","Collecting lightning-utilities\u003e=0.10.0 (from pytorch-lightning\u003e=2.0.0-\u003eneuralforecast)\n","  Downloading lightning_utilities-0.11.8-py3-none-any.whl.metadata (5.2 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast) (3.16.1)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast) (4.23.0)\n","Requirement already satisfied: msgpack\u003c2.0.0,\u003e=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast) (1.1.0)\n","Requirement already satisfied: protobuf!=3.19.5,\u003e=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast) (4.25.5)\n","Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast) (1.3.1)\n","Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast) (1.5.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast) (2.32.3)\n","Collecting tensorboardX\u003e=1.9 (from ray[tune]\u003e=2.2.0-\u003eneuralforecast)\n","  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch\u003e=2.0.0-\u003eneuralforecast) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=2.0.0-\u003eneuralforecast) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=2.0.0-\u003eneuralforecast) (1.13.1)\n","Requirement already satisfied: mpmath\u003c1.4,\u003e=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1-\u003etorch\u003e=2.0.0-\u003eneuralforecast) (1.3.0)\n","Collecting alembic\u003e=1.5.0 (from optuna-\u003eneuralforecast)\n","  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n","Collecting colorlog (from optuna-\u003eneuralforecast)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: sqlalchemy\u003e=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna-\u003eneuralforecast) (2.0.36)\n","Collecting Mako (from alembic\u003e=1.5.0-\u003eoptuna-\u003eneuralforecast)\n","  Downloading Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]\u003e=2022.5.0-\u003epytorch-lightning\u003e=2.0.0-\u003eneuralforecast) (3.10.10)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities\u003e=0.10.0-\u003epytorch-lightning\u003e=2.0.0-\u003eneuralforecast) (75.1.0)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil\u003e=2.8.2-\u003epandas\u003e=1.3.5-\u003eneuralforecast) (1.16.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy\u003e=1.4.2-\u003eoptuna-\u003eneuralforecast) (3.1.1)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch\u003e=2.0.0-\u003eneuralforecast) (3.0.2)\n","Requirement already satisfied: attrs\u003e=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema-\u003eray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast) (24.2.0)\n","Requirement already satisfied: jsonschema-specifications\u003e=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema-\u003eray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast) (2024.10.1)\n","Requirement already satisfied: referencing\u003e=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema-\u003eray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast) (0.35.1)\n","Requirement already satisfied: rpds-py\u003e=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema-\u003eray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast) (0.21.0)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests-\u003eray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast) (3.4.0)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003eray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast) (3.10)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003eray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast) (2.2.3)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003eray\u003e=2.2.0-\u003eray[tune]\u003e=2.2.0-\u003eneuralforecast) (2024.8.30)\n","Requirement already satisfied: aiohappyeyeballs\u003e=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003e=2022.5.0-\u003epytorch-lightning\u003e=2.0.0-\u003eneuralforecast) (2.4.3)\n","Requirement already satisfied: multidict\u003c7.0,\u003e=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003e=2022.5.0-\u003epytorch-lightning\u003e=2.0.0-\u003eneuralforecast) (6.1.0)\n","Requirement already satisfied: yarl\u003c2.0,\u003e=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003e=2022.5.0-\u003epytorch-lightning\u003e=2.0.0-\u003eneuralforecast) (1.17.1)\n","Requirement already satisfied: async-timeout\u003c5.0,\u003e=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003e=2022.5.0-\u003epytorch-lightning\u003e=2.0.0-\u003eneuralforecast) (4.0.3)\n","Requirement already satisfied: propcache\u003e=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl\u003c2.0,\u003e=1.12.0-\u003eaiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003e=2022.5.0-\u003epytorch-lightning\u003e=2.0.0-\u003eneuralforecast) (0.2.0)\n","Downloading neuralforecast-1.7.5-py3-none-any.whl (254 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.8/254.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading coreforecast-0.0.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (275 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.6/275.6 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dask_expr-1.1.16-py3-none-any.whl (243 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ray-2.39.0-cp310-cp310-manylinux2014_x86_64.whl (66.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.3/66.3 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading utilsforecast-0.2.8-py3-none-any.whl (41 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading optuna-4.1.0-py3-none-any.whl (364 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.4/364.4 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.14.0-py3-none-any.whl (233 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.11.8-py3-none-any.whl (26 kB)\n","Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchmetrics-1.6.0-py3-none-any.whl (926 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m926.4/926.4 kB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Downloading Mako-1.3.6-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tensorboardX, Mako, lightning-utilities, coreforecast, colorlog, alembic, utilsforecast, torchmetrics, optuna, dask-expr, ray, pytorch-lightning, neuralforecast\n","Successfully installed Mako-1.3.6 alembic-1.14.0 colorlog-6.9.0 coreforecast-0.0.15 dask-expr-1.1.16 lightning-utilities-0.11.8 neuralforecast-1.7.5 optuna-4.1.0 pytorch-lightning-2.4.0 ray-2.39.0 tensorboardX-2.6.2.2 torchmetrics-1.6.0 utilsforecast-0.2.8\n"]}],"source":["!pip install neuralforecast dask[dataframe]\n","import os\n","os.environ['NIXTLA_ID_AS_COL'] = '1'"]},{"cell_type":"markdown","metadata":{"id":"3EQ6trnW9CBB"},"source":["# Modelling original (20 years total, 14 train, 2 val, 4 test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c02WoJ-x9Bhf"},"outputs":[],"source":["from neuralforecast.core import NeuralForecast\n","from ray import tune\n","from ray.tune.search.hyperopt import HyperOptSearch\n","from neuralforecast.losses.pytorch import DistributionLoss\n","from neuralforecast.auto import  AutoNHITS"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4798,"status":"ok","timestamp":1731497905024,"user":{"displayName":"John Alejandro Mendoza Ortiz","userId":"11091031515932154769"},"user_tz":-60},"id":"NtJU-Fb69K7s","outputId":"308c26ed-69d2-41fe-9c52-2140472501ee"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u003cclass 'pandas.core.frame.DataFrame'\u003e\n","RangeIndex: 2008545 entries, 0 to 2008544\n","Data columns (total 21 columns):\n"," #   Column     Dtype         \n","---  ------     -----         \n"," 0   unique_id  object        \n"," 1   ds         datetime64[ns]\n"," 2   y          float32       \n"," 3   MA(2,9)    int32         \n"," 4   MA(3,9)    int32         \n"," 5   MA(1,12)   int32         \n"," 6   MA(2,12)   int32         \n"," 7   MA(3,12)   int32         \n"," 8   MOM(9)     int32         \n"," 9   MOM(12)    int32         \n"," 10  RSI(7)     int32         \n"," 11  RSI(14)    int32         \n"," 12  EMA(3,9)   int32         \n"," 13  EMA(5,9)   int32         \n"," 14  EMA(5,12)  int32         \n"," 15  DY         float32       \n"," 16  PTBV       float32       \n"," 17  P          float32       \n"," 18  PO         float32       \n"," 19  VO         float32       \n"," 20  PE         float32       \n","dtypes: datetime64[ns](1), float32(7), int32(12), object(1)\n","memory usage: 176.2+ MB\n"]}],"source":["df = pd.read_csv('Data/S\u0026P500/3ProSP500.csv')\n","df['ds'] = pd.to_datetime(df['ds'])\n","df = df.astype({col: 'int32' if dtype == 'int64' else 'float32' if dtype == 'float64' else dtype\n","                for col, dtype in df.dtypes.items()})\n","df = df.rename(columns={'840E': 'y'})\n","df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":125},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1731497905024,"user":{"displayName":"John Alejandro Mendoza Ortiz","userId":"11091031515932154769"},"user_tz":-60},"id":"imeLITtw9NiG","outputId":"d7a7561d-a744-477b-8c7c-f184d3312443"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"summary_counts\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"Period\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"2018-2019\",\n          \"2020-2023\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unique Days Covered\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 368,\n        \"min\": 522,\n        \"max\": 1043,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          522,\n          1043\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"summary_counts"},"text/html":["\n","  \u003cdiv id=\"df-d9c09aaf-346f-485f-a791-ae7f55b5c355\" class=\"colab-df-container\"\u003e\n","    \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003ePeriod\u003c/th\u003e\n","      \u003cth\u003eUnique Days Covered\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e2020-2023\u003c/td\u003e\n","      \u003ctd\u003e1043\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e2018-2019\u003c/td\u003e\n","      \u003ctd\u003e522\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e\n","    \u003cdiv class=\"colab-df-buttons\"\u003e\n","\n","  \u003cdiv class=\"colab-df-container\"\u003e\n","    \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d9c09aaf-346f-485f-a791-ae7f55b5c355')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\"\u003e\n","\n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\"\u003e\n","    \u003cpath d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/\u003e\n","  \u003c/svg\u003e\n","    \u003c/button\u003e\n","\n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","    \u003cscript\u003e\n","      const buttonEl =\n","        document.querySelector('#df-d9c09aaf-346f-485f-a791-ae7f55b5c355 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-d9c09aaf-346f-485f-a791-ae7f55b5c355');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    \u003c/script\u003e\n","  \u003c/div\u003e\n","\n","\n","\u003cdiv id=\"df-b72b5298-553f-429b-a6dc-411e0ac9fe7e\"\u003e\n","  \u003cbutton class=\"colab-df-quickchart\" onclick=\"quickchart('df-b72b5298-553f-429b-a6dc-411e0ac9fe7e')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\"\u003e\n","\n","\u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\"\u003e\n","    \u003cg\u003e\n","        \u003cpath d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/\u003e\n","    \u003c/g\u003e\n","\u003c/svg\u003e\n","  \u003c/button\u003e\n","\n","\u003cstyle\u003e\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","\u003c/style\u003e\n","\n","  \u003cscript\u003e\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() =\u003e {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-b72b5298-553f-429b-a6dc-411e0ac9fe7e button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  \u003c/script\u003e\n","\u003c/div\u003e\n","\n","  \u003cdiv id=\"id_ad7df089-1c01-47da-b428-606887994961\"\u003e\n","    \u003cstyle\u003e\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    \u003c/style\u003e\n","    \u003cbutton class=\"colab-df-generate\" onclick=\"generateWithVariable('summary_counts')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\"\u003e\n","\n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/\u003e\n","  \u003c/svg\u003e\n","    \u003c/button\u003e\n","    \u003cscript\u003e\n","      (() =\u003e {\n","      const buttonEl =\n","        document.querySelector('#id_ad7df089-1c01-47da-b428-606887994961 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () =\u003e {\n","        google.colab.notebook.generateWithVariable('summary_counts');\n","      }\n","      })();\n","    \u003c/script\u003e\n","  \u003c/div\u003e\n","\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n"],"text/plain":["      Period  Unique Days Covered\n","0  2020-2023                 1043\n","1  2018-2019                  522"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["start_2020_2023 = '2020-01-01'\n","end_2020_2023 = '2023-12-31'\n","\n","start_2018_2019 = '2018-01-01'\n","end_2018_2019 = '2019-12-31'\n","\n","# Filter the DataFrame for each time range and extract unique days\n","unique_days_2020_2023 = df[(df['ds'] \u003e= start_2020_2023) \u0026 (df['ds'] \u003c= end_2020_2023)]['ds'].dt.date.unique()\n","unique_days_2018_2019 = df[(df['ds'] \u003e= start_2018_2019) \u0026 (df['ds'] \u003c= end_2018_2019)]['ds'].dt.date.unique()\n","\n","# Count the number of unique days in each range\n","count_unique_days_2020_2023 = len(unique_days_2020_2023)\n","count_unique_days_2018_2019 = len(unique_days_2018_2019)\n","\n","# Create a summary DataFrame\n","summary_counts = pd.DataFrame({\n","    'Period': ['2020-2023', '2018-2019'],\n","    'Unique Days Covered': [count_unique_days_2020_2023, count_unique_days_2018_2019]\n","})\n","summary_counts"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dV811ty09Pyf"},"outputs":[],"source":["import os\n","from time import time\n","\n","class AutoNHITSTrainer:\n","    def __init__(self, horizons, levels, exog_list, df, val_size, test_size):\n","        self.horizons = horizons\n","        self.levels = levels\n","        self.exog_list = exog_list\n","        self.df = df\n","        self.val_size = val_size\n","        self.test_size = test_size\n","\n","    def check_existing_files(self, horizon):\n","        \"\"\"Checks if model and CSV already exist for a given horizon.\"\"\"\n","        model_path = f'Trained Models/AutoNHITS/horizon_{horizon}/'\n","        output_csv = f'Data/Test/nhits_model0_1_horizon_{horizon}.csv'\n","        return os.path.exists(model_path) and os.path.exists(output_csv)\n","\n","    def save_results(self, nf, horizon, Y_hat_df):\n","        \"\"\"Saves the trained model and prediction results.\"\"\"\n","        model_path = f'Trained Models/AutoNHITS/horizon_{horizon}/'\n","        output_csv = f'Data/Test/horizon_{horizon}/nhits_model0_1_horizon_{horizon}.csv'\n","\n","        # Create model directory if it doesn't exist\n","        os.makedirs(model_path, exist_ok=True)\n","\n","        # Save the model, predictions and hyperparameter search\n","        for idx, model in enumerate(nf.models):\n","          hpo = f'HPO/horizon_{horizon}/nhits_model{idx}_horizon_{horizon}_hpo.csv'\n","          results = model.results.get_dataframe()\n","          results.to_csv(hpo, index=False)\n","\n","        nf.save(path=model_path, model_index=None, overwrite=True, save_dataset=False)\n","\n","        for col in Y_hat_df.select_dtypes(include='float32').columns:\n","          Y_hat_df[col] = Y_hat_df[col].astype('float16')\n","\n","        Y_hat_df.to_csv(output_csv, index=False)\n","\n","\n","    def configure_models(self, horizon):\n","        \"\"\"Configures two AutoNHITS models for the given horizon.\"\"\"\n","        # Model 0 Configuration\n","        nhits_config0 = AutoNHITS.get_default_config(h=horizon, backend=\"ray\")\n","        nhits_config0['random_seed'] = 42\n","        nhits_config0['learning_rate'] = tune.choice([0.01, 0.005, 0.001, 0.0005, 0.0001, 0.0005, 0.0001, 0.00005, 0.00001])\n","\n","        # Model 1 Configuration\n","        nhits_config1 = AutoNHITS.get_default_config(h=horizon, backend=\"ray\")\n","        nhits_config1['hist_exog_list'] = self.exog_list\n","        nhits_config1['random_seed'] = 42\n","        nhits_config1['learning_rate'] = tune.choice([0.01, 0.005, 0.001, 0.0005, 0.0001, 0.0005, 0.0001, 0.00005, 0.00001])\n","\n","        # Instantiate the models\n","        nhits_model0 = AutoNHITS(h=horizon,\n","                                 config=nhits_config0,\n","                                 search_alg=HyperOptSearch(), num_samples=30,\n","                                 backend='ray',\n","                                 loss=DistributionLoss(distribution='StudentT', level=self.levels),\n","                                 cpus = 12, gpus = 1)\n","\n","        nhits_model1 = AutoNHITS(h=horizon,\n","                                 config=nhits_config1,\n","                                 search_alg=HyperOptSearch(), num_samples=30,\n","                                 backend='ray',\n","                                 loss=DistributionLoss(distribution='StudentT', level=self.levels),\n","                                 cpus = 12, gpus = 1)\n","\n","        return nhits_model0, nhits_model1\n","\n","    def run_training(self):\n","        \"\"\"Runs the training loop over all horizons.\"\"\"\n","        for horizon in self.horizons:\n","            if self.check_existing_files(horizon):\n","                print(f\"Horizon {horizon}: Model and CSV already exist. Skipping this run.\")\n","                continue\n","\n","            # Configure the models\n","            nhits_model0, nhits_model1 = self.configure_models(horizon)\n","\n","            # Start training and cross-validation\n","            init = time()\n","            nf = NeuralForecast(models=[nhits_model0, nhits_model1], freq='B')\n","            Y_hat_df = nf.cross_validation(df=self.df,\n","                                           val_size=self.val_size,\n","                                           test_size=self.test_size,\n","                                           n_windows=None)\n","\n","            # Save results\n","            self.save_results(nf, horizon, Y_hat_df)\n","\n","            # Log the time taken\n","            end = time()\n","            print(f'Horizon {horizon} CV Minutes: {(end - init) / 60}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yLJHhyZoq4RM"},"outputs":[],"source":["exog_list = list(df.columns)\n","exog_list.remove('ds')\n","exog_list.remove('y')\n","exog_list.remove('unique_id')\n","\n","levels = [90]\n","val_size = count_unique_days_2018_2019\n","test_size = count_unique_days_2020_2023\n","horizons = [1, 5, 10]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","output_embedded_package_id":"1VoN6yaXmam83d5-bmqMfTMCDmlMODNKM"},"id":"CT6TAO5sq5If","outputId":"77da92e9-8a72-4e56-9df2-ffec0adc7c07"},"outputs":[{"data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{},"output_type":"display_data"}],"source":["trainer = AutoNHITSTrainer(horizons, levels, exog_list, df, val_size, test_size)\n","trainer.run_training()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Zh6sfyaXltcZ"},"outputs":[],"source":["exog_list = list(df.columns)\n","exog_list.remove('ds')\n","exog_list.remove('y')\n","exog_list.remove('unique_id')\n","\n","levels = [90]\n","val_size = count_unique_days_2018_2019\n","test_size = count_unique_days_2020_2023\n","horizons = [20]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Juxvd8hUl2QL","outputId":"f5c4a9cd-5804-40e3-fb58-75b8706fd3b0"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-11-12 18:17:40,208\tINFO worker.py:1816 -- Started a local Ray instance.\n","2024-11-12 18:17:41,607\tINFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n"]},{"name":"stdout","output_type":"stream","text":["+--------------------------------------------------------------------+\n","| Configuration for experiment     _train_tune_2024-11-12_18-17-38   |\n","+--------------------------------------------------------------------+\n","| Search algorithm                 SearchGenerator                   |\n","| Scheduler                        FIFOScheduler                     |\n","| Number of trials                 30                                |\n","+--------------------------------------------------------------------+\n","\n","View detailed results here: /root/ray_results/_train_tune_2024-11-12_18-17-38\n","To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-11-12_18-17-38_352630_2326/artifacts/2024-11-12_18-17-41/_train_tune_2024-11-12_18-17-38/driver_artifacts`\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=3770)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=3770)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=3770)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=3770)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=3770)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=3770)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=3770)\u001b[0m 2024-11-12 18:17:49.815176: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=3770)\u001b[0m 2024-11-12 18:17:49.835022: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=3770)\u001b[0m 2024-11-12 18:17:49.859783: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=3770)\u001b[0m 2024-11-12 18:17:49.867406: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=3770)\u001b[0m 2024-11-12 18:17:49.885514: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=3770)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=3770)\u001b[0m 2024-11-12 18:17:51.065001: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=3770)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=3770)\u001b[0m \n","\u001b[36m(_train_tune pid=3770)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=3770)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=3770)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=3770)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=3770)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=3770)\u001b[0m 3 | blocks       | ModuleList       | 2.4 M  | train\n","\u001b[36m(_train_tune pid=3770)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=3770)\u001b[0m 2.4 M     Trainable params\n","\u001b[36m(_train_tune pid=3770)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=3770)\u001b[0m 2.4 M     Total params\n","\u001b[36m(_train_tune pid=3770)\u001b[0m 9.772     Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=3770)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=3770)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/13 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.716, train_loss_epoch=-0.0958]\n","Epoch 2:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.08, train_loss_epoch=-0.946]\n","Epoch 3:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.08]\n","Epoch 4:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.09]\n","Epoch 5:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.13]\n","Epoch 6:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.10]\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.11]\n","\u001b[36m(_train_tune pid=3770)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=3770)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.43it/s]\u001b[A\n","\u001b[36m(_train_tune pid=3770)\u001b[0m \n","Epoch 7:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.11, valid_loss=-1.18]\n","Epoch 8:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.12, valid_loss=-1.18]\n","Epoch 9:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.02, train_loss_epoch=-1.09, valid_loss=-1.18]\n","Epoch 10:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.14, valid_loss=-1.18]\n","Epoch 11:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.17, valid_loss=-1.18]\n","Epoch 12:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.08, valid_loss=-1.18]\n","Epoch 13:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.17, valid_loss=-1.18]\n","Epoch 14:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.09, train_loss_epoch=-1.13, valid_loss=-1.18]\n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.848, train_loss_epoch=-1.12, valid_loss=-1.18]\n","\u001b[36m(_train_tune pid=3770)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=3770)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.44it/s]\u001b[A\n","Epoch 15:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.09, train_loss_epoch=-1.12, valid_loss=-1.20] \n","Epoch 16:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.13, valid_loss=-1.20]\n","Epoch 17:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.15, valid_loss=-1.20]\n","Epoch 18:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.14, valid_loss=-1.20]\n","Epoch 19:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.18, valid_loss=-1.20]\n","Epoch 20:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.14, valid_loss=-1.20]\n","Epoch 21:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.17, valid_loss=-1.20]\n","Epoch 22:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.19, valid_loss=-1.20]\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.15, valid_loss=-1.20]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=3770)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.47it/s]\u001b[A\n","Epoch 23:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.15, valid_loss=-1.17]\n","Epoch 24:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.16, valid_loss=-1.17]\n","Epoch 25:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.14, valid_loss=-1.17]\n","Epoch 26:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.19, valid_loss=-1.17]\n","Epoch 27:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.16, valid_loss=-1.17]\n","Epoch 28:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.767, train_loss_epoch=-1.15, valid_loss=-1.17]\n","Epoch 29:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.16, valid_loss=-1.17]\n","Epoch 30:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.18, valid_loss=-1.17]\n","\u001b[36m(_train_tune pid=3770)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=3770)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.37it/s]\u001b[A\n","Epoch 30:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.18, valid_loss=-1.22]\n","Epoch 31:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.19, valid_loss=-1.22]\n","Epoch 32:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.17, valid_loss=-1.22]\n","Epoch 33:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.09, train_loss_epoch=-1.19, valid_loss=-1.22]\n","Epoch 34:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.18, valid_loss=-1.22]\n","Epoch 34: 100%|██████████| 13/13 [00:00\u003c00:00, 38.10it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.16, valid_loss=-1.22]\n","Epoch 35:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.16, valid_loss=-1.22]\n","Epoch 36:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.17, valid_loss=-1.22]\n","Epoch 37:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.17, valid_loss=-1.22]\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.17, valid_loss=-1.22]\n","\u001b[36m(_train_tune pid=3770)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=3770)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.39it/s]\u001b[A\n","Epoch 38:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.17, valid_loss=-1.21]\n","Epoch 39:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.18, valid_loss=-1.21]\n","Epoch 40:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.972, train_loss_epoch=-1.17, valid_loss=-1.21]\n","Epoch 41:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.20, valid_loss=-1.21]\n","Epoch 42:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.20, valid_loss=-1.21]\n","Epoch 43:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.18, valid_loss=-1.21]\n","Epoch 44:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.18, valid_loss=-1.21]\n","Epoch 45:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.19, valid_loss=-1.21]\n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.19, valid_loss=-1.21]\n","\u001b[36m(_train_tune pid=3770)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=3770)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.44it/s]\u001b[A\n","Epoch 46:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.19, valid_loss=-1.22]\n","Epoch 47:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.842, train_loss_epoch=-1.16, valid_loss=-1.22]\n","Epoch 48:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.20, valid_loss=-1.22]\n","Epoch 49:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.19, valid_loss=-1.22]\n","Epoch 50:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.20, valid_loss=-1.22]\n","Epoch 51:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.18, valid_loss=-1.22]\n","Epoch 52:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.20, valid_loss=-1.22]\n","Epoch 53:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.18, valid_loss=-1.22]\n","\u001b[36m(_train_tune pid=3770)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=3770)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.40it/s]\u001b[A\n","Epoch 53:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.18, valid_loss=-1.20]\n","Epoch 54:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.18, valid_loss=-1.20]\n","Epoch 55:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.04, train_loss_epoch=-1.20, valid_loss=-1.20]\n","Epoch 56:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.19, valid_loss=-1.20]\n","Epoch 57:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.18, valid_loss=-1.20]\n","Epoch 58:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.19, valid_loss=-1.20]\n","Epoch 59:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.18, valid_loss=-1.20]\n","Epoch 60:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.16, valid_loss=-1.20]\n","Epoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.18, valid_loss=-1.20]\n","\u001b[36m(_train_tune pid=3770)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=3770)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.41it/s]\u001b[A\n","Epoch 61:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.18, valid_loss=-1.22]\n","Epoch 62:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.18, valid_loss=-1.22]\n","Epoch 63:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.18, valid_loss=-1.22]\n","Epoch 64:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.22, valid_loss=-1.22]\n","Epoch 65:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.22, valid_loss=-1.22]\n","Epoch 66:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.19, valid_loss=-1.22]\n","Epoch 67:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.17, valid_loss=-1.22]\n","Epoch 68:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.18, valid_loss=-1.22]\n","Epoch 69:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.778, train_loss_epoch=-1.14, valid_loss=-1.22]\n","\u001b[36m(_train_tune pid=3770)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=3770)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.38it/s]\u001b[A\n","Epoch 69:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.14, valid_loss=-1.23] \n","Epoch 70:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 71:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 72:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 73:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 74:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.18, valid_loss=-1.23]\n","Epoch 75:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.926, train_loss_epoch=-1.14, valid_loss=-1.23]\n","Epoch 76:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.21, valid_loss=-1.23]\n","\u001b[36m(_train_tune pid=3770)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=3770)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.43it/s]\u001b[A\n","Epoch 77:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.21, valid_loss=-1.21]\n","Epoch 78:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.19, valid_loss=-1.21]\n","Epoch 79:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.17, valid_loss=-1.21]\n","Epoch 80:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.19, valid_loss=-1.21]\n","Epoch 81:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.22, valid_loss=-1.21]\n","Epoch 81: 100%|██████████| 13/13 [00:00\u003c00:00, 38.72it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.23, valid_loss=-1.21]\n","Epoch 82:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.23, valid_loss=-1.21]\n","Epoch 83:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.08, train_loss_epoch=-1.17, valid_loss=-1.21]\n","Epoch 84:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.19, valid_loss=-1.21]\n","\u001b[36m(_train_tune pid=3770)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=3770)\u001b[0m `Trainer.fit` stopped: `max_steps=1100.0` reached.\n","2024-11-12 18:18:42,416\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (180, 60, 1), 'n_pool_kernel_size': (16, 8, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=3770)\u001b[0m \n","\u001b[36m(_train_tune pid=3770)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.38it/s]\u001b[A\n","\u001b[36m(_train_tune pid=3770)\u001b[0m \r                                                                        \u001b[A\rEpoch 84:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.19, valid_loss=-1.23]\rEpoch 84:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.19, valid_loss=-1.23]\rEpoch 84:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.19, valid_loss=-1.23]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=4118)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=4118)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=4118)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=4118)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=4118)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=4118)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=4118)\u001b[0m 2024-11-12 18:18:51.009411: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=4118)\u001b[0m 2024-11-12 18:18:51.029759: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=4118)\u001b[0m 2024-11-12 18:18:51.054632: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=4118)\u001b[0m 2024-11-12 18:18:51.062268: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=4118)\u001b[0m 2024-11-12 18:18:51.080857: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=4118)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=4118)\u001b[0m 2024-11-12 18:18:52.265293: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=4118)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=4118)\u001b[0m \n","\u001b[36m(_train_tune pid=4118)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=4118)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=4118)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=4118)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=4118)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=4118)\u001b[0m 3 | blocks       | ModuleList       | 2.6 M  | train\n","\u001b[36m(_train_tune pid=4118)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=4118)\u001b[0m 2.6 M     Trainable params\n","\u001b[36m(_train_tune pid=4118)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=4118)\u001b[0m 2.6 M     Total params\n","\u001b[36m(_train_tune pid=4118)\u001b[0m 10.342    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=4118)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=4118)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 18:18:55,261\tERROR tune_controller.py:1331 -- Trial task failed for trial _train_tune_5c12a5b4\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n","    result = ray.get(future)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n","    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(OutOfMemoryError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=4118, ip=172.28.0.12, actor_id=09017d54d75602b91562621001000000, repr=_train_tune)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n","    raise skipped from exception_cause(skipped)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n","    self._ret = self._target(*self._args, **self._kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in \u003clambda\u003e\n","    training_func=lambda: self._trainable_func(self.config),\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n","    output = fn()\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/util.py\", line 130, in inner\n","    return trainable(config, **fn_kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_auto.py\", line 214, in _train_tune\n","    _ = self._fit_model(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_auto.py\", line 362, in _fit_model\n","    model = model.fit(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_windows.py\", line 661, in fit\n","    return self._fit(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_model.py\", line 356, in _fit\n","    trainer.fit(model, datamodule=datamodule)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 538, in fit\n","    call._call_and_handle_interrupt(\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 47, in _call_and_handle_interrupt\n","    return trainer_fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 574, in _fit_impl\n","    self._run(model, ckpt_path=ckpt_path)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 981, in _run\n","    results = self._run_stage()\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1023, in _run_stage\n","    self._run_sanity_check()\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1052, in _run_sanity_check\n","    val_loop.run()\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py\", line 178, in _decorator\n","    return loop_run(self, *args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 135, in run\n","    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 396, in _evaluation_step\n","    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 319, in _call_strategy_hook\n","    output = fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py\", line 411, in validation_step\n","    return self.lightning_module.validation_step(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_windows.py\", line 534, in validation_step\n","    valid_loss_batch = self._compute_valid_loss(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_windows.py\", line 461, in _compute_valid_loss\n","    _, sample_mean, quants = self.loss.sample(distr_args=distr_args)\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/losses/pytorch.py\", line 2002, in sample\n","    samples = distr.sample(sample_shape=(num_samples,))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributions/distribution.py\", line 167, in sample\n","    return self.rsample(sample_shape)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributions/studentT.py\", line 90, in rsample\n","    Y = X * torch.rsqrt(Z / self.df)\n","torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.60 GiB. GPU 0 has a total capacity of 39.56 GiB of which 7.84 GiB is free. Process 27231 has 416.00 MiB memory in use. Process 39459 has 31.31 GiB memory in use. Of the allocated memory 29.95 GiB is allocated by PyTorch, and 896.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial _train_tune_5c12a5b4 errored after 0 iterations at 2024-11-12 18:18:55. Total running time: 1min 13s\n","Error file: /tmp/ray/session_2024-11-12_18-17-38_352630_2326/artifacts/2024-11-12_18-17-41/_train_tune_2024-11-12_18-17-38/driver_artifacts/_train_tune_5c12a5b4_2_batch_size=256,h=20,input_size=80,learning_rate=0.0005,loss=ref_ph_de895953,max_steps=1500.0000,n_freq_down_2024-11-12_18-17-49/error.txt\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=4227)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=4227)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=4227)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=4227)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=4227)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=4227)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=4227)\u001b[0m 2024-11-12 18:19:02.962189: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=4227)\u001b[0m 2024-11-12 18:19:02.981436: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=4227)\u001b[0m 2024-11-12 18:19:03.005859: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=4227)\u001b[0m 2024-11-12 18:19:03.013261: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=4227)\u001b[0m 2024-11-12 18:19:03.030691: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=4227)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=4227)\u001b[0m 2024-11-12 18:19:04.200707: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=4227)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=4227)\u001b[0m \n","\u001b[36m(_train_tune pid=4227)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=4227)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=4227)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=4227)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=4227)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=4227)\u001b[0m 3 | blocks       | ModuleList       | 2.6 M  | train\n","\u001b[36m(_train_tune pid=4227)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=4227)\u001b[0m 2.6 M     Trainable params\n","\u001b[36m(_train_tune pid=4227)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=4227)\u001b[0m 2.6 M     Total params\n","\u001b[36m(_train_tune pid=4227)\u001b[0m 10.518    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=4227)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=4227)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","                                                                           \n","Epoch 0:   0%|          | 0/4 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.46]\n","Epoch 2:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.62]\n","Epoch 3:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.60]\n","Epoch 4:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.58]\n","Epoch 5:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.69]\n","Epoch 6:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.59]\n","Epoch 7:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.71]\n","Epoch 8:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.62]\n","Epoch 9:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.82]\n","Epoch 10:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.67]\n","Epoch 11:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-1.91]\n","Epoch 12:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.74]\n","Epoch 13:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.77]\n","Epoch 14:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.77]\n","Epoch 15:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.76]\n","Epoch 16:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.77]\n","Epoch 17:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.93]\n","Epoch 18:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-1.99]\n","Epoch 19:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-1.92]\n","Epoch 20:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.83]\n","Epoch 21:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.00]\n","Epoch 22:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.87]\n","Epoch 23:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.89]\n","Epoch 24:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.82]\n","Epoch 24: 100%|██████████| 4/4 [00:00\u003c00:00, 10.98it/s, v_num=0, train_loss_step=-2.48, train_loss_epoch=-1.82]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4227)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4227)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.79it/s]\u001b[A\n","Epoch 25:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.48, train_loss_epoch=-2.04, valid_loss=-1.91]\n","Epoch 26:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-1.95, valid_loss=-1.91]\n","Epoch 27:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.91, valid_loss=-1.91]\n","Epoch 28:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.88, valid_loss=-1.91]\n","Epoch 29:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.70, valid_loss=-1.91]\n","Epoch 30:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-1.90, valid_loss=-1.91]\n","Epoch 30:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.95, valid_loss=-1.91]        \n","Epoch 31:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.95, valid_loss=-1.91]\n","Epoch 32:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.87, valid_loss=-1.91]\n","Epoch 33:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.86, valid_loss=-1.91]\n","Epoch 34:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.88, valid_loss=-1.91]\n","Epoch 35:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.91, valid_loss=-1.91]\n","Epoch 36:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.93, valid_loss=-1.91]\n","Epoch 37:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-1.99, valid_loss=-1.91]\n","Epoch 38:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.41, train_loss_epoch=-2.05, valid_loss=-1.91]\n","Epoch 38: 100%|██████████| 4/4 [00:00\u003c00:00, 10.91it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.95, valid_loss=-1.91]\n","Epoch 39:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.95, valid_loss=-1.91]\n","Epoch 40:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.88, valid_loss=-1.91]\n","Epoch 41:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.84, valid_loss=-1.91]\n","Epoch 42:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.84, valid_loss=-1.91]\n","Epoch 43:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.96, valid_loss=-1.91]\n","Epoch 43: 100%|██████████| 4/4 [00:00\u003c00:00, 10.77it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.96, valid_loss=-1.91]\n","Epoch 44:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.96, valid_loss=-1.91]\n","Epoch 45:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.95, valid_loss=-1.91]\n","Epoch 46:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.94, valid_loss=-1.91]\n","Epoch 47:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.95, valid_loss=-1.91]\n","Epoch 48:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.04, valid_loss=-1.91]\n","Epoch 49:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.87, valid_loss=-1.91]\n","Epoch 49: 100%|██████████| 4/4 [00:00\u003c00:00, 11.08it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.87, valid_loss=-1.91]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4227)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4227)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.79it/s]\u001b[A\n","Epoch 50:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.98, valid_loss=-1.96]\n","Epoch 51:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.98, valid_loss=-1.96]\n","Epoch 52:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.99, valid_loss=-1.96]\n","Epoch 53:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.97, valid_loss=-1.96]\n","Epoch 54:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.06, valid_loss=-1.96]\n","Epoch 55:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.92, valid_loss=-1.96]\n","Epoch 56:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.97, valid_loss=-1.96]\n","Epoch 57:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-2.01, valid_loss=-1.96]\n","Epoch 58:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.91, valid_loss=-1.96]\n","Epoch 59:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-2.04, valid_loss=-1.96]\n","Epoch 60:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.00, valid_loss=-1.96]\n","Epoch 61:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.98, valid_loss=-1.96]\n","Epoch 62:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.90, valid_loss=-1.96]\n","Epoch 63:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.99, valid_loss=-1.96]\n","Epoch 64:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.97, valid_loss=-1.96]\n","Epoch 65:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.89, valid_loss=-1.96]\n","Epoch 66:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.92, valid_loss=-1.96]\n","Epoch 67:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.92, valid_loss=-1.96]\n","Epoch 68:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-1.97, valid_loss=-1.96]\n","Epoch 69:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.97, valid_loss=-1.96]\n","Epoch 69: 100%|██████████| 4/4 [00:00\u003c00:00, 10.65it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.89, valid_loss=-1.96]\n","Epoch 70:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.89, valid_loss=-1.96]\n","Epoch 71:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.90, valid_loss=-1.96]\n","Epoch 72:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.81, valid_loss=-1.96]\n","Epoch 73:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.96, valid_loss=-1.96]\n","Epoch 74:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.99, valid_loss=-1.96]\n","Epoch 74: 100%|██████████| 4/4 [00:00\u003c00:00, 10.77it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.99, valid_loss=-1.96]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4227)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.80it/s]\u001b[A\n","Epoch 75:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.86, valid_loss=-1.98]\n","Epoch 76:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.96, valid_loss=-1.98]\n","Epoch 77:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.99, valid_loss=-1.98]\n","Epoch 78:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.93, valid_loss=-1.98]\n","Epoch 79:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-2.02, valid_loss=-1.98]\n","Epoch 80:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.97, valid_loss=-1.98]\n","Epoch 81:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.06, valid_loss=-1.98]\n","Epoch 82:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.88, valid_loss=-1.98]\n","Epoch 83:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.92, valid_loss=-1.98]\n","Epoch 84:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.08, train_loss_epoch=-1.77, valid_loss=-1.98]\n","Epoch 85:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.97, valid_loss=-1.98]\n","Epoch 86:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.99, valid_loss=-1.98]\n","Epoch 87:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.00, valid_loss=-1.98]\n","Epoch 88:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-2.07, valid_loss=-1.98]\n","Epoch 89:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.96, valid_loss=-1.98]\n","Epoch 90:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.84, valid_loss=-1.98]\n","Epoch 91:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.91, valid_loss=-1.98]\n","Epoch 92:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-2.05, valid_loss=-1.98]\n","Epoch 93:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.94, valid_loss=-1.98]\n","Epoch 94:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-2.07, valid_loss=-1.98]\n","Epoch 95:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-2.09, valid_loss=-1.98]\n","Epoch 96:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.90, valid_loss=-1.98]\n","Epoch 97:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.03, valid_loss=-1.98]\n","Epoch 98:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-2.03, valid_loss=-1.98]\n","Epoch 99:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.94, valid_loss=-1.98]\n","Epoch 99: 100%|██████████| 4/4 [00:00\u003c00:00, 10.59it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.94, valid_loss=-1.98]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4227)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4227)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.79it/s]\u001b[A\n","Epoch 100:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.96, valid_loss=-1.99]\n","Epoch 101:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-2.08, valid_loss=-1.99]\n","Epoch 102:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.95, valid_loss=-1.99]\n","Epoch 103:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.00, valid_loss=-1.99]\n","Epoch 104:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.96, valid_loss=-1.99]\n","Epoch 105:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.02, valid_loss=-1.99]\n","Epoch 106:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.04, valid_loss=-1.99]\n","Epoch 107:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.98, valid_loss=-1.99]\n","Epoch 108:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.01, valid_loss=-1.99]\n","Epoch 109:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.92, valid_loss=-1.99]\n","Epoch 110:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-1.99, valid_loss=-1.99]\n","Epoch 111:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.99, valid_loss=-1.99]\n","Epoch 112:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.95, valid_loss=-1.99]\n","Epoch 113:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.40, train_loss_epoch=-2.11, valid_loss=-1.99]\n","Epoch 113: 100%|██████████| 4/4 [00:00\u003c00:00, 10.95it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.10, valid_loss=-1.99]\n","Epoch 114:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.10, valid_loss=-1.99]\n","Epoch 115:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.02, valid_loss=-1.99]\n","Epoch 116:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.94, valid_loss=-1.99]\n","Epoch 117:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.90, valid_loss=-1.99]\n","Epoch 118:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.86, valid_loss=-1.99]\n","Epoch 119:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-2.06, valid_loss=-1.99]\n","Epoch 120:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.88, valid_loss=-1.99]\n","Epoch 121:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.99, valid_loss=-1.99]\n","Epoch 122:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.97, valid_loss=-1.99]\n","Epoch 123:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.98, valid_loss=-1.99]\n","Epoch 124:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.01, valid_loss=-1.99]\n","Epoch 124: 100%|██████████| 4/4 [00:00\u003c00:00, 10.90it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.01, valid_loss=-1.99]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4227)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4227)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.80it/s]\u001b[A\n","Epoch 124: 100%|██████████| 4/4 [00:02\u003c00:00,  1.49it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.02, valid_loss=-2.00]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=4227)\u001b[0m `Trainer.fit` stopped: `max_steps=500.0` reached.\n","2024-11-12 18:20:06,117\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (1, 1, 1), 'n_pool_kernel_size': (16, 8, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=4585)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=4585)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=4585)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=4585)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=4585)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=4585)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=4585)\u001b[0m 2024-11-12 18:20:13.958414: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=4585)\u001b[0m 2024-11-12 18:20:13.978641: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=4585)\u001b[0m 2024-11-12 18:20:14.003629: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=4585)\u001b[0m 2024-11-12 18:20:14.011251: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=4585)\u001b[0m 2024-11-12 18:20:14.029076: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=4585)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=4585)\u001b[0m 2024-11-12 18:20:15.189705: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=4585)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=4585)\u001b[0m \n","\u001b[36m(_train_tune pid=4585)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=4585)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=4585)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=4585)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=4585)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=4585)\u001b[0m 3 | blocks       | ModuleList       | 2.4 M  | train\n","\u001b[36m(_train_tune pid=4585)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=4585)\u001b[0m 2.4 M     Trainable params\n","\u001b[36m(_train_tune pid=4585)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=4585)\u001b[0m 2.4 M     Total params\n","\u001b[36m(_train_tune pid=4585)\u001b[0m 9.757     Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=4585)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=4585)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/13 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.876, train_loss_epoch=2.200]\n","Epoch 2:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.453, train_loss_epoch=0.238]\n","Epoch 3:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.932, train_loss_epoch=-0.573]\n","Epoch 4:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.04, train_loss_epoch=-1.02]\n","Epoch 5:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.10]\n","Epoch 6:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.10]\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.13]\n","\u001b[36m(_train_tune pid=4585)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4585)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.42it/s]\u001b[A\n","Epoch 7:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.13, valid_loss=-1.17]\n","Epoch 8:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.13, valid_loss=-1.17]\n","Epoch 9:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.04, train_loss_epoch=-1.14, valid_loss=-1.17]\n","Epoch 10:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.13, valid_loss=-1.17]\n","Epoch 11:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.17, valid_loss=-1.17]\n","Epoch 12:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.15, valid_loss=-1.17]\n","Epoch 13:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.18, valid_loss=-1.17]\n","Epoch 14:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.08, train_loss_epoch=-1.14, valid_loss=-1.17]\n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.922, train_loss_epoch=-1.14, valid_loss=-1.17]\n","\u001b[36m(_train_tune pid=4585)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4585)\u001b[0m \n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4585)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.28it/s]\u001b[A\n","Epoch 15:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.14, valid_loss=-1.20] \n","Epoch 16:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.16, valid_loss=-1.20]\n","Epoch 17:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.14, valid_loss=-1.20]\n","Epoch 18:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.15, valid_loss=-1.20]\n","Epoch 19:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.16, valid_loss=-1.20]\n","Epoch 20:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.17, valid_loss=-1.20]\n","Epoch 21:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.16, valid_loss=-1.20]\n","Epoch 22:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.18, valid_loss=-1.20]\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.18, valid_loss=-1.20]\n","\u001b[36m(_train_tune pid=4585)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4585)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.44it/s]\u001b[A\n","Epoch 23:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.18, valid_loss=-1.19]\n","Epoch 24:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.18, valid_loss=-1.19]\n","Epoch 25:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.17, valid_loss=-1.19]\n","Epoch 26:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.17, valid_loss=-1.19]\n","Epoch 27:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.17, valid_loss=-1.19]\n","Epoch 28:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.834, train_loss_epoch=-1.16, valid_loss=-1.19]\n","Epoch 29:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.18, valid_loss=-1.19]\n","Epoch 30:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.19, valid_loss=-1.19]\n","\u001b[36m(_train_tune pid=4585)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4585)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.35it/s]\u001b[A\n","Epoch 31:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.19, valid_loss=-1.22]\n","Epoch 32:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.18, valid_loss=-1.22]\n","Epoch 33:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.18, valid_loss=-1.22]\n","Epoch 34:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.18, valid_loss=-1.22]\n","Epoch 35:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.18, valid_loss=-1.22]\n","Epoch 36:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.19, valid_loss=-1.22]\n","Epoch 37:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.17, valid_loss=-1.22]\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.19, valid_loss=-1.22]\n","\u001b[36m(_train_tune pid=4585)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4585)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.42it/s]\u001b[A\n","Epoch 38:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.19, valid_loss=-1.22]\n","Epoch 39:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.19, valid_loss=-1.22]\n","Epoch 40:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.951, train_loss_epoch=-1.17, valid_loss=-1.22]\n","Epoch 41:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.20, valid_loss=-1.22]\n","Epoch 42:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.19, valid_loss=-1.22]\n","Epoch 43:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.19, valid_loss=-1.22]\n","Epoch 44:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.18, valid_loss=-1.22]\n","Epoch 45:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.19, valid_loss=-1.22]\n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.20, valid_loss=-1.22]\n","\u001b[36m(_train_tune pid=4585)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4585)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.45it/s]\u001b[A\n","Epoch 46:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.20, valid_loss=-1.19]\n","Epoch 46: 100%|██████████| 13/13 [00:02\u003c00:00,  6.27it/s, v_num=0, train_loss_step=-0.944, train_loss_epoch=-1.20, valid_loss=-1.19]\n","Epoch 46: 100%|██████████| 13/13 [00:02\u003c00:00,  6.27it/s, v_num=0, train_loss_step=-0.944, train_loss_epoch=-1.16, valid_loss=-1.19]\n","Epoch 47:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.944, train_loss_epoch=-1.16, valid_loss=-1.19]\n","Epoch 48:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.19, valid_loss=-1.19]\n","Epoch 49:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.19, valid_loss=-1.19]\n","Epoch 50:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.20, valid_loss=-1.19]\n","Epoch 51:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.18, valid_loss=-1.19]\n","Epoch 52:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.19, valid_loss=-1.19]\n","Epoch 53:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.19, valid_loss=-1.19]\n","\u001b[36m(_train_tune pid=4585)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4585)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.41it/s]\u001b[A\n","Epoch 53:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 54:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 55:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.904, train_loss_epoch=-1.18, valid_loss=-1.23]\n","Epoch 56:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 57:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 58:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 59:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 60:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.18, valid_loss=-1.23]\n","Epoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.17, valid_loss=-1.23]\n","\u001b[36m(_train_tune pid=4585)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4585)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.43it/s]\u001b[A\n","Epoch 61:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.17, valid_loss=-1.22]\n","Epoch 62:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.18, valid_loss=-1.22]\n","Epoch 63:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.19, valid_loss=-1.22]\n","Epoch 64:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.20, valid_loss=-1.22]\n","Epoch 65:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.20, valid_loss=-1.22]\n","Epoch 66:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.19, valid_loss=-1.22]\n","Epoch 67:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.18, valid_loss=-1.22]\n","Epoch 68:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.19, valid_loss=-1.22]\n","Epoch 69:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.752, train_loss_epoch=-1.15, valid_loss=-1.22]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4585)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.45it/s]\u001b[A\n","Epoch 69:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.15, valid_loss=-1.22] \n","Epoch 70:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.21, valid_loss=-1.22]\n","Epoch 71:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.19, valid_loss=-1.22]\n","Epoch 72:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.20, valid_loss=-1.22]\n","Epoch 73:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.19, valid_loss=-1.22]\n","Epoch 74:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.19, valid_loss=-1.22]\n","Epoch 75:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.03, train_loss_epoch=-1.17, valid_loss=-1.22]\n","Epoch 76:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.20, valid_loss=-1.22]\n","\u001b[36m(_train_tune pid=4585)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=4585)\u001b[0m `Trainer.fit` stopped: `max_steps=1000.0` reached.\n","2024-11-12 18:21:00,083\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (60, 8, 1), 'n_pool_kernel_size': (4, 4, 4), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=4585)\u001b[0m \n","\u001b[36m(_train_tune pid=4585)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.40it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4585)\u001b[0m \r                                                                        \u001b[A\rEpoch 76:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.20, valid_loss=-1.23]\rEpoch 76:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.20, valid_loss=-1.23]\rEpoch 76:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.20, valid_loss=-1.23]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=4868)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=4868)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=4868)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=4868)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=4868)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=4868)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=4868)\u001b[0m 2024-11-12 18:21:07.990954: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=4868)\u001b[0m 2024-11-12 18:21:08.011430: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=4868)\u001b[0m 2024-11-12 18:21:08.036473: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=4868)\u001b[0m 2024-11-12 18:21:08.044163: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=4868)\u001b[0m 2024-11-12 18:21:08.061818: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=4868)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=4868)\u001b[0m 2024-11-12 18:21:09.240644: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=4868)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=4868)\u001b[0m \n","\u001b[36m(_train_tune pid=4868)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=4868)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=4868)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=4868)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=4868)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=4868)\u001b[0m 3 | blocks       | ModuleList       | 2.5 M  | train\n","\u001b[36m(_train_tune pid=4868)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=4868)\u001b[0m 2.5 M     Trainable params\n","\u001b[36m(_train_tune pid=4868)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=4868)\u001b[0m 2.5 M     Total params\n","\u001b[36m(_train_tune pid=4868)\u001b[0m 10.139    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=4868)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=4868)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/13 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.14]\n","Epoch 2:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-1.65]\n","Epoch 3:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.79]\n","Epoch 4:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.84]\n","Epoch 5:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-1.93]\n","Epoch 6:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.90]\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.94]\n","\u001b[36m(_train_tune pid=4868)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4868)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  6.87it/s]\u001b[A\n","Epoch 7:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.94, valid_loss=-1.92]\n","Epoch 8:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.93, valid_loss=-1.92]\n","Epoch 9:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.93, valid_loss=-1.92]\n","Epoch 10:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.94, valid_loss=-1.92]\n","Epoch 11:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.47, train_loss_epoch=-2.01, valid_loss=-1.92]\n","Epoch 12:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.95, valid_loss=-1.92]\n","Epoch 13:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.00, valid_loss=-1.92]\n","Epoch 14:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.97, valid_loss=-1.92]\n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.96, valid_loss=-1.92]\n","\u001b[36m(_train_tune pid=4868)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4868)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  6.79it/s]\u001b[A\n","Epoch 15:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.96, valid_loss=-1.97]\n","Epoch 16:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-2.01, valid_loss=-1.97]\n","Epoch 17:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.50, train_loss_epoch=-2.03, valid_loss=-1.97]\n","Epoch 18:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-2.00, valid_loss=-1.97]\n","Epoch 19:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.98, valid_loss=-1.97]\n","Epoch 20:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.49, train_loss_epoch=-2.04, valid_loss=-1.97]\n","Epoch 21:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.00, valid_loss=-1.97]\n","Epoch 22:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.98, valid_loss=-1.97]\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.96, valid_loss=-1.97]\n","\u001b[36m(_train_tune pid=4868)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4868)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  6.92it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4868)\u001b[0m \n","Epoch 23:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.96, valid_loss=-1.98]\n","Epoch 24:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.53, train_loss_epoch=-2.04, valid_loss=-1.98]\n","Epoch 25:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.01, valid_loss=-1.98]\n","Epoch 26:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.01, valid_loss=-1.98]\n","Epoch 27:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.00, valid_loss=-1.98]\n","Epoch 28:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.96, valid_loss=-1.98]\n","Epoch 29:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.02, valid_loss=-1.98]\n","Epoch 30:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.02, valid_loss=-1.98]\n","\u001b[36m(_train_tune pid=4868)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4868)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  6.93it/s]\u001b[A\n","Epoch 30:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.02, valid_loss=-1.99]\n","Epoch 31:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-2.00, valid_loss=-1.99]\n","Epoch 32:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 33:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.96, valid_loss=-1.99]\n","Epoch 34:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.01, valid_loss=-1.99]\n","Epoch 35:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-2.01, valid_loss=-1.99]\n","Epoch 36:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.51, train_loss_epoch=-2.06, valid_loss=-1.99]\n","Epoch 37:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.01, valid_loss=-1.99]\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-2.00, valid_loss=-1.99]\n","\u001b[36m(_train_tune pid=4868)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4868)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  6.92it/s]\u001b[A\n","Epoch 38:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.00, valid_loss=-2.01]\n","Epoch 39:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.02, valid_loss=-2.01]\n","Epoch 40:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.99, valid_loss=-2.01]\n","Epoch 41:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-2.03, valid_loss=-2.01]\n","Epoch 42:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.01, valid_loss=-2.01]\n","Epoch 43:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.04, valid_loss=-2.01]\n","Epoch 44:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.02, valid_loss=-2.01]\n","Epoch 45:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.01, valid_loss=-2.01]\n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.05, valid_loss=-2.01]\n","\u001b[36m(_train_tune pid=4868)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4868)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  6.85it/s]\u001b[A\n","Epoch 46:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.05, valid_loss=-2.01]\n","Epoch 47:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-2.00, valid_loss=-2.01]\n","Epoch 48:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.05, valid_loss=-2.01]\n","Epoch 49:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.02, valid_loss=-2.01]\n","Epoch 50:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.04, valid_loss=-2.01]\n","Epoch 51:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.03, valid_loss=-2.01]\n","Epoch 52:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.47, train_loss_epoch=-2.05, valid_loss=-2.01]\n","Epoch 53:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.02, valid_loss=-2.01]\n","\u001b[36m(_train_tune pid=4868)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4868)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  6.84it/s]\u001b[A\n","Epoch 54:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.02, valid_loss=-2.01]\n","Epoch 55:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.99, valid_loss=-2.01]\n","Epoch 56:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.04, valid_loss=-2.01]\n","Epoch 57:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.03, valid_loss=-2.01]\n","Epoch 58:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-2.05, valid_loss=-2.01]\n","Epoch 59:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-2.01, valid_loss=-2.01]\n","Epoch 60:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.03, valid_loss=-2.01]\n","Epoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-2.02, valid_loss=-2.01]\n","\u001b[36m(_train_tune pid=4868)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4868)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  6.92it/s]\u001b[A\n","Epoch 61:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.02, valid_loss=-2.01]\n","Epoch 62:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-2.03, valid_loss=-2.01]\n","Epoch 63:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.01, valid_loss=-2.01]\n","Epoch 63: 100%|██████████| 13/13 [00:00\u003c00:00, 36.49it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-2.00, valid_loss=-2.01]\n","Epoch 64:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-2.00, valid_loss=-2.01]\n","Epoch 65:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.03, valid_loss=-2.01]\n","Epoch 66:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.04, valid_loss=-2.01]\n","Epoch 67:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-2.01, valid_loss=-2.01]\n","Epoch 68:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-2.02, valid_loss=-2.01]\n","Epoch 69:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.96, valid_loss=-2.01]\n","\u001b[36m(_train_tune pid=4868)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4868)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  6.91it/s]\u001b[A\n","Epoch 69:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.96, valid_loss=-2.02]\n","Epoch 70:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.03, valid_loss=-2.02]\n","Epoch 71:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.05, valid_loss=-2.02]\n","Epoch 72:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.36, train_loss_epoch=-2.06, valid_loss=-2.02]\n","Epoch 73:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.05, valid_loss=-2.02]\n","Epoch 74:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.04, valid_loss=-2.02]\n","Epoch 75:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.02, valid_loss=-2.02]\n","Epoch 76:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-2.06, valid_loss=-2.02]\n","\u001b[36m(_train_tune pid=4868)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4868)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  6.88it/s]\u001b[A\n","                                                                        \u001b[A\n","Epoch 76:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.06, valid_loss=-2.02]\n","Epoch 77:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.05, valid_loss=-2.02]\n","Epoch 78:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.03, valid_loss=-2.02]\n","Epoch 79:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-2.03, valid_loss=-2.02]\n","Epoch 80:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.05, valid_loss=-2.02]\n","Epoch 81:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-2.07, valid_loss=-2.02]\n","Epoch 82:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.54, train_loss_epoch=-2.06, valid_loss=-2.02]\n","Epoch 83:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-2.02, valid_loss=-2.02]\n","Epoch 84:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.06, valid_loss=-2.02]\n","\u001b[36m(_train_tune pid=4868)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4868)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  6.92it/s]\u001b[A\n","Epoch 84:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.06, valid_loss=-2.02]\n","Epoch 84: 100%|██████████| 13/13 [00:02\u003c00:00,  5.73it/s, v_num=0, train_loss_step=-2.55, train_loss_epoch=-2.06, valid_loss=-2.02]\n","Epoch 85:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.55, train_loss_epoch=-2.08, valid_loss=-2.02]\n","Epoch 86:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-2.00, valid_loss=-2.02]\n","Epoch 86: 100%|██████████| 13/13 [00:00\u003c00:00, 36.03it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-2.00, valid_loss=-2.02]\n","Epoch 87:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-2.01, valid_loss=-2.02]\n","Epoch 88:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.06, valid_loss=-2.02]\n","Epoch 89:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.03, valid_loss=-2.02]\n","Epoch 90:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-2.07, valid_loss=-2.02]\n","Epoch 91:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-2.07, valid_loss=-2.02]\n","Epoch 92:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-2.01, valid_loss=-2.02]\n","\u001b[36m(_train_tune pid=4868)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4868)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  6.90it/s]\u001b[A\n","Epoch 92:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.01, valid_loss=-2.03]\n","Epoch 93:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.05, valid_loss=-2.03]\n","Epoch 94:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.04, valid_loss=-2.03]\n","Epoch 95:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-2.06, valid_loss=-2.03]\n","Epoch 96:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-2.03, valid_loss=-2.03]\n","Epoch 97:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-2.07, valid_loss=-2.03]\n","Epoch 98:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.04, valid_loss=-2.03]\n","Epoch 99:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.03, valid_loss=-2.03]\n","Epoch 99: 100%|██████████| 13/13 [00:00\u003c00:00, 38.75it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.03, valid_loss=-2.03]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4868)\u001b[0m \n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=4868)\u001b[0m `Trainer.fit` stopped: `max_steps=1300.0` reached.\n","2024-11-12 18:22:12,932\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (1, 1, 1), 'n_pool_kernel_size': (4, 4, 4), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=4868)\u001b[0m \n","\u001b[36m(_train_tune pid=4868)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  6.84it/s]\u001b[A\n","\u001b[36m(_train_tune pid=4868)\u001b[0m \r                                                                        \u001b[A\rEpoch 99: 100%|██████████| 13/13 [00:02\u003c00:00,  5.76it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.03, valid_loss=-2.03]\rEpoch 99: 100%|██████████| 13/13 [00:02\u003c00:00,  5.76it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.03, valid_loss=-2.03]\rEpoch 99: 100%|██████████| 13/13 [00:02\u003c00:00,  5.75it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.03, valid_loss=-2.03]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=5230)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=5230)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=5230)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=5230)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=5230)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=5230)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=5230)\u001b[0m 2024-11-12 18:22:21.066134: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=5230)\u001b[0m 2024-11-12 18:22:21.086863: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=5230)\u001b[0m 2024-11-12 18:22:21.112388: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=5230)\u001b[0m 2024-11-12 18:22:21.120382: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=5230)\u001b[0m 2024-11-12 18:22:21.138599: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=5230)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=5230)\u001b[0m 2024-11-12 18:22:22.307910: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=5230)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=5230)\u001b[0m \n","\u001b[36m(_train_tune pid=5230)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=5230)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=5230)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=5230)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=5230)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=5230)\u001b[0m 3 | blocks       | ModuleList       | 2.5 M  | train\n","\u001b[36m(_train_tune pid=5230)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=5230)\u001b[0m 2.5 M     Trainable params\n","\u001b[36m(_train_tune pid=5230)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=5230)\u001b[0m 2.5 M     Total params\n","\u001b[36m(_train_tune pid=5230)\u001b[0m 10.059    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=5230)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=5230)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/4 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.47]\n","Epoch 2:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.57]\n","Epoch 3:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.53]\n","Epoch 4:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.42]\n","Epoch 5:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.61]\n","Epoch 6:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.47]\n","Epoch 7:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.50]\n","Epoch 8:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.44]\n","Epoch 9:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.49]\n","Epoch 10:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.44]\n","Epoch 11:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.66]\n","Epoch 12:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.09, train_loss_epoch=-1.46]\n","Epoch 13:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.45]\n","Epoch 14:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.42]\n","Epoch 15:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.43]\n","Epoch 16:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.45]\n","Epoch 17:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.58]\n","Epoch 18:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.63]\n","Epoch 19:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.56]\n","Epoch 20:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.54]\n","Epoch 21:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.60]\n","Epoch 22:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.49]\n","Epoch 23:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.60]\n","Epoch 24:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.53]\n","Epoch 24: 100%|██████████| 4/4 [00:00\u003c00:00, 13.52it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.53]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5230)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5230)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.78it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5230)\u001b[0m \n","Epoch 25:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.58, valid_loss=-1.56]\n","Epoch 26:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.66, valid_loss=-1.56]\n","Epoch 27:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.58, valid_loss=-1.56]\n","Epoch 28:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.59, valid_loss=-1.56]\n","Epoch 29:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.849, train_loss_epoch=-1.42, valid_loss=-1.56]\n","Epoch 30:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.65, valid_loss=-1.56]\n","Epoch 31:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.61, valid_loss=-1.56]\n","Epoch 32:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.55, valid_loss=-1.56]\n","Epoch 33:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.60, valid_loss=-1.56]\n","Epoch 34:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.60, valid_loss=-1.56]\n","Epoch 35:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.60, valid_loss=-1.56]\n","Epoch 36:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.64, valid_loss=-1.56]\n","Epoch 37:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.76, valid_loss=-1.56]\n","Epoch 37: 100%|██████████| 4/4 [00:00\u003c00:00, 14.23it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.76, valid_loss=-1.56]\n","Epoch 38:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.72, valid_loss=-1.56]\n","Epoch 39:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.68, valid_loss=-1.56]\n","Epoch 40:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.50, valid_loss=-1.56]\n","Epoch 41:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.60, valid_loss=-1.56]\n","Epoch 42:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.992, train_loss_epoch=-1.51, valid_loss=-1.56]\n","Epoch 43:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.73, valid_loss=-1.56]\n","Epoch 44:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.65, valid_loss=-1.56]\n","Epoch 45:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.73, valid_loss=-1.56]\n","Epoch 46:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.66, valid_loss=-1.56]\n","Epoch 47:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.67, valid_loss=-1.56]\n","Epoch 48:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.83, valid_loss=-1.56]\n","Epoch 49:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.64, valid_loss=-1.56]\n","Epoch 49: 100%|██████████| 4/4 [00:00\u003c00:00, 14.26it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.64, valid_loss=-1.56]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5230)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5230)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.77it/s]\u001b[A\n","Epoch 50:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.74, valid_loss=-1.74]\n","Epoch 51:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.69, valid_loss=-1.74]\n","Epoch 52:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.78, valid_loss=-1.74]\n","Epoch 53:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.75, valid_loss=-1.74]\n","Epoch 54:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-1.82, valid_loss=-1.74]\n","Epoch 55:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.82, valid_loss=-1.74]\n","Epoch 56:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.76, valid_loss=-1.74]\n","Epoch 57:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.82, valid_loss=-1.74]\n","Epoch 58:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.72, valid_loss=-1.74]\n","Epoch 59:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-1.86, valid_loss=-1.74]\n","Epoch 60:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.77, valid_loss=-1.74]\n","Epoch 61:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.83, valid_loss=-1.74]\n","Epoch 62:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.73, valid_loss=-1.74]\n","Epoch 63:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.80, valid_loss=-1.74]\n","Epoch 64:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.81, valid_loss=-1.74]\n","Epoch 65:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.72, valid_loss=-1.74]\n","Epoch 66:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.78, valid_loss=-1.74]\n","Epoch 66: 100%|██████████| 4/4 [00:00\u003c00:00, 13.95it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.78, valid_loss=-1.74]\n","Epoch 67:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.73, valid_loss=-1.74]\n","Epoch 68:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.86, valid_loss=-1.74]\n","Epoch 69:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.80, valid_loss=-1.74]\n","Epoch 70:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.71, valid_loss=-1.74]\n","Epoch 71:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.74, valid_loss=-1.74]\n","Epoch 72:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.965, train_loss_epoch=-1.65, valid_loss=-1.74]\n","Epoch 73:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.82, valid_loss=-1.74]\n","Epoch 74:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.90, valid_loss=-1.74]\n","Epoch 74: 100%|██████████| 4/4 [00:00\u003c00:00, 13.81it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.90, valid_loss=-1.74]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5230)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5230)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.40it/s]\u001b[A\n","Epoch 75:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.71, valid_loss=-1.83]\n","Epoch 76:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.77, valid_loss=-1.83]\n","Epoch 77:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.82, valid_loss=-1.83]\n","Epoch 78:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.81, valid_loss=-1.83]\n","Epoch 79:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.87, valid_loss=-1.83]\n","Epoch 80:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.81, valid_loss=-1.83]\n","Epoch 81:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.89, valid_loss=-1.83]\n","Epoch 82:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.80, valid_loss=-1.83]\n","Epoch 83:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.76, valid_loss=-1.83]\n","Epoch 84:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.973, train_loss_epoch=-1.63, valid_loss=-1.83]\n","Epoch 85:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.81, valid_loss=-1.83]\n","Epoch 86:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.79, valid_loss=-1.83]\n","Epoch 87:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.86, valid_loss=-1.83]\n","Epoch 88:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-1.90, valid_loss=-1.83]\n","Epoch 89:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.77, valid_loss=-1.83]\n","Epoch 90:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.70, valid_loss=-1.83]\n","Epoch 91:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.72, valid_loss=-1.83]\n","Epoch 92:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-1.96, valid_loss=-1.83]\n","Epoch 93:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.80, valid_loss=-1.83]\n","Epoch 94:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.91, valid_loss=-1.83]\n","Epoch 95:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-1.91, valid_loss=-1.83]\n","Epoch 96:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.78, valid_loss=-1.83]\n","Epoch 97:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.88, valid_loss=-1.83]\n","Epoch 98:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-1.97, valid_loss=-1.83]\n","Epoch 99:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.72, valid_loss=-1.83]\n","Epoch 99: 100%|██████████| 4/4 [00:00\u003c00:00, 14.20it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.72, valid_loss=-1.83]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5230)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5230)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.54it/s]\u001b[A\n","Epoch 100:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.85, valid_loss=-1.87]\n","Epoch 101:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-1.95, valid_loss=-1.87]\n","Epoch 102:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.81, valid_loss=-1.87]\n","Epoch 103:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.82, valid_loss=-1.87]\n","Epoch 104:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.81, valid_loss=-1.87]\n","Epoch 105:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.95, valid_loss=-1.87]\n","Epoch 106:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.89, valid_loss=-1.87]\n","Epoch 107:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.79, valid_loss=-1.87]\n","Epoch 108:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.85, valid_loss=-1.87]\n","Epoch 109:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.86, valid_loss=-1.87]\n","Epoch 110:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.87, valid_loss=-1.87]\n","Epoch 111:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.81, valid_loss=-1.87]\n","Epoch 112:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.79, valid_loss=-1.87]\n","Epoch 113:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-1.98, valid_loss=-1.87]\n","Epoch 114:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-1.94, valid_loss=-1.87]\n","Epoch 115:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.90, valid_loss=-1.87]\n","Epoch 116:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.79, valid_loss=-1.87]\n","Epoch 117:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.79, valid_loss=-1.87]\n","Epoch 118:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.69, valid_loss=-1.87]\n","Epoch 119:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.94, valid_loss=-1.87]\n","Epoch 120:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.79, valid_loss=-1.87]\n","Epoch 120: 100%|██████████| 4/4 [00:00\u003c00:00, 13.85it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.79, valid_loss=-1.87]\n","Epoch 121:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.91, valid_loss=-1.87]\n","Epoch 122:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.79, valid_loss=-1.87]\n","Epoch 123:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.86, valid_loss=-1.87]\n","Epoch 124:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.84, valid_loss=-1.87]\n","Epoch 124: 100%|██████████| 4/4 [00:00\u003c00:00, 14.01it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.84, valid_loss=-1.87]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5230)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5230)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.75it/s]\u001b[A\n","Epoch 125:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.85, valid_loss=-1.88]\n","Epoch 126:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-1.88, valid_loss=-1.88]\n","Epoch 127:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.78, valid_loss=-1.88]\n","Epoch 128:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.89, valid_loss=-1.88]\n","Epoch 129:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.91, valid_loss=-1.88]\n","Epoch 130:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.85, valid_loss=-1.88]\n","Epoch 131:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.86, valid_loss=-1.88]\n","Epoch 132:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.84, valid_loss=-1.88]\n","Epoch 133:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.89, valid_loss=-1.88]\n","Epoch 134:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.84, valid_loss=-1.88]\n","Epoch 135:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.84, valid_loss=-1.88]\n","Epoch 136:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.52, train_loss_epoch=-2.05, valid_loss=-1.88]\n","Epoch 137:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.83, valid_loss=-1.88]\n","Epoch 138:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-1.95, valid_loss=-1.88]\n","Epoch 139:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.83, valid_loss=-1.88]\n","Epoch 140:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.78, valid_loss=-1.88]\n","Epoch 141:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.89, valid_loss=-1.88]\n","Epoch 142:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.80, valid_loss=-1.88]\n","Epoch 143:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.86, valid_loss=-1.88]\n","Epoch 144:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.90, valid_loss=-1.88]\n","Epoch 145:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.87, valid_loss=-1.88]\n","Epoch 146:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.91, valid_loss=-1.88]\n","Epoch 147:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.82, valid_loss=-1.88]\n","Epoch 148:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.88, valid_loss=-1.88]\n","Epoch 149:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.74, valid_loss=-1.88]\n","Epoch 149: 100%|██████████| 4/4 [00:00\u003c00:00, 13.84it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.74, valid_loss=-1.88]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5230)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5230)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.97it/s]\u001b[A\n","Epoch 150:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.87, valid_loss=-1.89]\n","Epoch 151:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.86, valid_loss=-1.89]\n","Epoch 152:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.79, valid_loss=-1.89]\n","Epoch 153:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.83, valid_loss=-1.89]\n","Epoch 154:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.56, train_loss_epoch=-2.10, valid_loss=-1.89]\n","Epoch 155:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.76, valid_loss=-1.89]\n","Epoch 156:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.81, valid_loss=-1.89]\n","Epoch 157:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.92, valid_loss=-1.89]\n","Epoch 158:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.79, valid_loss=-1.89]\n","Epoch 159:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.40, train_loss_epoch=-2.01, valid_loss=-1.89]\n","Epoch 160:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.79, valid_loss=-1.89]\n","Epoch 161:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.53, train_loss_epoch=-2.09, valid_loss=-1.89]\n","Epoch 162:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-1.96, valid_loss=-1.89]\n","Epoch 163:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.88, valid_loss=-1.89]\n","Epoch 164:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.91, valid_loss=-1.89]\n","Epoch 165:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.78, valid_loss=-1.89]\n","Epoch 166:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.90, valid_loss=-1.89]\n","Epoch 167:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.90, valid_loss=-1.89]\n","Epoch 168:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.84, valid_loss=-1.89]\n","Epoch 169:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.46, train_loss_epoch=-2.03, valid_loss=-1.89]\n","Epoch 170:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.79, valid_loss=-1.89]\n","Epoch 171:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.93, valid_loss=-1.89]\n","Epoch 172:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-2.00, valid_loss=-1.89]\n","Epoch 173:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.75, valid_loss=-1.89]\n","Epoch 174:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.93, valid_loss=-1.89]\n","Epoch 174: 100%|██████████| 4/4 [00:00\u003c00:00, 13.80it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.93, valid_loss=-1.89]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5230)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 16.09it/s]\u001b[A\n","Epoch 175:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.98, valid_loss=-1.90]\n","Epoch 176:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.84, valid_loss=-1.90]\n","Epoch 177:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.93, valid_loss=-1.90]\n","Epoch 177: 100%|██████████| 4/4 [00:00\u003c00:00, 14.12it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.93, valid_loss=-1.90]\n","Epoch 178:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.85, valid_loss=-1.90]\n","Epoch 179:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.86, valid_loss=-1.90]\n","Epoch 180:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.84, valid_loss=-1.90]\n","Epoch 181:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.99, valid_loss=-1.90]\n","Epoch 182:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-1.96, valid_loss=-1.90]\n","Epoch 183:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.93, valid_loss=-1.90]\n","Epoch 184:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.85, valid_loss=-1.90]\n","Epoch 185:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.76, valid_loss=-1.90]\n","Epoch 186:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-1.98, valid_loss=-1.90]\n","Epoch 187:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.84, valid_loss=-1.90]\n","Epoch 188:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.76, valid_loss=-1.90]\n","Epoch 188: 100%|██████████| 4/4 [00:00\u003c00:00, 14.04it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.76, valid_loss=-1.90]\n","Epoch 188:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.76, valid_loss=-1.90]        \n","Epoch 189:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.76, valid_loss=-1.90]\n","Epoch 190:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.83, valid_loss=-1.90]\n","Epoch 191:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.75, valid_loss=-1.90]\n","Epoch 192:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.92, valid_loss=-1.90]\n","Epoch 193:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.84, valid_loss=-1.90]\n","Epoch 194:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.92, valid_loss=-1.90]\n","Epoch 195:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.89, valid_loss=-1.90]\n","Epoch 196:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.55, train_loss_epoch=-2.06, valid_loss=-1.90]\n","Epoch 197:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.89, valid_loss=-1.90]\n","Epoch 198:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.86, valid_loss=-1.90]\n","Epoch 199:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.83, valid_loss=-1.90]\n","Epoch 199: 100%|██████████| 4/4 [00:00\u003c00:00, 13.65it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.83, valid_loss=-1.90]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5230)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5230)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.91it/s]\u001b[A\n","Epoch 200:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.94, valid_loss=-1.91]\n","Epoch 201:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.89, valid_loss=-1.91]\n","Epoch 202:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.87, valid_loss=-1.91]\n","Epoch 203:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.85, valid_loss=-1.91]\n","Epoch 204:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.82, valid_loss=-1.91]\n","Epoch 205:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-1.94, valid_loss=-1.91]\n","Epoch 206:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-1.98, valid_loss=-1.91]\n","Epoch 207:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-1.95, valid_loss=-1.91]\n","Epoch 208:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.84, valid_loss=-1.91]\n","Epoch 209:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.70, valid_loss=-1.91]\n","Epoch 209: 100%|██████████| 4/4 [00:00\u003c00:00, 13.99it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.87, valid_loss=-1.91]\n","Epoch 210:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.87, valid_loss=-1.91]\n","Epoch 211:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.83, valid_loss=-1.91]\n","Epoch 212:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.92, valid_loss=-1.91]\n","Epoch 213:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.84, valid_loss=-1.91]\n","Epoch 214:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.95, valid_loss=-1.91]\n","Epoch 215:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-1.95, valid_loss=-1.91]\n","Epoch 216:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.83, valid_loss=-1.91]\n","Epoch 217:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-1.95, valid_loss=-1.91]\n","Epoch 218:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.76, valid_loss=-1.91]\n","Epoch 219:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.84, valid_loss=-1.91]\n","Epoch 220:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.74, valid_loss=-1.91]\n","Epoch 221:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.93, valid_loss=-1.91]\n","Epoch 221:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.87, valid_loss=-1.91]        \n","Epoch 222:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.87, valid_loss=-1.91]\n","Epoch 223:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-2.01, valid_loss=-1.91]\n","Epoch 224:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.89, valid_loss=-1.91]\n","Epoch 224: 100%|██████████| 4/4 [00:00\u003c00:00, 13.79it/s, v_num=0, train_loss_step=-1.00, train_loss_epoch=-1.89, valid_loss=-1.91]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5230)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5230)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.79it/s]\u001b[A\n","Epoch 225:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.00, train_loss_epoch=-1.70, valid_loss=-1.91]\n","Epoch 226:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.85, valid_loss=-1.91]\n","Epoch 227:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.98, valid_loss=-1.91]\n","Epoch 228:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.93, valid_loss=-1.91]\n","Epoch 229:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.83, valid_loss=-1.91]\n","Epoch 230:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.83, valid_loss=-1.91]\n","Epoch 230: 100%|██████████| 4/4 [00:00\u003c00:00, 13.84it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.83, valid_loss=-1.91]\n","Epoch 231:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.79, valid_loss=-1.91]\n","Epoch 232:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.74, valid_loss=-1.91]\n","Epoch 233:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.88, valid_loss=-1.91]\n","Epoch 234:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.84, valid_loss=-1.91]\n","Epoch 235:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.93, valid_loss=-1.91]\n","Epoch 236:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.88, valid_loss=-1.91]\n","Epoch 237:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.76, valid_loss=-1.91]\n","Epoch 238:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.93, valid_loss=-1.91]\n","Epoch 239:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.73, valid_loss=-1.91]\n","Epoch 240:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.86, valid_loss=-1.91]\n","Epoch 241:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.03, valid_loss=-1.91]\n","Epoch 242:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.88, valid_loss=-1.91]\n","Epoch 243:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.87, valid_loss=-1.91]\n","Epoch 244:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.90, valid_loss=-1.91]\n","Epoch 245:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.83, valid_loss=-1.91]\n","Epoch 246:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.87, valid_loss=-1.91]\n","Epoch 247:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.84, valid_loss=-1.91]\n","Epoch 248:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.86, valid_loss=-1.91]\n","Epoch 249:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-1.98, valid_loss=-1.91]\n","Epoch 249: 100%|██████████| 4/4 [00:00\u003c00:00, 13.62it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.98, valid_loss=-1.91]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5230)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=5230)\u001b[0m `Trainer.fit` stopped: `max_steps=1000.0` reached.\n","2024-11-12 18:23:40,773\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (24, 12, 1), 'n_pool_kernel_size': (4, 4, 4), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=5230)\u001b[0m \n","\u001b[36m(_train_tune pid=5230)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.61it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5230)\u001b[0m \r                                                                      \u001b[A\rEpoch 249: 100%|██████████| 4/4 [00:00\u003c00:00,  6.16it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.98, valid_loss=-1.92]\rEpoch 249: 100%|██████████| 4/4 [00:00\u003c00:00,  6.15it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.92, valid_loss=-1.92]\rEpoch 249: 100%|██████████| 4/4 [00:00\u003c00:00,  6.14it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.92, valid_loss=-1.92]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=5658)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=5658)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=5658)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=5658)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=5658)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=5658)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=5658)\u001b[0m 2024-11-12 18:23:48.965681: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=5658)\u001b[0m 2024-11-12 18:23:48.986747: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=5658)\u001b[0m 2024-11-12 18:23:49.012334: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=5658)\u001b[0m 2024-11-12 18:23:49.020196: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=5658)\u001b[0m 2024-11-12 18:23:49.038357: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=5658)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=5658)\u001b[0m 2024-11-12 18:23:50.259987: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=5658)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=5658)\u001b[0m \n","\u001b[36m(_train_tune pid=5658)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=5658)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=5658)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=5658)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=5658)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=5658)\u001b[0m 3 | blocks       | ModuleList       | 2.7 M  | train\n","\u001b[36m(_train_tune pid=5658)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=5658)\u001b[0m 2.7 M     Trainable params\n","\u001b[36m(_train_tune pid=5658)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=5658)\u001b[0m 2.7 M     Total params\n","\u001b[36m(_train_tune pid=5658)\u001b[0m 10.622    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=5658)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=5658)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 18:23:53,299\tERROR tune_controller.py:1331 -- Trial task failed for trial _train_tune_8be5387b\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n","    result = ray.get(future)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n","    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(OutOfMemoryError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=5658, ip=172.28.0.12, actor_id=9e114da6d7de35f45539735001000000, repr=_train_tune)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n","    raise skipped from exception_cause(skipped)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n","    self._ret = self._target(*self._args, **self._kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in \u003clambda\u003e\n","    training_func=lambda: self._trainable_func(self.config),\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n","    output = fn()\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/util.py\", line 130, in inner\n","    return trainable(config, **fn_kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_auto.py\", line 214, in _train_tune\n","    _ = self._fit_model(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_auto.py\", line 362, in _fit_model\n","    model = model.fit(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_windows.py\", line 661, in fit\n","    return self._fit(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_model.py\", line 356, in _fit\n","    trainer.fit(model, datamodule=datamodule)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 538, in fit\n","    call._call_and_handle_interrupt(\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 47, in _call_and_handle_interrupt\n","    return trainer_fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 574, in _fit_impl\n","    self._run(model, ckpt_path=ckpt_path)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 981, in _run\n","    results = self._run_stage()\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1023, in _run_stage\n","    self._run_sanity_check()\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1052, in _run_sanity_check\n","    val_loop.run()\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py\", line 178, in _decorator\n","    return loop_run(self, *args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 135, in run\n","    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 396, in _evaluation_step\n","    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 319, in _call_strategy_hook\n","    output = fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py\", line 411, in validation_step\n","    return self.lightning_module.validation_step(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_windows.py\", line 534, in validation_step\n","    valid_loss_batch = self._compute_valid_loss(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_windows.py\", line 461, in _compute_valid_loss\n","    _, sample_mean, quants = self.loss.sample(distr_args=distr_args)\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/losses/pytorch.py\", line 2002, in sample\n","    samples = distr.sample(sample_shape=(num_samples,))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributions/distribution.py\", line 167, in sample\n","    return self.rsample(sample_shape)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributions/studentT.py\", line 90, in rsample\n","    Y = X * torch.rsqrt(Z / self.df)\n","torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.60 GiB. GPU 0 has a total capacity of 39.56 GiB of which 7.45 GiB is free. Process 27231 has 416.00 MiB memory in use. Process 71553 has 31.69 GiB memory in use. Of the allocated memory 30.14 GiB is allocated by PyTorch, and 1.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial _train_tune_8be5387b errored after 0 iterations at 2024-11-12 18:23:53. Total running time: 6min 11s\n","Error file: /tmp/ray/session_2024-11-12_18-17-38_352630_2326/artifacts/2024-11-12_18-17-41/_train_tune_2024-11-12_18-17-38/driver_artifacts/_train_tune_8be5387b_7_batch_size=256,h=20,input_size=100,learning_rate=0.0010,loss=ref_ph_de895953,max_steps=600.0000,n_freq_down_2024-11-12_18-22-20/error.txt\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=5766)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=5766)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=5766)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=5766)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=5766)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=5766)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=5766)\u001b[0m 2024-11-12 18:24:01.068718: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=5766)\u001b[0m 2024-11-12 18:24:01.088249: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=5766)\u001b[0m 2024-11-12 18:24:01.113679: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=5766)\u001b[0m 2024-11-12 18:24:01.121394: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=5766)\u001b[0m 2024-11-12 18:24:01.139559: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=5766)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=5766)\u001b[0m 2024-11-12 18:24:02.339308: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=5766)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=5766)\u001b[0m \n","\u001b[36m(_train_tune pid=5766)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=5766)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=5766)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=5766)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=5766)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=5766)\u001b[0m 3 | blocks       | ModuleList       | 2.5 M  | train\n","\u001b[36m(_train_tune pid=5766)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=5766)\u001b[0m 2.5 M     Trainable params\n","\u001b[36m(_train_tune pid=5766)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=5766)\u001b[0m 2.5 M     Total params\n","\u001b[36m(_train_tune pid=5766)\u001b[0m 10.077    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=5766)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=5766)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/13 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-0.98]\n","Epoch 2:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.59]\n","Epoch 3:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.64]\n","Epoch 4:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.62]\n","Epoch 5:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.65]\n","Epoch 6:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.64]\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.66]\n","\u001b[36m(_train_tune pid=5766)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5766)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  6.73it/s]\u001b[A\n","Epoch 7:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.66, valid_loss=-1.63]\n","Epoch 8:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.66, valid_loss=-1.63]\n","Epoch 9:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.66, valid_loss=-1.63]\n","Epoch 10:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.67, valid_loss=-1.63]\n","Epoch 11:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-1.76, valid_loss=-1.63]\n","Epoch 12:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.64, valid_loss=-1.63]\n","Epoch 13:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.69, valid_loss=-1.63]\n","Epoch 14:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.65, valid_loss=-1.63]\n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.69, valid_loss=-1.63]\n","\u001b[36m(_train_tune pid=5766)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5766)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  6.83it/s]\u001b[A\n","Epoch 15:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.69, valid_loss=-1.65]\n","Epoch 16:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-1.70, valid_loss=-1.65]\n","Epoch 17:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-1.76, valid_loss=-1.65]\n","Epoch 18:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.72, valid_loss=-1.65]\n","Epoch 19:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.67, valid_loss=-1.65]\n","Epoch 20:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.73, valid_loss=-1.65]\n","Epoch 21:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.68, valid_loss=-1.65]\n","Epoch 22:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.74, valid_loss=-1.65]\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.68, valid_loss=-1.65]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5766)\u001b[0m \n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5766)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  6.76it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5766)\u001b[0m \n","Epoch 23:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.68, valid_loss=-1.67]\n","Epoch 24:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-1.77, valid_loss=-1.67]\n","Epoch 25:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.69, valid_loss=-1.67]\n","Epoch 26:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.74, valid_loss=-1.67]\n","Epoch 27:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.69, valid_loss=-1.67]\n","Epoch 28:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.868, train_loss_epoch=-1.64, valid_loss=-1.67]\n","Epoch 29:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.71, valid_loss=-1.67]\n","Epoch 30:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.71, valid_loss=-1.67]\n","\u001b[36m(_train_tune pid=5766)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5766)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  6.94it/s]\u001b[A\n","Epoch 30:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.71, valid_loss=-1.68]\n","Epoch 31:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.72, valid_loss=-1.68]\n","Epoch 32:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.72, valid_loss=-1.68]\n","Epoch 33:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.72, valid_loss=-1.68]\n","Epoch 34:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.72, valid_loss=-1.68]\n","Epoch 35:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.70, valid_loss=-1.68]\n","Epoch 36:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-1.76, valid_loss=-1.68]\n","Epoch 37:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.71, valid_loss=-1.68]\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.70, valid_loss=-1.68]\n","\u001b[36m(_train_tune pid=5766)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5766)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  6.79it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5766)\u001b[0m \n","Epoch 38:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.70, valid_loss=-1.69]\n","Epoch 39:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.72, valid_loss=-1.69]\n","Epoch 40:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.70, valid_loss=-1.69]\n","Epoch 41:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.76, valid_loss=-1.69]\n","Epoch 42:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.73, valid_loss=-1.69]\n","Epoch 43:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.71, valid_loss=-1.69]\n","Epoch 43: 100%|██████████| 13/13 [00:00\u003c00:00, 38.02it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.71, valid_loss=-1.69]\n","Epoch 44:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.73, valid_loss=-1.69]\n","Epoch 45:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.68, valid_loss=-1.69]\n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.76, valid_loss=-1.69]\n","\u001b[36m(_train_tune pid=5766)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5766)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  6.84it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5766)\u001b[0m \n","Epoch 46:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.76, valid_loss=-1.69]\n","Epoch 47:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.71, valid_loss=-1.69]\n","Epoch 48:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.77, valid_loss=-1.69]\n","Epoch 49:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.74, valid_loss=-1.69]\n","Epoch 50:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.76, valid_loss=-1.69]\n","Epoch 51:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.74, valid_loss=-1.69]\n","Epoch 52:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-1.76, valid_loss=-1.69]\n","Epoch 53:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.74, valid_loss=-1.69]\n","\u001b[36m(_train_tune pid=5766)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5766)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  6.88it/s]\u001b[A\n","Epoch 53:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.74, valid_loss=-1.70]\n","Epoch 54:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.75, valid_loss=-1.70]\n","Epoch 55:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.74, valid_loss=-1.70]\n","Epoch 56:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.79, valid_loss=-1.70]\n","Epoch 57:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.74, valid_loss=-1.70]\n","Epoch 58:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.74, valid_loss=-1.70]\n","Epoch 59:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.71, valid_loss=-1.70]\n","Epoch 60:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.72, valid_loss=-1.70]\n","Epoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.71, valid_loss=-1.70]\n","\u001b[36m(_train_tune pid=5766)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5766)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  6.87it/s]\u001b[A\n","Epoch 61:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.71, valid_loss=-1.70]\n","Epoch 62:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.73, valid_loss=-1.70]\n","Epoch 63:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.74, valid_loss=-1.70]\n","Epoch 64:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.75, valid_loss=-1.70]\n","Epoch 65:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.78, valid_loss=-1.70]\n","Epoch 66:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.73, valid_loss=-1.70]\n","Epoch 67:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.69, valid_loss=-1.70]\n","Epoch 68:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.73, valid_loss=-1.70]\n","Epoch 69:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.817, train_loss_epoch=-1.66, valid_loss=-1.70]\n","\u001b[36m(_train_tune pid=5766)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=5766)\u001b[0m `Trainer.fit` stopped: `max_steps=900.0` reached.\n","2024-11-12 18:24:45,347\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (1, 1, 1), 'n_pool_kernel_size': (1, 1, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=5766)\u001b[0m \n","\u001b[36m(_train_tune pid=5766)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  6.83it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5766)\u001b[0m \r                                                                        \u001b[A\rEpoch 69:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.66, valid_loss=-1.71] \rEpoch 69:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.73, valid_loss=-1.71]\rEpoch 69:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.73, valid_loss=-1.71]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=6044)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=6044)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=6044)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=6044)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=6044)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=6044)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=6044)\u001b[0m 2024-11-12 18:24:52.922543: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=6044)\u001b[0m 2024-11-12 18:24:52.943370: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=6044)\u001b[0m 2024-11-12 18:24:52.968474: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=6044)\u001b[0m 2024-11-12 18:24:52.976190: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=6044)\u001b[0m 2024-11-12 18:24:52.994186: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=6044)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=6044)\u001b[0m 2024-11-12 18:24:54.133132: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=6044)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=6044)\u001b[0m \n","\u001b[36m(_train_tune pid=6044)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=6044)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=6044)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=6044)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=6044)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=6044)\u001b[0m 3 | blocks       | ModuleList       | 2.6 M  | train\n","\u001b[36m(_train_tune pid=6044)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=6044)\u001b[0m 2.6 M     Trainable params\n","\u001b[36m(_train_tune pid=6044)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=6044)\u001b[0m 2.6 M     Total params\n","\u001b[36m(_train_tune pid=6044)\u001b[0m 10.417    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=6044)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=6044)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/7 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.01, train_loss_epoch=0.568]\n","Epoch 2:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.38]\n","Epoch 3:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.56]\n","Epoch 4:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.63]\n","Epoch 5:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.69]\n","Epoch 6:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.71]\n","Epoch 7:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.80]\n","Epoch 8:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.79]\n","Epoch 9:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.85]\n","Epoch 10:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.79]\n","Epoch 11:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-1.92]\n","Epoch 12:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.79]\n","Epoch 13:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.84]\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.84]\n","\u001b[36m(_train_tune pid=6044)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6044)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.62it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6044)\u001b[0m \n","Epoch 14:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.84, valid_loss=-1.85]\n","Epoch 15:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.83, valid_loss=-1.85]\n","Epoch 16:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-1.94, valid_loss=-1.85]\n","Epoch 17:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.83, valid_loss=-1.85]\n","Epoch 18:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.81, valid_loss=-1.85]\n","Epoch 19:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-1.96, valid_loss=-1.85]\n","Epoch 20:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.89, valid_loss=-1.85]\n","Epoch 21:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.88, valid_loss=-1.85]\n","Epoch 22:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.93, valid_loss=-1.85]\n","Epoch 23:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.86, valid_loss=-1.85]\n","Epoch 24:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.87, valid_loss=-1.85]\n","Epoch 25:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.94, valid_loss=-1.85]\n","Epoch 26:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.94, valid_loss=-1.85]\n","Epoch 27:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.86, valid_loss=-1.85]\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.94, valid_loss=-1.85]\n","\u001b[36m(_train_tune pid=6044)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6044)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.63it/s]\u001b[A\n","Epoch 28:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.94, valid_loss=-1.89]\n","Epoch 29:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.82, valid_loss=-1.89]\n","Epoch 30:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-2.00, valid_loss=-1.89]\n","Epoch 31:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.89, valid_loss=-1.89]\n","Epoch 32:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.85, valid_loss=-1.89]\n","Epoch 33:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.89, valid_loss=-1.89]\n","Epoch 34:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.86, valid_loss=-1.89]\n","Epoch 35:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.98, valid_loss=-1.89]\n","Epoch 36:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.89, valid_loss=-1.89]\n","Epoch 37:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.46, train_loss_epoch=-2.02, valid_loss=-1.89]\n","Epoch 38:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.90, valid_loss=-1.89]\n","Epoch 39:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.88, valid_loss=-1.89]\n","Epoch 40:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-1.98, valid_loss=-1.89]\n","Epoch 41:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.57, train_loss_epoch=-2.06, valid_loss=-1.89]\n","Epoch 42:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-1.99, valid_loss=-1.89]\n","\u001b[36m(_train_tune pid=6044)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6044)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.63it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6044)\u001b[0m \n","Epoch 42:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.99, valid_loss=-1.94]\n","Epoch 43:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.95, valid_loss=-1.94]\n","Epoch 44:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.94, valid_loss=-1.94]\n","Epoch 45:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.95, valid_loss=-1.94]\n","Epoch 46:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.94, valid_loss=-1.94]\n","Epoch 47:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.00, valid_loss=-1.94]\n","Epoch 48:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.90, valid_loss=-1.94]\n","Epoch 49:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.99, valid_loss=-1.94]\n","Epoch 50:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.85, valid_loss=-1.94]\n","Epoch 51:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.95, valid_loss=-1.94]\n","Epoch 52:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.94, valid_loss=-1.94]\n","Epoch 53:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.88, valid_loss=-1.94]\n","Epoch 54:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.95, valid_loss=-1.94]\n","Epoch 55:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.01, valid_loss=-1.94]\n","Epoch 56:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-1.99, valid_loss=-1.94]\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.79, valid_loss=-1.94]\n","\u001b[36m(_train_tune pid=6044)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6044)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.67it/s]\u001b[A\n","Epoch 57:   0%|          | 0/7 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.79, valid_loss=-1.94]\n","Epoch 58:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.02, valid_loss=-1.94]\n","Epoch 59:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.94, valid_loss=-1.94]\n","Epoch 60:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.93, valid_loss=-1.94]\n","Epoch 61:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.91, valid_loss=-1.94]\n","Epoch 62:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.97, valid_loss=-1.94]\n","Epoch 63:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.97, valid_loss=-1.94]\n","Epoch 64:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.89, valid_loss=-1.94]\n","Epoch 65:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.93, valid_loss=-1.94]\n","Epoch 66:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.92, valid_loss=-1.94]\n","Epoch 66: 100%|██████████| 7/7 [00:00\u003c00:00, 24.81it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.92, valid_loss=-1.94]\n","Epoch 67:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.98, valid_loss=-1.94]\n","Epoch 68:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.92, valid_loss=-1.94]\n","Epoch 69:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.91, valid_loss=-1.94]\n","Epoch 70:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.91, valid_loss=-1.94]\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.85, valid_loss=-1.94]\n","\u001b[36m(_train_tune pid=6044)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6044)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.66it/s]\u001b[A\n","Epoch 71:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.85, valid_loss=-1.95]\n","Epoch 72:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.93, valid_loss=-1.95]\n","Epoch 73:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.02, valid_loss=-1.95]\n","Epoch 74:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-1.97, valid_loss=-1.95]\n","Epoch 75:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.98, valid_loss=-1.95]\n","Epoch 76:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.97, valid_loss=-1.95]\n","Epoch 77:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.94, valid_loss=-1.95]\n","Epoch 78:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-1.97, valid_loss=-1.95]\n","Epoch 79:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.97, valid_loss=-1.95]\n","Epoch 80:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.01, valid_loss=-1.95]\n","Epoch 81:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.93, valid_loss=-1.95]\n","Epoch 82:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.94, valid_loss=-1.95]\n","Epoch 83:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.03, train_loss_epoch=-1.80, valid_loss=-1.95]\n","Epoch 84:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.96, valid_loss=-1.95]\n","Epoch 85:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.99, valid_loss=-1.95]\n","\u001b[36m(_train_tune pid=6044)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6044)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.67it/s]\u001b[A\n","Epoch 86:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.98, valid_loss=-1.97]\n","Epoch 87:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.47, train_loss_epoch=-2.05, valid_loss=-1.97]\n","Epoch 88:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-2.01, valid_loss=-1.97]\n","Epoch 89:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.96, valid_loss=-1.97]\n","Epoch 90:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.04, valid_loss=-1.97]\n","Epoch 91:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.05, valid_loss=-1.97]\n","Epoch 92:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.95, valid_loss=-1.97]\n","Epoch 93:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.96, valid_loss=-1.97]\n","Epoch 94:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.01, valid_loss=-1.97]\n","Epoch 95:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.95, valid_loss=-1.97]\n","Epoch 96:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-2.02, valid_loss=-1.97]\n","Epoch 97:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.99, valid_loss=-1.97]\n","Epoch 98:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.02, valid_loss=-1.97]\n","Epoch 99:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-2.00, valid_loss=-1.97]\n","Epoch 99: 100%|██████████| 7/7 [00:00\u003c00:00, 24.22it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-2.00, valid_loss=-1.97]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=6044)\u001b[0m `Trainer.fit` stopped: `max_steps=700.0` reached.\n","2024-11-12 18:25:39,867\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (180, 60, 1), 'n_pool_kernel_size': (2, 2, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=6044)\u001b[0m \n","\u001b[36m(_train_tune pid=6044)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.67it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6044)\u001b[0m \r                                                                      \u001b[A\rEpoch 99: 100%|██████████| 7/7 [00:02\u003c00:00,  3.14it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-2.00, valid_loss=-1.98]\rEpoch 99: 100%|██████████| 7/7 [00:02\u003c00:00,  3.14it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-2.04, valid_loss=-1.98]\rEpoch 99: 100%|██████████| 7/7 [00:02\u003c00:00,  3.14it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-2.04, valid_loss=-1.98]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=6333)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=6333)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=6333)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=6333)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=6333)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=6333)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=6333)\u001b[0m 2024-11-12 18:25:48.030618: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=6333)\u001b[0m 2024-11-12 18:25:48.052259: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=6333)\u001b[0m 2024-11-12 18:25:48.079338: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=6333)\u001b[0m 2024-11-12 18:25:48.087900: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=6333)\u001b[0m 2024-11-12 18:25:48.107284: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=6333)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=6333)\u001b[0m 2024-11-12 18:25:49.298230: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=6333)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=6333)\u001b[0m \n","\u001b[36m(_train_tune pid=6333)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=6333)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=6333)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=6333)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=6333)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=6333)\u001b[0m 3 | blocks       | ModuleList       | 2.4 M  | train\n","\u001b[36m(_train_tune pid=6333)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=6333)\u001b[0m 2.4 M     Trainable params\n","\u001b[36m(_train_tune pid=6333)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=6333)\u001b[0m 2.4 M     Total params\n","\u001b[36m(_train_tune pid=6333)\u001b[0m 9.784     Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=6333)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=6333)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/4 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.500, train_loss_epoch=4.240]\n","Epoch 2:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-0.207]\n","Epoch 3:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.16]\n","Epoch 4:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.29]\n","Epoch 5:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.41]\n","Epoch 6:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.41]\n","Epoch 7:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.50]\n","Epoch 8:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.02, train_loss_epoch=-1.46]\n","Epoch 9:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.59]\n","Epoch 10:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.44]\n","Epoch 11:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.69]\n","Epoch 12:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.39]\n","Epoch 13:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.48]\n","Epoch 14:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.55]\n","Epoch 15:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.42]\n","Epoch 16:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.07, train_loss_epoch=-1.42]\n","Epoch 17:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.62]\n","Epoch 18:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-1.73]\n","Epoch 19:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.71]\n","Epoch 20:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.36, train_loss_epoch=-1.49]\n","Epoch 21:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.74]\n","Epoch 22:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.59]\n","Epoch 23:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.58]\n","Epoch 24:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.57]\n","Epoch 24: 100%|██████████| 4/4 [00:00\u003c00:00, 15.85it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-1.57]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6333)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6333)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 16.07it/s]\u001b[A\n","Epoch 25:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-1.75, valid_loss=-1.67]\n","Epoch 26:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.63, valid_loss=-1.67]\n","Epoch 27:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.62, valid_loss=-1.67]\n","Epoch 28:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.69, valid_loss=-1.67]\n","Epoch 29:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.423, train_loss_epoch=-1.35, valid_loss=-1.67]\n","Epoch 30:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.73, valid_loss=-1.67]\n","Epoch 31:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.66, valid_loss=-1.67]\n","Epoch 32:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.59, valid_loss=-1.67]\n","Epoch 33:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.66, valid_loss=-1.67]\n","Epoch 34:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.52, valid_loss=-1.67]\n","Epoch 35:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.68, valid_loss=-1.67]\n","Epoch 36:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.60, valid_loss=-1.67]\n","Epoch 37:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.60, valid_loss=-1.67]\n","Epoch 38:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.75, valid_loss=-1.67]\n","Epoch 39:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.69, valid_loss=-1.67]\n","Epoch 40:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.58, valid_loss=-1.67]\n","Epoch 41:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.52, valid_loss=-1.67]\n","Epoch 42:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.996, train_loss_epoch=-1.50, valid_loss=-1.67]\n","Epoch 43:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.72, valid_loss=-1.67]\n","Epoch 44:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.65, valid_loss=-1.67]\n","Epoch 45:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.66, valid_loss=-1.67]\n","Epoch 46:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.66, valid_loss=-1.67]\n","Epoch 47:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.68, valid_loss=-1.67]\n","Epoch 48:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.69, valid_loss=-1.67]\n","Epoch 49:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.66, valid_loss=-1.67]\n","Epoch 49: 100%|██████████| 4/4 [00:00\u003c00:00, 15.37it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.66, valid_loss=-1.67]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6333)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.76it/s]\u001b[A\n","Epoch 50:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.68, valid_loss=-1.65]\n","Epoch 51:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.57, valid_loss=-1.65]\n","Epoch 52:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.67, valid_loss=-1.65]\n","Epoch 52: 100%|██████████| 4/4 [00:00\u003c00:00, 15.52it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.68, valid_loss=-1.65]\n","Epoch 52:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.68, valid_loss=-1.65]        \n","Epoch 53:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.68, valid_loss=-1.65]\n","Epoch 54:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-1.75, valid_loss=-1.65]\n","Epoch 55:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.59, valid_loss=-1.65]\n","Epoch 56:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.63, valid_loss=-1.65]\n","Epoch 57:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.79, valid_loss=-1.65]\n","Epoch 58:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.62, valid_loss=-1.65]\n","Epoch 59:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.73, valid_loss=-1.65]\n","Epoch 60:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.68, valid_loss=-1.65]\n","Epoch 61:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.72, valid_loss=-1.65]\n","Epoch 62:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.60, valid_loss=-1.65]\n","Epoch 63:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.71, valid_loss=-1.65]\n","Epoch 64:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.59, valid_loss=-1.65]\n","Epoch 65:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.63, valid_loss=-1.65]\n","Epoch 66:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.59, valid_loss=-1.65]\n","Epoch 67:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.62, valid_loss=-1.65]\n","Epoch 68:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.70, valid_loss=-1.65]\n","Epoch 69:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.72, valid_loss=-1.65]\n","Epoch 70:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.62, valid_loss=-1.65]\n","Epoch 71:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.66, valid_loss=-1.65]\n","Epoch 72:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.662, train_loss_epoch=-1.42, valid_loss=-1.65]\n","Epoch 73:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.67, valid_loss=-1.65]\n","Epoch 74:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.65, valid_loss=-1.65]\n","Epoch 74: 100%|██████████| 4/4 [00:00\u003c00:00, 15.39it/s, v_num=0, train_loss_step=-1.08, train_loss_epoch=-1.65, valid_loss=-1.65]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6333)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6333)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.93it/s]\u001b[A\n","Epoch 75:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.08, train_loss_epoch=-1.54, valid_loss=-1.70]\n","Epoch 76:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.62, valid_loss=-1.70]\n","Epoch 77:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.68, valid_loss=-1.70]\n","Epoch 78:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.64, valid_loss=-1.70]\n","Epoch 79:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.76, valid_loss=-1.70]\n","Epoch 80:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.68, valid_loss=-1.70]\n","Epoch 81:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.72, valid_loss=-1.70]\n","Epoch 82:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.63, valid_loss=-1.70]\n","Epoch 83:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.68, valid_loss=-1.70]\n","Epoch 84:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.63, train_loss_epoch=-1.38, valid_loss=-1.70]\n","Epoch 85:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.66, valid_loss=-1.70]\n","Epoch 86:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.71, valid_loss=-1.70]\n","Epoch 87:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.60, valid_loss=-1.70]\n","Epoch 88:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-1.79, valid_loss=-1.70]\n","Epoch 89:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.58, valid_loss=-1.70]\n","Epoch 90:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.53, valid_loss=-1.70]\n","Epoch 91:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.60, valid_loss=-1.70]\n","Epoch 92:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.77, valid_loss=-1.70]\n","Epoch 93:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.64, valid_loss=-1.70]\n","Epoch 94:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.76, valid_loss=-1.70]\n","Epoch 95:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.77, valid_loss=-1.70]\n","Epoch 96:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.62, valid_loss=-1.70]\n","Epoch 97:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.69, valid_loss=-1.70]\n","Epoch 98:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.71, valid_loss=-1.70]\n","Epoch 99:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.59, valid_loss=-1.70]\n","Epoch 99: 100%|██████████| 4/4 [00:00\u003c00:00, 15.37it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.59, valid_loss=-1.70]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6333)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.88it/s]\u001b[A\n","Epoch 100:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.60, valid_loss=-1.73]\n","Epoch 101:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.76, valid_loss=-1.73]\n","Epoch 102:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.62, valid_loss=-1.73]\n","Epoch 103:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.67, valid_loss=-1.73]\n","Epoch 104:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.36, train_loss_epoch=-1.65, valid_loss=-1.73]\n","Epoch 104: 100%|██████████| 4/4 [00:00\u003c00:00, 15.56it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.65, valid_loss=-1.73]\n","Epoch 105:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.77, valid_loss=-1.73]\n","Epoch 106:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.69, valid_loss=-1.73]\n","Epoch 107:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.67, valid_loss=-1.73]\n","Epoch 108:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.65, valid_loss=-1.73]\n","Epoch 109:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.65, valid_loss=-1.73]\n","Epoch 110:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.77, valid_loss=-1.73]\n","Epoch 111:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.64, valid_loss=-1.73]\n","Epoch 112:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.56, valid_loss=-1.73]\n","Epoch 113:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-1.82, valid_loss=-1.73]\n","Epoch 114:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.81, valid_loss=-1.73]\n","Epoch 115:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.70, valid_loss=-1.73]\n","Epoch 116:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.64, valid_loss=-1.73]\n","Epoch 117:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.68, valid_loss=-1.73]\n","Epoch 118:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.942, train_loss_epoch=-1.47, valid_loss=-1.73]\n","Epoch 119:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-1.85, valid_loss=-1.73]\n","Epoch 120:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.57, valid_loss=-1.73]\n","Epoch 121:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.68, valid_loss=-1.73]\n","Epoch 122:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.69, valid_loss=-1.73]\n","Epoch 123:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.68, valid_loss=-1.73]\n","Epoch 124:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.58, valid_loss=-1.73]\n","Epoch 124: 100%|██████████| 4/4 [00:00\u003c00:00, 15.52it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.58, valid_loss=-1.73]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6333)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6333)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.95it/s]\u001b[A\n","Epoch 125:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.67, valid_loss=-1.71]\n","Epoch 126:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.73, valid_loss=-1.71]\n","Epoch 127:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.58, valid_loss=-1.71]\n","Epoch 128:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.72, valid_loss=-1.71]\n","Epoch 129:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.67, valid_loss=-1.71]\n","Epoch 130:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.65, valid_loss=-1.71]\n","Epoch 131:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.66, valid_loss=-1.71]\n","Epoch 132:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.64, valid_loss=-1.71]\n","Epoch 133:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.71, valid_loss=-1.71]\n","Epoch 134:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.65, valid_loss=-1.71]\n","Epoch 135:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.66, valid_loss=-1.71]\n","Epoch 136:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-1.85, valid_loss=-1.71]\n","Epoch 137:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.59, valid_loss=-1.71]\n","Epoch 138:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.73, valid_loss=-1.71]\n","Epoch 139:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.77, valid_loss=-1.71]\n","Epoch 140:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.68, valid_loss=-1.71]\n","Epoch 141:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.71, valid_loss=-1.71]\n","Epoch 142:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.02, train_loss_epoch=-1.53, valid_loss=-1.71]\n","Epoch 143:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.79, valid_loss=-1.71]\n","Epoch 144:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.74, valid_loss=-1.71]\n","Epoch 145:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.64, valid_loss=-1.71]\n","Epoch 146:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.70, valid_loss=-1.71]\n","Epoch 146: 100%|██████████| 4/4 [00:00\u003c00:00, 14.49it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.70, valid_loss=-1.71]\n","Epoch 147:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.69, valid_loss=-1.71]\n","Epoch 148:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.66, valid_loss=-1.71]\n","Epoch 149:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.06, train_loss_epoch=-1.51, valid_loss=-1.71]\n","Epoch 149: 100%|██████████| 4/4 [00:00\u003c00:00, 14.84it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.51, valid_loss=-1.71]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6333)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=6333)\u001b[0m `Trainer.fit` stopped: `max_steps=600.0` reached.\n","2024-11-12 18:26:33,337\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (60, 8, 1), 'n_pool_kernel_size': (8, 4, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=6333)\u001b[0m \n","\u001b[36m(_train_tune pid=6333)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 16.24it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6333)\u001b[0m \r                                                                      \u001b[A\rEpoch 149: 100%|██████████| 4/4 [00:00\u003c00:00,  6.76it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.51, valid_loss=-1.75]\rEpoch 149: 100%|██████████| 4/4 [00:00\u003c00:00,  6.75it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.62, valid_loss=-1.75]\rEpoch 149: 100%|██████████| 4/4 [00:00\u003c00:00,  6.73it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.62, valid_loss=-1.75]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=6613)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=6613)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=6613)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=6613)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=6613)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=6613)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=6613)\u001b[0m 2024-11-12 18:26:40.967833: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=6613)\u001b[0m 2024-11-12 18:26:40.988516: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=6613)\u001b[0m 2024-11-12 18:26:41.014005: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=6613)\u001b[0m 2024-11-12 18:26:41.021733: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=6613)\u001b[0m 2024-11-12 18:26:41.040044: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=6613)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=6613)\u001b[0m 2024-11-12 18:26:42.296916: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=6613)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=6613)\u001b[0m \n","\u001b[36m(_train_tune pid=6613)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=6613)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=6613)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=6613)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=6613)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=6613)\u001b[0m 3 | blocks       | ModuleList       | 2.7 M  | train\n","\u001b[36m(_train_tune pid=6613)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=6613)\u001b[0m 2.7 M     Trainable params\n","\u001b[36m(_train_tune pid=6613)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=6613)\u001b[0m 2.7 M     Total params\n","\u001b[36m(_train_tune pid=6613)\u001b[0m 10.629    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=6613)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=6613)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/13 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.917, train_loss_epoch=0.899]\n","Epoch 2:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.810, train_loss_epoch=0.862]\n","Epoch 3:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.783, train_loss_epoch=0.829]\n","Epoch 4:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.779, train_loss_epoch=0.790]\n","Epoch 5:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.704, train_loss_epoch=0.736]\n","Epoch 6:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.632, train_loss_epoch=0.667]\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.546, train_loss_epoch=0.579]\n","\u001b[36m(_train_tune pid=6613)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6613)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 49.01it/s]\u001b[A\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.428, train_loss_epoch=0.579, valid_loss=0.440]\n","Epoch 8:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.385, train_loss_epoch=0.449, valid_loss=0.440]\n","Epoch 9:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.106, train_loss_epoch=0.271, valid_loss=0.440]\n","Epoch 10:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.108, train_loss_epoch=0.0488, valid_loss=0.440]\n","Epoch 11:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.119, train_loss_epoch=-0.207, valid_loss=0.440]\n","Epoch 12:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.617, train_loss_epoch=-0.521, valid_loss=0.440]\n","Epoch 13:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.835, train_loss_epoch=-0.779, valid_loss=0.440]\n","Epoch 14:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.945, train_loss_epoch=-0.937, valid_loss=0.440]\n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.671, train_loss_epoch=-0.999, valid_loss=0.440]\n","\u001b[36m(_train_tune pid=6613)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6613)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 49.41it/s]\u001b[A\n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.07, train_loss_epoch=-0.999, valid_loss=-1.09] \n","Epoch 16:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.06, valid_loss=-1.09]\n","Epoch 17:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.08, valid_loss=-1.09]\n","Epoch 18:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.10, valid_loss=-1.09]\n","Epoch 19:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.11, valid_loss=-1.09]\n","Epoch 20:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.13, valid_loss=-1.09]\n","Epoch 21:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.11, valid_loss=-1.09]\n","Epoch 22:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.05, train_loss_epoch=-1.12, valid_loss=-1.09]\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.06, train_loss_epoch=-1.11, valid_loss=-1.09]\n","\u001b[36m(_train_tune pid=6613)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6613)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 54.13it/s]\u001b[A\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.11, valid_loss=-1.17]\n","Epoch 24:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.13, valid_loss=-1.17]\n","Epoch 25:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.13, valid_loss=-1.17]\n","Epoch 26:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.15, valid_loss=-1.17]\n","Epoch 27:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.14, valid_loss=-1.17]\n","Epoch 28:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.773, train_loss_epoch=-1.11, valid_loss=-1.17]\n","Epoch 29:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.14, valid_loss=-1.17]\n","Epoch 30:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.14, valid_loss=-1.17]\n","\u001b[36m(_train_tune pid=6613)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6613)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 50.91it/s]\u001b[A\n","Epoch 30:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.14, valid_loss=-1.18]\n","Epoch 31:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.06, train_loss_epoch=-1.14, valid_loss=-1.18]\n","Epoch 32:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.14, valid_loss=-1.18]\n","Epoch 33:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.15, valid_loss=-1.18]\n","Epoch 34:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.17, valid_loss=-1.18]\n","Epoch 35:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.02, train_loss_epoch=-1.14, valid_loss=-1.18]\n","Epoch 35: 100%|██████████| 13/13 [00:00\u003c00:00, 47.04it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.15, valid_loss=-1.18]\n","Epoch 36:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.15, valid_loss=-1.18]\n","Epoch 37:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.15, valid_loss=-1.18]\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.15, valid_loss=-1.18]\n","\u001b[36m(_train_tune pid=6613)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6613)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 47.76it/s]\u001b[A\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.15, valid_loss=-1.19]\n","Epoch 39:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.14, valid_loss=-1.19]\n","Epoch 40:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.943, train_loss_epoch=-1.13, valid_loss=-1.19]\n","Epoch 41:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.15, valid_loss=-1.19]\n","Epoch 42:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.16, valid_loss=-1.19]\n","Epoch 43:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.16, valid_loss=-1.19]\n","Epoch 44:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.14, valid_loss=-1.19]\n","Epoch 45:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.16, valid_loss=-1.19]\n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.17, valid_loss=-1.19]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6613)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 48.63it/s]\u001b[A\n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.17, valid_loss=-1.20]\n","Epoch 47:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.617, train_loss_epoch=-1.11, valid_loss=-1.20]\n","Epoch 48:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.17, valid_loss=-1.20]\n","Epoch 49:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.17, valid_loss=-1.20]\n","Epoch 49: 100%|██████████| 13/13 [00:00\u003c00:00, 47.65it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.17, valid_loss=-1.20]\n","Epoch 49: 100%|██████████| 13/13 [00:00\u003c00:00, 47.31it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.16, valid_loss=-1.20]\n","Epoch 50:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.16, valid_loss=-1.20]\n","Epoch 51:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.09, train_loss_epoch=-1.15, valid_loss=-1.20]\n","Epoch 52:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.16, valid_loss=-1.20]\n","Epoch 53:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.15, valid_loss=-1.20]\n","\u001b[36m(_train_tune pid=6613)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6613)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 49.25it/s]\u001b[A\n","Epoch 53: 100%|██████████| 13/13 [00:00\u003c00:00, 22.29it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.15, valid_loss=-1.20]\n","Epoch 54:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.15, valid_loss=-1.20]\n","Epoch 55:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.919, train_loss_epoch=-1.14, valid_loss=-1.20]\n","Epoch 56:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.16, valid_loss=-1.20]\n","Epoch 57:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.15, valid_loss=-1.20]\n","Epoch 58:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.16, valid_loss=-1.20]\n","Epoch 59:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.16, valid_loss=-1.20]\n","Epoch 60:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.16, valid_loss=-1.20]\n","Epoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.07, train_loss_epoch=-1.16, valid_loss=-1.20]\n","\u001b[36m(_train_tune pid=6613)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6613)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 48.42it/s]\u001b[A\n","Epoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.16, valid_loss=-1.20]\n","Epoch 62:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.16, valid_loss=-1.20]\n","Epoch 63:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.16, valid_loss=-1.20]\n","Epoch 64:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.16, valid_loss=-1.20]\n","Epoch 65:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.17, valid_loss=-1.20]\n","Epoch 66:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.17, valid_loss=-1.20]\n","Epoch 67:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.07, train_loss_epoch=-1.16, valid_loss=-1.20]\n","Epoch 68:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.08, train_loss_epoch=-1.15, valid_loss=-1.20]\n","Epoch 69:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.699, train_loss_epoch=-1.13, valid_loss=-1.20]\n","\u001b[36m(_train_tune pid=6613)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6613)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 47.57it/s]\u001b[A\n","Epoch 69:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.13, valid_loss=-1.20] \n","Epoch 70:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.16, valid_loss=-1.20]\n","Epoch 71:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.18, valid_loss=-1.20]\n","Epoch 72:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.16, valid_loss=-1.20]\n","Epoch 73:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.17, valid_loss=-1.20]\n","Epoch 74:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.16, valid_loss=-1.20]\n","Epoch 75:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.751, train_loss_epoch=-1.13, valid_loss=-1.20]\n","Epoch 76:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.15, valid_loss=-1.20]\n","\u001b[36m(_train_tune pid=6613)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6613)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 47.60it/s]\u001b[A\n","Epoch 77:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.17, valid_loss=-1.20]\n","Epoch 78:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.17, valid_loss=-1.20]\n","Epoch 79:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.00, train_loss_epoch=-1.16, valid_loss=-1.20]\n","Epoch 80:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.16, valid_loss=-1.20]\n","Epoch 81:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.16, valid_loss=-1.20]\n","Epoch 82:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.17, valid_loss=-1.20]\n","Epoch 83:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.941, train_loss_epoch=-1.14, valid_loss=-1.20]\n","Epoch 84:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.16, valid_loss=-1.20]\n","\u001b[36m(_train_tune pid=6613)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6613)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 50.49it/s]\u001b[A\n","Epoch 84:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.09, train_loss_epoch=-1.16, valid_loss=-1.20]\n","Epoch 85:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.16, valid_loss=-1.20]\n","Epoch 86:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.946, train_loss_epoch=-1.16, valid_loss=-1.20]\n","Epoch 87:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.16, valid_loss=-1.20]\n","Epoch 88:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.19, valid_loss=-1.20]\n","Epoch 89:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.16, valid_loss=-1.20]\n","Epoch 90:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.17, valid_loss=-1.20]\n","Epoch 90: 100%|██████████| 13/13 [00:00\u003c00:00, 43.70it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.18, valid_loss=-1.20]\n","Epoch 91:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.18, valid_loss=-1.20]\n","Epoch 92:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.15, valid_loss=-1.20]\n","\u001b[36m(_train_tune pid=6613)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6613)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 46.97it/s]\u001b[A\n","Epoch 92:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.15, valid_loss=-1.20]\n","Epoch 93:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.18, valid_loss=-1.20]\n","Epoch 94:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.15, valid_loss=-1.20]\n","Epoch 95:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.18, valid_loss=-1.20]\n","Epoch 96:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.07, train_loss_epoch=-1.15, valid_loss=-1.20]\n","Epoch 97:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.17, valid_loss=-1.20]\n","Epoch 98:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.17, valid_loss=-1.20]\n","Epoch 99:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.17, valid_loss=-1.20]\n","Epoch 99: 100%|██████████| 13/13 [00:00\u003c00:00, 48.54it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.17, valid_loss=-1.20]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6613)\u001b[0m \n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=6613)\u001b[0m `Trainer.fit` stopped: `max_steps=1300.0` reached.2024-11-12 18:27:16,800\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (60, 8, 1), 'n_pool_kernel_size': (2, 2, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=6613)\u001b[0m \n","\u001b[36m(_train_tune pid=6613)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 49.79it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6613)\u001b[0m \r                                                                        \u001b[A\rEpoch 99: 100%|██████████| 13/13 [00:00\u003c00:00, 23.65it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.17, valid_loss=-1.21]\rEpoch 99: 100%|██████████| 13/13 [00:00\u003c00:00, 23.48it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.17, valid_loss=-1.21]\rEpoch 99: 100%|██████████| 13/13 [00:00\u003c00:00, 23.43it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.17, valid_loss=-1.21]\n"]},{"name":"stderr","output_type":"stream","text":["\n","\u001b[36m(_train_tune pid=6856)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=6856)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=6856)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=6856)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=6856)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=6856)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=6856)\u001b[0m 2024-11-12 18:27:25.058289: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=6856)\u001b[0m 2024-11-12 18:27:25.077610: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=6856)\u001b[0m 2024-11-12 18:27:25.103007: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=6856)\u001b[0m 2024-11-12 18:27:25.110347: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=6856)\u001b[0m 2024-11-12 18:27:25.127549: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=6856)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=6856)\u001b[0m 2024-11-12 18:27:26.297856: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=6856)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=6856)\u001b[0m \n","\u001b[36m(_train_tune pid=6856)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=6856)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=6856)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=6856)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=6856)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=6856)\u001b[0m 3 | blocks       | ModuleList       | 2.6 M  | train\n","\u001b[36m(_train_tune pid=6856)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=6856)\u001b[0m 2.6 M     Trainable params\n","\u001b[36m(_train_tune pid=6856)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=6856)\u001b[0m 2.6 M     Total params\n","\u001b[36m(_train_tune pid=6856)\u001b[0m 10.465    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=6856)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=6856)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/13 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.958, train_loss_epoch=-0.0748]\n","Epoch 2:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-0.931]\n","Epoch 3:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.11]\n","Epoch 4:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.11]\n","Epoch 5:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.15]\n","Epoch 6:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.14]\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.17]\n","\u001b[36m(_train_tune pid=6856)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6856)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 49.24it/s]\u001b[A\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.03, train_loss_epoch=-1.17, valid_loss=-1.19]\n","Epoch 8:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.09, train_loss_epoch=-1.13, valid_loss=-1.19]\n","Epoch 9:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.984, train_loss_epoch=-1.08, valid_loss=-1.19]\n","Epoch 10:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.09, train_loss_epoch=-1.12, valid_loss=-1.19]\n","Epoch 11:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.14, valid_loss=-1.19]\n","Epoch 12:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.13, valid_loss=-1.19]\n","Epoch 13:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.15, valid_loss=-1.19]\n","Epoch 14:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.06, train_loss_epoch=-1.15, valid_loss=-1.19]\n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.832, train_loss_epoch=-1.14, valid_loss=-1.19]\n","\u001b[36m(_train_tune pid=6856)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6856)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 50.53it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6856)\u001b[0m \n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.14, valid_loss=-1.20] \n","Epoch 16:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.18, valid_loss=-1.20]\n","Epoch 17:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.15, valid_loss=-1.20]\n","Epoch 18:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.18, valid_loss=-1.20]\n","Epoch 19:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.16, valid_loss=-1.20]\n","Epoch 20:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.19, valid_loss=-1.20]\n","Epoch 21:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.19, valid_loss=-1.20]\n","Epoch 22:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.17, valid_loss=-1.20]\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.17, valid_loss=-1.20]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6856)\u001b[0m \n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6856)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 48.24it/s]\u001b[A\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.17, valid_loss=-1.22]\n","Epoch 24:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.19, valid_loss=-1.22]\n","Epoch 25:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.18, valid_loss=-1.22]\n","Epoch 26:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.18, valid_loss=-1.22]\n","Epoch 27:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.20, valid_loss=-1.22]\n","Epoch 28:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.877, train_loss_epoch=-1.16, valid_loss=-1.22]\n","Epoch 29:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.16, valid_loss=-1.22]\n","Epoch 30:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.18, valid_loss=-1.22]\n","\u001b[36m(_train_tune pid=6856)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6856)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 49.95it/s]\u001b[A\n","Epoch 30:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.18, valid_loss=-1.23]\n","Epoch 31:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.18, valid_loss=-1.23]\n","Epoch 32:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.17, valid_loss=-1.23]\n","Epoch 33:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 34:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 35:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 36:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 37:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.17, valid_loss=-1.23]\n","\u001b[36m(_train_tune pid=6856)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6856)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 48.70it/s]\u001b[A\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.17, valid_loss=-1.23]\n","Epoch 39:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 40:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.03, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 41:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 42:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 43:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 44:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 45:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.22, valid_loss=-1.23]\n","\u001b[36m(_train_tune pid=6856)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6856)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 50.49it/s]\u001b[A\n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 47:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.914, train_loss_epoch=-1.18, valid_loss=-1.24]\n","Epoch 48:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 49:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 50:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 51:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 52:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 53:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.18, valid_loss=-1.24]\n","\u001b[36m(_train_tune pid=6856)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6856)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 51.62it/s]\u001b[A\n","Epoch 53:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.06, train_loss_epoch=-1.18, valid_loss=-1.23]\n","Epoch 54:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 55:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.993, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 56:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 57:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 58:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.23, valid_loss=-1.23]\n","Epoch 59:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.07, train_loss_epoch=-1.17, valid_loss=-1.23]\n","Epoch 60:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.19, valid_loss=-1.23]\n","\u001b[36m(_train_tune pid=6856)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6856)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 49.27it/s]\u001b[A\n","Epoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 62:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 63:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 64:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.23, valid_loss=-1.23]\n","Epoch 65:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 66:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 66: 100%|██████████| 13/13 [00:00\u003c00:00, 45.25it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 67:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.08, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 68:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 69:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.842, train_loss_epoch=-1.20, valid_loss=-1.23]\n","\u001b[36m(_train_tune pid=6856)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6856)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 45.18it/s]\u001b[A\n","Epoch 69:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.09, train_loss_epoch=-1.20, valid_loss=-1.24] \n","Epoch 70:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 71:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.24, valid_loss=-1.24]\n","Epoch 72:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.19, valid_loss=-1.24]\n","Epoch 73:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 74:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 75:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.999, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 76:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.20, valid_loss=-1.24]\n","\u001b[36m(_train_tune pid=6856)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6856)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 46.88it/s]\u001b[A\n","Epoch 77:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.19, valid_loss=-1.24]\n","Epoch 78:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 79:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 80:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 81:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 82:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 83:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.18, valid_loss=-1.24]\n","Epoch 84:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.22, valid_loss=-1.24]\n","\u001b[36m(_train_tune pid=6856)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6856)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 45.77it/s]\u001b[A\n","Epoch 84:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.22, valid_loss=-1.25]\n","Epoch 85:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.20, valid_loss=-1.25]\n","Epoch 86:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.09, train_loss_epoch=-1.21, valid_loss=-1.25]\n","Epoch 87:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.23, valid_loss=-1.25]\n","Epoch 88:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.20, valid_loss=-1.25]\n","Epoch 89:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.21, valid_loss=-1.25]\n","Epoch 90:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.23, valid_loss=-1.25]\n","Epoch 91:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.24, valid_loss=-1.25]\n","Epoch 92:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.23, valid_loss=-1.25]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6856)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 45.58it/s]\u001b[A\n","Epoch 92:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.23, valid_loss=-1.25]\n","Epoch 93:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.24, valid_loss=-1.25]\n","Epoch 94:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.20, valid_loss=-1.25]\n","Epoch 95:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.22, valid_loss=-1.25]\n","Epoch 96:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.17, valid_loss=-1.25]\n","Epoch 97:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.24, valid_loss=-1.25]\n","Epoch 98:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.20, valid_loss=-1.25]\n","Epoch 99:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.23, valid_loss=-1.25]\n","Epoch 99: 100%|██████████| 13/13 [00:00\u003c00:00, 48.65it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.23, valid_loss=-1.25]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 18:28:00,497\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (60, 8, 1), 'n_pool_kernel_size': (16, 8, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=6856)\u001b[0m `Trainer.fit` stopped: `max_steps=1300.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=6856)\u001b[0m \n","\u001b[36m(_train_tune pid=6856)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 50.49it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6856)\u001b[0m \r                                                                        \u001b[A\rEpoch 99: 100%|██████████| 13/13 [00:00\u003c00:00, 23.85it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.23, valid_loss=-1.25]\rEpoch 99: 100%|██████████| 13/13 [00:00\u003c00:00, 23.80it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.23, valid_loss=-1.25]\rEpoch 99: 100%|██████████| 13/13 [00:00\u003c00:00, 23.75it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.23, valid_loss=-1.25]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=7100)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=7100)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=7100)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=7100)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=7100)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=7100)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=7100)\u001b[0m 2024-11-12 18:28:09.096017: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=7100)\u001b[0m 2024-11-12 18:28:09.116484: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=7100)\u001b[0m 2024-11-12 18:28:09.141643: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=7100)\u001b[0m 2024-11-12 18:28:09.149420: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=7100)\u001b[0m 2024-11-12 18:28:09.167287: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=7100)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=7100)\u001b[0m 2024-11-12 18:28:10.339854: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=7100)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=7100)\u001b[0m \n","\u001b[36m(_train_tune pid=7100)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=7100)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=7100)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=7100)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=7100)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=7100)\u001b[0m 3 | blocks       | ModuleList       | 2.5 M  | train\n","\u001b[36m(_train_tune pid=7100)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=7100)\u001b[0m 2.5 M     Trainable params\n","\u001b[36m(_train_tune pid=7100)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=7100)\u001b[0m 2.5 M     Total params\n","\u001b[36m(_train_tune pid=7100)\u001b[0m 9.956     Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=7100)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=7100)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 18:28:13,152\tERROR tune_controller.py:1331 -- Trial task failed for trial _train_tune_a6b09eb8\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n","    result = ray.get(future)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n","    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(OutOfMemoryError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=7100, ip=172.28.0.12, actor_id=722af359fadf5808f2772e2e01000000, repr=_train_tune)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n","    raise skipped from exception_cause(skipped)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n","    self._ret = self._target(*self._args, **self._kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in \u003clambda\u003e\n","    training_func=lambda: self._trainable_func(self.config),\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n","    output = fn()\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/util.py\", line 130, in inner\n","    return trainable(config, **fn_kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_auto.py\", line 214, in _train_tune\n","    _ = self._fit_model(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_auto.py\", line 362, in _fit_model\n","    model = model.fit(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_windows.py\", line 661, in fit\n","    return self._fit(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_model.py\", line 356, in _fit\n","    trainer.fit(model, datamodule=datamodule)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 538, in fit\n","    call._call_and_handle_interrupt(\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 47, in _call_and_handle_interrupt\n","    return trainer_fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 574, in _fit_impl\n","    self._run(model, ckpt_path=ckpt_path)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 981, in _run\n","    results = self._run_stage()\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1023, in _run_stage\n","    self._run_sanity_check()\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1052, in _run_sanity_check\n","    val_loop.run()\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py\", line 178, in _decorator\n","    return loop_run(self, *args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 135, in run\n","    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 396, in _evaluation_step\n","    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 319, in _call_strategy_hook\n","    output = fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py\", line 411, in validation_step\n","    return self.lightning_module.validation_step(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_windows.py\", line 534, in validation_step\n","    valid_loss_batch = self._compute_valid_loss(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_windows.py\", line 461, in _compute_valid_loss\n","    _, sample_mean, quants = self.loss.sample(distr_args=distr_args)\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/losses/pytorch.py\", line 2002, in sample\n","    samples = distr.sample(sample_shape=(num_samples,))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributions/distribution.py\", line 167, in sample\n","    return self.rsample(sample_shape)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributions/studentT.py\", line 90, in rsample\n","    Y = X * torch.rsqrt(Z / self.df)\n","torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.60 GiB. GPU 0 has a total capacity of 39.56 GiB of which 8.61 GiB is free. Process 27231 has 416.00 MiB memory in use. Process 98574 has 30.54 GiB memory in use. Of the allocated memory 29.56 GiB is allocated by PyTorch, and 498.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial _train_tune_a6b09eb8 errored after 0 iterations at 2024-11-12 18:28:13. Total running time: 10min 31s\n","Error file: /tmp/ray/session_2024-11-12_18-17-38_352630_2326/artifacts/2024-11-12_18-17-41/_train_tune_2024-11-12_18-17-38/driver_artifacts/_train_tune_a6b09eb8_13_batch_size=256,h=20,input_size=40,learning_rate=0.0010,loss=ref_ph_de895953,max_steps=1200.0000,n_freq_dow_2024-11-12_18-27-24/error.txt\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=7210)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=7210)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=7210)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=7210)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=7210)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=7210)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=7210)\u001b[0m 2024-11-12 18:28:21.060380: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=7210)\u001b[0m 2024-11-12 18:28:21.080115: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=7210)\u001b[0m 2024-11-12 18:28:21.106114: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=7210)\u001b[0m 2024-11-12 18:28:21.113836: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=7210)\u001b[0m 2024-11-12 18:28:21.131562: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=7210)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=7210)\u001b[0m 2024-11-12 18:28:22.285787: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=7210)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=7210)\u001b[0m \n","\u001b[36m(_train_tune pid=7210)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=7210)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=7210)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=7210)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=7210)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=7210)\u001b[0m 3 | blocks       | ModuleList       | 2.5 M  | train\n","\u001b[36m(_train_tune pid=7210)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=7210)\u001b[0m 2.5 M     Trainable params\n","\u001b[36m(_train_tune pid=7210)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=7210)\u001b[0m 2.5 M     Total params\n","\u001b[36m(_train_tune pid=7210)\u001b[0m 10.013    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=7210)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=7210)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/4 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.34]\n","Epoch 2:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.55]\n","Epoch 3:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.53]\n","Epoch 4:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.43]\n","Epoch 5:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.62]\n","Epoch 6:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.57]\n","Epoch 7:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.60]\n","Epoch 8:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.57]\n","Epoch 8: 100%|██████████| 4/4 [00:00\u003c00:00, 14.92it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.72]\n","Epoch 9:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.72]\n","Epoch 10:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.63]\n","Epoch 11:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-1.89]\n","Epoch 12:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.65]\n","Epoch 13:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.71]\n","Epoch 14:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.76]\n","Epoch 15:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.72]\n","Epoch 16:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.71]\n","Epoch 17:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.88]\n","Epoch 18:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-1.99]\n","Epoch 19:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-1.92]\n","Epoch 20:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.81]\n","Epoch 21:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-1.95]\n","Epoch 22:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.89]\n","Epoch 23:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.89]\n","Epoch 24:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.81]\n","Epoch 24: 100%|██████████| 4/4 [00:00\u003c00:00, 14.28it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-1.81]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7210)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7210)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 16.22it/s]\u001b[A\n","Epoch 25:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-2.00, valid_loss=-1.92]\n","Epoch 26:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-1.96, valid_loss=-1.92]\n","Epoch 27:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.92, valid_loss=-1.92]\n","Epoch 28:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.89, valid_loss=-1.92]\n","Epoch 29:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.70, valid_loss=-1.92]\n","Epoch 30:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.97, valid_loss=-1.92]\n","Epoch 31:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.92, valid_loss=-1.92]\n","Epoch 32:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.83, valid_loss=-1.92]\n","Epoch 33:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.90, valid_loss=-1.92]\n","Epoch 34:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.90, valid_loss=-1.92]\n","Epoch 35:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.91, valid_loss=-1.92]\n","Epoch 36:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.91, valid_loss=-1.92]\n","Epoch 37:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.00, valid_loss=-1.92]\n","Epoch 38:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-2.01, valid_loss=-1.92]\n","Epoch 39:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.96, valid_loss=-1.92]\n","Epoch 40:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.88, valid_loss=-1.92]\n","Epoch 41:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.83, valid_loss=-1.92]\n","Epoch 42:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.81, valid_loss=-1.92]\n","Epoch 43:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.01, valid_loss=-1.92]\n","Epoch 43: 100%|██████████| 4/4 [00:00\u003c00:00, 15.10it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-2.01, valid_loss=-1.92]\n","Epoch 44:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.95, valid_loss=-1.92]\n","Epoch 45:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.92, valid_loss=-1.92]\n","Epoch 46:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.95, valid_loss=-1.92]\n","Epoch 47:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.93, valid_loss=-1.92]\n","Epoch 48:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.07, valid_loss=-1.92]\n","Epoch 49:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.89, valid_loss=-1.92]\n","Epoch 49: 100%|██████████| 4/4 [00:00\u003c00:00, 14.92it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.89, valid_loss=-1.92]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7210)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7210)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 16.47it/s]\u001b[A\n","Epoch 50:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.97, valid_loss=-1.99]\n","Epoch 51:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.97, valid_loss=-1.99]\n","Epoch 52:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-1.98, valid_loss=-1.99]\n","Epoch 53:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.96, valid_loss=-1.99]\n","Epoch 54:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.54, train_loss_epoch=-2.11, valid_loss=-1.99]\n","Epoch 55:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.93, valid_loss=-1.99]\n","Epoch 56:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.93, valid_loss=-1.99]\n","Epoch 57:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-2.06, valid_loss=-1.99]\n","Epoch 58:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.87, valid_loss=-1.99]\n","Epoch 59:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.36, train_loss_epoch=-2.06, valid_loss=-1.99]\n","Epoch 60:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.93, valid_loss=-1.99]\n","Epoch 61:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.00, valid_loss=-1.99]\n","Epoch 62:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.91, valid_loss=-1.99]\n","Epoch 63:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.02, valid_loss=-1.99]\n","Epoch 64:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.99, valid_loss=-1.99]\n","Epoch 65:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.90, valid_loss=-1.99]\n","Epoch 66:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.96, valid_loss=-1.99]\n","Epoch 67:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.92, valid_loss=-1.99]\n","Epoch 68:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.05, valid_loss=-1.99]\n","Epoch 69:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.96, valid_loss=-1.99]\n","Epoch 70:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.91, valid_loss=-1.99]\n","Epoch 71:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.92, valid_loss=-1.99]\n","Epoch 72:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.05, train_loss_epoch=-1.73, valid_loss=-1.99]\n","Epoch 73:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.95, valid_loss=-1.99]\n","Epoch 73: 100%|██████████| 4/4 [00:00\u003c00:00, 14.75it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.01, valid_loss=-1.99]\n","Epoch 74:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.01, valid_loss=-1.99]\n","Epoch 74: 100%|██████████| 4/4 [00:00\u003c00:00, 15.04it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-2.01, valid_loss=-1.99]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7210)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7210)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 16.08it/s]\u001b[A\n","Epoch 75:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.88, valid_loss=-2.00]\n","Epoch 76:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.97, valid_loss=-2.00]\n","Epoch 77:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.98, valid_loss=-2.00]\n","Epoch 78:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.89, valid_loss=-2.00]\n","Epoch 79:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.03, valid_loss=-2.00]\n","Epoch 80:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.02, valid_loss=-2.00]\n","Epoch 81:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.04, valid_loss=-2.00]\n","Epoch 82:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.94, valid_loss=-2.00]\n","Epoch 83:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.97, valid_loss=-2.00]\n","Epoch 84:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.77, valid_loss=-2.00]\n","Epoch 85:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.95, valid_loss=-2.00]\n","Epoch 86:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.96, valid_loss=-2.00]\n","Epoch 87:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.96, valid_loss=-2.00]\n","Epoch 88:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.45, train_loss_epoch=-2.09, valid_loss=-2.00]\n","Epoch 89:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.89, valid_loss=-2.00]\n","Epoch 90:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.90, valid_loss=-2.00]\n","Epoch 91:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.94, valid_loss=-2.00]\n","Epoch 92:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.07, valid_loss=-2.00]\n","Epoch 93:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.95, valid_loss=-2.00]\n","Epoch 94:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-2.11, valid_loss=-2.00]\n","Epoch 95:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-2.08, valid_loss=-2.00]\n","Epoch 96:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.99, valid_loss=-2.00]\n","Epoch 97:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.00, valid_loss=-2.00]\n","Epoch 98:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-2.08, valid_loss=-2.00]\n","Epoch 99:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.91, valid_loss=-2.00]\n","Epoch 99: 100%|██████████| 4/4 [00:00\u003c00:00, 14.99it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.91, valid_loss=-2.00]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7210)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 16.02it/s]\u001b[A\n","Epoch 100:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.98, valid_loss=-2.01]\n","Epoch 101:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-2.07, valid_loss=-2.01]\n","Epoch 102:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.95, valid_loss=-2.01]\n","Epoch 103:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.02, valid_loss=-2.01]\n","Epoch 104:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.99, valid_loss=-2.01]\n","Epoch 105:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-2.06, valid_loss=-2.01]\n","Epoch 106:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.05, valid_loss=-2.01]\n","Epoch 107:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.97, valid_loss=-2.01]\n","Epoch 108:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.97, valid_loss=-2.01]\n","Epoch 109:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.96, valid_loss=-2.01]\n","Epoch 110:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.05, valid_loss=-2.01]\n","Epoch 111:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.99, valid_loss=-2.01]\n","Epoch 112:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.94, valid_loss=-2.01]\n","Epoch 113:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-2.11, valid_loss=-2.01]\n","Epoch 114:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.08, valid_loss=-2.01]\n","Epoch 115:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.04, valid_loss=-2.01]\n","Epoch 116:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.97, valid_loss=-2.01]\n","Epoch 117:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.93, valid_loss=-2.01]\n","Epoch 118:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.90, valid_loss=-2.01]\n","Epoch 118: 100%|██████████| 4/4 [00:00\u003c00:00, 14.94it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.90, valid_loss=-2.01]\n","Epoch 119:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.47, train_loss_epoch=-2.11, valid_loss=-2.01]\n","Epoch 120:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.94, valid_loss=-2.01]\n","Epoch 121:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.04, valid_loss=-2.01]\n","Epoch 122:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.02, valid_loss=-2.01]\n","Epoch 123:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.02, valid_loss=-2.01]\n","Epoch 124:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.00, valid_loss=-2.01]\n","Epoch 124: 100%|██████████| 4/4 [00:00\u003c00:00, 14.97it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.00, valid_loss=-2.01]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7210)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7210)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.99it/s]\u001b[A\n","Epoch 124: 100%|██████████| 4/4 [00:00\u003c00:00,  6.73it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.02, valid_loss=-2.01]\n","Epoch 125:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.02, valid_loss=-2.01]\n","Epoch 126:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.05, valid_loss=-2.01]\n","Epoch 127:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.99, valid_loss=-2.01]\n","Epoch 128:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.02, valid_loss=-2.01]\n","Epoch 129:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.01, valid_loss=-2.01]\n","Epoch 130:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.00, valid_loss=-2.01]\n","Epoch 131:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.00, valid_loss=-2.01]\n","Epoch 132:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.97, valid_loss=-2.01]\n","Epoch 133:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.02, valid_loss=-2.01]\n","Epoch 134:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.99, valid_loss=-2.01]\n","Epoch 135:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.98, valid_loss=-2.01]\n","Epoch 136:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.54, train_loss_epoch=-2.15, valid_loss=-2.01]\n","Epoch 137:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.95, valid_loss=-2.01]\n","Epoch 138:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.46, train_loss_epoch=-2.12, valid_loss=-2.01]\n","Epoch 139:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.05, valid_loss=-2.01]\n","Epoch 140:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.92, valid_loss=-2.01]\n","Epoch 141:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.03, valid_loss=-2.01]\n","Epoch 142:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.89, valid_loss=-2.01]\n","Epoch 143:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-2.06, valid_loss=-2.01]\n","Epoch 144:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.05, valid_loss=-2.01]\n","Epoch 145:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.97, valid_loss=-2.01]\n","Epoch 146:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.99, valid_loss=-2.01]\n","Epoch 147:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.97, valid_loss=-2.01]\n","Epoch 148:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.07, valid_loss=-2.01]\n","Epoch 149:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.89, valid_loss=-2.01]\n","Epoch 149: 100%|██████████| 4/4 [00:00\u003c00:00, 14.97it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.89, valid_loss=-2.01]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7210)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7210)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.92it/s]\u001b[A\n","Epoch 150:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.01, valid_loss=-2.02]\n","Epoch 151:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.00, valid_loss=-2.02]\n","Epoch 152:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.92, valid_loss=-2.02]\n","Epoch 153:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.97, valid_loss=-2.02]\n","Epoch 154:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.53, train_loss_epoch=-2.14, valid_loss=-2.02]\n","Epoch 155:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.90, valid_loss=-2.02]\n","Epoch 156:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.93, valid_loss=-2.02]\n","Epoch 157:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.02, valid_loss=-2.02]\n","Epoch 158:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.91, valid_loss=-2.02]\n","Epoch 159:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.41, train_loss_epoch=-2.08, valid_loss=-2.02]\n","Epoch 160:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.94, valid_loss=-2.02]\n","Epoch 161:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.62, train_loss_epoch=-2.16, valid_loss=-2.02]\n","Epoch 162:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.45, train_loss_epoch=-2.12, valid_loss=-2.02]\n","Epoch 163:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-2.09, valid_loss=-2.02]\n","Epoch 164:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.07, valid_loss=-2.02]\n","Epoch 165:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.91, valid_loss=-2.02]\n","Epoch 166:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.02, valid_loss=-2.02]\n","Epoch 167:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.06, valid_loss=-2.02]\n","Epoch 168:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.01, valid_loss=-2.02]\n","Epoch 169:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.56, train_loss_epoch=-2.16, valid_loss=-2.02]\n","Epoch 170:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.95, valid_loss=-2.02]\n","Epoch 171:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.08, valid_loss=-2.02]\n","Epoch 172:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-2.11, valid_loss=-2.02]\n","Epoch 173:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.94, valid_loss=-2.02]\n","Epoch 174:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-2.11, valid_loss=-2.02]\n","Epoch 174: 100%|██████████| 4/4 [00:00\u003c00:00, 14.66it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-2.11, valid_loss=-2.02]\n","\u001b[36m(_train_tune pid=7210)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7210)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7210)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 16.25it/s]\u001b[A\n","Epoch 175:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-2.09, valid_loss=-2.02]\n","Epoch 176:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.05, valid_loss=-2.02]\n","Epoch 177:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.02, valid_loss=-2.02]\n","Epoch 177: 100%|██████████| 4/4 [00:00\u003c00:00, 15.00it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.95, valid_loss=-2.02]\n","Epoch 178:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.95, valid_loss=-2.02]\n","Epoch 179:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.99, valid_loss=-2.02]\n","Epoch 180:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.99, valid_loss=-2.02]\n","Epoch 181:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.07, valid_loss=-2.02]\n","Epoch 182:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-2.14, valid_loss=-2.02]\n","Epoch 183:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.01, valid_loss=-2.02]\n","Epoch 184:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.99, valid_loss=-2.02]\n","Epoch 185:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.94, valid_loss=-2.02]\n","Epoch 186:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-2.06, valid_loss=-2.02]\n","Epoch 187:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.95, valid_loss=-2.02]\n","Epoch 188:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.97, valid_loss=-2.02]\n","Epoch 189:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.87, valid_loss=-2.02]\n","Epoch 190:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-2.00, valid_loss=-2.02]\n","Epoch 191:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.83, valid_loss=-2.02]\n","Epoch 192:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.05, valid_loss=-2.02]\n","Epoch 193:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-2.00, valid_loss=-2.02]\n","Epoch 193: 100%|██████████| 4/4 [00:00\u003c00:00, 15.00it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-2.00, valid_loss=-2.02]\n","Epoch 194:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.02, valid_loss=-2.02]\n","Epoch 195:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.06, valid_loss=-2.02]\n","Epoch 196:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.63, train_loss_epoch=-2.18, valid_loss=-2.02]\n","Epoch 197:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.01, valid_loss=-2.02]\n","Epoch 198:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.00, valid_loss=-2.02]\n","Epoch 199:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.93, valid_loss=-2.02]\n","Epoch 199: 100%|██████████| 4/4 [00:00\u003c00:00, 14.61it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.93, valid_loss=-2.02]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7210)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7210)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 16.11it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7210)\u001b[0m \n","Epoch 200:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.99, valid_loss=-2.01]\n","Epoch 201:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.03, valid_loss=-2.01]\n","Epoch 202:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.00, valid_loss=-2.01]\n","Epoch 203:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.99, valid_loss=-2.01]\n","Epoch 204:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.94, valid_loss=-2.01]\n","Epoch 205:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-2.12, valid_loss=-2.01]\n","Epoch 206:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.44, train_loss_epoch=-2.13, valid_loss=-2.01]\n","Epoch 207:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-2.11, valid_loss=-2.01]\n","Epoch 208:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.97, valid_loss=-2.01]\n","Epoch 209:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.82, valid_loss=-2.01]\n","Epoch 210:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.02, valid_loss=-2.01]\n","Epoch 211:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.01, valid_loss=-2.01]\n","Epoch 212:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.97, valid_loss=-2.01]\n","Epoch 213:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.95, valid_loss=-2.01]\n","Epoch 214:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.07, valid_loss=-2.01]\n","Epoch 215:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.10, valid_loss=-2.01]\n","Epoch 216:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.00, valid_loss=-2.01]\n","Epoch 217:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.44, train_loss_epoch=-2.11, valid_loss=-2.01]\n","Epoch 218:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.88, valid_loss=-2.01]\n","Epoch 219:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-2.01, valid_loss=-2.01]\n","Epoch 220:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.88, valid_loss=-2.01]\n","Epoch 221:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.03, valid_loss=-2.01]\n","Epoch 222:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.96, valid_loss=-2.01]\n","Epoch 223:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.47, train_loss_epoch=-2.12, valid_loss=-2.01]\n","Epoch 224:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.00, valid_loss=-2.01]\n","Epoch 224: 100%|██████████| 4/4 [00:00\u003c00:00, 15.14it/s, v_num=0, train_loss_step=-1.05, train_loss_epoch=-2.00, valid_loss=-2.01]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7210)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7210)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.76it/s]\u001b[A\n","Epoch 225:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.05, train_loss_epoch=-1.77, valid_loss=-2.02]\n","Epoch 226:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.98, valid_loss=-2.02]\n","Epoch 227:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-2.11, valid_loss=-2.02]\n","Epoch 228:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.00, valid_loss=-2.02]\n","Epoch 229:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.92, valid_loss=-2.02]\n","Epoch 230:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.96, valid_loss=-2.02]\n","Epoch 231:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.87, valid_loss=-2.02]\n","Epoch 232:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.96, valid_loss=-2.02]\n","Epoch 233:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.05, valid_loss=-2.02]\n","Epoch 234:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.98, valid_loss=-2.02]\n","Epoch 235:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-2.07, valid_loss=-2.02]\n","Epoch 236:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.06, valid_loss=-2.02]\n","Epoch 237:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.81, valid_loss=-2.02]\n","Epoch 238:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.02, valid_loss=-2.02]\n","Epoch 239:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.87, valid_loss=-2.02]\n","Epoch 240:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.95, valid_loss=-2.02]\n","Epoch 241:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.06, valid_loss=-2.02]\n","Epoch 242:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.03, valid_loss=-2.02]\n","Epoch 243:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.96, valid_loss=-2.02]\n","Epoch 244:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.06, valid_loss=-2.02]\n","Epoch 245:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.93, valid_loss=-2.02]\n","Epoch 246:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.01, valid_loss=-2.02]\n","Epoch 247:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.97, valid_loss=-2.02]\n","Epoch 248:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.01, valid_loss=-2.02]\n","Epoch 249:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.45, train_loss_epoch=-2.13, valid_loss=-2.02]\n","Epoch 249: 100%|██████████| 4/4 [00:00\u003c00:00, 14.48it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.13, valid_loss=-2.02]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7210)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7210)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 16.17it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7210)\u001b[0m \n","Epoch 250:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.08, valid_loss=-2.03]\n","Epoch 251:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.91, valid_loss=-2.03]\n","Epoch 252:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.88, valid_loss=-2.03]\n","Epoch 253:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.98, valid_loss=-2.03]\n","Epoch 254:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.90, valid_loss=-2.03]\n","Epoch 255:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.99, valid_loss=-2.03]\n","Epoch 256:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-2.07, valid_loss=-2.03]\n","Epoch 257:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.06, valid_loss=-2.03]\n","Epoch 258:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.50, train_loss_epoch=-2.15, valid_loss=-2.03]\n","Epoch 259:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.85, valid_loss=-2.03]\n","Epoch 260:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.04, valid_loss=-2.03]\n","Epoch 261:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.98, valid_loss=-2.03]\n","Epoch 262:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-2.12, valid_loss=-2.03]\n","Epoch 263:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.82, valid_loss=-2.03]\n","Epoch 264:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.03, valid_loss=-2.03]\n","Epoch 265:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.53, train_loss_epoch=-2.14, valid_loss=-2.03]\n","Epoch 266:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-2.10, valid_loss=-2.03]\n","Epoch 267:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.94, valid_loss=-2.03]\n","Epoch 268:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.92, valid_loss=-2.03]\n","Epoch 269:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-2.11, valid_loss=-2.03]\n","Epoch 270:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.01, valid_loss=-2.03]\n","Epoch 271:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.97, valid_loss=-2.03]\n","Epoch 272:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.95, valid_loss=-2.03]\n","Epoch 273:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.93, valid_loss=-2.03]\n","Epoch 274:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.92, valid_loss=-2.03]\n","Epoch 274: 100%|██████████| 4/4 [00:00\u003c00:00, 14.78it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.92, valid_loss=-2.03]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7210)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7210)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 16.18it/s]\u001b[A\n","Epoch 275:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.86, valid_loss=-2.03]\n","Epoch 276:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.01, valid_loss=-2.03]\n","Epoch 277:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.97, valid_loss=-2.03]\n","Epoch 278:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.01, valid_loss=-2.03]\n","Epoch 279:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.94, valid_loss=-2.03]\n","Epoch 280:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.07, valid_loss=-2.03]\n","Epoch 281:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.03, valid_loss=-2.03]\n","Epoch 282:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.05, valid_loss=-2.03]\n","Epoch 283:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.04, valid_loss=-2.03]\n","Epoch 284:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.05, valid_loss=-2.03]\n","Epoch 285:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.03, valid_loss=-2.03]\n","Epoch 286:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.00, valid_loss=-2.03]\n","Epoch 287:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.11, valid_loss=-2.03]\n","Epoch 288:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.01, valid_loss=-2.03]\n","Epoch 289:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.06, valid_loss=-2.03]\n","Epoch 290:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.03, valid_loss=-2.03]\n","Epoch 291:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.02, valid_loss=-2.03]\n","Epoch 292:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.96, valid_loss=-2.03]\n","Epoch 293:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.02, valid_loss=-2.03]\n","Epoch 294:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-2.09, valid_loss=-2.03]\n","Epoch 295:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.42, train_loss_epoch=-2.13, valid_loss=-2.03]\n","Epoch 296:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.05, valid_loss=-2.03]\n","Epoch 297:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.02, valid_loss=-2.03]\n","Epoch 297: 100%|██████████| 4/4 [00:00\u003c00:00, 13.90it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.02, valid_loss=-2.03]\n","Epoch 298:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-2.01, valid_loss=-2.03]\n","Epoch 299:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.01, valid_loss=-2.03]\n","Epoch 299: 100%|██████████| 4/4 [00:00\u003c00:00, 15.06it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.01, valid_loss=-2.03]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 18:29:50,034\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (60, 8, 1), 'n_pool_kernel_size': (2, 2, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=7210)\u001b[0m `Trainer.fit` stopped: `max_steps=1200.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=7210)\u001b[0m \n","\u001b[36m(_train_tune pid=7210)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.93it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7210)\u001b[0m \r                                                                      \u001b[A\rEpoch 299: 100%|██████████| 4/4 [00:00\u003c00:00,  6.77it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.01, valid_loss=-2.03]\rEpoch 299: 100%|██████████| 4/4 [00:00\u003c00:00,  6.76it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.05, valid_loss=-2.03]\rEpoch 299: 100%|██████████| 4/4 [00:00\u003c00:00,  6.74it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.05, valid_loss=-2.03]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=7674)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=7674)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=7674)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=7674)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=7674)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=7674)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=7674)\u001b[0m 2024-11-12 18:29:57.928144: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=7674)\u001b[0m 2024-11-12 18:29:57.949267: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=7674)\u001b[0m 2024-11-12 18:29:57.975356: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=7674)\u001b[0m 2024-11-12 18:29:57.983481: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=7674)\u001b[0m 2024-11-12 18:29:58.002326: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=7674)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=7674)\u001b[0m 2024-11-12 18:29:59.160446: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=7674)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=7674)\u001b[0m \n","\u001b[36m(_train_tune pid=7674)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=7674)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=7674)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=7674)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=7674)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=7674)\u001b[0m 3 | blocks       | ModuleList       | 2.6 M  | train\n","\u001b[36m(_train_tune pid=7674)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=7674)\u001b[0m 2.6 M     Trainable params\n","\u001b[36m(_train_tune pid=7674)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=7674)\u001b[0m 2.6 M     Total params\n","\u001b[36m(_train_tune pid=7674)\u001b[0m 10.459    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=7674)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=7674)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/7 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=0.340]\n","Epoch 2:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-1.63]\n","Epoch 3:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.70]\n","Epoch 4:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.67]\n","Epoch 5:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.81]\n","Epoch 6:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.76]\n","Epoch 7:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.83]\n","Epoch 8:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.75]\n","Epoch 9:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-1.91]\n","Epoch 10:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.75]\n","Epoch 11:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.42, train_loss_epoch=-1.92]\n","Epoch 12:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.82]\n","Epoch 13:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.83]\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.83]\n","\u001b[36m(_train_tune pid=7674)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7674)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.65it/s]\u001b[A\n","Epoch 14:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.83, valid_loss=-1.89]\n","Epoch 15:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.85, valid_loss=-1.89]\n","Epoch 16:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-1.93, valid_loss=-1.89]\n","Epoch 17:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.87, valid_loss=-1.89]\n","Epoch 18:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.91, valid_loss=-1.89]\n","Epoch 19:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.41, train_loss_epoch=-1.97, valid_loss=-1.89]\n","Epoch 20:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.87, valid_loss=-1.89]\n","Epoch 21:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.91, valid_loss=-1.89]\n","Epoch 22:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.93, valid_loss=-1.89]\n","Epoch 23:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.88, valid_loss=-1.89]\n","Epoch 24:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.96, valid_loss=-1.89]\n","Epoch 25:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.96, valid_loss=-1.89]\n","Epoch 26:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-1.96, valid_loss=-1.89]\n","Epoch 27:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.97, valid_loss=-1.89]\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.95, valid_loss=-1.89]\n","\u001b[36m(_train_tune pid=7674)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7674)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7674)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.62it/s]\u001b[A\n","Epoch 28:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.95, valid_loss=-1.92]\n","Epoch 29:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.82, valid_loss=-1.92]\n","Epoch 30:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-2.00, valid_loss=-1.92]\n","Epoch 31:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.94, valid_loss=-1.92]\n","Epoch 32:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.87, valid_loss=-1.92]\n","Epoch 33:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.95, valid_loss=-1.92]\n","Epoch 34:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.93, valid_loss=-1.92]\n","Epoch 35:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.96, valid_loss=-1.92]\n","Epoch 36:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.90, valid_loss=-1.92]\n","Epoch 37:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.56, train_loss_epoch=-2.05, valid_loss=-1.92]\n","Epoch 38:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.93, valid_loss=-1.92]\n","Epoch 39:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.96, valid_loss=-1.92]\n","Epoch 39:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.94, valid_loss=-1.92]        \n","Epoch 40:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.94, valid_loss=-1.92]\n","Epoch 41:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.51, train_loss_epoch=-2.05, valid_loss=-1.92]\n","Epoch 42:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.00, valid_loss=-1.92]\n","\u001b[36m(_train_tune pid=7674)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7674)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.62it/s]\u001b[A\n","Epoch 42:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.00, valid_loss=-1.96]\n","Epoch 43:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.90, valid_loss=-1.96]\n","Epoch 44:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.98, valid_loss=-1.96]\n","Epoch 45:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.99, valid_loss=-1.96]\n","Epoch 46:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.92, valid_loss=-1.96]\n","Epoch 47:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-2.01, valid_loss=-1.96]\n","Epoch 48:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.87, valid_loss=-1.96]\n","Epoch 49:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.96, valid_loss=-1.96]\n","Epoch 50:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.90, valid_loss=-1.96]\n","Epoch 51:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.93, valid_loss=-1.96]\n","Epoch 52:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.93, valid_loss=-1.96]\n","Epoch 53:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.90, valid_loss=-1.96]\n","Epoch 54:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.94, valid_loss=-1.96]\n","Epoch 55:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-1.99, valid_loss=-1.96]\n","Epoch 56:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.92, valid_loss=-1.96]\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.87, valid_loss=-1.96]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7674)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7674)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.60it/s]\u001b[A\n","Epoch 57:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.87, valid_loss=-1.97]\n","Epoch 58:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.01, valid_loss=-1.97]\n","Epoch 59:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.96, valid_loss=-1.97]\n","Epoch 60:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.98, valid_loss=-1.97]\n","Epoch 61:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.92, valid_loss=-1.97]\n","Epoch 62:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.02, valid_loss=-1.97]\n","Epoch 63:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.97, valid_loss=-1.97]\n","Epoch 64:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.93, valid_loss=-1.97]\n","Epoch 65:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.96, valid_loss=-1.97]\n","Epoch 66:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.94, valid_loss=-1.97]\n","Epoch 67:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-1.98, valid_loss=-1.97]\n","Epoch 68:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.95, valid_loss=-1.97]\n","Epoch 68: 100%|██████████| 7/7 [00:00\u003c00:00, 23.61it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.95, valid_loss=-1.97]\n","Epoch 68: 100%|██████████| 7/7 [00:00\u003c00:00, 23.51it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.91, valid_loss=-1.97]\n","Epoch 69:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.91, valid_loss=-1.97]\n","Epoch 70:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.94, valid_loss=-1.97]\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.06, train_loss_epoch=-1.85, valid_loss=-1.97]\n","\u001b[36m(_train_tune pid=7674)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7674)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.65it/s]\u001b[A\n","Epoch 71:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.85, valid_loss=-2.00]\n","Epoch 72:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.99, valid_loss=-2.00]\n","Epoch 73:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.01, valid_loss=-2.00]\n","Epoch 74:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-2.04, valid_loss=-2.00]\n","Epoch 75:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.03, valid_loss=-2.00]\n","Epoch 76:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.08, valid_loss=-2.00]\n","Epoch 77:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.96, valid_loss=-2.00]\n","Epoch 78:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-2.06, valid_loss=-2.00]\n","Epoch 79:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.00, valid_loss=-2.00]\n","Epoch 80:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.00, valid_loss=-2.00]\n","Epoch 81:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.94, valid_loss=-2.00]\n","Epoch 82:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-2.00, valid_loss=-2.00]\n","Epoch 83:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.90, valid_loss=-2.00]\n","Epoch 84:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.95, valid_loss=-2.00]\n","Epoch 85:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.01, valid_loss=-2.00]\n","\u001b[36m(_train_tune pid=7674)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7674)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.64it/s]\u001b[A\n","Epoch 85:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.01, valid_loss=-2.00]\n","Epoch 86:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.00, valid_loss=-2.00]\n","Epoch 87:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.50, train_loss_epoch=-2.05, valid_loss=-2.00]\n","Epoch 88:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.06, valid_loss=-2.00]\n","Epoch 89:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.94, valid_loss=-2.00]\n","Epoch 90:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.01, valid_loss=-2.00]\n","Epoch 91:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.01, valid_loss=-2.00]\n","Epoch 92:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.97, valid_loss=-2.00]\n","Epoch 93:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.03, valid_loss=-2.00]\n","Epoch 94:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.94, valid_loss=-2.00]\n","Epoch 95:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.95, valid_loss=-2.00]\n","Epoch 96:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-2.02, valid_loss=-2.00]\n","Epoch 97:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.03, valid_loss=-2.00]\n","Epoch 98:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-2.05, valid_loss=-2.00]\n","Epoch 99:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.99, valid_loss=-2.00]\n","Epoch 99: 100%|██████████| 7/7 [00:00\u003c00:00, 22.39it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-1.99, valid_loss=-2.00]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7674)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.65it/s]\u001b[A\n","Epoch 100:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.04, valid_loss=-2.00]\n","Epoch 101:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.03, valid_loss=-2.00]\n","Epoch 102:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.98, valid_loss=-2.00]\n","Epoch 103:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.98, valid_loss=-2.00]\n","Epoch 104:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.45, train_loss_epoch=-2.09, valid_loss=-2.00]\n","Epoch 105:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.97, valid_loss=-2.00]\n","Epoch 106:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.90, valid_loss=-2.00]\n","Epoch 107:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.04, valid_loss=-2.00]\n","Epoch 108:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.01, valid_loss=-2.00]\n","Epoch 109:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.99, valid_loss=-2.00]\n","Epoch 110:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.90, valid_loss=-2.00]\n","Epoch 111:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.36, train_loss_epoch=-2.06, valid_loss=-2.00]\n","Epoch 112:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.00, valid_loss=-2.00]\n","Epoch 113:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.03, valid_loss=-2.00]\n","Epoch 114:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.60, train_loss_epoch=-2.13, valid_loss=-2.00]\n","\u001b[36m(_train_tune pid=7674)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7674)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.64it/s]\u001b[A\n","Epoch 114:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.13, valid_loss=-2.01]\n","Epoch 115:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.96, valid_loss=-2.01]\n","Epoch 116:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.94, valid_loss=-2.01]\n","Epoch 117:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-2.07, valid_loss=-2.01]\n","Epoch 118:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.98, valid_loss=-2.01]\n","Epoch 119:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.99, valid_loss=-2.01]\n","Epoch 120:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.05, valid_loss=-2.01]\n","Epoch 121:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.02, valid_loss=-2.01]\n","Epoch 122:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.01, valid_loss=-2.01]\n","Epoch 123:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.05, valid_loss=-2.01]\n","Epoch 124:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.03, valid_loss=-2.01]\n","Epoch 125:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.04, valid_loss=-2.01]\n","Epoch 126:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.04, valid_loss=-2.01]\n","Epoch 127:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.96, valid_loss=-2.01]\n","Epoch 128:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.00, valid_loss=-2.01]\n","\u001b[36m(_train_tune pid=7674)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7674)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.60it/s]\u001b[A\n","Epoch 128:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.00, valid_loss=-2.01]\n","Epoch 129:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.01, valid_loss=-2.01]\n","Epoch 130:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.00, valid_loss=-2.01]\n","Epoch 131:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.01, valid_loss=-2.01]\n","Epoch 132:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.98, valid_loss=-2.01]\n","Epoch 133:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-2.00, valid_loss=-2.01]\n","Epoch 134:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.56, train_loss_epoch=-2.12, valid_loss=-2.01]\n","Epoch 135:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-2.02, valid_loss=-2.01]\n","Epoch 136:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.50, train_loss_epoch=-2.12, valid_loss=-2.01]\n","Epoch 137:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.01, valid_loss=-2.01]\n","Epoch 138:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-2.01, valid_loss=-2.01]\n","Epoch 139:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.03, valid_loss=-2.01]\n","Epoch 140:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.93, valid_loss=-2.01]\n","Epoch 141:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.06, valid_loss=-2.01]\n","Epoch 142:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.07, valid_loss=-2.01]\n","\u001b[36m(_train_tune pid=7674)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7674)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.61it/s]\u001b[A\n","Epoch 143:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.98, valid_loss=-2.03]\n","Epoch 144:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.02, valid_loss=-2.03]\n","Epoch 145:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.07, valid_loss=-2.03]\n","Epoch 146:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-2.01, valid_loss=-2.03]\n","Epoch 147:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-2.09, valid_loss=-2.03]\n","Epoch 148:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-2.08, valid_loss=-2.03]\n","Epoch 149:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-2.02, valid_loss=-2.03]\n","Epoch 150:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-2.01, valid_loss=-2.03]\n","Epoch 151:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.94, valid_loss=-2.03]\n","Epoch 152:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.58, train_loss_epoch=-2.11, valid_loss=-2.03]\n","Epoch 153:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.98, valid_loss=-2.03]\n","Epoch 154:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.99, valid_loss=-2.03]\n","Epoch 155:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.02, valid_loss=-2.03]\n","Epoch 156:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.95, valid_loss=-2.03]\n","Epoch 157:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.52, train_loss_epoch=-2.06, valid_loss=-2.03]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7674)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.64it/s]\u001b[A\n","Epoch 157:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.06, valid_loss=-2.03]\n","Epoch 158:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.97, valid_loss=-2.03]\n","Epoch 159:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.99, valid_loss=-2.03]\n","Epoch 160:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-2.00, valid_loss=-2.03]\n","Epoch 161:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.97, valid_loss=-2.03]\n","Epoch 162:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.97, valid_loss=-2.03]\n","Epoch 163:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.62, train_loss_epoch=-2.10, valid_loss=-2.03]\n","Epoch 164:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.96, valid_loss=-2.03]\n","Epoch 165:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.02, valid_loss=-2.03]\n","Epoch 166:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.04, valid_loss=-2.03]\n","Epoch 167:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-2.02, valid_loss=-2.03]\n","Epoch 168:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.01, valid_loss=-2.03]\n","Epoch 169:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-2.00, valid_loss=-2.03]\n","Epoch 170:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.61, train_loss_epoch=-2.07, valid_loss=-2.03]\n","Epoch 171:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-2.01, valid_loss=-2.03]\n","\u001b[36m(_train_tune pid=7674)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7674)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.64it/s]\u001b[A\n","Epoch 171:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.01, valid_loss=-2.03]\n","Epoch 172:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.93, valid_loss=-2.03]\n","Epoch 173:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.99, valid_loss=-2.03]\n","Epoch 174:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.99, valid_loss=-2.03]\n","Epoch 175:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-2.03, valid_loss=-2.03]\n","Epoch 176:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.99, valid_loss=-2.03]\n","Epoch 177:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.00, valid_loss=-2.03]\n","Epoch 178:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-2.04, valid_loss=-2.03]\n","Epoch 179:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.97, valid_loss=-2.03]\n","Epoch 180:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.42, train_loss_epoch=-2.08, valid_loss=-2.03]\n","Epoch 181:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.94, valid_loss=-2.03]\n","Epoch 182:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.02, valid_loss=-2.03]\n","Epoch 183:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.05, valid_loss=-2.03]\n","Epoch 184:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-2.08, valid_loss=-2.03]\n","Epoch 185:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.09, valid_loss=-2.03]\n","\u001b[36m(_train_tune pid=7674)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7674)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.63it/s]\u001b[A\n","Epoch 185:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.09, valid_loss=-2.04]\n","Epoch 186:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.99, valid_loss=-2.04]\n","Epoch 187:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.03, valid_loss=-2.04]\n","Epoch 188:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.91, valid_loss=-2.04]\n","Epoch 189:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.06, valid_loss=-2.04]\n","Epoch 190:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.99, valid_loss=-2.04]\n","Epoch 191:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.99, valid_loss=-2.04]\n","Epoch 192:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.04, valid_loss=-2.04]\n","Epoch 193:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.73, train_loss_epoch=-2.15, valid_loss=-2.04]\n","Epoch 194:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-2.01, valid_loss=-2.04]\n","Epoch 195:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.05, valid_loss=-2.04]\n","Epoch 196:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.97, valid_loss=-2.04]\n","Epoch 197:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.05, valid_loss=-2.04]\n","Epoch 198:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.45, train_loss_epoch=-2.04, valid_loss=-2.04]\n","Epoch 199:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.06, valid_loss=-2.04]\n","Epoch 199: 100%|██████████| 7/7 [00:00\u003c00:00, 23.15it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.06, valid_loss=-2.04]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 18:31:29,607\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (40, 20, 1), 'n_pool_kernel_size': (16, 8, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=7674)\u001b[0m `Trainer.fit` stopped: `max_steps=1400.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=7674)\u001b[0m \n","\u001b[36m(_train_tune pid=7674)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.67it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7674)\u001b[0m \r                                                                      \u001b[A\rEpoch 199: 100%|██████████| 7/7 [00:02\u003c00:00,  3.12it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.06, valid_loss=-2.03]\rEpoch 199: 100%|██████████| 7/7 [00:02\u003c00:00,  3.12it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.01, valid_loss=-2.03]\rEpoch 199: 100%|██████████| 7/7 [00:02\u003c00:00,  3.12it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.01, valid_loss=-2.03]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=8146)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=8146)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=8146)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=8146)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=8146)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=8146)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=8146)\u001b[0m 2024-11-12 18:31:38.113999: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=8146)\u001b[0m 2024-11-12 18:31:38.134787: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=8146)\u001b[0m 2024-11-12 18:31:38.160010: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=8146)\u001b[0m 2024-11-12 18:31:38.167710: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=8146)\u001b[0m 2024-11-12 18:31:38.185808: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=8146)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=8146)\u001b[0m 2024-11-12 18:31:39.345387: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=8146)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=8146)\u001b[0m \n","\u001b[36m(_train_tune pid=8146)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=8146)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=8146)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=8146)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=8146)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=8146)\u001b[0m 3 | blocks       | ModuleList       | 2.5 M  | train\n","\u001b[36m(_train_tune pid=8146)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=8146)\u001b[0m 2.5 M     Trainable params\n","\u001b[36m(_train_tune pid=8146)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=8146)\u001b[0m 2.5 M     Total params\n","\u001b[36m(_train_tune pid=8146)\u001b[0m 9.948     Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=8146)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=8146)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/2 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.797, train_loss_epoch=0.877]\n","Epoch 2:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.325, train_loss_epoch=0.464]\n","Epoch 3:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.531, train_loss_epoch=-0.299]\n","Epoch 4:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.500, train_loss_epoch=0.533]\n","Epoch 5:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.472, train_loss_epoch=0.0399]\n","Epoch 5: 100%|██████████| 2/2 [00:00\u003c00:00,  8.52it/s, v_num=0, train_loss_step=-0.704, train_loss_epoch=-0.154]\n","Epoch 6:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.704, train_loss_epoch=-0.154]\n","Epoch 7:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.137, train_loss_epoch=-0.358]\n","Epoch 8:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.381, train_loss_epoch=-0.273]\n","Epoch 9:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.527, train_loss_epoch=-0.527]\n","Epoch 10:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.394, train_loss_epoch=-0.426]\n","Epoch 11:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.403, train_loss_epoch=-0.391]\n","Epoch 12:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.635, train_loss_epoch=-0.586]\n","Epoch 13:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.705, train_loss_epoch=-0.704]\n","Epoch 14:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.797, train_loss_epoch=-0.747]\n","Epoch 15:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.907, train_loss_epoch=-0.915]\n","Epoch 16:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.97, train_loss_epoch=-0.927]\n","Epoch 17:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.948, train_loss_epoch=-0.978]\n","Epoch 18:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.00, train_loss_epoch=-1.03]\n","Epoch 19:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.06, train_loss_epoch=-1.03]\n","Epoch 20:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.07]\n","Epoch 21:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.07]\n","Epoch 22:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.06]\n","Epoch 23:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.08]\n","Epoch 24:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.11]\n","Epoch 25:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.12]\n","Epoch 26:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.12]\n","Epoch 27:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.14]\n","Epoch 28:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.13]\n","Epoch 29:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.14]\n","Epoch 30:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.14]\n","Epoch 31:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.15]\n","Epoch 32:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.14]\n","Epoch 33:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.11]\n","Epoch 34:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.13]\n","Epoch 35:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.15]\n","Epoch 36:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.15]\n","Epoch 37:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.14]\n","Epoch 38:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.16]\n","Epoch 39:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.15]\n","Epoch 40:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.17]\n","Epoch 41:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.17]\n","Epoch 42:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.14]\n","Epoch 43:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.17]\n","Epoch 44:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.09, train_loss_epoch=-1.13]\n","Epoch 45:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.15]\n","Epoch 46:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.15]\n","Epoch 47:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.15]\n","Epoch 48:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.15]\n","Epoch 49:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.13]\n","Epoch 49: 100%|██████████| 2/2 [00:00\u003c00:00,  8.51it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.13]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8146)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8146)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 11.65it/s]\u001b[A\n","Epoch 50:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.16, valid_loss=-1.19]\n","Epoch 51:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.17, valid_loss=-1.19]\n","Epoch 52:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.17, valid_loss=-1.19]\n","Epoch 53:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.17, valid_loss=-1.19]\n","Epoch 54:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.17, valid_loss=-1.19]\n","Epoch 55:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.16, valid_loss=-1.19]\n","Epoch 56:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.17, valid_loss=-1.19]\n","Epoch 57:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.17, valid_loss=-1.19]\n","Epoch 58:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.16, valid_loss=-1.19]\n","Epoch 59:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.13, valid_loss=-1.19]\n","Epoch 60:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.15, valid_loss=-1.19]\n","Epoch 61:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.17, valid_loss=-1.19]\n","Epoch 62:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.15, valid_loss=-1.19]\n","Epoch 63:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.16, valid_loss=-1.19]\n","Epoch 64:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.17, valid_loss=-1.19]\n","Epoch 65:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.12, valid_loss=-1.19]\n","Epoch 66:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.18, valid_loss=-1.19]\n","Epoch 67:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.17, valid_loss=-1.19]\n","Epoch 68:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.16, valid_loss=-1.19]\n","Epoch 69:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.15, valid_loss=-1.19]\n","Epoch 70:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.17, valid_loss=-1.19]\n","Epoch 71:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.14, valid_loss=-1.19]\n","Epoch 72:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.18, valid_loss=-1.19]\n","Epoch 73:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.17, valid_loss=-1.19]\n","Epoch 74:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.19, valid_loss=-1.19]\n","Epoch 75:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.16, valid_loss=-1.19]\n","Epoch 76:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.16, valid_loss=-1.19]\n","Epoch 76: 100%|██████████| 2/2 [00:00\u003c00:00,  8.12it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.16, valid_loss=-1.19]\n","Epoch 76:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.16, valid_loss=-1.19]        \n","Epoch 77:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.16, valid_loss=-1.19]\n","Epoch 78:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.10, valid_loss=-1.19]\n","Epoch 79:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.14, valid_loss=-1.19]\n","Epoch 80:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.13, valid_loss=-1.19]\n","Epoch 81:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.11, valid_loss=-1.19]\n","Epoch 82:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.13, valid_loss=-1.19]\n","Epoch 83:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.14, valid_loss=-1.19]\n","Epoch 84:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.18, valid_loss=-1.19]\n","Epoch 85:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.17, valid_loss=-1.19]\n","Epoch 86:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.15, valid_loss=-1.19]\n","Epoch 87:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.18, valid_loss=-1.19]\n","Epoch 88:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.15, valid_loss=-1.19]\n","Epoch 89:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.17, valid_loss=-1.19]\n","Epoch 90:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.16, valid_loss=-1.19]\n","Epoch 90: 100%|██████████| 2/2 [00:00\u003c00:00,  8.07it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.16, valid_loss=-1.19]\n","Epoch 91:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.16, valid_loss=-1.19]\n","Epoch 92:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.15, valid_loss=-1.19]\n","Epoch 93:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.18, valid_loss=-1.19]\n","Epoch 94:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.18, valid_loss=-1.19]\n","Epoch 95:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.17, valid_loss=-1.19]\n","Epoch 96:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.18, valid_loss=-1.19]\n","Epoch 97:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.16, valid_loss=-1.19]\n","Epoch 98:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.18, valid_loss=-1.19]\n","Epoch 99:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.16, valid_loss=-1.19]\n","Epoch 99: 100%|██████████| 2/2 [00:00\u003c00:00,  8.40it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.16, valid_loss=-1.19]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8146)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8146)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 11.49it/s]\u001b[A\n","Epoch 100:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.16, valid_loss=-1.19]\n","Epoch 101:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.18, valid_loss=-1.19]\n","Epoch 102:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.17, valid_loss=-1.19]\n","Epoch 103:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.18, valid_loss=-1.19]\n","Epoch 104:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.19, valid_loss=-1.19]\n","Epoch 105:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.16, valid_loss=-1.19]\n","Epoch 106:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.18, valid_loss=-1.19]\n","Epoch 107:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.18, valid_loss=-1.19]\n","Epoch 108:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.16, valid_loss=-1.19]\n","Epoch 109:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.17, valid_loss=-1.19]\n","Epoch 110:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.19, valid_loss=-1.19]\n","Epoch 111:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.17, valid_loss=-1.19]\n","Epoch 112:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.18, valid_loss=-1.19]\n","Epoch 113:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.15, valid_loss=-1.19]\n","Epoch 114:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.18, valid_loss=-1.19]\n","Epoch 115:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.17, valid_loss=-1.19]\n","Epoch 116:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.19, valid_loss=-1.19]\n","Epoch 117:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.19, valid_loss=-1.19]\n","Epoch 118:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.19, valid_loss=-1.19]\n","Epoch 119:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.18, valid_loss=-1.19]\n","Epoch 120:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.17, valid_loss=-1.19]\n","Epoch 121:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.18, valid_loss=-1.19]\n","Epoch 122:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.18, valid_loss=-1.19]\n","Epoch 123:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.20, valid_loss=-1.19]\n","Epoch 124:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.19, valid_loss=-1.19]\n","Epoch 125:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.20, valid_loss=-1.19]\n","Epoch 126:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.20, valid_loss=-1.19]\n","Epoch 127:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.981, train_loss_epoch=-1.04, valid_loss=-1.19]\n","Epoch 128:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.12, valid_loss=-1.19]\n","Epoch 129:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.08, valid_loss=-1.19]\n","Epoch 130:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.07, train_loss_epoch=-1.10, valid_loss=-1.19]\n","Epoch 131:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.16, valid_loss=-1.19]\n","Epoch 132:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.17, valid_loss=-1.19]\n","Epoch 133:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.17, valid_loss=-1.19]\n","Epoch 134:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.19, valid_loss=-1.19]\n","Epoch 135:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.18, valid_loss=-1.19]\n","Epoch 136:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.17, valid_loss=-1.19]\n","Epoch 137:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.18, valid_loss=-1.19]\n","Epoch 138:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.17, valid_loss=-1.19]\n","Epoch 139:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.17, valid_loss=-1.19]\n","Epoch 140:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.18, valid_loss=-1.19]\n","Epoch 141:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.17, valid_loss=-1.19]\n","Epoch 142:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.19, valid_loss=-1.19]\n","Epoch 143:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.19, valid_loss=-1.19]\n","Epoch 144:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.19, valid_loss=-1.19]\n","Epoch 145:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.17, valid_loss=-1.19]\n","Epoch 146:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.19, valid_loss=-1.19]\n","Epoch 147:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.19, valid_loss=-1.19]\n","Epoch 148:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.20, valid_loss=-1.19]\n","Epoch 149:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.18, valid_loss=-1.19]\n","Epoch 149: 100%|██████████| 2/2 [00:00\u003c00:00,  8.56it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.18, valid_loss=-1.19]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8146)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8146)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 11.54it/s]\u001b[A\n","Epoch 150:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.20, valid_loss=-1.21]\n","Epoch 151:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.19, valid_loss=-1.21]\n","Epoch 152:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.19, valid_loss=-1.21]\n","Epoch 153:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.19, valid_loss=-1.21]\n","Epoch 154:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.21, valid_loss=-1.21]\n","Epoch 155:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.22, valid_loss=-1.21]\n","Epoch 156:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.21, valid_loss=-1.21]\n","Epoch 157:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.20, valid_loss=-1.21]\n","Epoch 158:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.20, valid_loss=-1.21]\n","Epoch 159:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.19, valid_loss=-1.21]\n","Epoch 160:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.20, valid_loss=-1.21]\n","Epoch 161:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.18, valid_loss=-1.21]\n","Epoch 162:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.19, valid_loss=-1.21]\n","Epoch 163:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.20, valid_loss=-1.21]\n","Epoch 164:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.18, valid_loss=-1.21]\n","Epoch 165:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.20, valid_loss=-1.21]\n","Epoch 166:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.20, valid_loss=-1.21]\n","Epoch 167:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.20, valid_loss=-1.21]\n","Epoch 168:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.20, valid_loss=-1.21]\n","Epoch 169:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.20, valid_loss=-1.21]\n","Epoch 170:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.20, valid_loss=-1.21]\n","Epoch 171:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.19, valid_loss=-1.21]\n","Epoch 172:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.20, valid_loss=-1.21]\n","Epoch 173:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.19, valid_loss=-1.21]\n","Epoch 174:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.21, valid_loss=-1.21]\n","Epoch 175:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.22, valid_loss=-1.21]\n","Epoch 176:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.21, valid_loss=-1.21]\n","Epoch 177:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.19, valid_loss=-1.21]\n","Epoch 178:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.20, valid_loss=-1.21]\n","Epoch 179:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.20, valid_loss=-1.21]\n","Epoch 180:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.20, valid_loss=-1.21]\n","Epoch 181:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.19, valid_loss=-1.21]\n","Epoch 182:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.21, valid_loss=-1.21]\n","Epoch 183:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.19, valid_loss=-1.21]\n","Epoch 184:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.21, valid_loss=-1.21]\n","Epoch 185:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.19, valid_loss=-1.21]\n","Epoch 186:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.22, valid_loss=-1.21]\n","Epoch 187:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.22, valid_loss=-1.21]\n","Epoch 188:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.20, valid_loss=-1.21]\n","Epoch 189:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.22, valid_loss=-1.21]\n","Epoch 190:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.19, valid_loss=-1.21]\n","Epoch 191:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.20, valid_loss=-1.21]\n","Epoch 192:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.20, valid_loss=-1.21]\n","Epoch 193:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.19, valid_loss=-1.21]\n","Epoch 194:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.21]\n","Epoch 195:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.21, valid_loss=-1.21]\n","Epoch 196:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.23, valid_loss=-1.21]\n","Epoch 197:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.19, valid_loss=-1.21]\n","Epoch 198:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.21, valid_loss=-1.21]\n","Epoch 199:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.21, valid_loss=-1.21]\n","Epoch 199: 100%|██████████| 2/2 [00:00\u003c00:00,  8.37it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.21, valid_loss=-1.21]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8146)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8146)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 11.73it/s]\u001b[A\n","Epoch 200:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.16, valid_loss=-1.19]\n","Epoch 201:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.20, valid_loss=-1.19]\n","Epoch 201: 100%|██████████| 2/2 [00:00\u003c00:00,  8.46it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.20, valid_loss=-1.19]\n","Epoch 202:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.17, valid_loss=-1.19]\n","Epoch 203:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.20, valid_loss=-1.19]\n","Epoch 204:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.20, valid_loss=-1.19]\n","Epoch 205:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.22, valid_loss=-1.19]\n","Epoch 206:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.19, valid_loss=-1.19]\n","Epoch 207:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.20, valid_loss=-1.19]\n","Epoch 208:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.19]\n","Epoch 209:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.19, valid_loss=-1.19]\n","Epoch 210:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.19]\n","Epoch 211:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.19, valid_loss=-1.19]\n","Epoch 212:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.19, valid_loss=-1.19]\n","Epoch 213:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.23, valid_loss=-1.19]\n","Epoch 214:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.22, valid_loss=-1.19]\n","Epoch 215:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.21, valid_loss=-1.19]\n","Epoch 216:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.19, valid_loss=-1.19]\n","Epoch 217:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.20, valid_loss=-1.19]\n","Epoch 218:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.20, valid_loss=-1.19]\n","Epoch 219:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.21, valid_loss=-1.19]\n","Epoch 220:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.19]\n","Epoch 221:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.19, valid_loss=-1.19]\n","Epoch 222:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.20, valid_loss=-1.19]\n","Epoch 223:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.19, valid_loss=-1.19]\n","Epoch 224:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.20, valid_loss=-1.19]\n","Epoch 225:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.22, valid_loss=-1.19]\n","Epoch 226:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.22, valid_loss=-1.19]\n","Epoch 227:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.22, valid_loss=-1.19]\n","Epoch 228:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.19, valid_loss=-1.19]\n","Epoch 229:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.19, valid_loss=-1.19]\n","Epoch 230:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.19]\n","Epoch 231:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.21, valid_loss=-1.19]\n","Epoch 232:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.20, valid_loss=-1.19]\n","Epoch 233:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.22, valid_loss=-1.19]\n","Epoch 234:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.20, valid_loss=-1.19]\n","Epoch 235:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.21, valid_loss=-1.19]\n","Epoch 236:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.21, valid_loss=-1.19]\n","Epoch 237:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.22, valid_loss=-1.19]\n","Epoch 238:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.22, valid_loss=-1.19]\n","Epoch 239:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.20, valid_loss=-1.19]\n","Epoch 240:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.21, valid_loss=-1.19]\n","Epoch 241:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.20, valid_loss=-1.19]\n","Epoch 242:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.18, valid_loss=-1.19]\n","Epoch 243:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.20, valid_loss=-1.19]\n","Epoch 244:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.20, valid_loss=-1.19]\n","Epoch 245:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.19, valid_loss=-1.19]\n","Epoch 246:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.20, valid_loss=-1.19]\n","Epoch 247:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.20, valid_loss=-1.19]\n","Epoch 248:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.19, valid_loss=-1.19]\n","Epoch 249:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.21, valid_loss=-1.19]\n","Epoch 249: 100%|██████████| 2/2 [00:00\u003c00:00,  8.48it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.21, valid_loss=-1.19]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8146)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8146)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 11.46it/s]\u001b[A\n","Epoch 250:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 251:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 252:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 253:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 254:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 255:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 256:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 257:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 258:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 259:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 260:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 261:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 262:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 263:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 264:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 265:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 266:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 267:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.23, valid_loss=-1.23]\n","Epoch 268:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 269:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.23, valid_loss=-1.23]\n","Epoch 270:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 271:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 272:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 273:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 274:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 275:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 276:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 277:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.23, valid_loss=-1.23]\n","Epoch 278:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.23, valid_loss=-1.23]\n","Epoch 279:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 280:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 281:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 282:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 283:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 284:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.23, valid_loss=-1.23]\n","Epoch 285:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 286:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 287:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 288:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 289:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 290:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 291:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 292:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 293:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 294:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 295:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 296:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 297:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 298:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 299:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.23, valid_loss=-1.23]\n","Epoch 299: 100%|██████████| 2/2 [00:00\u003c00:00,  8.39it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.23, valid_loss=-1.23]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8146)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8146)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 11.51it/s]\u001b[A\n","Epoch 300:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 301:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 302:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 303:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 304:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 305:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 306:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 307:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 308:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 309:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 310:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 311:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.24, valid_loss=-1.24]\n","Epoch 311: 100%|██████████| 2/2 [00:00\u003c00:00,  8.43it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 312:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 313:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.18, valid_loss=-1.24]\n","Epoch 314:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 314: 100%|██████████| 2/2 [00:00\u003c00:00,  8.46it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 315:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 316:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 317:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 318:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 319:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 320:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 321:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 322:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 323:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 323: 100%|██████████| 2/2 [00:00\u003c00:00,  8.37it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 324:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 325:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 326:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 327:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 328:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 329:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 330:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 331:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 332:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 332: 100%|██████████| 2/2 [00:00\u003c00:00,  8.41it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 333:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 334:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 335:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 336:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 337:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 338:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 339:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.19, valid_loss=-1.24]\n","Epoch 340:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 341:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.19, valid_loss=-1.24]\n","Epoch 342:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 343:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.24, valid_loss=-1.24]\n","Epoch 344:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 345:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 346:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 347:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 348:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 349:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 349: 100%|██████████| 2/2 [00:00\u003c00:00,  8.33it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8146)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8146)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 11.57it/s]\u001b[A\n","Epoch 350:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 351:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 352:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 353:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 354:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 355:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 356:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 357:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 358:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 359:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.19, valid_loss=-1.24]\n","Epoch 360:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 361:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 362:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 363:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 364:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 365:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.19, valid_loss=-1.24]\n","Epoch 366:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 367:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 368:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 369:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 370:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 371:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 372:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 373:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 374:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 375:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 376:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 377:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 378:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 379:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 380:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 381:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 382:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 383:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.24, valid_loss=-1.24]\n","Epoch 384:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 385:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 386:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 387:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 388:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 389:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 390:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 391:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 392:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 393:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 394:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 395:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 396:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 397:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.24, valid_loss=-1.24]\n","Epoch 398:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 399:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 399: 100%|██████████| 2/2 [00:00\u003c00:00,  8.33it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8146)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 18:33:20,332\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (60, 8, 1), 'n_pool_kernel_size': (16, 8, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=8146)\u001b[0m `Trainer.fit` stopped: `max_steps=800.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=8146)\u001b[0m \n","\u001b[36m(_train_tune pid=8146)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 11.65it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8146)\u001b[0m \r                                                                      \u001b[A\rEpoch 399: 100%|██████████| 2/2 [00:00\u003c00:00,  3.65it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.22, valid_loss=-1.24]\rEpoch 399: 100%|██████████| 2/2 [00:00\u003c00:00,  3.62it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.20, valid_loss=-1.24]\rEpoch 399: 100%|██████████| 2/2 [00:00\u003c00:00,  3.61it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.20, valid_loss=-1.24]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=8667)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=8667)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=8667)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=8667)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=8667)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=8667)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=8667)\u001b[0m 2024-11-12 18:33:28.113358: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=8667)\u001b[0m 2024-11-12 18:33:28.133980: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=8667)\u001b[0m 2024-11-12 18:33:28.159515: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=8667)\u001b[0m 2024-11-12 18:33:28.167346: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=8667)\u001b[0m 2024-11-12 18:33:28.186491: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=8667)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=8667)\u001b[0m 2024-11-12 18:33:29.374982: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=8667)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=8667)\u001b[0m \n","\u001b[36m(_train_tune pid=8667)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=8667)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=8667)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=8667)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=8667)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=8667)\u001b[0m 3 | blocks       | ModuleList       | 2.6 M  | train\n","\u001b[36m(_train_tune pid=8667)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=8667)\u001b[0m 2.6 M     Trainable params\n","\u001b[36m(_train_tune pid=8667)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=8667)\u001b[0m 2.6 M     Total params\n","\u001b[36m(_train_tune pid=8667)\u001b[0m 10.335    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=8667)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=8667)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/13 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.30]\n","Epoch 2:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.52]\n","Epoch 3:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.60]\n","Epoch 4:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.72]\n","Epoch 5:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.75]\n","Epoch 6:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.80]\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.83]\n","\u001b[36m(_train_tune pid=8667)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8667)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.21it/s]\u001b[A\n","Epoch 7:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.83, valid_loss=-1.80]\n","Epoch 8:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.83, valid_loss=-1.80]\n","Epoch 9:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.81, valid_loss=-1.80]\n","Epoch 10:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.83, valid_loss=-1.80]\n","Epoch 11:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-1.87, valid_loss=-1.80]\n","Epoch 12:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.89, valid_loss=-1.80]\n","Epoch 13:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.89, valid_loss=-1.80]\n","Epoch 14:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.82, valid_loss=-1.80]\n","Epoch 14:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.86, valid_loss=-1.80]         \n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.86, valid_loss=-1.80]\n","\u001b[36m(_train_tune pid=8667)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8667)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.02it/s]\u001b[A\n","Epoch 15:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.86, valid_loss=-1.88]\n","Epoch 16:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-1.91, valid_loss=-1.88]\n","Epoch 17:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-1.92, valid_loss=-1.88]\n","Epoch 18:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.93, valid_loss=-1.88]\n","Epoch 19:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.90, valid_loss=-1.88]\n","Epoch 20:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-1.97, valid_loss=-1.88]\n","Epoch 21:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.90, valid_loss=-1.88]\n","Epoch 21: 100%|██████████| 13/13 [00:00\u003c00:00, 34.03it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.90, valid_loss=-1.88]\n","Epoch 22:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.96, valid_loss=-1.88]\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.94, valid_loss=-1.88]\n","\u001b[36m(_train_tune pid=8667)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8667)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.23it/s]\u001b[A\n","Epoch 23:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.94, valid_loss=-1.90]\n","Epoch 24:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-1.93, valid_loss=-1.90]\n","Epoch 25:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-1.97, valid_loss=-1.90]\n","Epoch 26:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.93, valid_loss=-1.90]\n","Epoch 27:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.93, valid_loss=-1.90]\n","Epoch 28:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.85, valid_loss=-1.90]\n","Epoch 29:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.91, valid_loss=-1.90]\n","Epoch 30:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.92, valid_loss=-1.90]\n","\u001b[36m(_train_tune pid=8667)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8667)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.25it/s]\u001b[A\n","Epoch 31:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.89, valid_loss=-1.92]\n","Epoch 32:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.87, valid_loss=-1.92]\n","Epoch 33:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.92, valid_loss=-1.92]\n","Epoch 34:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.95, valid_loss=-1.92]\n","Epoch 35:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.93, valid_loss=-1.92]\n","Epoch 36:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.51, train_loss_epoch=-1.94, valid_loss=-1.92]\n","Epoch 37:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.96, valid_loss=-1.92]\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.88, valid_loss=-1.92]\n","\u001b[36m(_train_tune pid=8667)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 18:33:55,152\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (40, 20, 1), 'n_pool_kernel_size': (1, 1, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=8667)\u001b[0m `Trainer.fit` stopped: `max_steps=500.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=8667)\u001b[0m \n","\u001b[36m(_train_tune pid=8667)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.32it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8667)\u001b[0m \r                                                                        \u001b[A\rEpoch 38:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.88, valid_loss=-1.92]\rEpoch 38:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.98, valid_loss=-1.92]\rEpoch 38:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.98, valid_loss=-1.92]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=8875)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=8875)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=8875)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=8875)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=8875)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=8875)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=8875)\u001b[0m 2024-11-12 18:34:03.136477: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=8875)\u001b[0m 2024-11-12 18:34:03.157311: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=8875)\u001b[0m 2024-11-12 18:34:03.183271: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=8875)\u001b[0m 2024-11-12 18:34:03.191059: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=8875)\u001b[0m 2024-11-12 18:34:03.209243: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=8875)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=8875)\u001b[0m 2024-11-12 18:34:04.385817: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=8875)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=8875)\u001b[0m \n","\u001b[36m(_train_tune pid=8875)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=8875)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=8875)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=8875)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=8875)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=8875)\u001b[0m 3 | blocks       | ModuleList       | 2.6 M  | train\n","\u001b[36m(_train_tune pid=8875)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=8875)\u001b[0m 2.6 M     Trainable params\n","\u001b[36m(_train_tune pid=8875)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=8875)\u001b[0m 2.6 M     Total params\n","\u001b[36m(_train_tune pid=8875)\u001b[0m 10.366    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=8875)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=8875)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/13 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.49]\n","Epoch 2:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-1.74]\n","Epoch 3:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.81]\n","Epoch 4:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.81]\n","Epoch 5:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-1.89]\n","Epoch 6:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.89]\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.91]\n","\u001b[36m(_train_tune pid=8875)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8875)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 44.16it/s]\u001b[A\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.91, valid_loss=-1.88]\n","Epoch 8:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.93, valid_loss=-1.88]\n","Epoch 9:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.86, valid_loss=-1.88]\n","Epoch 10:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.91, valid_loss=-1.88]\n","Epoch 11:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-1.98, valid_loss=-1.88]\n","Epoch 12:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.93, valid_loss=-1.88]\n","Epoch 13:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.01, valid_loss=-1.88]\n","Epoch 14:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.92, valid_loss=-1.88]\n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.93, valid_loss=-1.88]\n","\u001b[36m(_train_tune pid=8875)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8875)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 45.85it/s]\u001b[A\n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.93, valid_loss=-1.87]\n","Epoch 16:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.48, train_loss_epoch=-1.97, valid_loss=-1.87]\n","Epoch 17:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.48, train_loss_epoch=-2.04, valid_loss=-1.87]\n","Epoch 18:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.01, valid_loss=-1.87]\n","Epoch 19:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.99, valid_loss=-1.87]\n","Epoch 19: 100%|██████████| 13/13 [00:00\u003c00:00, 47.31it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-2.02, valid_loss=-1.87]\n","Epoch 20:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-2.02, valid_loss=-1.87]\n","Epoch 21:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.99, valid_loss=-1.87]\n","Epoch 22:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.00, valid_loss=-1.87]\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.97, valid_loss=-1.87]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8875)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 45.56it/s]\u001b[A\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-1.97, valid_loss=-1.92]\n","Epoch 24:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-2.01, valid_loss=-1.92]\n","Epoch 25:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.00, valid_loss=-1.92]\n","Epoch 26:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.99, valid_loss=-1.92]\n","Epoch 27:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.02, valid_loss=-1.92]\n","Epoch 28:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-2.00, valid_loss=-1.92]\n","Epoch 29:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.04, valid_loss=-1.92]\n","Epoch 30:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.05, valid_loss=-1.92]\n","\u001b[36m(_train_tune pid=8875)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8875)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 45.08it/s]\u001b[A\n","Epoch 31:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-2.04, valid_loss=-1.95]\n","Epoch 32:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.07, valid_loss=-1.95]\n","Epoch 33:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-2.01, valid_loss=-1.95]\n","Epoch 34:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.06, valid_loss=-1.95]\n","Epoch 35:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.99, valid_loss=-1.95]\n","Epoch 36:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.63, train_loss_epoch=-2.10, valid_loss=-1.95]\n","Epoch 37:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.04, valid_loss=-1.95]\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-2.04, valid_loss=-1.95]\n","\u001b[36m(_train_tune pid=8875)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8875)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 45.64it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8875)\u001b[0m \n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.04, valid_loss=-1.93]\n","Epoch 39:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-2.08, valid_loss=-1.93]\n","Epoch 40:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.99, valid_loss=-1.93]\n","Epoch 41:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-2.07, valid_loss=-1.93]\n","Epoch 42:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.05, valid_loss=-1.93]\n","Epoch 43:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.02, valid_loss=-1.93]\n","Epoch 44:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.04, valid_loss=-1.93]\n","Epoch 45:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.07, valid_loss=-1.93]\n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.36, train_loss_epoch=-2.08, valid_loss=-1.93]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8875)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 44.83it/s]\u001b[A\n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.08, valid_loss=-1.96]\n","Epoch 47:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-2.08, valid_loss=-1.96]\n","Epoch 48:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.09, valid_loss=-1.96]\n","Epoch 49:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.07, valid_loss=-1.96]\n","Epoch 50:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.08, valid_loss=-1.96]\n","Epoch 51:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.10, valid_loss=-1.96]\n","Epoch 52:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.59, train_loss_epoch=-2.12, valid_loss=-1.96]\n","Epoch 53:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.07, valid_loss=-1.96]\n","\u001b[36m(_train_tune pid=8875)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8875)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 44.72it/s]\u001b[A\n","Epoch 53:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.07, valid_loss=-1.96]\n","Epoch 54:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.06, valid_loss=-1.96]\n","Epoch 54: 100%|██████████| 13/13 [00:00\u003c00:00, 43.56it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-2.06, valid_loss=-1.96]\n","Epoch 55:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-2.05, valid_loss=-1.96]\n","Epoch 56:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-2.10, valid_loss=-1.96]\n","Epoch 57:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.07, valid_loss=-1.96]\n","Epoch 58:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.40, train_loss_epoch=-2.10, valid_loss=-1.96]\n","Epoch 59:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-2.06, valid_loss=-1.96]\n","Epoch 60:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.03, valid_loss=-1.96]\n","Epoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-2.07, valid_loss=-1.96]\n","\u001b[36m(_train_tune pid=8875)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8875)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 43.51it/s]\u001b[A\n","Epoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.07, valid_loss=-1.95]\n","Epoch 62:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-2.08, valid_loss=-1.95]\n","Epoch 62: 100%|██████████| 13/13 [00:00\u003c00:00, 44.91it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.05, valid_loss=-1.95]\n","Epoch 63:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.05, valid_loss=-1.95]\n","Epoch 64:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-2.04, valid_loss=-1.95]\n","Epoch 65:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-2.08, valid_loss=-1.95]\n","Epoch 66:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.08, valid_loss=-1.95]\n","Epoch 67:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-2.05, valid_loss=-1.95]\n","Epoch 68:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.04, valid_loss=-1.95]\n","Epoch 68: 100%|██████████| 13/13 [00:00\u003c00:00, 45.37it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-2.03, valid_loss=-1.95]\n","Epoch 69:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-2.03, valid_loss=-1.95]\n","\u001b[36m(_train_tune pid=8875)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 18:34:29,625\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (168, 24, 1), 'n_pool_kernel_size': (4, 4, 4), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=8875)\u001b[0m `Trainer.fit` stopped: `max_steps=900.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=8875)\u001b[0m \n","\u001b[36m(_train_tune pid=8875)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 47.46it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8875)\u001b[0m \r                                                                        \u001b[A\rEpoch 69:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.03, valid_loss=-1.97]\rEpoch 69:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.02, valid_loss=-1.97]\rEpoch 69:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.02, valid_loss=-1.97]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=9084)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=9084)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=9084)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=9084)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=9084)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=9084)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=9084)\u001b[0m 2024-11-12 18:34:38.023364: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=9084)\u001b[0m 2024-11-12 18:34:38.042819: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=9084)\u001b[0m 2024-11-12 18:34:38.069379: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=9084)\u001b[0m 2024-11-12 18:34:38.077201: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=9084)\u001b[0m 2024-11-12 18:34:38.095425: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=9084)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=9084)\u001b[0m 2024-11-12 18:34:39.342352: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=9084)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=9084)\u001b[0m \n","\u001b[36m(_train_tune pid=9084)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=9084)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=9084)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=9084)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=9084)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=9084)\u001b[0m 3 | blocks       | ModuleList       | 2.6 M  | train\n","\u001b[36m(_train_tune pid=9084)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=9084)\u001b[0m 2.6 M     Trainable params\n","\u001b[36m(_train_tune pid=9084)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=9084)\u001b[0m 2.6 M     Total params\n","\u001b[36m(_train_tune pid=9084)\u001b[0m 10.335    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=9084)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=9084)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/13 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.50]\n","Epoch 2:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.76]\n","Epoch 3:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-1.74]\n","Epoch 4:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.82]\n","Epoch 5:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.84]\n","Epoch 6:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.86]\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.87]\n","\u001b[36m(_train_tune pid=9084)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9084)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.22it/s]\u001b[A\n","Epoch 7:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.87, valid_loss=-1.84]\n","Epoch 8:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.86, valid_loss=-1.84]\n","Epoch 9:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.85, valid_loss=-1.84]\n","Epoch 10:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.88, valid_loss=-1.84]\n","Epoch 11:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.45, train_loss_epoch=-1.91, valid_loss=-1.84]\n","Epoch 12:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.92, valid_loss=-1.84]\n","Epoch 13:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-1.91, valid_loss=-1.84]\n","Epoch 14:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.84, valid_loss=-1.84]\n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.87, valid_loss=-1.84]\n","\u001b[36m(_train_tune pid=9084)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9084)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.36it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9084)\u001b[0m \n","Epoch 15:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.87, valid_loss=-1.88]\n","Epoch 16:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-1.93, valid_loss=-1.88]\n","Epoch 17:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-1.95, valid_loss=-1.88]\n","Epoch 18:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-1.96, valid_loss=-1.88]\n","Epoch 19:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.92, valid_loss=-1.88]\n","Epoch 20:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-2.00, valid_loss=-1.88]\n","Epoch 21:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.91, valid_loss=-1.88]\n","Epoch 22:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.98, valid_loss=-1.88]\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.96, valid_loss=-1.88]\n","\u001b[36m(_train_tune pid=9084)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9084)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.17it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9084)\u001b[0m \n","Epoch 23:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.96, valid_loss=-1.92]\n","Epoch 24:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-1.95, valid_loss=-1.92]\n","Epoch 25:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-1.99, valid_loss=-1.92]\n","Epoch 26:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.95, valid_loss=-1.92]\n","Epoch 27:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.95, valid_loss=-1.92]\n","Epoch 28:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.87, valid_loss=-1.92]\n","Epoch 29:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.92, valid_loss=-1.92]\n","Epoch 30:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.95, valid_loss=-1.92]\n","\u001b[36m(_train_tune pid=9084)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9084)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.26it/s]\u001b[A\n","Epoch 30:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.95, valid_loss=-1.95]\n","Epoch 31:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.91, valid_loss=-1.95]\n","Epoch 32:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.92, valid_loss=-1.95]\n","Epoch 33:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.96, valid_loss=-1.95]\n","Epoch 34:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-1.99, valid_loss=-1.95]\n","Epoch 35:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.97, valid_loss=-1.95]\n","Epoch 35: 100%|██████████| 13/13 [00:00\u003c00:00, 38.90it/s, v_num=0, train_loss_step=-2.55, train_loss_epoch=-1.99, valid_loss=-1.95]\n","Epoch 36:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.55, train_loss_epoch=-1.99, valid_loss=-1.95]\n","Epoch 37:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.00, valid_loss=-1.95]\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.92, valid_loss=-1.95]\n","\u001b[36m(_train_tune pid=9084)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9084)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.29it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9084)\u001b[0m \n","                                                                        \u001b[A\n","Epoch 38:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.92, valid_loss=-1.97]\n","Epoch 39:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.01, valid_loss=-1.97]\n","Epoch 40:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.97, valid_loss=-1.97]\n","Epoch 41:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.99, valid_loss=-1.97]\n","Epoch 42:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.98, valid_loss=-1.97]\n","Epoch 43:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.93, valid_loss=-1.97]\n","Epoch 44:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.96, valid_loss=-1.97]\n","Epoch 45:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.02, valid_loss=-1.97]\n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-1.98, valid_loss=-1.97]\n","\u001b[36m(_train_tune pid=9084)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9084)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.27it/s]\u001b[A\n","Epoch 46:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.98, valid_loss=-1.98]\n","Epoch 47:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-2.01, valid_loss=-1.98]\n","Epoch 48:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-1.96, valid_loss=-1.98]\n","Epoch 49:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.00, valid_loss=-1.98]\n","Epoch 50:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.00, valid_loss=-1.98]\n","Epoch 51:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.03, valid_loss=-1.98]\n","Epoch 52:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.45, train_loss_epoch=-2.05, valid_loss=-1.98]\n","Epoch 53:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.02, valid_loss=-1.98]\n","\u001b[36m(_train_tune pid=9084)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9084)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.21it/s]\u001b[A\n","Epoch 53:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.02, valid_loss=-1.99]\n","Epoch 54:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.98, valid_loss=-1.99]\n","Epoch 55:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.93, valid_loss=-1.99]\n","Epoch 56:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 57:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 58:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-2.06, valid_loss=-1.99]\n","Epoch 59:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.98, valid_loss=-1.99]\n","Epoch 60:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.95, valid_loss=-1.99]\n","Epoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.99, valid_loss=-1.99]\n","\u001b[36m(_train_tune pid=9084)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9084)\u001b[0m \n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9084)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.33it/s]\u001b[A\n","Epoch 61:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.99, valid_loss=-1.99]\n","Epoch 62:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.02, valid_loss=-1.99]\n","Epoch 63:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.00, valid_loss=-1.99]\n","Epoch 64:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-2.01, valid_loss=-1.99]\n","Epoch 65:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-2.04, valid_loss=-1.99]\n","Epoch 66:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.06, valid_loss=-1.99]\n","Epoch 67:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-2.01, valid_loss=-1.99]\n","Epoch 68:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.97, valid_loss=-1.99]\n","Epoch 69:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.96, valid_loss=-1.99]\n","\u001b[36m(_train_tune pid=9084)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9084)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.21it/s]\u001b[A\n","Epoch 69:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.96, valid_loss=-2.00]\n","Epoch 70:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.98, valid_loss=-2.00]\n","Epoch 71:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.03, valid_loss=-2.00]\n","Epoch 72:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-2.02, valid_loss=-2.00]\n","Epoch 73:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.03, valid_loss=-2.00]\n","Epoch 74:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.03, valid_loss=-2.00]\n","Epoch 75:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-2.01, valid_loss=-2.00]\n","Epoch 76:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.99, valid_loss=-2.00]\n","\u001b[36m(_train_tune pid=9084)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9084)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.23it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9084)\u001b[0m \n","Epoch 77:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.05, valid_loss=-2.01]\n","Epoch 78:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.03, valid_loss=-2.01]\n","Epoch 79:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.98, valid_loss=-2.01]\n","Epoch 80:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.02, valid_loss=-2.01]\n","Epoch 81:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-2.05, valid_loss=-2.01]\n","Epoch 82:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.46, train_loss_epoch=-2.08, valid_loss=-2.01]\n","Epoch 83:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-2.00, valid_loss=-2.01]\n","Epoch 84:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.00, valid_loss=-2.01]\n","\u001b[36m(_train_tune pid=9084)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9084)\u001b[0m \n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9084)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.24it/s]\u001b[A\n","Epoch 84:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.00, valid_loss=-2.01]\n","Epoch 85:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.60, train_loss_epoch=-2.08, valid_loss=-2.01]\n","Epoch 86:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.99, valid_loss=-2.01]\n","Epoch 87:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-2.05, valid_loss=-2.01]\n","Epoch 88:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.40, train_loss_epoch=-1.99, valid_loss=-2.01]\n","Epoch 89:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-2.04, valid_loss=-2.01]\n","Epoch 90:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.55, train_loss_epoch=-2.05, valid_loss=-2.01]\n","Epoch 91:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.07, valid_loss=-2.01]\n","Epoch 92:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.00, valid_loss=-2.01]\n","\u001b[36m(_train_tune pid=9084)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9084)\u001b[0m \n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=9084)\u001b[0m `Trainer.fit` stopped: `max_steps=1200.0` reached.\n","2024-11-12 18:35:35,971\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (180, 60, 1), 'n_pool_kernel_size': (1, 1, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=9084)\u001b[0m \n","\u001b[36m(_train_tune pid=9084)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.16it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9084)\u001b[0m \r                                                                        \u001b[A\rEpoch 92:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.00, valid_loss=-2.02]\rEpoch 92:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.06, valid_loss=-2.02]\rEpoch 92:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.06, valid_loss=-2.02]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=9416)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=9416)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=9416)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=9416)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=9416)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=9416)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=9416)\u001b[0m 2024-11-12 18:35:44.184211: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=9416)\u001b[0m 2024-11-12 18:35:44.205865: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=9416)\u001b[0m 2024-11-12 18:35:44.232996: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=9416)\u001b[0m 2024-11-12 18:35:44.241261: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=9416)\u001b[0m 2024-11-12 18:35:44.261277: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=9416)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=9416)\u001b[0m 2024-11-12 18:35:45.438839: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=9416)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=9416)\u001b[0m \n","\u001b[36m(_train_tune pid=9416)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=9416)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=9416)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=9416)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=9416)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=9416)\u001b[0m 3 | blocks       | ModuleList       | 2.6 M  | train\n","\u001b[36m(_train_tune pid=9416)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=9416)\u001b[0m 2.6 M     Trainable params\n","\u001b[36m(_train_tune pid=9416)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=9416)\u001b[0m 2.6 M     Total params\n","\u001b[36m(_train_tune pid=9416)\u001b[0m 10.341    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=9416)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=9416)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:02\u003c00:00,  0.82it/s]\n","Epoch 0:   0%|          | 0/4 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.27]\n","Epoch 2:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.58]\n","Epoch 3:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.66]\n","Epoch 4:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.57]\n","Epoch 5:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.66]\n","Epoch 6:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.65]\n","Epoch 7:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.74]\n","Epoch 8:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.63]\n","Epoch 9:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.84]\n","Epoch 10:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.71]\n","Epoch 11:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-1.90]\n","Epoch 12:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.73]\n","Epoch 13:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.82]\n","Epoch 14:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.78]\n","Epoch 15:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.81]\n","Epoch 16:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.80]\n","Epoch 17:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-1.94]\n","Epoch 18:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-1.95]\n","Epoch 19:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-1.90]\n","Epoch 20:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.85]\n","Epoch 21:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-2.03]\n","Epoch 22:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.89]\n","Epoch 23:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.83]\n","Epoch 24:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.80]\n","Epoch 24: 100%|██████████| 4/4 [00:00\u003c00:00, 11.15it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-1.80]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9416)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9416)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.01it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9416)\u001b[0m \n","Epoch 25:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-1.99, valid_loss=-1.87]\n","Epoch 26:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.93, valid_loss=-1.87]\n","Epoch 27:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.88, valid_loss=-1.87]\n","Epoch 28:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.87, valid_loss=-1.87]\n","Epoch 29:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.76, valid_loss=-1.87]\n","Epoch 30:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.90, valid_loss=-1.87]\n","Epoch 31:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.92, valid_loss=-1.87]\n","Epoch 32:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.80, valid_loss=-1.87]\n","Epoch 33:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.83, valid_loss=-1.87]\n","Epoch 34:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.75, valid_loss=-1.87]\n","Epoch 35:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.92, valid_loss=-1.87]\n","Epoch 36:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.94, valid_loss=-1.87]\n","Epoch 37:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-1.97, valid_loss=-1.87]\n","Epoch 38:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-1.99, valid_loss=-1.87]\n","Epoch 39:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.93, valid_loss=-1.87]\n","Epoch 40:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.87, valid_loss=-1.87]\n","Epoch 41:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.69, valid_loss=-1.87]\n","Epoch 42:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.78, valid_loss=-1.87]\n","Epoch 43:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-2.07, valid_loss=-1.87]\n","Epoch 44:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.94, valid_loss=-1.87]\n","Epoch 45:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.97, valid_loss=-1.87]\n","Epoch 46:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.96, valid_loss=-1.87]\n","Epoch 47:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.96, valid_loss=-1.87]\n","Epoch 48:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.06, valid_loss=-1.87]\n","Epoch 49:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.82, valid_loss=-1.87]\n","Epoch 49: 100%|██████████| 4/4 [00:00\u003c00:00, 12.05it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.82, valid_loss=-1.87]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9416)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.03it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9416)\u001b[0m \n","Epoch 50:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.96, valid_loss=-1.94]\n","Epoch 51:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.98, valid_loss=-1.94]\n","Epoch 52:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.95, valid_loss=-1.94]\n","Epoch 53:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.91, valid_loss=-1.94]\n","Epoch 54:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-2.06, valid_loss=-1.94]\n","Epoch 55:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.92, valid_loss=-1.94]\n","Epoch 56:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.99, valid_loss=-1.94]\n","Epoch 57:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-1.99, valid_loss=-1.94]\n","Epoch 58:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.90, valid_loss=-1.94]\n","Epoch 59:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.45, train_loss_epoch=-2.07, valid_loss=-1.94]\n","Epoch 60:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.93, valid_loss=-1.94]\n","Epoch 61:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.99, valid_loss=-1.94]\n","Epoch 62:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.89, valid_loss=-1.94]\n","Epoch 63:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.94, valid_loss=-1.94]\n","Epoch 64:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.91, valid_loss=-1.94]\n","Epoch 65:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.98, valid_loss=-1.94]\n","Epoch 66:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.91, valid_loss=-1.94]\n","Epoch 67:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.84, valid_loss=-1.94]\n","Epoch 68:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.95, valid_loss=-1.94]\n","Epoch 69:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.02, valid_loss=-1.94]\n","Epoch 70:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.87, valid_loss=-1.94]\n","Epoch 71:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.98, valid_loss=-1.94]\n","Epoch 72:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.847, train_loss_epoch=-1.70, valid_loss=-1.94]\n","Epoch 73:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.06, valid_loss=-1.94]\n","Epoch 74:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.99, valid_loss=-1.94]\n","Epoch 74: 100%|██████████| 4/4 [00:00\u003c00:00, 12.24it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.99, valid_loss=-1.94]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9416)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.02it/s]\u001b[A\n","Epoch 75:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.93, valid_loss=-1.98]\n","Epoch 76:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.99, valid_loss=-1.98]\n","Epoch 77:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.95, valid_loss=-1.98]\n","Epoch 78:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.88, valid_loss=-1.98]\n","Epoch 79:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.98, valid_loss=-1.98]\n","Epoch 80:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.02, valid_loss=-1.98]\n","Epoch 81:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-1.98, valid_loss=-1.98]\n","Epoch 82:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.85, valid_loss=-1.98]\n","Epoch 83:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.97, valid_loss=-1.98]\n","Epoch 84:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.05, train_loss_epoch=-1.79, valid_loss=-1.98]\n","Epoch 85:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.89, valid_loss=-1.98]\n","Epoch 86:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.02, valid_loss=-1.98]\n","Epoch 87:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.94, valid_loss=-1.98]\n","Epoch 88:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.44, train_loss_epoch=-2.09, valid_loss=-1.98]\n","Epoch 89:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.93, valid_loss=-1.98]\n","Epoch 90:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.85, valid_loss=-1.98]\n","Epoch 91:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.82, valid_loss=-1.98]\n","Epoch 92:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.45, train_loss_epoch=-2.15, valid_loss=-1.98]\n","Epoch 93:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.97, valid_loss=-1.98]\n","Epoch 94:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.52, train_loss_epoch=-2.13, valid_loss=-1.98]\n","Epoch 95:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-2.11, valid_loss=-1.98]\n","Epoch 96:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.93, valid_loss=-1.98]\n","Epoch 97:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.05, valid_loss=-1.98]\n","Epoch 98:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.06, valid_loss=-1.98]\n","Epoch 99:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.93, valid_loss=-1.98]\n","Epoch 99: 100%|██████████| 4/4 [00:00\u003c00:00, 12.07it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.93, valid_loss=-1.98]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9416)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9416)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.01it/s]\u001b[A\n","Epoch 100:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-2.00, valid_loss=-2.01]\n","Epoch 101:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.08, valid_loss=-2.01]\n","Epoch 102:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.95, valid_loss=-2.01]\n","Epoch 103:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.08, valid_loss=-2.01]\n","Epoch 104:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-2.04, valid_loss=-2.01]\n","Epoch 105:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.00, valid_loss=-2.01]\n","Epoch 106:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.00, valid_loss=-2.01]\n","Epoch 107:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.91, valid_loss=-2.01]\n","Epoch 108:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.97, valid_loss=-2.01]\n","Epoch 109:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.96, valid_loss=-2.01]\n","Epoch 110:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.07, valid_loss=-2.01]\n","Epoch 111:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.92, valid_loss=-2.01]\n","Epoch 112:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.98, valid_loss=-2.01]\n","Epoch 113:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-2.10, valid_loss=-2.01]\n","Epoch 114:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-2.06, valid_loss=-2.01]\n","Epoch 114: 100%|██████████| 4/4 [00:00\u003c00:00, 11.38it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.00, valid_loss=-2.01]\n","Epoch 115:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.00, valid_loss=-2.01]\n","Epoch 116:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-2.02, valid_loss=-2.01]\n","Epoch 117:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.86, valid_loss=-2.01]\n","Epoch 118:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.91, valid_loss=-2.01]\n","Epoch 119:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.49, train_loss_epoch=-2.17, valid_loss=-2.01]\n","Epoch 120:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.93, valid_loss=-2.01]\n","Epoch 121:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.09, valid_loss=-2.01]\n","Epoch 121: 100%|██████████| 4/4 [00:00\u003c00:00, 11.89it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.09, valid_loss=-2.01]\n","Epoch 122:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.00, valid_loss=-2.01]\n","Epoch 123:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.01, valid_loss=-2.01]\n","Epoch 124:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.96, valid_loss=-2.01]\n","Epoch 124: 100%|██████████| 4/4 [00:00\u003c00:00, 11.83it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-1.96, valid_loss=-2.01]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9416)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9416)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.02it/s]\u001b[A\n","Epoch 124: 100%|██████████| 4/4 [00:02\u003c00:00,  1.67it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-1.98, valid_loss=-2.01]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=9416)\u001b[0m `Trainer.fit` stopped: `max_steps=500.0` reached.\n","2024-11-12 18:36:42,105\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (60, 8, 1), 'n_pool_kernel_size': (1, 1, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=9752)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=9752)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=9752)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=9752)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=9752)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=9752)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=9752)\u001b[0m 2024-11-12 18:36:50.169927: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=9752)\u001b[0m 2024-11-12 18:36:50.190883: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=9752)\u001b[0m 2024-11-12 18:36:50.216315: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=9752)\u001b[0m 2024-11-12 18:36:50.224170: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=9752)\u001b[0m 2024-11-12 18:36:50.242493: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=9752)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=9752)\u001b[0m 2024-11-12 18:36:51.433052: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=9752)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=9752)\u001b[0m \n","\u001b[36m(_train_tune pid=9752)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=9752)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=9752)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=9752)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=9752)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=9752)\u001b[0m 3 | blocks       | ModuleList       | 2.6 M  | train\n","\u001b[36m(_train_tune pid=9752)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=9752)\u001b[0m 2.6 M     Trainable params\n","\u001b[36m(_train_tune pid=9752)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=9752)\u001b[0m 2.6 M     Total params\n","\u001b[36m(_train_tune pid=9752)\u001b[0m 10.417    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=9752)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=9752)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/2 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.723, train_loss_epoch=0.845]\n","Epoch 2:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.187, train_loss_epoch=0.0342]\n","Epoch 3:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.190, train_loss_epoch=2.180]\n","Epoch 4:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.218, train_loss_epoch=0.0407]\n","Epoch 5:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.474, train_loss_epoch=0.443]\n","Epoch 6:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.474, train_loss_epoch=0.479]\n","Epoch 7:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.470, train_loss_epoch=0.480]\n","Epoch 8:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.399, train_loss_epoch=0.421]\n","Epoch 9:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.200, train_loss_epoch=0.264]\n","Epoch 10:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.185, train_loss_epoch=-0.0773]\n","Epoch 11:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.715, train_loss_epoch=-0.574]\n","Epoch 12:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.0392, train_loss_epoch=-0.408]\n","Epoch 13:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.974, train_loss_epoch=-0.824]\n","Epoch 14:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.04, train_loss_epoch=-0.824]\n","Epoch 15:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.997, train_loss_epoch=-0.902]\n","Epoch 16:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.98, train_loss_epoch=-0.945]\n","Epoch 17:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.974, train_loss_epoch=-0.965]\n","Epoch 18:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.01, train_loss_epoch=-1.02]\n","Epoch 19:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.00, train_loss_epoch=-1.03]\n","Epoch 20:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.07, train_loss_epoch=-1.09]\n","Epoch 21:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.07]\n","Epoch 22:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.943, train_loss_epoch=-0.983]\n","Epoch 23:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.08, train_loss_epoch=-1.06]\n","Epoch 24:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.06]\n","Epoch 25:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.02, train_loss_epoch=-1.06]\n","Epoch 26:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.11]\n","Epoch 27:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.14]\n","Epoch 28:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.06, train_loss_epoch=-1.10]\n","Epoch 29:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.14]\n","Epoch 30:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.08, train_loss_epoch=-1.09]\n","Epoch 31:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.14]\n","Epoch 32:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.16]\n","Epoch 33:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.17]\n","Epoch 34:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.13]\n","Epoch 35:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.08, train_loss_epoch=-1.08]\n","Epoch 36:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.16]\n","Epoch 37:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.16]\n","Epoch 38:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.19]\n","Epoch 39:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.13]\n","Epoch 40:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.17]\n","Epoch 41:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.16]\n","Epoch 42:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.01, train_loss_epoch=-1.07]\n","Epoch 43:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.787, train_loss_epoch=-0.802]\n","Epoch 44:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.06, train_loss_epoch=-1.12]\n","Epoch 45:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.08, train_loss_epoch=-1.12]\n","Epoch 46:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.09, train_loss_epoch=-1.07]\n","Epoch 47:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.04, train_loss_epoch=-1.07]\n","Epoch 48:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.07, train_loss_epoch=-1.09]\n","Epoch 49:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.10]\n","Epoch 49: 100%|██████████| 2/2 [00:00\u003c00:00,  7.73it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.10]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9752)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9752)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 11.36it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9752)\u001b[0m \n","Epoch 50:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.14, valid_loss=-1.17]\n","Epoch 51:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.14, valid_loss=-1.17]\n","Epoch 52:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.16, valid_loss=-1.17]\n","Epoch 53:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.12, valid_loss=-1.17]\n","Epoch 54:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.15, valid_loss=-1.17]\n","Epoch 55:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.19, valid_loss=-1.17]\n","Epoch 56:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.17, valid_loss=-1.17]\n","Epoch 57:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.15, valid_loss=-1.17]\n","Epoch 58:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.18, valid_loss=-1.17]\n","Epoch 59:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.14, valid_loss=-1.17]\n","Epoch 60:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.20, valid_loss=-1.17]\n","Epoch 61:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.17, valid_loss=-1.17]\n","Epoch 62:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.09, valid_loss=-1.17]\n","Epoch 63:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.13, valid_loss=-1.17]\n","Epoch 64:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.19, valid_loss=-1.17]\n","Epoch 65:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.19, valid_loss=-1.17]\n","Epoch 66:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.13, valid_loss=-1.17]\n","Epoch 67:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.11, valid_loss=-1.17]\n","Epoch 68:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.18, valid_loss=-1.17]\n","Epoch 69:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.15, valid_loss=-1.17]\n","Epoch 70:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.18, valid_loss=-1.17]\n","Epoch 71:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.18, valid_loss=-1.17]\n","Epoch 72:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.18, valid_loss=-1.17]\n","Epoch 73:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.16, valid_loss=-1.17]\n","Epoch 74:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.20, valid_loss=-1.17]\n","Epoch 75:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.20, valid_loss=-1.17]\n","Epoch 76:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.15, valid_loss=-1.17]\n","Epoch 77:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.13, valid_loss=-1.17]\n","Epoch 78:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.21, valid_loss=-1.17]\n","Epoch 79:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.14, valid_loss=-1.17]\n","Epoch 80:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.19, valid_loss=-1.17]\n","Epoch 81:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.16, valid_loss=-1.17]\n","Epoch 82:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.21, valid_loss=-1.17]\n","Epoch 83:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.18, valid_loss=-1.17]\n","Epoch 84:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.18, valid_loss=-1.17]\n","Epoch 85:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.17, valid_loss=-1.17]\n","Epoch 86:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.21, valid_loss=-1.17]\n","Epoch 87:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.21, valid_loss=-1.17]\n","Epoch 88:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.17, valid_loss=-1.17]\n","Epoch 89:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.09, train_loss_epoch=-1.14, valid_loss=-1.17]\n","Epoch 90:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.21, valid_loss=-1.17]\n","Epoch 91:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.18, valid_loss=-1.17]\n","Epoch 92:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.16, valid_loss=-1.17]\n","Epoch 93:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.17, valid_loss=-1.17]\n","Epoch 94:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.20, valid_loss=-1.17]\n","Epoch 95:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.17, valid_loss=-1.17]\n","Epoch 96:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.19, valid_loss=-1.17]\n","Epoch 97:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.08, train_loss_epoch=-1.14, valid_loss=-1.17]\n","Epoch 98:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.19, valid_loss=-1.17]\n","Epoch 99:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.18, valid_loss=-1.17]\n","Epoch 99: 100%|██████████| 2/2 [00:00\u003c00:00,  7.09it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.18, valid_loss=-1.17]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9752)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9752)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 11.20it/s]\u001b[A\n","Epoch 100:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.18, valid_loss=-1.21]\n","Epoch 101:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.15, valid_loss=-1.21]\n","Epoch 102:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.18, valid_loss=-1.21]\n","Epoch 103:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.18, valid_loss=-1.21]\n","Epoch 104:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.14, valid_loss=-1.21]\n","Epoch 105:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.22, valid_loss=-1.21]\n","Epoch 106:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.21, valid_loss=-1.21]\n","Epoch 107:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.16, valid_loss=-1.21]\n","Epoch 108:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.19, valid_loss=-1.21]\n","Epoch 109:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.16, valid_loss=-1.21]\n","Epoch 110:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.18, valid_loss=-1.21]\n","Epoch 111:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.22, valid_loss=-1.21]\n","Epoch 112:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.20, valid_loss=-1.21]\n","Epoch 113:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.20, valid_loss=-1.21]\n","Epoch 114:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.19, valid_loss=-1.21]\n","Epoch 115:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.16, valid_loss=-1.21]\n","Epoch 116:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.20, valid_loss=-1.21]\n","Epoch 117:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.21]\n","Epoch 118:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.20, valid_loss=-1.21]\n","Epoch 119:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.09, train_loss_epoch=-1.15, valid_loss=-1.21]\n","Epoch 120:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.23, valid_loss=-1.21]\n","Epoch 121:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.20, valid_loss=-1.21]\n","Epoch 122:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.21, valid_loss=-1.21]\n","Epoch 123:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.21, valid_loss=-1.21]\n","Epoch 124:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.17, valid_loss=-1.21]\n","Epoch 125:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.19, valid_loss=-1.21]\n","Epoch 125: 100%|██████████| 2/2 [00:00\u003c00:00,  6.88it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.20, valid_loss=-1.21]\n","Epoch 126:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.20, valid_loss=-1.21]\n","Epoch 127:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.18, valid_loss=-1.21]\n","Epoch 128:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.18, valid_loss=-1.21]\n","Epoch 129:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.11, valid_loss=-1.21]\n","Epoch 130:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.16, valid_loss=-1.21]\n","Epoch 131:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.20, valid_loss=-1.21]\n","Epoch 132:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.16, valid_loss=-1.21]\n","Epoch 133:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.19, valid_loss=-1.21]\n","Epoch 134:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.16, valid_loss=-1.21]\n","Epoch 135:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.19, valid_loss=-1.21]\n","Epoch 136:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.17, valid_loss=-1.21]\n","Epoch 137:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.15, valid_loss=-1.21]\n","Epoch 138:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.23, valid_loss=-1.21]\n","Epoch 139:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.16, valid_loss=-1.21]\n","Epoch 140:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.20, valid_loss=-1.21]\n","Epoch 141:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.17, valid_loss=-1.21]\n","Epoch 142:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.22, valid_loss=-1.21]\n","Epoch 143:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.17, valid_loss=-1.21]\n","Epoch 144:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.17, valid_loss=-1.21]\n","Epoch 145:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.20, valid_loss=-1.21]\n","Epoch 146:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.17, valid_loss=-1.21]\n","Epoch 147:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.16, valid_loss=-1.21]\n","Epoch 148:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.23, valid_loss=-1.21]\n","Epoch 149:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.18, valid_loss=-1.21]\n","Epoch 149: 100%|██████████| 2/2 [00:00\u003c00:00,  7.73it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.18, valid_loss=-1.21]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9752)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9752)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 11.18it/s]\u001b[A\n","Epoch 150:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.12, valid_loss=-1.23]\n","Epoch 151:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.18, valid_loss=-1.23]\n","Epoch 152:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 153:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 154:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 155:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 156:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 157:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 158:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 159:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 160:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 161:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 162:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 163:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 164:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 165:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.18, valid_loss=-1.23]\n","Epoch 166:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 167:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 168:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 169:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 170:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 171:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 172:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 173:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 174:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 175:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 176:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 177:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.17, valid_loss=-1.23]\n","Epoch 178:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 179:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 180:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.18, valid_loss=-1.23]\n","Epoch 181:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 182:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 183:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 184:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 185:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.16, valid_loss=-1.23]\n","Epoch 186:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 187:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 188:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.24, valid_loss=-1.23]\n","Epoch 189:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 190:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 191:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 192:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.23, valid_loss=-1.23]\n","Epoch 193:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 194:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 195:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 196:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 197:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 198:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.23, valid_loss=-1.23]\n","Epoch 199:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.17, valid_loss=-1.23]\n","Epoch 199: 100%|██████████| 2/2 [00:00\u003c00:00,  7.02it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.17, valid_loss=-1.23]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9752)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9752)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 11.19it/s]\u001b[A\n","Epoch 200:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.25, valid_loss=-1.23]\n","Epoch 201:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 202:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 203:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 204:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.24, valid_loss=-1.23]\n","Epoch 205:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.23, valid_loss=-1.23]\n","Epoch 206:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 207:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.23, valid_loss=-1.23]\n","Epoch 208:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 209:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 210:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 211:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.16, valid_loss=-1.23]\n","Epoch 212:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 213:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 214:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 215:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 216:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.17, valid_loss=-1.23]\n","Epoch 217:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 218:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.24, valid_loss=-1.23]\n","Epoch 219:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 219: 100%|██████████| 2/2 [00:00\u003c00:00,  6.96it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 220:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.23, valid_loss=-1.23]\n","Epoch 221:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 222:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 223:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.25, valid_loss=-1.23]\n","Epoch 224:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.18, valid_loss=-1.23]\n","Epoch 225:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.24, valid_loss=-1.23]\n","Epoch 226:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.24, valid_loss=-1.23]\n","Epoch 227:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.25, valid_loss=-1.23]\n","Epoch 228:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 229:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.23, valid_loss=-1.23]\n","Epoch 230:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 231:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 232:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 233:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.24, valid_loss=-1.23]\n","Epoch 234:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.24, valid_loss=-1.23]\n","Epoch 235:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 236:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.18, valid_loss=-1.23]\n","Epoch 237:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 238:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 239:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.18, valid_loss=-1.23]\n","Epoch 240:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 241:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.15, valid_loss=-1.23]\n","Epoch 242:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 243:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 244:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 245:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 246:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.23, valid_loss=-1.23]\n","Epoch 247:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.24, valid_loss=-1.23]\n","Epoch 248:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 249:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.18, valid_loss=-1.23]\n","Epoch 249: 100%|██████████| 2/2 [00:00\u003c00:00,  7.73it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.18, valid_loss=-1.23]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9752)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9752)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 11.28it/s]\u001b[A\n","Epoch 250:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.25, valid_loss=-1.23]\n","Epoch 251:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 252:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 253:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 254:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.24, valid_loss=-1.23]\n","Epoch 255:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 256:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 257:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 258:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 259:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.16, valid_loss=-1.23]\n","Epoch 260:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 261:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 262:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 263:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 264:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 265:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.16, valid_loss=-1.23]\n","Epoch 266:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.23, valid_loss=-1.23]\n","Epoch 267:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 268:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 269:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 270:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 271:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.23, valid_loss=-1.23]\n","Epoch 272:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 273:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 274:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.18, valid_loss=-1.23]\n","Epoch 275:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.23, valid_loss=-1.23]\n","Epoch 276:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 277:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.23, valid_loss=-1.23]\n","Epoch 278:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.24, valid_loss=-1.23]\n","Epoch 279:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.18, valid_loss=-1.23]\n","Epoch 279: 100%|██████████| 2/2 [00:00\u003c00:00,  7.72it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.18, valid_loss=-1.23]\n","Epoch 280:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 281:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.23, valid_loss=-1.23]\n","Epoch 282:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 283:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 284:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 285:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 286:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 287:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 288:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.17, valid_loss=-1.23]\n","Epoch 289:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 290:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.19, valid_loss=-1.23]\n","Epoch 291:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.23, valid_loss=-1.23]\n","Epoch 292:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 293:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.17, valid_loss=-1.23]\n","Epoch 294:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 295:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.20, valid_loss=-1.23]\n","Epoch 296:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.17, valid_loss=-1.23]\n","Epoch 297:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 298:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.21, valid_loss=-1.23]\n","Epoch 299:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.23, valid_loss=-1.23]\n","Epoch 299: 100%|██████████| 2/2 [00:00\u003c00:00,  7.29it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.23, valid_loss=-1.23]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9752)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9752)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 11.19it/s]\u001b[A\n","Epoch 300:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.23, valid_loss=-1.22]\n","Epoch 301:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.22, valid_loss=-1.22]\n","Epoch 302:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.22, valid_loss=-1.22]\n","Epoch 303:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.22, valid_loss=-1.22]\n","Epoch 304:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.23, valid_loss=-1.22]\n","Epoch 305:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.19, valid_loss=-1.22]\n","Epoch 306:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.21, valid_loss=-1.22]\n","Epoch 307:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.24, valid_loss=-1.22]\n","Epoch 308:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.24, valid_loss=-1.22]\n","Epoch 309:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.21, valid_loss=-1.22]\n","Epoch 310:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.23, valid_loss=-1.22]\n","Epoch 311:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.24, valid_loss=-1.22]\n","Epoch 312:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.22, valid_loss=-1.22]\n","Epoch 313:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.24, valid_loss=-1.22]\n","Epoch 314:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.22, valid_loss=-1.22]\n","Epoch 315:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.20, valid_loss=-1.22]\n","Epoch 316:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.19, valid_loss=-1.22]\n","Epoch 317:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.23, valid_loss=-1.22]\n","Epoch 318:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.23, valid_loss=-1.22]\n","Epoch 319:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.25, valid_loss=-1.22]\n","Epoch 320:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.21, valid_loss=-1.22]\n","Epoch 321:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.23, valid_loss=-1.22]\n","Epoch 322:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.25, valid_loss=-1.22]\n","Epoch 323:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.22, valid_loss=-1.22]\n","Epoch 324:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.24, valid_loss=-1.22]\n","Epoch 325:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.23, valid_loss=-1.22]\n","Epoch 326:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.25, valid_loss=-1.22]\n","Epoch 327:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.24, valid_loss=-1.22]\n","Epoch 328:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.23, valid_loss=-1.22]\n","Epoch 329:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.21, valid_loss=-1.22]\n","Epoch 330:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.22, valid_loss=-1.22]\n","Epoch 331:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.25, valid_loss=-1.22]\n","Epoch 332:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.23, valid_loss=-1.22]\n","Epoch 333:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.24, valid_loss=-1.22]\n","Epoch 334:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.19, valid_loss=-1.22]\n","Epoch 335:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.25, valid_loss=-1.22]\n","Epoch 336:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.21, valid_loss=-1.22]\n","Epoch 337:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.20, valid_loss=-1.22]\n","Epoch 338:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.20, valid_loss=-1.22]\n","Epoch 339:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.20, valid_loss=-1.22]\n","Epoch 340:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.18, valid_loss=-1.22]\n","Epoch 341:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.25, valid_loss=-1.22]\n","Epoch 342:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.25, valid_loss=-1.22]\n","Epoch 343:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.23, valid_loss=-1.22]\n","Epoch 344:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.24, valid_loss=-1.22]\n","Epoch 345:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.25, valid_loss=-1.22]\n","Epoch 346:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.23, valid_loss=-1.22]\n","Epoch 347:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.24, valid_loss=-1.22]\n","Epoch 348:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.20, valid_loss=-1.22]\n","Epoch 349:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.25, valid_loss=-1.22]\n","Epoch 349: 100%|██████████| 2/2 [00:00\u003c00:00,  6.83it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.25, valid_loss=-1.22]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9752)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9752)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 11.45it/s]\u001b[A\n","Epoch 350:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.19, valid_loss=-1.24]\n","Epoch 351:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.24, valid_loss=-1.24]\n","Epoch 352:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 353:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 354:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 355:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.24, valid_loss=-1.24]\n","Epoch 356:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.19, valid_loss=-1.24]\n","Epoch 357:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.17, valid_loss=-1.24]\n","Epoch 358:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.16, valid_loss=-1.24]\n","Epoch 359:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.24, valid_loss=-1.24]\n","Epoch 360:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.24, valid_loss=-1.24]\n","Epoch 361:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.19, valid_loss=-1.24]\n","Epoch 362:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 363:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 364:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.25, valid_loss=-1.24]\n","Epoch 365:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 366:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 367:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.25, valid_loss=-1.24]\n","Epoch 368:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.15, valid_loss=-1.24]\n","Epoch 369:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.24, valid_loss=-1.24]\n","Epoch 370:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.24, valid_loss=-1.24]\n","Epoch 371:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.24, valid_loss=-1.24]\n","Epoch 372:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 373:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 374:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.24, valid_loss=-1.24]\n","Epoch 375:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 376:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 377:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.24, valid_loss=-1.24]\n","Epoch 378:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.24, valid_loss=-1.24]\n","Epoch 379:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 380:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.25, valid_loss=-1.24]\n","Epoch 381:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 382:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.17, valid_loss=-1.24]\n","Epoch 383:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.25, valid_loss=-1.24]\n","Epoch 384:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 385:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 385: 100%|██████████| 2/2 [00:00\u003c00:00,  7.71it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 386:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 387:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 388:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 389:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.24, valid_loss=-1.24]\n","Epoch 390:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.24, valid_loss=-1.24]\n","Epoch 391:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 392:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 393:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.25, valid_loss=-1.24]\n","Epoch 394:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.24, valid_loss=-1.24]\n","Epoch 395:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.18, valid_loss=-1.24]\n","Epoch 396:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 397:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.19, valid_loss=-1.24]\n","Epoch 398:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 399:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.24, valid_loss=-1.24]\n","Epoch 399: 100%|██████████| 2/2 [00:00\u003c00:00,  7.54it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.24, valid_loss=-1.24]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9752)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9752)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 11.14it/s]\u001b[A\n","Epoch 400:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 401:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.19, valid_loss=-1.24]\n","Epoch 402:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 403:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.18, valid_loss=-1.24]\n","Epoch 404:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 405:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.26, valid_loss=-1.24]\n","Epoch 405: 100%|██████████| 2/2 [00:00\u003c00:00,  7.53it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 406:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 407:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 408:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 409:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.24, valid_loss=-1.24]\n","Epoch 410:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 411:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 412:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 413:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 414:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.19, valid_loss=-1.24]\n","Epoch 414: 100%|██████████| 2/2 [00:00\u003c00:00,  7.70it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 415:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 416:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 417:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 418:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 419:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.24, valid_loss=-1.24]\n","Epoch 420:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.24, valid_loss=-1.24]\n","Epoch 421:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.16, valid_loss=-1.24]\n","Epoch 422:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.24, valid_loss=-1.24]\n","Epoch 423:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 424:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 425:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 426:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.26, valid_loss=-1.24]\n","Epoch 427:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 428:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 428: 100%|██████████| 2/2 [00:00\u003c00:00,  7.52it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 429:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 430:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 431:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 432:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.24, valid_loss=-1.24]\n","Epoch 433:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 434:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.26, valid_loss=-1.24]\n","Epoch 435:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.24, valid_loss=-1.24]\n","Epoch 436:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 437:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 438:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.25, valid_loss=-1.24]\n","Epoch 439:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 440:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 441:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.26, valid_loss=-1.24]\n","Epoch 442:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.24, valid_loss=-1.24]\n","Epoch 443:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.19, valid_loss=-1.24]\n","Epoch 444:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.25, valid_loss=-1.24]\n","Epoch 445:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 446:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.25, valid_loss=-1.24]\n","Epoch 447:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 448:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 449:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.24, valid_loss=-1.24]\n","Epoch 449: 100%|██████████| 2/2 [00:00\u003c00:00,  7.68it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.24, valid_loss=-1.24]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9752)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 18:39:01,588\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (40, 20, 1), 'n_pool_kernel_size': (2, 2, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=9752)\u001b[0m `Trainer.fit` stopped: `max_steps=900.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=9752)\u001b[0m \n","\u001b[36m(_train_tune pid=9752)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 11.29it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9752)\u001b[0m \r                                                                      \u001b[A\rEpoch 449: 100%|██████████| 2/2 [00:00\u003c00:00,  3.34it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.24, valid_loss=-1.24]\rEpoch 449: 100%|██████████| 2/2 [00:00\u003c00:00,  3.31it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.24, valid_loss=-1.24]\rEpoch 449: 100%|██████████| 2/2 [00:00\u003c00:00,  3.31it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.24, valid_loss=-1.24]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=10397)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=10397)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=10397)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=10397)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=10397)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=10397)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=10397)\u001b[0m 2024-11-12 18:39:10.140336: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=10397)\u001b[0m 2024-11-12 18:39:10.160818: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=10397)\u001b[0m 2024-11-12 18:39:10.185658: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=10397)\u001b[0m 2024-11-12 18:39:10.193391: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=10397)\u001b[0m 2024-11-12 18:39:10.211203: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=10397)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=10397)\u001b[0m 2024-11-12 18:39:11.380858: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=10397)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=10397)\u001b[0m \n","\u001b[36m(_train_tune pid=10397)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=10397)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=10397)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=10397)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=10397)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=10397)\u001b[0m 3 | blocks       | ModuleList       | 2.5 M  | train\n","\u001b[36m(_train_tune pid=10397)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=10397)\u001b[0m 2.5 M     Trainable params\n","\u001b[36m(_train_tune pid=10397)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=10397)\u001b[0m 2.5 M     Total params\n","\u001b[36m(_train_tune pid=10397)\u001b[0m 10.059    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=10397)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=10397)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/7 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.30, train_loss_epoch=163.0]\n","Epoch 2:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.378, train_loss_epoch=1.380]\n","Epoch 3:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.146, train_loss_epoch=1.930]\n","Epoch 4:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.0514, train_loss_epoch=0.619]\n","Epoch 5:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.983, train_loss_epoch=-0.498]\n","Epoch 6:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.909, train_loss_epoch=-0.86]\n","Epoch 7:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.02]\n","Epoch 8:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.99, train_loss_epoch=-1.05]\n","Epoch 9:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.08]\n","Epoch 10:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.991, train_loss_epoch=-1.07]\n","Epoch 11:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.11]\n","Epoch 12:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.05, train_loss_epoch=-1.11]\n","Epoch 13:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.10]\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.09, train_loss_epoch=-1.11]\n","\u001b[36m(_train_tune pid=10397)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10397)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.71it/s]\u001b[A\n","Epoch 14:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.11, valid_loss=-1.13]\n","Epoch 15:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.05, train_loss_epoch=-1.12, valid_loss=-1.13]\n","Epoch 16:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.18, valid_loss=-1.13]\n","Epoch 17:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.14, valid_loss=-1.13]\n","Epoch 18:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.14, valid_loss=-1.13]\n","Epoch 19:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.17, valid_loss=-1.13]\n","Epoch 20:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.967, train_loss_epoch=-1.12, valid_loss=-1.13]\n","Epoch 21:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.16, valid_loss=-1.13]\n","Epoch 21: 100%|██████████| 7/7 [00:00\u003c00:00, 25.80it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.16, valid_loss=-1.13]\n","Epoch 22:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.13, valid_loss=-1.13]\n","Epoch 23:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.13, valid_loss=-1.13]\n","Epoch 24:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.16, valid_loss=-1.13]\n","Epoch 25:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.18, valid_loss=-1.13]\n","Epoch 26:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.14, valid_loss=-1.13]\n","Epoch 27:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.16, valid_loss=-1.13]\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.18, valid_loss=-1.13]\n","\u001b[36m(_train_tune pid=10397)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10397)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.71it/s]\u001b[A\n","Epoch 28:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.18, valid_loss=-1.01]\n","Epoch 29:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.534, train_loss_epoch=-0.98, valid_loss=-1.01]\n","Epoch 30:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.13, valid_loss=-1.01]\n","Epoch 31:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.06, train_loss_epoch=-1.13, valid_loss=-1.01]\n","Epoch 32:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.05, train_loss_epoch=-1.13, valid_loss=-1.01]\n","Epoch 33:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.15, valid_loss=-1.01]\n","Epoch 34:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.14, valid_loss=-1.01]\n","Epoch 35:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.19, valid_loss=-1.01]\n","Epoch 36:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.19, valid_loss=-1.01]\n","Epoch 37:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.20, valid_loss=-1.01]\n","Epoch 38:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.20, valid_loss=-1.01]\n","Epoch 39:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.18, valid_loss=-1.01]\n","Epoch 40:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.18, valid_loss=-1.01]\n","Epoch 41:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.20, valid_loss=-1.01]\n","Epoch 42:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.18, valid_loss=-1.01]\n","\u001b[36m(_train_tune pid=10397)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10397)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.70it/s]\u001b[A\n","Epoch 43:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.18, valid_loss=-1.21]\n","Epoch 44:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.18, valid_loss=-1.21]\n","Epoch 45:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.19, valid_loss=-1.21]\n","Epoch 46:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.19, valid_loss=-1.21]\n","Epoch 47:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.19, valid_loss=-1.21]\n","Epoch 48:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.02, train_loss_epoch=-1.16, valid_loss=-1.21]\n","Epoch 49:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.20, valid_loss=-1.21]\n","Epoch 50:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.19, valid_loss=-1.21]\n","Epoch 51:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.16, valid_loss=-1.21]\n","Epoch 52:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.18, valid_loss=-1.21]\n","Epoch 53:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.802, train_loss_epoch=-1.08, valid_loss=-1.21]\n","Epoch 54:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.16, valid_loss=-1.21]\n","Epoch 55:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.19, valid_loss=-1.21]\n","Epoch 56:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.19, valid_loss=-1.21]\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.961, train_loss_epoch=-1.06, valid_loss=-1.21]\n","\u001b[36m(_train_tune pid=10397)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10397)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.75it/s]\u001b[A\n","Epoch 57:   0%|          | 0/7 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.06, valid_loss=-1.20] \n","Epoch 58:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.17, valid_loss=-1.20]\n","Epoch 59:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.19, valid_loss=-1.20]\n","Epoch 60:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.18, valid_loss=-1.20]\n","Epoch 61:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.19, valid_loss=-1.20]\n","Epoch 62:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.19, valid_loss=-1.20]\n","Epoch 63:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.20, valid_loss=-1.20]\n","Epoch 64:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.20, valid_loss=-1.20]\n","Epoch 65:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.20, valid_loss=-1.20]\n","Epoch 66:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.20, valid_loss=-1.20]\n","Epoch 67:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.21, valid_loss=-1.20]\n","Epoch 68:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.20, valid_loss=-1.20]\n","Epoch 69:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.20, valid_loss=-1.20]\n","Epoch 70:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.21, valid_loss=-1.20]\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.734, train_loss_epoch=-1.14, valid_loss=-1.20]\n","\u001b[36m(_train_tune pid=10397)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10397)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10397)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.69it/s]\u001b[A\n","Epoch 71:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.14, valid_loss=-1.24] \n","Epoch 72:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 73:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 74:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 75:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 76:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 77:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.08, train_loss_epoch=-1.17, valid_loss=-1.24]\n","Epoch 78:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 79:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.19, valid_loss=-1.24]\n","Epoch 80:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 81:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 82:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 83:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.617, train_loss_epoch=-1.13, valid_loss=-1.24]\n","Epoch 84:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.05, train_loss_epoch=-1.17, valid_loss=-1.24]\n","Epoch 85:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.18, valid_loss=-1.24]\n","\u001b[36m(_train_tune pid=10397)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10397)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.62it/s]\u001b[A\n","Epoch 86:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.16, valid_loss=-1.24]\n","Epoch 87:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 88:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 89:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 90:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 91:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 92:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.19, valid_loss=-1.24]\n","Epoch 93:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 94:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 95:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.19, valid_loss=-1.24]\n","Epoch 96:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 97:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 98:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 99:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.19, valid_loss=-1.24]\n","Epoch 99: 100%|██████████| 7/7 [00:00\u003c00:00, 24.01it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.19, valid_loss=-1.24]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10397)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.71it/s]\u001b[A\n","Epoch 99: 100%|██████████| 7/7 [00:02\u003c00:00,  3.17it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.23, valid_loss=-1.22]\n","Epoch 100:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.23, valid_loss=-1.22]\n","Epoch 101:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.18, valid_loss=-1.22]\n","Epoch 102:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.21, valid_loss=-1.22]\n","Epoch 103:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.19, valid_loss=-1.22]\n","Epoch 104:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.23, valid_loss=-1.22]\n","Epoch 105:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.19, valid_loss=-1.22]\n","Epoch 106:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.872, train_loss_epoch=-1.16, valid_loss=-1.22]\n","Epoch 107:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.23, valid_loss=-1.22]\n","Epoch 108:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.24, valid_loss=-1.22]\n","Epoch 109:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.20, valid_loss=-1.22]\n","Epoch 110:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.957, train_loss_epoch=-1.18, valid_loss=-1.22]\n","Epoch 111:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.22, valid_loss=-1.22]\n","Epoch 112:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.23, valid_loss=-1.22]\n","Epoch 113:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.21, valid_loss=-1.22]\n","Epoch 114:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.21, valid_loss=-1.22]\n","\u001b[36m(_train_tune pid=10397)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10397)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.72it/s]\u001b[A\n","Epoch 114:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 115:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 116:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.959, train_loss_epoch=-1.17, valid_loss=-1.24]\n","Epoch 117:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 118:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.06, train_loss_epoch=-1.17, valid_loss=-1.24]\n","Epoch 119:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.17, valid_loss=-1.24]\n","Epoch 120:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.18, valid_loss=-1.24]\n","Epoch 121:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 122:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 123:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.19, valid_loss=-1.24]\n","Epoch 124:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.19, valid_loss=-1.24]\n","Epoch 125:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 126:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 127:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.828, train_loss_epoch=-1.14, valid_loss=-1.24]\n","Epoch 128:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.20, valid_loss=-1.24]\n","\u001b[36m(_train_tune pid=10397)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10397)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.72it/s]\u001b[A\n","Epoch 128:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 129:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 130:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.19, valid_loss=-1.24]\n","Epoch 131:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 132:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 133:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 134:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 135:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.07, train_loss_epoch=-1.19, valid_loss=-1.24]\n","Epoch 136:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 137:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 137: 100%|██████████| 7/7 [00:00\u003c00:00, 26.05it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 138:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.19, valid_loss=-1.24]\n","Epoch 139:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 140:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.03, train_loss_epoch=-1.19, valid_loss=-1.24]\n","Epoch 141:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 142:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.22, valid_loss=-1.24]\n","\u001b[36m(_train_tune pid=10397)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10397)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10397)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.70it/s]\u001b[A\n","Epoch 143:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.21, valid_loss=-1.25]\n","Epoch 144:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.23, valid_loss=-1.25]\n","Epoch 145:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.25, valid_loss=-1.25]\n","Epoch 146:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.21, valid_loss=-1.25]\n","Epoch 147:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.24, valid_loss=-1.25]\n","Epoch 148:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.22, valid_loss=-1.25]\n","Epoch 149:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.20, valid_loss=-1.25]\n","Epoch 150:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.22, valid_loss=-1.25]\n","Epoch 151:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.20, valid_loss=-1.25]\n","Epoch 152:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.23, valid_loss=-1.25]\n","Epoch 153:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.875, train_loss_epoch=-1.18, valid_loss=-1.25]\n","Epoch 154:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.20, valid_loss=-1.25]\n","Epoch 155:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.19, valid_loss=-1.25]\n","Epoch 156:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.22, valid_loss=-1.25]\n","Epoch 157:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.22, valid_loss=-1.25]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10397)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10397)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.76it/s]\u001b[A\n","Epoch 157:   0%|          | 0/7 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22, valid_loss=-1.25]\n","Epoch 158:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.17, valid_loss=-1.25]\n","Epoch 159:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.22, valid_loss=-1.25]\n","Epoch 160:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.22, valid_loss=-1.25]\n","Epoch 161:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.20, valid_loss=-1.25]\n","Epoch 162:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.22, valid_loss=-1.25]\n","Epoch 163:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.24, valid_loss=-1.25]\n","Epoch 164:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.928, train_loss_epoch=-1.18, valid_loss=-1.25]\n","Epoch 165:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.20, valid_loss=-1.25]\n","Epoch 166:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.23, valid_loss=-1.25]\n","Epoch 167:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.20, valid_loss=-1.25]\n","Epoch 168:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.21, valid_loss=-1.25]\n","Epoch 169:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.18, valid_loss=-1.25]\n","Epoch 170:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.22, valid_loss=-1.25]\n","Epoch 171:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.19, valid_loss=-1.25]\n","\u001b[36m(_train_tune pid=10397)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10397)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.74it/s]\u001b[A\n","Epoch 171:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.19, valid_loss=-1.24]\n","Epoch 172:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 173:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 174:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 175:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 176:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 177:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 178:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 179:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.03, train_loss_epoch=-1.20, valid_loss=-1.24]\n","Epoch 180:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.23, valid_loss=-1.24]\n","Epoch 181:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.951, train_loss_epoch=-1.19, valid_loss=-1.24]\n","Epoch 182:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.21, valid_loss=-1.24]\n","Epoch 183:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.22, valid_loss=-1.24]\n","Epoch 184:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.24, valid_loss=-1.24]\n","Epoch 185:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.22, valid_loss=-1.24]\n","\u001b[36m(_train_tune pid=10397)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=10397)\u001b[0m `Trainer.fit` stopped: `max_steps=1300.0` reached.\n","2024-11-12 18:40:31,263\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (24, 12, 1), 'n_pool_kernel_size': (4, 4, 4), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=10397)\u001b[0m \n","\u001b[36m(_train_tune pid=10397)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.82it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10397)\u001b[0m \r                                                                      \u001b[A\rEpoch 185:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.22, valid_loss=-1.25]\rEpoch 185:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.24, valid_loss=-1.25]\rEpoch 185:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.24, valid_loss=-1.25]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=10824)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=10824)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=10824)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=10824)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=10824)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=10824)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=10824)\u001b[0m 2024-11-12 18:40:39.241572: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=10824)\u001b[0m 2024-11-12 18:40:39.262246: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=10824)\u001b[0m 2024-11-12 18:40:39.288372: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=10824)\u001b[0m 2024-11-12 18:40:39.296201: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=10824)\u001b[0m 2024-11-12 18:40:39.314661: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=10824)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=10824)\u001b[0m 2024-11-12 18:40:40.549721: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=10824)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=10824)\u001b[0m \n","\u001b[36m(_train_tune pid=10824)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=10824)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=10824)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=10824)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=10824)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=10824)\u001b[0m 3 | blocks       | ModuleList       | 2.5 M  | train\n","\u001b[36m(_train_tune pid=10824)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=10824)\u001b[0m 2.5 M     Trainable params\n","\u001b[36m(_train_tune pid=10824)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=10824)\u001b[0m 2.5 M     Total params\n","\u001b[36m(_train_tune pid=10824)\u001b[0m 10.176    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=10824)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=10824)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00,  2.03it/s]\n","Epoch 0:   0%|          | 0/4 [00:00\u003c?, ?it/s] \n","Epoch 0: 100%|██████████| 4/4 [00:00\u003c00:00, 10.00it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.48]\n","Epoch 1:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.48]\n","Epoch 2:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.36, train_loss_epoch=-1.80]\n","Epoch 3:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-1.83]\n","Epoch 4:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.78]\n","Epoch 5:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-1.91]\n","Epoch 6:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.87]\n","Epoch 7:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.92]\n","Epoch 7: 100%|██████████| 4/4 [00:00\u003c00:00, 14.52it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.86]\n","Epoch 8:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.86]\n","Epoch 9:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.01]\n","Epoch 10:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.85]\n","Epoch 11:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.54, train_loss_epoch=-2.07]\n","Epoch 12:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.87]\n","Epoch 13:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.87]\n","Epoch 14:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.94]\n","Epoch 15:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.87]\n","Epoch 16:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.82]\n","Epoch 17:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.04]\n","Epoch 18:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.40, train_loss_epoch=-2.08]\n","Epoch 19:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-2.05]\n","Epoch 20:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.95]\n","Epoch 21:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.07]\n","Epoch 22:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.01]\n","Epoch 23:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.97]\n","Epoch 24:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.91]\n","Epoch 24: 100%|██████████| 4/4 [00:00\u003c00:00, 13.73it/s, v_num=0, train_loss_step=-2.53, train_loss_epoch=-1.91]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10824)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10824)\u001b[0m \n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10824)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.40it/s]\u001b[A\n","Epoch 25:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.53, train_loss_epoch=-2.11, valid_loss=-1.99]\n","Epoch 26:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.06, valid_loss=-1.99]\n","Epoch 27:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.98, valid_loss=-1.99]\n","Epoch 28:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.99, valid_loss=-1.99]\n","Epoch 29:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.85, valid_loss=-1.99]\n","Epoch 30:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.05, valid_loss=-1.99]\n","Epoch 31:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.02, valid_loss=-1.99]\n","Epoch 32:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.93, valid_loss=-1.99]\n","Epoch 33:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.96, valid_loss=-1.99]\n","Epoch 34:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.95, valid_loss=-1.99]\n","Epoch 35:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.92, valid_loss=-1.99]\n","Epoch 36:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.98, valid_loss=-1.99]\n","Epoch 37:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.08, valid_loss=-1.99]\n","Epoch 38:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.48, train_loss_epoch=-2.10, valid_loss=-1.99]\n","Epoch 39:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.97, valid_loss=-1.99]\n","Epoch 40:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.96, valid_loss=-1.99]\n","Epoch 41:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.90, valid_loss=-1.99]\n","Epoch 42:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.95, valid_loss=-1.99]\n","Epoch 43:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 44:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.95, valid_loss=-1.99]\n","Epoch 45:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.98, valid_loss=-1.99]\n","Epoch 46:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.99, valid_loss=-1.99]\n","Epoch 47:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.99, valid_loss=-1.99]\n","Epoch 48:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-2.07, valid_loss=-1.99]\n","Epoch 49:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.94, valid_loss=-1.99]\n","Epoch 49: 100%|██████████| 4/4 [00:00\u003c00:00, 14.50it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.94, valid_loss=-1.99]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10824)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10824)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 14.32it/s]\u001b[A\n","Epoch 50:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.03, valid_loss=-2.00]\n","Epoch 51:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.01, valid_loss=-2.00]\n","Epoch 52:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.04, valid_loss=-2.00]\n","Epoch 53:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.00, valid_loss=-2.00]\n","Epoch 54:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.56, train_loss_epoch=-2.14, valid_loss=-2.00]\n","Epoch 55:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.00, valid_loss=-2.00]\n","Epoch 56:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.01, valid_loss=-2.00]\n","Epoch 57:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-2.09, valid_loss=-2.00]\n","Epoch 58:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.95, valid_loss=-2.00]\n","Epoch 59:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-2.12, valid_loss=-2.00]\n","Epoch 60:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.00, valid_loss=-2.00]\n","Epoch 61:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.02, valid_loss=-2.00]\n","Epoch 62:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-2.01, valid_loss=-2.00]\n","Epoch 63:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.04, valid_loss=-2.00]\n","Epoch 64:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.02, valid_loss=-2.00]\n","Epoch 65:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.96, valid_loss=-2.00]\n","Epoch 66:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.99, valid_loss=-2.00]\n","Epoch 67:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.98, valid_loss=-2.00]\n","Epoch 68:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.04, valid_loss=-2.00]\n","Epoch 69:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.00, valid_loss=-2.00]\n","Epoch 70:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.93, valid_loss=-2.00]\n","Epoch 71:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.97, valid_loss=-2.00]\n","Epoch 72:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.82, valid_loss=-2.00]\n","Epoch 73:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.99, valid_loss=-2.00]\n","Epoch 74:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.07, valid_loss=-2.00]\n","Epoch 74: 100%|██████████| 4/4 [00:00\u003c00:00, 13.87it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-2.07, valid_loss=-2.00]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10824)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.55it/s]\u001b[A\n","Epoch 75:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.93, valid_loss=-2.06]\n","Epoch 76:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.00, valid_loss=-2.06]\n","Epoch 77:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.06, valid_loss=-2.06]\n","Epoch 78:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.98, valid_loss=-2.06]\n","Epoch 79:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-2.06, valid_loss=-2.06]\n","Epoch 80:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.04, valid_loss=-2.06]\n","Epoch 81:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.09, valid_loss=-2.06]\n","Epoch 82:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.94, valid_loss=-2.06]\n","Epoch 83:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-2.00, valid_loss=-2.06]\n","Epoch 84:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.82, valid_loss=-2.06]\n","Epoch 85:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.02, valid_loss=-2.06]\n","Epoch 86:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.03, valid_loss=-2.06]\n","Epoch 87:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.01, valid_loss=-2.06]\n","Epoch 88:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.45, train_loss_epoch=-2.14, valid_loss=-2.06]\n","Epoch 89:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.95, valid_loss=-2.06]\n","Epoch 90:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.93, valid_loss=-2.06]\n","Epoch 91:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-2.00, valid_loss=-2.06]\n","Epoch 92:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.42, train_loss_epoch=-2.13, valid_loss=-2.06]\n","Epoch 93:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-2.02, valid_loss=-2.06]\n","Epoch 94:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.48, train_loss_epoch=-2.12, valid_loss=-2.06]\n","Epoch 95:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.42, train_loss_epoch=-2.14, valid_loss=-2.06]\n","Epoch 96:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-2.03, valid_loss=-2.06]\n","Epoch 97:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.05, valid_loss=-2.06]\n","Epoch 98:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.42, train_loss_epoch=-2.12, valid_loss=-2.06]\n","Epoch 99:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.96, valid_loss=-2.06]\n","Epoch 99: 100%|██████████| 4/4 [00:00\u003c00:00, 13.75it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.96, valid_loss=-2.06]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10824)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10824)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.47it/s]\u001b[A\n","Epoch 100:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.02, valid_loss=-2.05]\n","Epoch 100: 100%|██████████| 4/4 [00:00\u003c00:00, 13.84it/s, v_num=0, train_loss_step=-2.40, train_loss_epoch=-2.12, valid_loss=-2.05]\n","Epoch 101:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.40, train_loss_epoch=-2.12, valid_loss=-2.05]\n","Epoch 102:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-2.01, valid_loss=-2.05]\n","Epoch 103:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.03, valid_loss=-2.05]\n","Epoch 104:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.01, valid_loss=-2.05]\n","Epoch 105:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.36, train_loss_epoch=-2.14, valid_loss=-2.05]\n","Epoch 106:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.07, valid_loss=-2.05]\n","Epoch 107:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.02, valid_loss=-2.05]\n","Epoch 108:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.07, valid_loss=-2.05]\n","Epoch 109:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.03, valid_loss=-2.05]\n","Epoch 110:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.10, valid_loss=-2.05]\n","Epoch 111:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.05, valid_loss=-2.05]\n","Epoch 112:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.99, valid_loss=-2.05]\n","Epoch 113:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.51, train_loss_epoch=-2.18, valid_loss=-2.05]\n","Epoch 114:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-2.13, valid_loss=-2.05]\n","Epoch 115:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.13, valid_loss=-2.05]\n","Epoch 116:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.99, valid_loss=-2.05]\n","Epoch 117:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-2.03, valid_loss=-2.05]\n","Epoch 118:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.95, valid_loss=-2.05]\n","Epoch 119:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.44, train_loss_epoch=-2.18, valid_loss=-2.05]\n","Epoch 120:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-2.00, valid_loss=-2.05]\n","Epoch 121:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.13, valid_loss=-2.05]\n","Epoch 122:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.09, valid_loss=-2.05]\n","Epoch 123:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.09, valid_loss=-2.05]\n","Epoch 124:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.08, valid_loss=-2.05]\n","Epoch 124: 100%|██████████| 4/4 [00:00\u003c00:00, 13.96it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.08, valid_loss=-2.05]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10824)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10824)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.21it/s]\u001b[A\n","Epoch 125:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.10, valid_loss=-2.05]\n","Epoch 126:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-2.12, valid_loss=-2.05]\n","Epoch 127:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-2.02, valid_loss=-2.05]\n","Epoch 128:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.10, valid_loss=-2.05]\n","Epoch 129:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.13, valid_loss=-2.05]\n","Epoch 130:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.05, valid_loss=-2.05]\n","Epoch 130: 100%|██████████| 4/4 [00:00\u003c00:00, 13.86it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.03, valid_loss=-2.05]\n","Epoch 131:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.03, valid_loss=-2.05]\n","Epoch 132:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-2.02, valid_loss=-2.05]\n","Epoch 133:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.07, valid_loss=-2.05]\n","Epoch 134:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.05, valid_loss=-2.05]\n","Epoch 135:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.04, valid_loss=-2.05]\n","Epoch 136:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.67, train_loss_epoch=-2.22, valid_loss=-2.05]\n","Epoch 137:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-2.01, valid_loss=-2.05]\n","Epoch 138:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.55, train_loss_epoch=-2.18, valid_loss=-2.05]\n","Epoch 139:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.11, valid_loss=-2.05]\n","Epoch 140:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.97, valid_loss=-2.05]\n","Epoch 141:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.11, valid_loss=-2.05]\n","Epoch 142:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.96, valid_loss=-2.05]\n","Epoch 143:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-2.13, valid_loss=-2.05]\n","Epoch 144:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.12, valid_loss=-2.05]\n","Epoch 145:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.07, valid_loss=-2.05]\n","Epoch 146:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.10, valid_loss=-2.05]\n","Epoch 147:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.11, valid_loss=-2.05]\n","Epoch 148:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-2.18, valid_loss=-2.05]\n","Epoch 149:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.97, valid_loss=-2.05]\n","Epoch 149: 100%|██████████| 4/4 [00:00\u003c00:00, 14.13it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.97, valid_loss=-2.05]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10824)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10824)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.30it/s]\u001b[A\n","Epoch 150:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.05, valid_loss=-2.05]\n","Epoch 151:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.06, valid_loss=-2.05]\n","Epoch 152:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-2.01, valid_loss=-2.05]\n","Epoch 153:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.06, valid_loss=-2.05]\n","Epoch 154:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.73, train_loss_epoch=-2.26, valid_loss=-2.05]\n","Epoch 155:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.97, valid_loss=-2.05]\n","Epoch 156:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-2.06, valid_loss=-2.05]\n","Epoch 157:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.11, valid_loss=-2.05]\n","Epoch 158:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.98, valid_loss=-2.05]\n","Epoch 159:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.57, train_loss_epoch=-2.20, valid_loss=-2.05]\n","Epoch 160:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-2.03, valid_loss=-2.05]\n","Epoch 161:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.69, train_loss_epoch=-2.26, valid_loss=-2.05]\n","Epoch 162:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.54, train_loss_epoch=-2.25, valid_loss=-2.05]\n","Epoch 163:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-2.15, valid_loss=-2.05]\n","Epoch 164:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-2.15, valid_loss=-2.05]\n","Epoch 165:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.99, valid_loss=-2.05]\n","Epoch 166:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.09, valid_loss=-2.05]\n","Epoch 167:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.08, valid_loss=-2.05]\n","Epoch 168:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.02, valid_loss=-2.05]\n","Epoch 169:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.60, train_loss_epoch=-2.19, valid_loss=-2.05]\n","Epoch 170:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.10, valid_loss=-2.05]\n","Epoch 171:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-2.17, valid_loss=-2.05]\n","Epoch 172:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.47, train_loss_epoch=-2.20, valid_loss=-2.05]\n","Epoch 173:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-2.01, valid_loss=-2.05]\n","Epoch 174:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-2.15, valid_loss=-2.05]\n","Epoch 174: 100%|██████████| 4/4 [00:00\u003c00:00, 14.40it/s, v_num=0, train_loss_step=-2.41, train_loss_epoch=-2.15, valid_loss=-2.05]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10824)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10824)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.41it/s]\u001b[A\n","Epoch 175:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.41, train_loss_epoch=-2.17, valid_loss=-2.04]\n","Epoch 176:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.09, valid_loss=-2.04]\n","Epoch 177:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.10, valid_loss=-2.04]\n","Epoch 178:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.08, valid_loss=-2.04]\n","Epoch 179:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.02, valid_loss=-2.04]\n","Epoch 180:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.06, valid_loss=-2.04]\n","Epoch 181:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.41, train_loss_epoch=-2.17, valid_loss=-2.04]\n","Epoch 182:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-2.18, valid_loss=-2.04]\n","Epoch 183:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.09, valid_loss=-2.04]\n","Epoch 184:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.07, valid_loss=-2.04]\n","Epoch 185:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-2.02, valid_loss=-2.04]\n","Epoch 186:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-2.15, valid_loss=-2.04]\n","Epoch 187:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.98, valid_loss=-2.04]\n","Epoch 188:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.07, valid_loss=-2.04]\n","Epoch 189:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.96, valid_loss=-2.04]\n","Epoch 190:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.06, valid_loss=-2.04]\n","Epoch 191:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.36, train_loss_epoch=-1.95, valid_loss=-2.04]\n","Epoch 192:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.13, valid_loss=-2.04]\n","Epoch 193:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.07, valid_loss=-2.04]\n","Epoch 194:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.08, valid_loss=-2.04]\n","Epoch 195:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.16, valid_loss=-2.04]\n","Epoch 196:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.69, train_loss_epoch=-2.24, valid_loss=-2.04]\n","Epoch 197:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.09, valid_loss=-2.04]\n","Epoch 198:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.10, valid_loss=-2.04]\n","Epoch 199:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-2.03, valid_loss=-2.04]\n","Epoch 199: 100%|██████████| 4/4 [00:00\u003c00:00, 13.96it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.03, valid_loss=-2.04]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10824)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10824)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.49it/s]\u001b[A\n","Epoch 200:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.04, valid_loss=-2.06]\n","Epoch 201:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.10, valid_loss=-2.06]\n","Epoch 202:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.10, valid_loss=-2.06]\n","Epoch 203:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-2.05, valid_loss=-2.06]\n","Epoch 204:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-2.02, valid_loss=-2.06]\n","Epoch 205:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-2.19, valid_loss=-2.06]\n","Epoch 206:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.51, train_loss_epoch=-2.18, valid_loss=-2.06]\n","Epoch 207:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.45, train_loss_epoch=-2.19, valid_loss=-2.06]\n","Epoch 208:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-2.07, valid_loss=-2.06]\n","Epoch 209:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.89, valid_loss=-2.06]\n","Epoch 210:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.11, valid_loss=-2.06]\n","Epoch 211:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.09, valid_loss=-2.06]\n","Epoch 212:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-2.03, valid_loss=-2.06]\n","Epoch 213:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-2.05, valid_loss=-2.06]\n","Epoch 214:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.11, valid_loss=-2.06]\n","Epoch 215:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-2.19, valid_loss=-2.06]\n","Epoch 216:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.12, valid_loss=-2.06]\n","Epoch 217:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.46, train_loss_epoch=-2.21, valid_loss=-2.06]\n","Epoch 218:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.99, valid_loss=-2.06]\n","Epoch 219:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.10, valid_loss=-2.06]\n","Epoch 220:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.97, valid_loss=-2.06]\n","Epoch 221:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.15, valid_loss=-2.06]\n","Epoch 222:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-2.07, valid_loss=-2.06]\n","Epoch 223:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.47, train_loss_epoch=-2.19, valid_loss=-2.06]\n","Epoch 224:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.13, valid_loss=-2.06]\n","Epoch 224: 100%|██████████| 4/4 [00:00\u003c00:00, 13.63it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-2.13, valid_loss=-2.06]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10824)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10824)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.36it/s]\u001b[A\n","Epoch 225:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.88, valid_loss=-2.05]\n","Epoch 226:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.09, valid_loss=-2.05]\n","Epoch 227:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.41, train_loss_epoch=-2.22, valid_loss=-2.05]\n","Epoch 228:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.09, valid_loss=-2.05]\n","Epoch 229:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.95, valid_loss=-2.05]\n","Epoch 230:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-2.06, valid_loss=-2.05]\n","Epoch 231:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.98, valid_loss=-2.05]\n","Epoch 232:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-2.07, valid_loss=-2.05]\n","Epoch 233:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.11, valid_loss=-2.05]\n","Epoch 234:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.09, valid_loss=-2.05]\n","Epoch 235:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.19, valid_loss=-2.05]\n","Epoch 236:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.12, valid_loss=-2.05]\n","Epoch 237:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.93, valid_loss=-2.05]\n","Epoch 238:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.12, valid_loss=-2.05]\n","Epoch 239:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.99, valid_loss=-2.05]\n","Epoch 240:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.09, valid_loss=-2.05]\n","Epoch 241:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.18, valid_loss=-2.05]\n","Epoch 242:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.10, valid_loss=-2.05]\n","Epoch 243:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.12, valid_loss=-2.05]\n","Epoch 244:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.13, valid_loss=-2.05]\n","Epoch 245:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-2.06, valid_loss=-2.05]\n","Epoch 246:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.14, valid_loss=-2.05]\n","Epoch 247:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-2.05, valid_loss=-2.05]\n","Epoch 248:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.09, valid_loss=-2.05]\n","Epoch 249:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.56, train_loss_epoch=-2.22, valid_loss=-2.05]\n","Epoch 249: 100%|██████████| 4/4 [00:00\u003c00:00, 13.96it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-2.22, valid_loss=-2.05]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10824)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10824)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.56it/s]\u001b[A\n","Epoch 250:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-2.19, valid_loss=-2.06]\n","Epoch 251:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-2.01, valid_loss=-2.06]\n","Epoch 252:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-2.00, valid_loss=-2.06]\n","Epoch 253:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.11, valid_loss=-2.06]\n","Epoch 254:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-2.00, valid_loss=-2.06]\n","Epoch 255:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.05, valid_loss=-2.06]\n","Epoch 256:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-2.14, valid_loss=-2.06]\n","Epoch 257:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.20, valid_loss=-2.06]\n","Epoch 258:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.52, train_loss_epoch=-2.26, valid_loss=-2.06]\n","Epoch 259:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-2.02, valid_loss=-2.06]\n","Epoch 260:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-2.18, valid_loss=-2.06]\n","Epoch 261:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.07, valid_loss=-2.06]\n","Epoch 262:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-2.20, valid_loss=-2.06]\n","Epoch 263:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.90, valid_loss=-2.06]\n","Epoch 264:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.11, valid_loss=-2.06]\n","Epoch 265:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.69, train_loss_epoch=-2.23, valid_loss=-2.06]\n","Epoch 266:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.48, train_loss_epoch=-2.21, valid_loss=-2.06]\n","Epoch 267:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-2.07, valid_loss=-2.06]\n","Epoch 268:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.97, valid_loss=-2.06]\n","Epoch 269:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.57, train_loss_epoch=-2.22, valid_loss=-2.06]\n","Epoch 270:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.11, valid_loss=-2.06]\n","Epoch 271:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-2.08, valid_loss=-2.06]\n","Epoch 272:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-2.04, valid_loss=-2.06]\n","Epoch 273:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-2.07, valid_loss=-2.06]\n","Epoch 274:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-2.02, valid_loss=-2.06]\n","Epoch 274: 100%|██████████| 4/4 [00:00\u003c00:00, 14.05it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-2.02, valid_loss=-2.06]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10824)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10824)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.17it/s]\u001b[A\n","Epoch 275:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.94, valid_loss=-2.07]\n","Epoch 276:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.16, valid_loss=-2.07]\n","Epoch 277:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.06, valid_loss=-2.07]\n","Epoch 278:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.12, valid_loss=-2.07]\n","Epoch 279:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-2.09, valid_loss=-2.07]\n","Epoch 280:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.23, valid_loss=-2.07]\n","Epoch 281:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.16, valid_loss=-2.07]\n","Epoch 282:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-2.14, valid_loss=-2.07]\n","Epoch 283:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-2.17, valid_loss=-2.07]\n","Epoch 284:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.12, valid_loss=-2.07]\n","Epoch 285:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.14, valid_loss=-2.07]\n","Epoch 286:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-2.07, valid_loss=-2.07]\n","Epoch 287:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.48, train_loss_epoch=-2.24, valid_loss=-2.07]\n","Epoch 288:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.20, valid_loss=-2.07]\n","Epoch 289:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-2.18, valid_loss=-2.07]\n","Epoch 290:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.21, valid_loss=-2.07]\n","Epoch 290: 100%|██████████| 4/4 [00:00\u003c00:00, 14.19it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.21, valid_loss=-2.07]\n","Epoch 291:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.15, valid_loss=-2.07]\n","Epoch 292:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-2.05, valid_loss=-2.07]\n","Epoch 293:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.11, valid_loss=-2.07]\n","Epoch 294:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-2.23, valid_loss=-2.07]\n","Epoch 295:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.54, train_loss_epoch=-2.19, valid_loss=-2.07]\n","Epoch 296:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-2.18, valid_loss=-2.07]\n","Epoch 297:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.16, valid_loss=-2.07]\n","Epoch 298:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-2.06, valid_loss=-2.07]\n","Epoch 299:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.18, valid_loss=-2.07]\n","Epoch 299: 100%|██████████| 4/4 [00:00\u003c00:00, 14.37it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-2.18, valid_loss=-2.07]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10824)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10824)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.42it/s]\u001b[A\n","Epoch 300:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-2.19, valid_loss=-2.04]\n","Epoch 301:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.12, valid_loss=-2.04]\n","Epoch 302:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.12, valid_loss=-2.04]\n","Epoch 303:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.15, valid_loss=-2.04]\n","Epoch 304:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.57, train_loss_epoch=-2.27, valid_loss=-2.04]\n","Epoch 305:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.15, valid_loss=-2.04]\n","Epoch 305: 100%|██████████| 4/4 [00:00\u003c00:00, 13.94it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.18, valid_loss=-2.04]\n","Epoch 306:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.18, valid_loss=-2.04]\n","Epoch 307:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.14, valid_loss=-2.04]\n","Epoch 308:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.97, valid_loss=-2.04]\n","Epoch 309:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-2.17, valid_loss=-2.04]\n","Epoch 310:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.08, valid_loss=-2.04]\n","Epoch 311:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.53, train_loss_epoch=-2.26, valid_loss=-2.04]\n","Epoch 312:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.48, train_loss_epoch=-2.23, valid_loss=-2.04]\n","Epoch 313:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.16, valid_loss=-2.04]\n","Epoch 314:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.40, train_loss_epoch=-2.20, valid_loss=-2.04]\n","Epoch 315:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.17, valid_loss=-2.04]\n","Epoch 316:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-2.17, valid_loss=-2.04]\n","Epoch 317:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.60, train_loss_epoch=-2.26, valid_loss=-2.04]\n","Epoch 318:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-2.07, valid_loss=-2.04]\n","Epoch 319:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.13, valid_loss=-2.04]\n","Epoch 320:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-2.17, valid_loss=-2.04]\n","Epoch 321:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.13, valid_loss=-2.04]\n","Epoch 322:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-2.19, valid_loss=-2.04]\n","Epoch 323:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.40, train_loss_epoch=-2.21, valid_loss=-2.04]\n","Epoch 324:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-2.21, valid_loss=-2.04]\n","Epoch 324: 100%|██████████| 4/4 [00:00\u003c00:00, 14.08it/s, v_num=0, train_loss_step=-2.58, train_loss_epoch=-2.21, valid_loss=-2.04]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10824)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 18:42:21,310\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (1, 1, 1), 'n_pool_kernel_size': (16, 8, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=10824)\u001b[0m `Trainer.fit` stopped: `max_steps=1300.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=10824)\u001b[0m \n","\u001b[36m(_train_tune pid=10824)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.64it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10824)\u001b[0m \r                                                                      \u001b[A\rEpoch 324: 100%|██████████| 4/4 [00:00\u003c00:00,  6.32it/s, v_num=0, train_loss_step=-2.58, train_loss_epoch=-2.21, valid_loss=-2.05]\rEpoch 324: 100%|██████████| 4/4 [00:00\u003c00:00,  6.31it/s, v_num=0, train_loss_step=-2.58, train_loss_epoch=-2.24, valid_loss=-2.05]\rEpoch 324: 100%|██████████| 4/4 [00:00\u003c00:00,  6.30it/s, v_num=0, train_loss_step=-2.58, train_loss_epoch=-2.24, valid_loss=-2.05]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=11343)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=11343)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=11343)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=11343)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=11343)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=11343)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=11343)\u001b[0m 2024-11-12 18:42:29.153289: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=11343)\u001b[0m 2024-11-12 18:42:29.174824: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=11343)\u001b[0m 2024-11-12 18:42:29.202316: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=11343)\u001b[0m 2024-11-12 18:42:29.210676: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=11343)\u001b[0m 2024-11-12 18:42:29.230539: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=11343)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=11343)\u001b[0m 2024-11-12 18:42:30.404117: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=11343)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=11343)\u001b[0m \n","\u001b[36m(_train_tune pid=11343)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=11343)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=11343)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=11343)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=11343)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=11343)\u001b[0m 3 | blocks       | ModuleList       | 2.6 M  | train\n","\u001b[36m(_train_tune pid=11343)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=11343)\u001b[0m 2.6 M     Trainable params\n","\u001b[36m(_train_tune pid=11343)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=11343)\u001b[0m 2.6 M     Total params\n","\u001b[36m(_train_tune pid=11343)\u001b[0m 10.520    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=11343)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=11343)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/7 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.50]\n","Epoch 2:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-1.62]\n","Epoch 3:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.62]\n","Epoch 4:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.56]\n","Epoch 5:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.69]\n","Epoch 6:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.67]\n","Epoch 7:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.74]\n","Epoch 8:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.75]\n","Epoch 9:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.84]\n","Epoch 10:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.78]\n","Epoch 11:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-1.92]\n","Epoch 12:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.83]\n","Epoch 13:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.82]\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.84]\n","\u001b[36m(_train_tune pid=11343)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11343)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.52it/s]\u001b[A\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.84, valid_loss=-1.86]\n","Epoch 15:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.85, valid_loss=-1.86]\n","Epoch 16:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-1.93, valid_loss=-1.86]\n","Epoch 17:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.87, valid_loss=-1.86]\n","Epoch 18:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.92, valid_loss=-1.86]\n","Epoch 19:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.42, train_loss_epoch=-1.98, valid_loss=-1.86]\n","Epoch 20:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.91, valid_loss=-1.86]\n","Epoch 21:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.95, valid_loss=-1.86]\n","Epoch 22:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.94, valid_loss=-1.86]\n","Epoch 23:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.95, valid_loss=-1.86]\n","Epoch 24:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.95, valid_loss=-1.86]\n","Epoch 25:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-1.98, valid_loss=-1.86]\n","Epoch 26:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.00, valid_loss=-1.86]\n","Epoch 27:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.97, valid_loss=-1.86]\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.96, valid_loss=-1.86]\n","\u001b[36m(_train_tune pid=11343)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11343)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.39it/s]\u001b[A\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.96, valid_loss=-1.95]\n","Epoch 28: 100%|██████████| 7/7 [00:00\u003c00:00, 13.05it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.96, valid_loss=-1.95]\n","Epoch 28: 100%|██████████| 7/7 [00:00\u003c00:00, 13.00it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.89, valid_loss=-1.95]\n","Epoch 29:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.89, valid_loss=-1.95]\n","Epoch 30:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-2.05, valid_loss=-1.95]\n","Epoch 31:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.92, valid_loss=-1.95]\n","Epoch 32:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.95, valid_loss=-1.95]\n","Epoch 33:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.98, valid_loss=-1.95]\n","Epoch 34:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.96, valid_loss=-1.95]\n","Epoch 35:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.01, valid_loss=-1.95]\n","Epoch 36:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.98, valid_loss=-1.95]\n","Epoch 37:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.52, train_loss_epoch=-2.06, valid_loss=-1.95]\n","Epoch 38:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.99, valid_loss=-1.95]\n","Epoch 39:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.96, valid_loss=-1.95]\n","Epoch 40:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.01, valid_loss=-1.95]\n","Epoch 41:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.47, train_loss_epoch=-2.04, valid_loss=-1.95]\n","Epoch 42:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.02, valid_loss=-1.95]\n","\u001b[36m(_train_tune pid=11343)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11343)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 27.05it/s]\u001b[A\n","Epoch 43:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.95, valid_loss=-1.97]\n","Epoch 44:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.99, valid_loss=-1.97]\n","Epoch 45:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.98, valid_loss=-1.97]\n","Epoch 46:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.99, valid_loss=-1.97]\n","Epoch 47:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.03, valid_loss=-1.97]\n","Epoch 48:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.95, valid_loss=-1.97]\n","Epoch 49:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.99, valid_loss=-1.97]\n","Epoch 50:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.98, valid_loss=-1.97]\n","Epoch 51:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.03, valid_loss=-1.97]\n","Epoch 52:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.02, valid_loss=-1.97]\n","Epoch 53:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.93, valid_loss=-1.97]\n","Epoch 54:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.02, valid_loss=-1.97]\n","Epoch 55:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.03, valid_loss=-1.97]\n","Epoch 56:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.05, valid_loss=-1.97]\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.92, valid_loss=-1.97]\n","\u001b[36m(_train_tune pid=11343)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11343)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 27.19it/s]\u001b[A\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.92, valid_loss=-1.97]\n","Epoch 58:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-2.06, valid_loss=-1.97]\n","Epoch 59:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.00, valid_loss=-1.97]\n","Epoch 60:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.04, valid_loss=-1.97]\n","Epoch 61:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-2.00, valid_loss=-1.97]\n","Epoch 62:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.02, valid_loss=-1.97]\n","Epoch 63:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.04, valid_loss=-1.97]\n","Epoch 64:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.99, valid_loss=-1.97]\n","Epoch 65:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.00, valid_loss=-1.97]\n","Epoch 66:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.99, valid_loss=-1.97]\n","Epoch 67:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 68:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.06, valid_loss=-1.97]\n","Epoch 69:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.99, valid_loss=-1.97]\n","Epoch 70:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.98, valid_loss=-1.97]\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.93, valid_loss=-1.97]\n","\u001b[36m(_train_tune pid=11343)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11343)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.86it/s]\u001b[A\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.93, valid_loss=-1.98]\n","Epoch 72:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.02, valid_loss=-1.98]\n","Epoch 73:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.07, valid_loss=-1.98]\n","Epoch 74:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-2.07, valid_loss=-1.98]\n","Epoch 75:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.04, valid_loss=-1.98]\n","Epoch 76:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.05, valid_loss=-1.98]\n","Epoch 77:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.99, valid_loss=-1.98]\n","Epoch 78:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-2.07, valid_loss=-1.98]\n","Epoch 79:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.07, valid_loss=-1.98]\n","Epoch 80:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-2.07, valid_loss=-1.98]\n","Epoch 81:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.97, valid_loss=-1.98]\n","Epoch 82:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-2.03, valid_loss=-1.98]\n","Epoch 83:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.94, valid_loss=-1.98]\n","Epoch 84:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.04, valid_loss=-1.98]\n","Epoch 85:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.04, valid_loss=-1.98]\n","\u001b[36m(_train_tune pid=11343)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11343)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.57it/s]\u001b[A\n","Epoch 85:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.04, valid_loss=-1.99]\n","Epoch 86:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.05, valid_loss=-1.99]\n","Epoch 87:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.67, train_loss_epoch=-2.13, valid_loss=-1.99]\n","Epoch 88:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-2.10, valid_loss=-1.99]\n","Epoch 89:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.04, valid_loss=-1.99]\n","Epoch 90:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.04, valid_loss=-1.99]\n","Epoch 91:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.06, valid_loss=-1.99]\n","Epoch 92:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-2.04, valid_loss=-1.99]\n","Epoch 93:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 94:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.05, valid_loss=-1.99]\n","Epoch 95:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-2.00, valid_loss=-1.99]\n","Epoch 96:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-2.07, valid_loss=-1.99]\n","Epoch 97:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.07, valid_loss=-1.99]\n","Epoch 98:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-2.07, valid_loss=-1.99]\n","Epoch 99:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-2.04, valid_loss=-1.99]\n","Epoch 99: 100%|██████████| 7/7 [00:00\u003c00:00, 29.39it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.04, valid_loss=-1.99]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11343)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.88it/s]\u001b[A\n","Epoch 100:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.11, valid_loss=-1.98]\n","Epoch 101:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.07, valid_loss=-1.98]\n","Epoch 102:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-2.03, valid_loss=-1.98]\n","Epoch 103:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.05, valid_loss=-1.98]\n","Epoch 104:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-2.11, valid_loss=-1.98]\n","Epoch 105:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.99, valid_loss=-1.98]\n","Epoch 106:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-2.00, valid_loss=-1.98]\n","Epoch 107:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.07, valid_loss=-1.98]\n","Epoch 108:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.41, train_loss_epoch=-2.12, valid_loss=-1.98]\n","Epoch 109:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.05, valid_loss=-1.98]\n","Epoch 110:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.97, valid_loss=-1.98]\n","Epoch 111:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.08, valid_loss=-1.98]\n","Epoch 112:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.07, valid_loss=-1.98]\n","Epoch 112: 100%|██████████| 7/7 [00:00\u003c00:00, 29.34it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.07, valid_loss=-1.98]\n","Epoch 113:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.07, valid_loss=-1.98]\n","Epoch 114:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.56, train_loss_epoch=-2.11, valid_loss=-1.98]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11343)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11343)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.68it/s]\u001b[A\n","Epoch 114:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.11, valid_loss=-1.99]\n","Epoch 115:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 116:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.97, valid_loss=-1.99]\n","Epoch 117:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.45, train_loss_epoch=-2.12, valid_loss=-1.99]\n","Epoch 118:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-2.01, valid_loss=-1.99]\n","Epoch 119:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.07, valid_loss=-1.99]\n","Epoch 120:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.05, valid_loss=-1.99]\n","Epoch 121:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.04, valid_loss=-1.99]\n","Epoch 122:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.06, valid_loss=-1.99]\n","Epoch 123:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.05, valid_loss=-1.99]\n","Epoch 124:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.06, valid_loss=-1.99]\n","Epoch 125:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.08, valid_loss=-1.99]\n","Epoch 126:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.07, valid_loss=-1.99]\n","Epoch 127:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.98, valid_loss=-1.99]\n","Epoch 128:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.04, valid_loss=-1.99]\n","\u001b[36m(_train_tune pid=11343)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11343)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.45it/s]\u001b[A\n","Epoch 128:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.04, valid_loss=-2.00]\n","Epoch 129:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.07, valid_loss=-2.00]\n","Epoch 130:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-2.03, valid_loss=-2.00]\n","Epoch 131:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.08, valid_loss=-2.00]\n","Epoch 132:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.04, valid_loss=-2.00]\n","Epoch 133:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-2.03, valid_loss=-2.00]\n","Epoch 134:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.71, train_loss_epoch=-2.13, valid_loss=-2.00]\n","Epoch 135:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-2.04, valid_loss=-2.00]\n","Epoch 136:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.51, train_loss_epoch=-2.12, valid_loss=-2.00]\n","Epoch 137:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.09, valid_loss=-2.00]\n","Epoch 138:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-2.02, valid_loss=-2.00]\n","Epoch 139:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.08, valid_loss=-2.00]\n","Epoch 140:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.99, valid_loss=-2.00]\n","Epoch 141:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.36, train_loss_epoch=-2.11, valid_loss=-2.00]\n","Epoch 142:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.08, valid_loss=-2.00]\n","\u001b[36m(_train_tune pid=11343)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11343)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.93it/s]\u001b[A\n","Epoch 142: 100%|██████████| 7/7 [00:00\u003c00:00, 13.11it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.08, valid_loss=-1.98]\n","Epoch 142:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.06, valid_loss=-1.98]        \n","Epoch 143:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.06, valid_loss=-1.98]\n","Epoch 144:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.09, valid_loss=-1.98]\n","Epoch 145:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.09, valid_loss=-1.98]\n","Epoch 146:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.06, valid_loss=-1.98]\n","Epoch 147:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.41, train_loss_epoch=-2.11, valid_loss=-1.98]\n","Epoch 148:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.36, train_loss_epoch=-2.10, valid_loss=-1.98]\n","Epoch 149:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.04, valid_loss=-1.98]\n","Epoch 150:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-2.02, valid_loss=-1.98]\n","Epoch 151:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-2.05, valid_loss=-1.98]\n","Epoch 152:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.69, train_loss_epoch=-2.16, valid_loss=-1.98]\n","Epoch 153:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.99, valid_loss=-1.98]\n","Epoch 154:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-2.03, valid_loss=-1.98]\n","Epoch 155:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.09, valid_loss=-1.98]\n","Epoch 156:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-2.01, valid_loss=-1.98]\n","Epoch 157:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.52, train_loss_epoch=-2.15, valid_loss=-1.98]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11343)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.82it/s]\u001b[A\n","Epoch 157:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.15, valid_loss=-1.99]\n","Epoch 158:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-2.02, valid_loss=-1.99]\n","Epoch 159:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-2.05, valid_loss=-1.99]\n","Epoch 160:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.04, valid_loss=-1.99]\n","Epoch 161:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 162:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 163:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.72, train_loss_epoch=-2.16, valid_loss=-1.99]\n","Epoch 164:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.99, valid_loss=-1.99]\n","Epoch 165:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-2.02, valid_loss=-1.99]\n","Epoch 166:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-2.11, valid_loss=-1.99]\n","Epoch 167:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 168:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.07, valid_loss=-1.99]\n","Epoch 169:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 170:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.60, train_loss_epoch=-2.14, valid_loss=-1.99]\n","Epoch 171:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-2.04, valid_loss=-1.99]\n","\u001b[36m(_train_tune pid=11343)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11343)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.06it/s]\u001b[A\n","Epoch 171:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.04, valid_loss=-1.99]\n","Epoch 172:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-2.01, valid_loss=-1.99]\n","Epoch 173:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 174:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 175:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-2.11, valid_loss=-1.99]\n","Epoch 176:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-2.05, valid_loss=-1.99]\n","Epoch 177:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.06, valid_loss=-1.99]\n","Epoch 178:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.04, valid_loss=-1.99]\n","Epoch 179:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-2.02, valid_loss=-1.99]\n","Epoch 180:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.51, train_loss_epoch=-2.14, valid_loss=-1.99]\n","Epoch 181:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.99, valid_loss=-1.99]\n","Epoch 182:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.08, valid_loss=-1.99]\n","Epoch 183:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.07, valid_loss=-1.99]\n","Epoch 184:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.42, train_loss_epoch=-2.11, valid_loss=-1.99]\n","Epoch 184: 100%|██████████| 7/7 [00:00\u003c00:00, 29.72it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.11, valid_loss=-1.99]\n","Epoch 185:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.06, valid_loss=-1.99]\n","\u001b[36m(_train_tune pid=11343)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11343)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11343)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 27.07it/s]\u001b[A\n","Epoch 185:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.06, valid_loss=-1.98]\n","Epoch 186:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-2.06, valid_loss=-1.98]\n","Epoch 187:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.06, valid_loss=-1.98]\n","Epoch 188:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.99, valid_loss=-1.98]\n","Epoch 189:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-2.09, valid_loss=-1.98]\n","Epoch 190:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.05, valid_loss=-1.98]\n","Epoch 191:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.09, valid_loss=-1.98]\n","Epoch 192:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.08, valid_loss=-1.98]\n","Epoch 193:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.65, train_loss_epoch=-2.17, valid_loss=-1.98]\n","Epoch 194:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.05, valid_loss=-1.98]\n","Epoch 195:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.07, valid_loss=-1.98]\n","Epoch 196:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-2.02, valid_loss=-1.98]\n","Epoch 197:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.07, valid_loss=-1.98]\n","Epoch 197: 100%|██████████| 7/7 [00:00\u003c00:00, 28.75it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-2.12, valid_loss=-1.98]\n","Epoch 198:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-2.12, valid_loss=-1.98]\n","Epoch 199:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-2.11, valid_loss=-1.98]\n","Epoch 199: 100%|██████████| 7/7 [00:00\u003c00:00, 29.58it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.11, valid_loss=-1.98]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11343)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.96it/s]\u001b[A\n","Epoch 200:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.05, valid_loss=-1.99]\n","Epoch 201:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-2.04, valid_loss=-1.99]\n","Epoch 202:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.40, train_loss_epoch=-2.13, valid_loss=-1.99]\n","Epoch 203:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.49, train_loss_epoch=-2.12, valid_loss=-1.99]\n","Epoch 203: 100%|██████████| 7/7 [00:00\u003c00:00, 29.33it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-2.12, valid_loss=-1.99]\n","Epoch 204:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-2.13, valid_loss=-1.99]\n","Epoch 205:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.06, valid_loss=-1.99]\n","Epoch 206:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.99, valid_loss=-1.99]\n","Epoch 207:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.07, valid_loss=-1.99]\n","Epoch 208:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.09, valid_loss=-1.99]\n","Epoch 209:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 210:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-2.05, valid_loss=-1.99]\n","Epoch 211:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.10, valid_loss=-1.99]\n","Epoch 212:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-2.12, valid_loss=-1.99]\n","Epoch 213:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.08, valid_loss=-1.99]\n","Epoch 214:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-2.12, valid_loss=-1.99]\n","\u001b[36m(_train_tune pid=11343)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 18:43:28,861\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (40, 20, 1), 'n_pool_kernel_size': (2, 2, 2), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=11343)\u001b[0m \n","\u001b[36m(_train_tune pid=11343)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 31.46it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11343)\u001b[0m \n","\u001b[36m(_train_tune pid=11343)\u001b[0m \r                                                                      \u001b[A\rEpoch 214:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.12, valid_loss=-1.99]\rEpoch 214:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.10, valid_loss=-1.99]\rEpoch 214:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.10, valid_loss=-1.99]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=11343)\u001b[0m `Trainer.fit` stopped: `max_steps=1500.0` reached.\n","\u001b[36m(_train_tune pid=11686)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=11686)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=11686)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=11686)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=11686)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=11686)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=11686)\u001b[0m 2024-11-12 18:43:37.214306: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=11686)\u001b[0m 2024-11-12 18:43:37.235420: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=11686)\u001b[0m 2024-11-12 18:43:37.261106: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=11686)\u001b[0m 2024-11-12 18:43:37.268840: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=11686)\u001b[0m 2024-11-12 18:43:37.287164: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=11686)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=11686)\u001b[0m 2024-11-12 18:43:38.492773: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=11686)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=11686)\u001b[0m \n","\u001b[36m(_train_tune pid=11686)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=11686)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=11686)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=11686)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=11686)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=11686)\u001b[0m 3 | blocks       | ModuleList       | 2.5 M  | train\n","\u001b[36m(_train_tune pid=11686)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=11686)\u001b[0m 2.5 M     Trainable params\n","\u001b[36m(_train_tune pid=11686)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=11686)\u001b[0m 2.5 M     Total params\n","\u001b[36m(_train_tune pid=11686)\u001b[0m 9.942     Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=11686)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=11686)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/7 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-0.888]\n","Epoch 2:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-1.75]\n","Epoch 3:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.82]\n","Epoch 4:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.82]\n","Epoch 5:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.90]\n","Epoch 6:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.87]\n","Epoch 7:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.93]\n","Epoch 8:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.89]\n","Epoch 9:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-1.96]\n","Epoch 10:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.93]\n","Epoch 11:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.36, train_loss_epoch=-1.97]\n","Epoch 12:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.91]\n","Epoch 13:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.92]\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.93]\n","\u001b[36m(_train_tune pid=11686)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11686)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.69it/s]\u001b[A\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.93, valid_loss=-1.95]\n","Epoch 15:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.91, valid_loss=-1.95]\n","Epoch 16:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.40, train_loss_epoch=-2.03, valid_loss=-1.95]\n","Epoch 17:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.94, valid_loss=-1.95]\n","Epoch 18:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.97, valid_loss=-1.95]\n","Epoch 19:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.48, train_loss_epoch=-2.07, valid_loss=-1.95]\n","Epoch 20:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.93, valid_loss=-1.95]\n","Epoch 21:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.00, valid_loss=-1.95]\n","Epoch 22:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.98, valid_loss=-1.95]\n","Epoch 23:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.97, valid_loss=-1.95]\n","Epoch 24:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.97, valid_loss=-1.95]\n","Epoch 25:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.99, valid_loss=-1.95]\n","Epoch 26:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-2.02, valid_loss=-1.95]\n","Epoch 27:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.02, valid_loss=-1.95]\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.95, valid_loss=-1.95]\n","\u001b[36m(_train_tune pid=11686)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11686)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11686)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 27.86it/s]\u001b[A\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.95, valid_loss=-1.96]\n","Epoch 29:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.90, valid_loss=-1.96]\n","Epoch 30:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-2.02, valid_loss=-1.96]\n","Epoch 31:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.93, valid_loss=-1.96]\n","Epoch 32:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.96, valid_loss=-1.96]\n","Epoch 33:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.01, valid_loss=-1.96]\n","Epoch 34:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.98, valid_loss=-1.96]\n","Epoch 35:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.03, valid_loss=-1.96]\n","Epoch 36:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.97, valid_loss=-1.96]\n","Epoch 37:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.57, train_loss_epoch=-2.09, valid_loss=-1.96]\n","Epoch 38:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.99, valid_loss=-1.96]\n","Epoch 39:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.97, valid_loss=-1.96]\n","Epoch 40:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.02, valid_loss=-1.96]\n","Epoch 41:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.47, train_loss_epoch=-2.06, valid_loss=-1.96]\n","Epoch 42:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.02, valid_loss=-1.96]\n","\u001b[36m(_train_tune pid=11686)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11686)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.22it/s]\u001b[A\n","Epoch 43:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.97, valid_loss=-1.97]\n","Epoch 44:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.01, valid_loss=-1.97]\n","Epoch 45:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.01, valid_loss=-1.97]\n","Epoch 46:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.02, valid_loss=-1.97]\n","Epoch 47:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 48:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.98, valid_loss=-1.97]\n","Epoch 49:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.03, valid_loss=-1.97]\n","Epoch 50:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.97, valid_loss=-1.97]\n","Epoch 51:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.02, valid_loss=-1.97]\n","Epoch 51: 100%|██████████| 7/7 [00:00\u003c00:00, 33.25it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.02, valid_loss=-1.97]\n","Epoch 52:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.01, valid_loss=-1.97]\n","Epoch 52: 100%|██████████| 7/7 [00:00\u003c00:00, 33.41it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-2.01, valid_loss=-1.97]\n","Epoch 53:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.99, valid_loss=-1.97]\n","Epoch 54:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.00, valid_loss=-1.97]\n","Epoch 55:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.01, valid_loss=-1.97]\n","Epoch 56:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.03, valid_loss=-1.97]\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.91, valid_loss=-1.97]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11686)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11686)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 28.19it/s]\u001b[A\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.91, valid_loss=-1.96]\n","Epoch 58:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.03, valid_loss=-1.96]\n","Epoch 59:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-2.00, valid_loss=-1.96]\n","Epoch 60:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.01, valid_loss=-1.96]\n","Epoch 61:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-2.00, valid_loss=-1.96]\n","Epoch 62:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.03, valid_loss=-1.96]\n","Epoch 63:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.03, valid_loss=-1.96]\n","Epoch 64:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.99, valid_loss=-1.96]\n","Epoch 65:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-2.01, valid_loss=-1.96]\n","Epoch 66:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.01, valid_loss=-1.96]\n","Epoch 67:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.06, valid_loss=-1.96]\n","Epoch 68:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.03, valid_loss=-1.96]\n","Epoch 69:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-2.00, valid_loss=-1.96]\n","Epoch 70:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.99, valid_loss=-1.96]\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.05, train_loss_epoch=-1.90, valid_loss=-1.96]\n","\u001b[36m(_train_tune pid=11686)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11686)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 31.27it/s]\u001b[A\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.90, valid_loss=-1.98]\n","Epoch 72:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-2.00, valid_loss=-1.98]\n","Epoch 73:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.08, valid_loss=-1.98]\n","Epoch 74:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.42, train_loss_epoch=-2.12, valid_loss=-1.98]\n","Epoch 75:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.04, valid_loss=-1.98]\n","Epoch 76:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.09, valid_loss=-1.98]\n","Epoch 77:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-2.03, valid_loss=-1.98]\n","Epoch 78:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-2.08, valid_loss=-1.98]\n","Epoch 79:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.08, valid_loss=-1.98]\n","Epoch 80:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-2.09, valid_loss=-1.98]\n","Epoch 81:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-2.03, valid_loss=-1.98]\n","Epoch 82:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.02, valid_loss=-1.98]\n","Epoch 83:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.96, valid_loss=-1.98]\n","Epoch 84:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.04, valid_loss=-1.98]\n","Epoch 85:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.05, valid_loss=-1.98]\n","\u001b[36m(_train_tune pid=11686)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11686)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.41it/s]\u001b[A\n","Epoch 85:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.05, valid_loss=-2.01]\n","Epoch 86:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.06, valid_loss=-2.01]\n","Epoch 87:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.72, train_loss_epoch=-2.13, valid_loss=-2.01]\n","Epoch 88:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-2.10, valid_loss=-2.01]\n","Epoch 89:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.07, valid_loss=-2.01]\n","Epoch 90:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.10, valid_loss=-2.01]\n","Epoch 91:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.12, valid_loss=-2.01]\n","Epoch 92:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-2.03, valid_loss=-2.01]\n","Epoch 92:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.05, valid_loss=-2.01]        \n","Epoch 93:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.05, valid_loss=-2.01]\n","Epoch 94:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.10, valid_loss=-2.01]\n","Epoch 95:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-2.00, valid_loss=-2.01]\n","Epoch 96:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-2.11, valid_loss=-2.01]\n","Epoch 97:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.08, valid_loss=-2.01]\n","Epoch 98:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-2.09, valid_loss=-2.01]\n","Epoch 99:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.05, valid_loss=-2.01]\n","Epoch 99: 100%|██████████| 7/7 [00:00\u003c00:00, 30.08it/s, v_num=0, train_loss_step=-2.36, train_loss_epoch=-2.05, valid_loss=-2.01]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11686)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 30.65it/s]\u001b[A\n","Epoch 100:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.36, train_loss_epoch=-2.12, valid_loss=-1.99]\n","Epoch 101:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.08, valid_loss=-1.99]\n","Epoch 102:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-2.04, valid_loss=-1.99]\n","Epoch 103:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.10, valid_loss=-1.99]\n","Epoch 104:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.50, train_loss_epoch=-2.16, valid_loss=-1.99]\n","Epoch 105:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-2.02, valid_loss=-1.99]\n","Epoch 106:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 107:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.07, valid_loss=-1.99]\n","Epoch 108:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.40, train_loss_epoch=-2.10, valid_loss=-1.99]\n","Epoch 109:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.09, valid_loss=-1.99]\n","Epoch 110:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.98, valid_loss=-1.99]\n","Epoch 111:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-2.11, valid_loss=-1.99]\n","Epoch 112:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.08, valid_loss=-1.99]\n","Epoch 113:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.12, valid_loss=-1.99]\n","Epoch 114:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.52, train_loss_epoch=-2.14, valid_loss=-1.99]\n","\u001b[36m(_train_tune pid=11686)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11686)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.69it/s]\u001b[A\n","Epoch 114:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.14, valid_loss=-1.99]\n","Epoch 115:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.07, valid_loss=-1.99]\n","Epoch 116:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-2.02, valid_loss=-1.99]\n","Epoch 117:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-2.13, valid_loss=-1.99]\n","Epoch 118:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-2.04, valid_loss=-1.99]\n","Epoch 119:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-2.09, valid_loss=-1.99]\n","Epoch 120:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.08, valid_loss=-1.99]\n","Epoch 121:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.09, valid_loss=-1.99]\n","Epoch 122:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.10, valid_loss=-1.99]\n","Epoch 123:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-2.13, valid_loss=-1.99]\n","Epoch 124:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.13, valid_loss=-1.99]\n","Epoch 125:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-2.13, valid_loss=-1.99]\n","Epoch 126:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.11, valid_loss=-1.99]\n","Epoch 127:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-2.00, valid_loss=-1.99]\n","Epoch 128:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.09, valid_loss=-1.99]\n","\u001b[36m(_train_tune pid=11686)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11686)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11686)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 28.72it/s]\u001b[A\n","Epoch 128:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.09, valid_loss=-2.00]\n","Epoch 129:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.10, valid_loss=-2.00]\n","Epoch 130:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.07, valid_loss=-2.00]\n","Epoch 131:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.09, valid_loss=-2.00]\n","Epoch 132:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-2.05, valid_loss=-2.00]\n","Epoch 133:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.05, valid_loss=-2.00]\n","Epoch 134:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.71, train_loss_epoch=-2.20, valid_loss=-2.00]\n","Epoch 135:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-2.05, valid_loss=-2.00]\n","Epoch 136:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.52, train_loss_epoch=-2.15, valid_loss=-2.00]\n","Epoch 137:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.08, valid_loss=-2.00]\n","Epoch 138:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-2.04, valid_loss=-2.00]\n","Epoch 139:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.10, valid_loss=-2.00]\n","Epoch 140:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-2.03, valid_loss=-2.00]\n","Epoch 141:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-2.12, valid_loss=-2.00]\n","Epoch 142:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.12, valid_loss=-2.00]\n","\u001b[36m(_train_tune pid=11686)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11686)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11686)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 27.15it/s]\u001b[A\n","Epoch 143:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.09, valid_loss=-1.99]\n","Epoch 144:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.12, valid_loss=-1.99]\n","Epoch 145:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-2.16, valid_loss=-1.99]\n","Epoch 146:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.08, valid_loss=-1.99]\n","Epoch 147:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.57, train_loss_epoch=-2.17, valid_loss=-1.99]\n","Epoch 148:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-2.15, valid_loss=-1.99]\n","Epoch 149:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.13, valid_loss=-1.99]\n","Epoch 150:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-2.09, valid_loss=-1.99]\n","Epoch 151:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-2.09, valid_loss=-1.99]\n","Epoch 152:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.65, train_loss_epoch=-2.19, valid_loss=-1.99]\n","Epoch 153:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-2.05, valid_loss=-1.99]\n","Epoch 154:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-2.10, valid_loss=-1.99]\n","Epoch 155:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.14, valid_loss=-1.99]\n","Epoch 156:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-2.08, valid_loss=-1.99]\n","Epoch 157:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.56, train_loss_epoch=-2.18, valid_loss=-1.99]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11686)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.54it/s]\u001b[A\n","Epoch 157:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.18, valid_loss=-1.99]\n","Epoch 158:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-2.08, valid_loss=-1.99]\n","Epoch 159:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-2.07, valid_loss=-1.99]\n","Epoch 160:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.11, valid_loss=-1.99]\n","Epoch 161:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-2.11, valid_loss=-1.99]\n","Epoch 162:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-2.11, valid_loss=-1.99]\n","Epoch 163:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.69, train_loss_epoch=-2.21, valid_loss=-1.99]\n","Epoch 164:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-2.02, valid_loss=-1.99]\n","Epoch 165:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-2.07, valid_loss=-1.99]\n","Epoch 166:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-2.16, valid_loss=-1.99]\n","Epoch 167:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.07, valid_loss=-1.99]\n","Epoch 168:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.12, valid_loss=-1.99]\n","Epoch 169:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-2.11, valid_loss=-1.99]\n","Epoch 170:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.63, train_loss_epoch=-2.19, valid_loss=-1.99]\n","Epoch 171:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-2.12, valid_loss=-1.99]\n","\u001b[36m(_train_tune pid=11686)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11686)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 27.75it/s]\u001b[A\n","Epoch 171:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.12, valid_loss=-1.99]\n","Epoch 172:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-2.09, valid_loss=-1.99]\n","Epoch 173:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-2.06, valid_loss=-1.99]\n","Epoch 174:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.11, valid_loss=-1.99]\n","Epoch 175:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-2.16, valid_loss=-1.99]\n","Epoch 176:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.11, valid_loss=-1.99]\n","Epoch 177:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.14, valid_loss=-1.99]\n","Epoch 178:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-2.11, valid_loss=-1.99]\n","Epoch 179:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.10, valid_loss=-1.99]\n","Epoch 180:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.57, train_loss_epoch=-2.22, valid_loss=-1.99]\n","Epoch 181:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-2.05, valid_loss=-1.99]\n","Epoch 182:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.16, valid_loss=-1.99]\n","Epoch 183:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.15, valid_loss=-1.99]\n","Epoch 184:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.51, train_loss_epoch=-2.19, valid_loss=-1.99]\n","Epoch 185:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.13, valid_loss=-1.99]\n","\u001b[36m(_train_tune pid=11686)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11686)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 29.54it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11686)\u001b[0m \n","Epoch 186:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.10, valid_loss=-1.99]\n","Epoch 187:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.12, valid_loss=-1.99]\n","Epoch 188:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-2.01, valid_loss=-1.99]\n","Epoch 189:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.16, valid_loss=-1.99]\n","Epoch 189: 100%|██████████| 7/7 [00:00\u003c00:00, 32.35it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.16, valid_loss=-1.99]\n","Epoch 190:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.15, valid_loss=-1.99]\n","Epoch 191:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.15, valid_loss=-1.99]\n","Epoch 192:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.14, valid_loss=-1.99]\n","Epoch 193:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.74, train_loss_epoch=-2.24, valid_loss=-1.99]\n","Epoch 194:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.13, valid_loss=-1.99]\n","Epoch 195:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.15, valid_loss=-1.99]\n","Epoch 196:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-2.10, valid_loss=-1.99]\n","Epoch 196: 100%|██████████| 7/7 [00:00\u003c00:00, 31.23it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.14, valid_loss=-1.99]\n","Epoch 197:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.14, valid_loss=-1.99]\n","Epoch 198:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.51, train_loss_epoch=-2.19, valid_loss=-1.99]\n","Epoch 199:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-2.17, valid_loss=-1.99]\n","Epoch 199: 100%|██████████| 7/7 [00:00\u003c00:00, 31.04it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-2.17, valid_loss=-1.99]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11686)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 28.42it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11686)\u001b[0m \n","Epoch 200:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-2.13, valid_loss=-2.00]\n","Epoch 201:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-2.10, valid_loss=-2.00]\n","Epoch 202:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.57, train_loss_epoch=-2.22, valid_loss=-2.00]\n","Epoch 203:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.60, train_loss_epoch=-2.18, valid_loss=-2.00]\n","Epoch 204:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.60, train_loss_epoch=-2.20, valid_loss=-2.00]\n","Epoch 205:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.12, valid_loss=-2.00]\n","Epoch 206:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-2.05, valid_loss=-2.00]\n","Epoch 207:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.12, valid_loss=-2.00]\n","Epoch 208:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.16, valid_loss=-2.00]\n","Epoch 209:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-2.10, valid_loss=-2.00]\n","Epoch 209: 100%|██████████| 7/7 [00:00\u003c00:00, 30.97it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.09, valid_loss=-2.00]\n","Epoch 210:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.09, valid_loss=-2.00]\n","Epoch 211:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-2.19, valid_loss=-2.00]\n","Epoch 212:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-2.20, valid_loss=-2.00]\n","Epoch 213:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.14, valid_loss=-2.00]\n","Epoch 214:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.46, train_loss_epoch=-2.19, valid_loss=-2.00]\n","\u001b[36m(_train_tune pid=11686)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=11686)\u001b[0m `Trainer.fit` stopped: `max_steps=1500.0` reached.\n","2024-11-12 18:44:32,708\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (40, 20, 1), 'n_pool_kernel_size': (16, 8, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=11686)\u001b[0m \n","\u001b[36m(_train_tune pid=11686)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 29.22it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11686)\u001b[0m \n","\u001b[36m(_train_tune pid=11686)\u001b[0m \r                                                                      \u001b[A\rEpoch 214:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.19, valid_loss=-1.99]\n","\u001b[36m(_train_tune pid=11686)\u001b[0m \rEpoch 214:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.15, valid_loss=-1.99]\rEpoch 214:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.15, valid_loss=-1.99]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=12013)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=12013)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=12013)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=12013)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=12013)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=12013)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=12013)\u001b[0m 2024-11-12 18:44:41.182293: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=12013)\u001b[0m 2024-11-12 18:44:41.203748: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=12013)\u001b[0m 2024-11-12 18:44:41.231087: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=12013)\u001b[0m 2024-11-12 18:44:41.239453: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=12013)\u001b[0m 2024-11-12 18:44:41.258744: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=12013)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=12013)\u001b[0m 2024-11-12 18:44:42.460258: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=12013)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=12013)\u001b[0m \n","\u001b[36m(_train_tune pid=12013)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=12013)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=12013)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=12013)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=12013)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=12013)\u001b[0m 3 | blocks       | ModuleList       | 2.6 M  | train\n","\u001b[36m(_train_tune pid=12013)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=12013)\u001b[0m 2.6 M     Trainable params\n","\u001b[36m(_train_tune pid=12013)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=12013)\u001b[0m 2.6 M     Total params\n","\u001b[36m(_train_tune pid=12013)\u001b[0m 10.495    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=12013)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=12013)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/7 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-0.126]\n","Epoch 2:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-1.70]\n","Epoch 3:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.75]\n","Epoch 4:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.77]\n","Epoch 5:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.87]\n","Epoch 6:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.84]\n","Epoch 7:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.89]\n","Epoch 8:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.81]\n","Epoch 9:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-1.94]\n","Epoch 10:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.84]\n","Epoch 11:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.41, train_loss_epoch=-1.99]\n","Epoch 12:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.86]\n","Epoch 13:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.87]\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.86]\n","\u001b[36m(_train_tune pid=12013)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12013)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.63it/s]\u001b[A\n","Epoch 14:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.86, valid_loss=-1.92]\n","Epoch 15:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.89, valid_loss=-1.92]\n","Epoch 16:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-2.00, valid_loss=-1.92]\n","Epoch 17:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.92, valid_loss=-1.92]\n","Epoch 18:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.94, valid_loss=-1.92]\n","Epoch 19:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.46, train_loss_epoch=-2.02, valid_loss=-1.92]\n","Epoch 20:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.93, valid_loss=-1.92]\n","Epoch 21:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.96, valid_loss=-1.92]\n","Epoch 22:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.95, valid_loss=-1.92]\n","Epoch 23:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.96, valid_loss=-1.92]\n","Epoch 24:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.99, valid_loss=-1.92]\n","Epoch 25:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.99, valid_loss=-1.92]\n","Epoch 26:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.00, valid_loss=-1.92]\n","Epoch 27:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.01, valid_loss=-1.92]\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.96, valid_loss=-1.92]\n","\u001b[36m(_train_tune pid=12013)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12013)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.61it/s]\u001b[A\n","Epoch 28:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.96, valid_loss=-1.98]\n","Epoch 29:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.88, valid_loss=-1.98]\n","Epoch 30:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.36, train_loss_epoch=-2.00, valid_loss=-1.98]\n","Epoch 31:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.94, valid_loss=-1.98]\n","Epoch 32:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.92, valid_loss=-1.98]\n","Epoch 33:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.98, valid_loss=-1.98]\n","Epoch 34:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.97, valid_loss=-1.98]\n","Epoch 35:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.01, valid_loss=-1.98]\n","Epoch 36:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.96, valid_loss=-1.98]\n","Epoch 37:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.54, train_loss_epoch=-2.07, valid_loss=-1.98]\n","Epoch 38:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.95, valid_loss=-1.98]\n","Epoch 39:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.96, valid_loss=-1.98]\n","Epoch 40:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.00, valid_loss=-1.98]\n","Epoch 41:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.56, train_loss_epoch=-2.06, valid_loss=-1.98]\n","Epoch 42:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.02, valid_loss=-1.98]\n","\u001b[36m(_train_tune pid=12013)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12013)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.61it/s]\u001b[A\n","Epoch 43:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.94, valid_loss=-2.00]\n","Epoch 44:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.98, valid_loss=-2.00]\n","Epoch 45:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.99, valid_loss=-2.00]\n","Epoch 46:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.98, valid_loss=-2.00]\n","Epoch 47:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-2.04, valid_loss=-2.00]\n","Epoch 48:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.93, valid_loss=-2.00]\n","Epoch 49:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.00, valid_loss=-2.00]\n","Epoch 50:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.94, valid_loss=-2.00]\n","Epoch 51:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.00, valid_loss=-2.00]\n","Epoch 52:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.98, valid_loss=-2.00]\n","Epoch 53:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.93, valid_loss=-2.00]\n","Epoch 54:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.97, valid_loss=-2.00]\n","Epoch 55:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.03, valid_loss=-2.00]\n","Epoch 56:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.98, valid_loss=-2.00]\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.91, valid_loss=-2.00]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12013)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.71it/s]\u001b[A\n","Epoch 57:   0%|          | 0/7 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.91, valid_loss=-1.99]\n","Epoch 58:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-2.06, valid_loss=-1.99]\n","Epoch 59:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.98, valid_loss=-1.99]\n","Epoch 60:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 61:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.95, valid_loss=-1.99]\n","Epoch 62:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.05, valid_loss=-1.99]\n","Epoch 63:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.99, valid_loss=-1.99]\n","Epoch 64:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.97, valid_loss=-1.99]\n","Epoch 65:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-2.01, valid_loss=-1.99]\n","Epoch 66:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.00, valid_loss=-1.99]\n","Epoch 67:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.04, valid_loss=-1.99]\n","Epoch 68:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.99, valid_loss=-1.99]\n","Epoch 69:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.95, valid_loss=-1.99]\n","Epoch 70:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.96, valid_loss=-1.99]\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.09, train_loss_epoch=-1.90, valid_loss=-1.99]\n","\u001b[36m(_train_tune pid=12013)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12013)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.62it/s]\u001b[A\n","Epoch 71:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.90, valid_loss=-2.02]\n","Epoch 72:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.99, valid_loss=-2.02]\n","Epoch 73:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.04, valid_loss=-2.02]\n","Epoch 74:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-2.08, valid_loss=-2.02]\n","Epoch 75:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.07, valid_loss=-2.02]\n","Epoch 76:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.08, valid_loss=-2.02]\n","Epoch 77:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.99, valid_loss=-2.02]\n","Epoch 78:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.40, train_loss_epoch=-2.08, valid_loss=-2.02]\n","Epoch 79:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.03, valid_loss=-2.02]\n","Epoch 80:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.04, valid_loss=-2.02]\n","Epoch 81:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-2.00, valid_loss=-2.02]\n","Epoch 82:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-2.03, valid_loss=-2.02]\n","Epoch 83:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.92, valid_loss=-2.02]\n","Epoch 83: 100%|██████████| 7/7 [00:00\u003c00:00, 23.24it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.99, valid_loss=-2.02]\n","Epoch 84:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.99, valid_loss=-2.02]\n","Epoch 84: 100%|██████████| 7/7 [00:00\u003c00:00, 22.49it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.01, valid_loss=-2.02]\n","Epoch 85:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.01, valid_loss=-2.02]\n","\u001b[36m(_train_tune pid=12013)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12013)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.70it/s]\u001b[A\n","Epoch 86:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.05, valid_loss=-2.02]\n","Epoch 87:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.58, train_loss_epoch=-2.08, valid_loss=-2.02]\n","Epoch 88:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.08, valid_loss=-2.02]\n","Epoch 89:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.98, valid_loss=-2.02]\n","Epoch 90:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.03, valid_loss=-2.02]\n","Epoch 91:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.05, valid_loss=-2.02]\n","Epoch 92:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-2.00, valid_loss=-2.02]\n","Epoch 93:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.06, valid_loss=-2.02]\n","Epoch 94:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.99, valid_loss=-2.02]\n","Epoch 95:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.98, valid_loss=-2.02]\n","Epoch 96:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-2.06, valid_loss=-2.02]\n","Epoch 97:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.06, valid_loss=-2.02]\n","Epoch 98:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-2.09, valid_loss=-2.02]\n","Epoch 99:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.02, valid_loss=-2.02]\n","Epoch 99: 100%|██████████| 7/7 [00:00\u003c00:00, 23.53it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-2.02, valid_loss=-2.02]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12013)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.69it/s]\u001b[A\n","Epoch 100:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-2.08, valid_loss=-2.03]\n","Epoch 101:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.05, valid_loss=-2.03]\n","Epoch 102:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.98, valid_loss=-2.03]\n","Epoch 103:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.03, valid_loss=-2.03]\n","Epoch 104:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.53, train_loss_epoch=-2.13, valid_loss=-2.03]\n","Epoch 105:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-2.01, valid_loss=-2.03]\n","Epoch 106:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.95, valid_loss=-2.03]\n","Epoch 107:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.06, valid_loss=-2.03]\n","Epoch 107: 100%|██████████| 7/7 [00:00\u003c00:00, 23.82it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-2.05, valid_loss=-2.03]\n","Epoch 108:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-2.05, valid_loss=-2.03]\n","Epoch 109:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.02, valid_loss=-2.03]\n","Epoch 110:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.95, valid_loss=-2.03]\n","Epoch 111:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-2.09, valid_loss=-2.03]\n","Epoch 112:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.01, valid_loss=-2.03]\n","Epoch 113:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.04, valid_loss=-2.03]\n","Epoch 114:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.56, train_loss_epoch=-2.14, valid_loss=-2.03]\n","\u001b[36m(_train_tune pid=12013)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12013)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.68it/s]\u001b[A\n","Epoch 114:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.14, valid_loss=-2.04]\n","Epoch 115:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.00, valid_loss=-2.04]\n","Epoch 116:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.96, valid_loss=-2.04]\n","Epoch 117:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.40, train_loss_epoch=-2.09, valid_loss=-2.04]\n","Epoch 118:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-2.00, valid_loss=-2.04]\n","Epoch 119:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.04, valid_loss=-2.04]\n","Epoch 120:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.06, valid_loss=-2.04]\n","Epoch 121:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.02, valid_loss=-2.04]\n","Epoch 122:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.04, valid_loss=-2.04]\n","Epoch 123:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.07, valid_loss=-2.04]\n","Epoch 124:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.06, valid_loss=-2.04]\n","Epoch 125:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.06, valid_loss=-2.04]\n","Epoch 126:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.08, valid_loss=-2.04]\n","Epoch 127:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.98, valid_loss=-2.04]\n","Epoch 128:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.04, valid_loss=-2.04]\n","\u001b[36m(_train_tune pid=12013)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12013)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.69it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12013)\u001b[0m \n","Epoch 128:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.04, valid_loss=-2.03]\n","Epoch 129:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.05, valid_loss=-2.03]\n","Epoch 130:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.04, valid_loss=-2.03]\n","Epoch 131:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.04, valid_loss=-2.03]\n","Epoch 132:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-2.00, valid_loss=-2.03]\n","Epoch 133:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-2.03, valid_loss=-2.03]\n","Epoch 134:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.57, train_loss_epoch=-2.14, valid_loss=-2.03]\n","Epoch 135:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-2.04, valid_loss=-2.03]\n","Epoch 136:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.50, train_loss_epoch=-2.13, valid_loss=-2.03]\n","Epoch 137:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.05, valid_loss=-2.03]\n","Epoch 138:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-2.03, valid_loss=-2.03]\n","Epoch 139:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.06, valid_loss=-2.03]\n","Epoch 140:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.97, valid_loss=-2.03]\n","Epoch 141:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-2.08, valid_loss=-2.03]\n","Epoch 142:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.08, valid_loss=-2.03]\n","\u001b[36m(_train_tune pid=12013)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12013)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.64it/s]\u001b[A\n","Epoch 143:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.01, valid_loss=-2.06]\n","Epoch 144:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.05, valid_loss=-2.06]\n","Epoch 145:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-2.09, valid_loss=-2.06]\n","Epoch 146:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-2.04, valid_loss=-2.06]\n","Epoch 147:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-2.11, valid_loss=-2.06]\n","Epoch 148:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-2.11, valid_loss=-2.06]\n","Epoch 149:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-2.05, valid_loss=-2.06]\n","Epoch 150:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-2.02, valid_loss=-2.06]\n","Epoch 151:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-2.01, valid_loss=-2.06]\n","Epoch 152:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.56, train_loss_epoch=-2.13, valid_loss=-2.06]\n","Epoch 153:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.99, valid_loss=-2.06]\n","Epoch 154:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.02, valid_loss=-2.06]\n","Epoch 155:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.05, valid_loss=-2.06]\n","Epoch 156:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-2.00, valid_loss=-2.06]\n","Epoch 157:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.47, train_loss_epoch=-2.09, valid_loss=-2.06]\n","\u001b[36m(_train_tune pid=12013)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12013)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.66it/s]\u001b[A\n","Epoch 157:   0%|          | 0/7 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.09, valid_loss=-2.06]\n","Epoch 158:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-2.01, valid_loss=-2.06]\n","Epoch 159:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-2.01, valid_loss=-2.06]\n","Epoch 160:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.02, valid_loss=-2.06]\n","Epoch 161:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-2.02, valid_loss=-2.06]\n","Epoch 162:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-2.02, valid_loss=-2.06]\n","Epoch 163:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.71, train_loss_epoch=-2.14, valid_loss=-2.06]\n","Epoch 164:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-2.02, valid_loss=-2.06]\n","Epoch 165:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.02, valid_loss=-2.06]\n","Epoch 165: 100%|██████████| 7/7 [00:00\u003c00:00, 22.31it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.08, valid_loss=-2.06]\n","Epoch 166:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.08, valid_loss=-2.06]\n","Epoch 167:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-2.04, valid_loss=-2.06]\n","Epoch 168:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.04, valid_loss=-2.06]\n","Epoch 169:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-2.03, valid_loss=-2.06]\n","Epoch 170:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.60, train_loss_epoch=-2.12, valid_loss=-2.06]\n","Epoch 171:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-2.04, valid_loss=-2.06]\n","\u001b[36m(_train_tune pid=12013)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12013)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.70it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12013)\u001b[0m \n","Epoch 171:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.04, valid_loss=-2.06]\n","Epoch 172:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.98, valid_loss=-2.06]\n","Epoch 173:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-2.02, valid_loss=-2.06]\n","Epoch 174:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-2.02, valid_loss=-2.06]\n","Epoch 175:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.09, valid_loss=-2.06]\n","Epoch 176:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.04, valid_loss=-2.06]\n","Epoch 177:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.05, valid_loss=-2.06]\n","Epoch 178:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-2.06, valid_loss=-2.06]\n","Epoch 179:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.99, valid_loss=-2.06]\n","Epoch 180:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.49, train_loss_epoch=-2.12, valid_loss=-2.06]\n","Epoch 181:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.96, valid_loss=-2.06]\n","Epoch 182:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.06, valid_loss=-2.06]\n","Epoch 183:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.07, valid_loss=-2.06]\n","Epoch 184:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-2.12, valid_loss=-2.06]\n","Epoch 185:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.09, valid_loss=-2.06]\n","\u001b[36m(_train_tune pid=12013)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12013)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.63it/s]\u001b[A\n","Epoch 186:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-2.03, valid_loss=-2.06]\n","Epoch 187:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.05, valid_loss=-2.06]\n","Epoch 188:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.96, valid_loss=-2.06]\n","Epoch 189:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.09, valid_loss=-2.06]\n","Epoch 190:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-2.04, valid_loss=-2.06]\n","Epoch 191:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.04, valid_loss=-2.06]\n","Epoch 192:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.06, valid_loss=-2.06]\n","Epoch 193:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.70, train_loss_epoch=-2.18, valid_loss=-2.06]\n","Epoch 194:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-2.05, valid_loss=-2.06]\n","Epoch 195:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.07, valid_loss=-2.06]\n","Epoch 196:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-2.01, valid_loss=-2.06]\n","Epoch 197:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.09, valid_loss=-2.06]\n","Epoch 198:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.46, train_loss_epoch=-2.10, valid_loss=-2.06]\n","Epoch 199:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.08, valid_loss=-2.06]\n","Epoch 199: 100%|██████████| 7/7 [00:00\u003c00:00, 22.84it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.08, valid_loss=-2.06]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12013)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=12013)\u001b[0m `Trainer.fit` stopped: `max_steps=1400.0` reached.\n","2024-11-12 18:46:14,624\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (168, 24, 1), 'n_pool_kernel_size': (8, 4, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=12013)\u001b[0m \n","\u001b[36m(_train_tune pid=12013)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.70it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12013)\u001b[0m \r                                                                      \u001b[A\rEpoch 199: 100%|██████████| 7/7 [00:02\u003c00:00,  3.14it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.08, valid_loss=-2.06]\rEpoch 199: 100%|██████████| 7/7 [00:02\u003c00:00,  3.13it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.06, valid_loss=-2.06]\rEpoch 199: 100%|██████████| 7/7 [00:02\u003c00:00,  3.13it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.06, valid_loss=-2.06]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=12495)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=12495)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=12495)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=12495)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=12495)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=12495)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=12495)\u001b[0m 2024-11-12 18:46:23.522018: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=12495)\u001b[0m 2024-11-12 18:46:23.543244: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=12495)\u001b[0m 2024-11-12 18:46:23.568432: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=12495)\u001b[0m 2024-11-12 18:46:23.576178: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=12495)\u001b[0m 2024-11-12 18:46:23.594391: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=12495)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=12495)\u001b[0m 2024-11-12 18:46:24.781733: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=12495)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=12495)\u001b[0m \n","\u001b[36m(_train_tune pid=12495)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=12495)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=12495)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=12495)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=12495)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=12495)\u001b[0m 3 | blocks       | ModuleList       | 2.6 M  | train\n","\u001b[36m(_train_tune pid=12495)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=12495)\u001b[0m 2.6 M     Trainable params\n","\u001b[36m(_train_tune pid=12495)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=12495)\u001b[0m 2.6 M     Total params\n","\u001b[36m(_train_tune pid=12495)\u001b[0m 10.200    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=12495)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=12495)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","                                                                           \n","Epoch 0:   0%|          | 0/7 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.50]\n","Epoch 2:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-1.82]\n","Epoch 3:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.88]\n","Epoch 4:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.88]\n","Epoch 5:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-1.96]\n","Epoch 6:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.91]\n","Epoch 7:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.97]\n","Epoch 8:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.92]\n","Epoch 9:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.00]\n","Epoch 10:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.95]\n","Epoch 11:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-2.01]\n","Epoch 12:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.95]\n","Epoch 13:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.94]\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.95]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12495)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12495)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.52it/s]\u001b[A\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.95, valid_loss=-1.97]\n","Epoch 15:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.94, valid_loss=-1.97]\n","Epoch 16:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.40, train_loss_epoch=-2.05, valid_loss=-1.97]\n","Epoch 17:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.96, valid_loss=-1.97]\n","Epoch 18:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.99, valid_loss=-1.97]\n","Epoch 19:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.44, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 20:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.95, valid_loss=-1.97]\n","Epoch 21:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.01, valid_loss=-1.97]\n","Epoch 22:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.01, valid_loss=-1.97]\n","Epoch 23:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.00, valid_loss=-1.97]\n","Epoch 24:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.01, valid_loss=-1.97]\n","Epoch 25:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.03, valid_loss=-1.97]\n","Epoch 26:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-2.05, valid_loss=-1.97]\n","Epoch 27:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.05, valid_loss=-1.97]\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.98, valid_loss=-1.97]\n","\u001b[36m(_train_tune pid=12495)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12495)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.83it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12495)\u001b[0m \n","                                                                      \u001b[A\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.98, valid_loss=-1.96]\n","Epoch 29:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.93, valid_loss=-1.96]\n","Epoch 30:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.36, train_loss_epoch=-2.05, valid_loss=-1.96]\n","Epoch 31:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.97, valid_loss=-1.96]\n","Epoch 32:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.99, valid_loss=-1.96]\n","Epoch 33:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.04, valid_loss=-1.96]\n","Epoch 34:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-2.00, valid_loss=-1.96]\n","Epoch 35:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.06, valid_loss=-1.96]\n","Epoch 36:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.01, valid_loss=-1.96]\n","Epoch 37:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.61, train_loss_epoch=-2.13, valid_loss=-1.96]\n","Epoch 38:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.01, valid_loss=-1.96]\n","Epoch 39:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-2.00, valid_loss=-1.96]\n","Epoch 40:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.06, valid_loss=-1.96]\n","Epoch 41:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.47, train_loss_epoch=-2.09, valid_loss=-1.96]\n","Epoch 42:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-2.05, valid_loss=-1.96]\n","\u001b[36m(_train_tune pid=12495)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12495)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 27.59it/s]\u001b[A\n","Epoch 42: 100%|██████████| 7/7 [00:00\u003c00:00, 13.76it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.00, valid_loss=-1.98]\n","Epoch 43:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.00, valid_loss=-1.98]\n","Epoch 44:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.05, valid_loss=-1.98]\n","Epoch 45:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.03, valid_loss=-1.98]\n","Epoch 46:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.05, valid_loss=-1.98]\n","Epoch 47:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-2.11, valid_loss=-1.98]\n","Epoch 48:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-2.01, valid_loss=-1.98]\n","Epoch 49:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.07, valid_loss=-1.98]\n","Epoch 50:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-2.00, valid_loss=-1.98]\n","Epoch 51:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.06, valid_loss=-1.98]\n","Epoch 52:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.05, valid_loss=-1.98]\n","Epoch 53:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-2.01, valid_loss=-1.98]\n","Epoch 54:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.03, valid_loss=-1.98]\n","Epoch 55:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-2.05, valid_loss=-1.98]\n","Epoch 56:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.06, valid_loss=-1.98]\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.96, valid_loss=-1.98]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12495)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12495)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 30.30it/s]\u001b[A\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.96, valid_loss=-1.99]\n","Epoch 58:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-2.07, valid_loss=-1.99]\n","Epoch 59:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 60:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 61:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-2.04, valid_loss=-1.99]\n","Epoch 62:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.08, valid_loss=-1.99]\n","Epoch 63:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.07, valid_loss=-1.99]\n","Epoch 64:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-2.02, valid_loss=-1.99]\n","Epoch 65:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.05, valid_loss=-1.99]\n","Epoch 66:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-2.04, valid_loss=-1.99]\n","Epoch 66: 100%|██████████| 7/7 [00:00\u003c00:00, 31.49it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.04, valid_loss=-1.99]\n","Epoch 67:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.10, valid_loss=-1.99]\n","Epoch 68:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.08, valid_loss=-1.99]\n","Epoch 69:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-2.05, valid_loss=-1.99]\n","Epoch 70:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.95, valid_loss=-1.99]\n","\u001b[36m(_train_tune pid=12495)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12495)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 30.15it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12495)\u001b[0m \n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.95, valid_loss=-2.02]\n","Epoch 72:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.06, valid_loss=-2.02]\n","Epoch 73:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.12, valid_loss=-2.02]\n","Epoch 74:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.48, train_loss_epoch=-2.16, valid_loss=-2.02]\n","Epoch 75:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.08, valid_loss=-2.02]\n","Epoch 76:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.13, valid_loss=-2.02]\n","Epoch 77:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-2.06, valid_loss=-2.02]\n","Epoch 78:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-2.12, valid_loss=-2.02]\n","Epoch 79:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.11, valid_loss=-2.02]\n","Epoch 80:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-2.11, valid_loss=-2.02]\n","Epoch 81:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-2.06, valid_loss=-2.02]\n","Epoch 82:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.06, valid_loss=-2.02]\n","Epoch 83:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.99, valid_loss=-2.02]\n","Epoch 84:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.07, valid_loss=-2.02]\n","Epoch 85:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.09, valid_loss=-2.02]\n","\u001b[36m(_train_tune pid=12495)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12495)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12495)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 30.12it/s]\u001b[A\n","Epoch 86:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.09, valid_loss=-2.03]\n","Epoch 87:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.76, train_loss_epoch=-2.16, valid_loss=-2.03]\n","Epoch 88:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.42, train_loss_epoch=-2.13, valid_loss=-2.03]\n","Epoch 89:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.10, valid_loss=-2.03]\n","Epoch 90:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.12, valid_loss=-2.03]\n","Epoch 91:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.15, valid_loss=-2.03]\n","Epoch 92:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-2.07, valid_loss=-2.03]\n","Epoch 93:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.09, valid_loss=-2.03]\n","Epoch 94:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.13, valid_loss=-2.03]\n","Epoch 95:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-2.02, valid_loss=-2.03]\n","Epoch 96:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.13, valid_loss=-2.03]\n","Epoch 97:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.10, valid_loss=-2.03]\n","Epoch 98:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.36, train_loss_epoch=-2.12, valid_loss=-2.03]\n","Epoch 99:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.08, valid_loss=-2.03]\n","Epoch 99: 100%|██████████| 7/7 [00:00\u003c00:00, 31.52it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-2.08, valid_loss=-2.03]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12495)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 29.51it/s]\u001b[A\n","Epoch 100:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-2.16, valid_loss=-2.01]\n","Epoch 101:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.12, valid_loss=-2.01]\n","Epoch 102:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.07, valid_loss=-2.01]\n","Epoch 103:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.12, valid_loss=-2.01]\n","Epoch 104:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.49, train_loss_epoch=-2.18, valid_loss=-2.01]\n","Epoch 105:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-2.06, valid_loss=-2.01]\n","Epoch 106:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-2.07, valid_loss=-2.01]\n","Epoch 107:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.10, valid_loss=-2.01]\n","Epoch 108:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.45, train_loss_epoch=-2.13, valid_loss=-2.01]\n","Epoch 109:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.13, valid_loss=-2.01]\n","Epoch 110:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-2.01, valid_loss=-2.01]\n","Epoch 111:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-2.14, valid_loss=-2.01]\n","Epoch 112:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.12, valid_loss=-2.01]\n","Epoch 113:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.15, valid_loss=-2.01]\n","Epoch 114:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.51, train_loss_epoch=-2.16, valid_loss=-2.01]\n","\u001b[36m(_train_tune pid=12495)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12495)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.79it/s]\u001b[A\n","Epoch 114:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.16, valid_loss=-2.02]\n","Epoch 115:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.11, valid_loss=-2.02]\n","Epoch 116:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-2.06, valid_loss=-2.02]\n","Epoch 117:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.50, train_loss_epoch=-2.17, valid_loss=-2.02]\n","Epoch 118:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-2.08, valid_loss=-2.02]\n","Epoch 119:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-2.14, valid_loss=-2.02]\n","Epoch 120:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.12, valid_loss=-2.02]\n","Epoch 121:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.13, valid_loss=-2.02]\n","Epoch 122:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.13, valid_loss=-2.02]\n","Epoch 123:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-2.17, valid_loss=-2.02]\n","Epoch 124:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.17, valid_loss=-2.02]\n","Epoch 125:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-2.15, valid_loss=-2.02]\n","Epoch 126:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.15, valid_loss=-2.02]\n","Epoch 127:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-2.04, valid_loss=-2.02]\n","Epoch 128:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.12, valid_loss=-2.02]\n","\u001b[36m(_train_tune pid=12495)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12495)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12495)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 28.47it/s]\u001b[A\n","Epoch 128:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.12, valid_loss=-2.01]\n","Epoch 129:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.14, valid_loss=-2.01]\n","Epoch 130:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.10, valid_loss=-2.01]\n","Epoch 131:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.13, valid_loss=-2.01]\n","Epoch 132:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.08, valid_loss=-2.01]\n","Epoch 133:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.09, valid_loss=-2.01]\n","Epoch 134:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.76, train_loss_epoch=-2.24, valid_loss=-2.01]\n","Epoch 135:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.09, valid_loss=-2.01]\n","Epoch 136:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.58, train_loss_epoch=-2.18, valid_loss=-2.01]\n","Epoch 137:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.13, valid_loss=-2.01]\n","Epoch 138:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-2.09, valid_loss=-2.01]\n","Epoch 139:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.15, valid_loss=-2.01]\n","Epoch 140:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-2.08, valid_loss=-2.01]\n","Epoch 141:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.44, train_loss_epoch=-2.18, valid_loss=-2.01]\n","Epoch 142:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.17, valid_loss=-2.01]\n","\u001b[36m(_train_tune pid=12495)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12495)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 29.01it/s]\u001b[A\n","Epoch 143:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.14, valid_loss=-2.02]\n","Epoch 144:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.17, valid_loss=-2.02]\n","Epoch 145:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.54, train_loss_epoch=-2.21, valid_loss=-2.02]\n","Epoch 146:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.12, valid_loss=-2.02]\n","Epoch 147:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.63, train_loss_epoch=-2.22, valid_loss=-2.02]\n","Epoch 148:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.44, train_loss_epoch=-2.18, valid_loss=-2.02]\n","Epoch 149:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.16, valid_loss=-2.02]\n","Epoch 150:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-2.13, valid_loss=-2.02]\n","Epoch 151:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-2.13, valid_loss=-2.02]\n","Epoch 152:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.70, train_loss_epoch=-2.22, valid_loss=-2.02]\n","Epoch 153:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-2.09, valid_loss=-2.02]\n","Epoch 154:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-2.13, valid_loss=-2.02]\n","Epoch 155:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.18, valid_loss=-2.02]\n","Epoch 156:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.11, valid_loss=-2.02]\n","Epoch 157:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.62, train_loss_epoch=-2.22, valid_loss=-2.02]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12495)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12495)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 29.92it/s]\u001b[A\n","Epoch 157:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.22, valid_loss=-2.03]\n","Epoch 158:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-2.11, valid_loss=-2.03]\n","Epoch 159:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.11, valid_loss=-2.03]\n","Epoch 160:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.13, valid_loss=-2.03]\n","Epoch 161:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-2.14, valid_loss=-2.03]\n","Epoch 162:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-2.15, valid_loss=-2.03]\n","Epoch 163:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.73, train_loss_epoch=-2.24, valid_loss=-2.03]\n","Epoch 164:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-2.06, valid_loss=-2.03]\n","Epoch 165:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-2.13, valid_loss=-2.03]\n","Epoch 166:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-2.19, valid_loss=-2.03]\n","Epoch 167:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.11, valid_loss=-2.03]\n","Epoch 168:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.15, valid_loss=-2.03]\n","Epoch 169:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-2.14, valid_loss=-2.03]\n","Epoch 170:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.67, train_loss_epoch=-2.23, valid_loss=-2.03]\n","Epoch 171:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.16, valid_loss=-2.03]\n","\u001b[36m(_train_tune pid=12495)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12495)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 30.30it/s]\u001b[A\n","Epoch 171:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.16, valid_loss=-2.01]\n","Epoch 171: 100%|██████████| 7/7 [00:00\u003c00:00, 14.35it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.12, valid_loss=-2.01]\n","Epoch 172:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.12, valid_loss=-2.01]\n","Epoch 173:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-2.10, valid_loss=-2.01]\n","Epoch 174:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.14, valid_loss=-2.01]\n","Epoch 175:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-2.19, valid_loss=-2.01]\n","Epoch 176:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.14, valid_loss=-2.01]\n","Epoch 177:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.18, valid_loss=-2.01]\n","Epoch 178:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-2.14, valid_loss=-2.01]\n","Epoch 179:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-2.14, valid_loss=-2.01]\n","Epoch 180:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.60, train_loss_epoch=-2.26, valid_loss=-2.01]\n","Epoch 181:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-2.10, valid_loss=-2.01]\n","Epoch 182:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.20, valid_loss=-2.01]\n","Epoch 183:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.19, valid_loss=-2.01]\n","Epoch 184:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.54, train_loss_epoch=-2.22, valid_loss=-2.01]\n","Epoch 185:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.16, valid_loss=-2.01]\n","\u001b[36m(_train_tune pid=12495)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12495)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 30.15it/s]\u001b[A\n","Epoch 185:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.16, valid_loss=-2.00]\n","Epoch 186:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-2.13, valid_loss=-2.00]\n","Epoch 187:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.15, valid_loss=-2.00]\n","Epoch 188:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-2.04, valid_loss=-2.00]\n","Epoch 189:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.19, valid_loss=-2.00]\n","Epoch 190:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.17, valid_loss=-2.00]\n","Epoch 191:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.17, valid_loss=-2.00]\n","Epoch 191: 100%|██████████| 7/7 [00:00\u003c00:00, 31.24it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.17, valid_loss=-2.00]\n","Epoch 192:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.18, valid_loss=-2.00]\n","Epoch 193:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.78, train_loss_epoch=-2.29, valid_loss=-2.00]\n","Epoch 194:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.18, valid_loss=-2.00]\n","Epoch 195:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.19, valid_loss=-2.00]\n","Epoch 196:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-2.15, valid_loss=-2.00]\n","Epoch 197:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.17, valid_loss=-2.00]\n","Epoch 198:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.50, train_loss_epoch=-2.22, valid_loss=-2.00]\n","Epoch 199:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-2.22, valid_loss=-2.00]\n","Epoch 199: 100%|██████████| 7/7 [00:00\u003c00:00, 30.69it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-2.22, valid_loss=-2.00]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=12495)\u001b[0m `Trainer.fit` stopped: `max_steps=1400.0` reached.\n","2024-11-12 18:47:15,999\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (1, 1, 1), 'n_pool_kernel_size': (2, 2, 2), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=12495)\u001b[0m \n","\u001b[36m(_train_tune pid=12495)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 27.58it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12495)\u001b[0m \r                                                                      \u001b[A\rEpoch 199: 100%|██████████| 7/7 [00:00\u003c00:00, 13.48it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-2.22, valid_loss=-2.01]\rEpoch 199: 100%|██████████| 7/7 [00:00\u003c00:00, 13.46it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-2.18, valid_loss=-2.01]\rEpoch 199: 100%|██████████| 7/7 [00:00\u003c00:00, 13.43it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-2.18, valid_loss=-2.01]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=12811)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=12811)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=12811)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=12811)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=12811)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=12811)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=12811)\u001b[0m 2024-11-12 18:47:24.399726: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=12811)\u001b[0m 2024-11-12 18:47:24.420547: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=12811)\u001b[0m 2024-11-12 18:47:24.445818: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=12811)\u001b[0m 2024-11-12 18:47:24.453530: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=12811)\u001b[0m 2024-11-12 18:47:24.471536: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=12811)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=12811)\u001b[0m 2024-11-12 18:47:25.654476: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=12811)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=12811)\u001b[0m \n","\u001b[36m(_train_tune pid=12811)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=12811)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=12811)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=12811)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=12811)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=12811)\u001b[0m 3 | blocks       | ModuleList       | 2.5 M  | train\n","\u001b[36m(_train_tune pid=12811)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=12811)\u001b[0m 2.5 M     Trainable params\n","\u001b[36m(_train_tune pid=12811)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=12811)\u001b[0m 2.5 M     Total params\n","\u001b[36m(_train_tune pid=12811)\u001b[0m 9.956     Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=12811)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=12811)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:02\u003c00:00,  0.82it/s]\n","Epoch 0:   0%|          | 0/4 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.33]\n","Epoch 2:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.62]\n","Epoch 3:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.61]\n","Epoch 4:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.55]\n","Epoch 5:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.68]\n","Epoch 6:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.66]\n","Epoch 7:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.80]\n","Epoch 8:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.65]\n","Epoch 9:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.91]\n","Epoch 10:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.79]\n","Epoch 11:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-1.97]\n","Epoch 12:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.77]\n","Epoch 12: 100%|██████████| 4/4 [00:00\u003c00:00, 11.26it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.77]\n","Epoch 13:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.82]\n","Epoch 14:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.81]\n","Epoch 15:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.84]\n","Epoch 16:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.75]\n","Epoch 17:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.94]\n","Epoch 18:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-2.03]\n","Epoch 19:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-1.99]\n","Epoch 20:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.91]\n","Epoch 21:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-2.00]\n","Epoch 22:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.94]\n","Epoch 23:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.92]\n","Epoch 24:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.87]\n","Epoch 24: 100%|██████████| 4/4 [00:00\u003c00:00, 11.40it/s, v_num=0, train_loss_step=-2.46, train_loss_epoch=-1.87]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12811)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12811)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.04it/s]\u001b[A\n","Epoch 25:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.46, train_loss_epoch=-2.06, valid_loss=-1.95]\n","Epoch 26:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.04, valid_loss=-1.95]\n","Epoch 27:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.94, valid_loss=-1.95]\n","Epoch 28:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.96, valid_loss=-1.95]\n","Epoch 29:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.80, valid_loss=-1.95]\n","Epoch 30:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.02, valid_loss=-1.95]\n","Epoch 31:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.96, valid_loss=-1.95]\n","Epoch 32:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.86, valid_loss=-1.95]\n","Epoch 33:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.93, valid_loss=-1.95]\n","Epoch 34:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.90, valid_loss=-1.95]\n","Epoch 35:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.97, valid_loss=-1.95]\n","Epoch 36:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.96, valid_loss=-1.95]\n","Epoch 37:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.03, valid_loss=-1.95]\n","Epoch 38:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.03, valid_loss=-1.95]\n","Epoch 39:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.01, valid_loss=-1.95]\n","Epoch 40:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.90, valid_loss=-1.95]\n","Epoch 41:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.88, valid_loss=-1.95]\n","Epoch 42:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.85, valid_loss=-1.95]\n","Epoch 43:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.03, valid_loss=-1.95]\n","Epoch 44:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.94, valid_loss=-1.95]\n","Epoch 45:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.97, valid_loss=-1.95]\n","Epoch 46:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.99, valid_loss=-1.95]\n","Epoch 47:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.92, valid_loss=-1.95]\n","Epoch 48:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.03, valid_loss=-1.95]\n","Epoch 49:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.86, valid_loss=-1.95]\n","Epoch 49: 100%|██████████| 4/4 [00:00\u003c00:00, 11.57it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.86, valid_loss=-1.95]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12811)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12811)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.04it/s]\u001b[A\n","Epoch 50:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.02, valid_loss=-1.99]\n","Epoch 51:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.98, valid_loss=-1.99]\n","Epoch 52:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 53:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.00, valid_loss=-1.99]\n","Epoch 54:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.47, train_loss_epoch=-2.09, valid_loss=-1.99]\n","Epoch 55:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.93, valid_loss=-1.99]\n","Epoch 56:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.01, valid_loss=-1.99]\n","Epoch 57:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.04, valid_loss=-1.99]\n","Epoch 58:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.93, valid_loss=-1.99]\n","Epoch 59:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-2.08, valid_loss=-1.99]\n","Epoch 60:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.92, valid_loss=-1.99]\n","Epoch 61:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.99, valid_loss=-1.99]\n","Epoch 62:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.89, valid_loss=-1.99]\n","Epoch 63:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 64:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.00, valid_loss=-1.99]\n","Epoch 65:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.94, valid_loss=-1.99]\n","Epoch 66:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.98, valid_loss=-1.99]\n","Epoch 67:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.93, valid_loss=-1.99]\n","Epoch 68:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.04, valid_loss=-1.99]\n","Epoch 69:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.02, valid_loss=-1.99]\n","Epoch 70:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.89, valid_loss=-1.99]\n","Epoch 71:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.92, valid_loss=-1.99]\n","Epoch 72:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.79, valid_loss=-1.99]\n","Epoch 73:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.99, valid_loss=-1.99]\n","Epoch 74:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.00, valid_loss=-1.99]\n","Epoch 74: 100%|██████████| 4/4 [00:00\u003c00:00, 11.38it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-2.00, valid_loss=-1.99]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12811)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12811)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.94it/s]\u001b[A\n","Epoch 75:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.87, valid_loss=-2.01]\n","Epoch 76:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.95, valid_loss=-2.01]\n","Epoch 77:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.00, valid_loss=-2.01]\n","Epoch 78:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.89, valid_loss=-2.01]\n","Epoch 78: 100%|██████████| 4/4 [00:00\u003c00:00, 11.89it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.02, valid_loss=-2.01]\n","Epoch 79:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.02, valid_loss=-2.01]\n","Epoch 80:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.00, valid_loss=-2.01]\n","Epoch 81:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.00, valid_loss=-2.01]\n","Epoch 82:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.93, valid_loss=-2.01]\n","Epoch 83:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.94, valid_loss=-2.01]\n","Epoch 84:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.79, valid_loss=-2.01]\n","Epoch 85:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.94, valid_loss=-2.01]\n","Epoch 86:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.95, valid_loss=-2.01]\n","Epoch 87:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.01, valid_loss=-2.01]\n","Epoch 88:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.44, train_loss_epoch=-2.10, valid_loss=-2.01]\n","Epoch 89:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.91, valid_loss=-2.01]\n","Epoch 90:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.88, valid_loss=-2.01]\n","Epoch 91:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.93, valid_loss=-2.01]\n","Epoch 92:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.36, train_loss_epoch=-2.06, valid_loss=-2.01]\n","Epoch 93:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.96, valid_loss=-2.01]\n","Epoch 94:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-2.08, valid_loss=-2.01]\n","Epoch 95:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-2.07, valid_loss=-2.01]\n","Epoch 96:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.97, valid_loss=-2.01]\n","Epoch 97:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.00, valid_loss=-2.01]\n","Epoch 97: 100%|██████████| 4/4 [00:00\u003c00:00, 11.63it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-2.00, valid_loss=-2.01]\n","Epoch 98:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-2.07, valid_loss=-2.01]\n","Epoch 99:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.94, valid_loss=-2.01]\n","Epoch 99: 100%|██████████| 4/4 [00:00\u003c00:00, 11.54it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.94, valid_loss=-2.01]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12811)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12811)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.03it/s]\u001b[A\n","Epoch 100:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.01, valid_loss=-2.02]\n","Epoch 101:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-2.07, valid_loss=-2.02]\n","Epoch 102:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.96, valid_loss=-2.02]\n","Epoch 103:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.01, valid_loss=-2.02]\n","Epoch 104:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.99, valid_loss=-2.02]\n","Epoch 105:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-2.06, valid_loss=-2.02]\n","Epoch 106:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.05, valid_loss=-2.02]\n","Epoch 107:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.97, valid_loss=-2.02]\n","Epoch 108:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.99, valid_loss=-2.02]\n","Epoch 109:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.97, valid_loss=-2.02]\n","Epoch 110:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.03, valid_loss=-2.02]\n","Epoch 111:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.95, valid_loss=-2.02]\n","Epoch 112:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.96, valid_loss=-2.02]\n","Epoch 113:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-2.07, valid_loss=-2.02]\n","Epoch 114:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.36, train_loss_epoch=-2.07, valid_loss=-2.02]\n","Epoch 115:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.05, valid_loss=-2.02]\n","Epoch 116:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.94, valid_loss=-2.02]\n","Epoch 117:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.92, valid_loss=-2.02]\n","Epoch 118:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.90, valid_loss=-2.02]\n","Epoch 119:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.10, valid_loss=-2.02]\n","Epoch 120:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.92, valid_loss=-2.02]\n","Epoch 121:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.01, valid_loss=-2.02]\n","Epoch 122:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.03, valid_loss=-2.02]\n","Epoch 123:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.00, valid_loss=-2.02]\n","Epoch 124:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.05, valid_loss=-2.02]\n","Epoch 124: 100%|██████████| 4/4 [00:00\u003c00:00, 11.62it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.05, valid_loss=-2.02]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12811)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.05it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12811)\u001b[0m \n","Epoch 125:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.02, valid_loss=-2.03]\n","Epoch 126:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-2.10, valid_loss=-2.03]\n","Epoch 127:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.00, valid_loss=-2.03]\n","Epoch 128:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.03, valid_loss=-2.03]\n","Epoch 129:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.01, valid_loss=-2.03]\n","Epoch 130:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.01, valid_loss=-2.03]\n","Epoch 131:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.01, valid_loss=-2.03]\n","Epoch 132:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.95, valid_loss=-2.03]\n","Epoch 133:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.02, valid_loss=-2.03]\n","Epoch 134:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.97, valid_loss=-2.03]\n","Epoch 135:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.96, valid_loss=-2.03]\n","Epoch 136:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.53, train_loss_epoch=-2.11, valid_loss=-2.03]\n","Epoch 137:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.91, valid_loss=-2.03]\n","Epoch 138:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.40, train_loss_epoch=-2.14, valid_loss=-2.03]\n","Epoch 139:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.01, valid_loss=-2.03]\n","Epoch 140:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.92, valid_loss=-2.03]\n","Epoch 141:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.02, valid_loss=-2.03]\n","Epoch 142:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.89, valid_loss=-2.03]\n","Epoch 143:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.05, valid_loss=-2.03]\n","Epoch 144:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.05, valid_loss=-2.03]\n","Epoch 145:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.00, valid_loss=-2.03]\n","Epoch 146:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.00, valid_loss=-2.03]\n","Epoch 147:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.99, valid_loss=-2.03]\n","Epoch 148:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.04, valid_loss=-2.03]\n","Epoch 148: 100%|██████████| 4/4 [00:00\u003c00:00, 11.64it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.92, valid_loss=-2.03]\n","Epoch 149:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.92, valid_loss=-2.03]\n","Epoch 149: 100%|██████████| 4/4 [00:00\u003c00:00, 11.37it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.92, valid_loss=-2.03]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12811)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12811)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.04it/s]\u001b[A\n","Epoch 150:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.03, valid_loss=-2.03]\n","Epoch 151:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.98, valid_loss=-2.03]\n","Epoch 152:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.97, valid_loss=-2.03]\n","Epoch 153:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.98, valid_loss=-2.03]\n","Epoch 154:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.58, train_loss_epoch=-2.14, valid_loss=-2.03]\n","Epoch 155:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.89, valid_loss=-2.03]\n","Epoch 156:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.93, valid_loss=-2.03]\n","Epoch 157:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.04, valid_loss=-2.03]\n","Epoch 158:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.90, valid_loss=-2.03]\n","Epoch 159:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.40, train_loss_epoch=-2.10, valid_loss=-2.03]\n","Epoch 160:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.95, valid_loss=-2.03]\n","Epoch 161:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.58, train_loss_epoch=-2.17, valid_loss=-2.03]\n","Epoch 162:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.52, train_loss_epoch=-2.15, valid_loss=-2.03]\n","Epoch 163:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-2.08, valid_loss=-2.03]\n","Epoch 164:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.04, valid_loss=-2.03]\n","Epoch 165:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.91, valid_loss=-2.03]\n","Epoch 166:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.99, valid_loss=-2.03]\n","Epoch 166: 100%|██████████| 4/4 [00:00\u003c00:00, 11.28it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-1.99, valid_loss=-2.03]\n","Epoch 167:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.02, valid_loss=-2.03]\n","Epoch 168:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.00, valid_loss=-2.03]\n","Epoch 169:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-2.15, valid_loss=-2.03]\n","Epoch 170:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.93, valid_loss=-2.03]\n","Epoch 171:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.05, valid_loss=-2.03]\n","Epoch 172:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.11, valid_loss=-2.03]\n","Epoch 173:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.93, valid_loss=-2.03]\n","Epoch 174:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.07, valid_loss=-2.03]\n","Epoch 174: 100%|██████████| 4/4 [00:00\u003c00:00, 11.59it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.07, valid_loss=-2.03]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12811)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.03it/s]\u001b[A\n","Epoch 175:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.03, valid_loss=-2.04]\n","Epoch 176:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.05, valid_loss=-2.04]\n","Epoch 177:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.02, valid_loss=-2.04]\n","Epoch 178:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.98, valid_loss=-2.04]\n","Epoch 179:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.99, valid_loss=-2.04]\n","Epoch 179: 100%|██████████| 4/4 [00:00\u003c00:00, 11.57it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.01, valid_loss=-2.04]\n","Epoch 180:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.01, valid_loss=-2.04]\n","Epoch 181:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.07, valid_loss=-2.04]\n","Epoch 182:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.06, valid_loss=-2.04]\n","Epoch 183:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.03, valid_loss=-2.04]\n","Epoch 184:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.96, valid_loss=-2.04]\n","Epoch 185:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.91, valid_loss=-2.04]\n","Epoch 186:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-2.06, valid_loss=-2.04]\n","Epoch 187:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-2.01, valid_loss=-2.04]\n","Epoch 188:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-2.00, valid_loss=-2.04]\n","Epoch 189:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.87, valid_loss=-2.04]\n","Epoch 190:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.99, valid_loss=-2.04]\n","Epoch 191:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.85, valid_loss=-2.04]\n","Epoch 192:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.02, valid_loss=-2.04]\n","Epoch 193:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.96, valid_loss=-2.04]\n","Epoch 194:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.04, valid_loss=-2.04]\n","Epoch 195:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.01, valid_loss=-2.04]\n","Epoch 196:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.60, train_loss_epoch=-2.19, valid_loss=-2.04]\n","Epoch 197:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.98, valid_loss=-2.04]\n","Epoch 198:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.01, valid_loss=-2.04]\n","Epoch 199:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.95, valid_loss=-2.04]\n","Epoch 199: 100%|██████████| 4/4 [00:00\u003c00:00, 11.17it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.95, valid_loss=-2.04]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12811)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.03it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12811)\u001b[0m \n","Epoch 199: 100%|██████████| 4/4 [00:02\u003c00:00,  1.65it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.95, valid_loss=-2.04]\n","Epoch 200:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.04, valid_loss=-2.04]\n","Epoch 201:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.01, valid_loss=-2.04]\n","Epoch 202:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.00, valid_loss=-2.04]\n","Epoch 203:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-2.00, valid_loss=-2.04]\n","Epoch 204:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.93, valid_loss=-2.04]\n","Epoch 205:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.12, valid_loss=-2.04]\n","Epoch 206:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-2.09, valid_loss=-2.04]\n","Epoch 207:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.10, valid_loss=-2.04]\n","Epoch 208:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.99, valid_loss=-2.04]\n","Epoch 209:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.83, valid_loss=-2.04]\n","Epoch 210:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-2.00, valid_loss=-2.04]\n","Epoch 211:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.99, valid_loss=-2.04]\n","Epoch 212:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.95, valid_loss=-2.04]\n","Epoch 213:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.93, valid_loss=-2.04]\n","Epoch 214:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.08, valid_loss=-2.04]\n","Epoch 215:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-2.07, valid_loss=-2.04]\n","Epoch 216:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-2.02, valid_loss=-2.04]\n","Epoch 217:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.50, train_loss_epoch=-2.15, valid_loss=-2.04]\n","Epoch 218:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.94, valid_loss=-2.04]\n","Epoch 219:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.98, valid_loss=-2.04]\n","Epoch 220:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.92, valid_loss=-2.04]\n","Epoch 221:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.03, valid_loss=-2.04]\n","Epoch 222:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.91, valid_loss=-2.04]\n","Epoch 223:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-2.08, valid_loss=-2.04]\n","Epoch 224:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.04, valid_loss=-2.04]\n","Epoch 224: 100%|██████████| 4/4 [00:00\u003c00:00, 11.43it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-2.04, valid_loss=-2.04]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12811)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12811)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.02it/s]\u001b[A\n","Epoch 225:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.78, valid_loss=-2.04]\n","Epoch 226:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.96, valid_loss=-2.04]\n","Epoch 227:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.07, valid_loss=-2.04]\n","Epoch 228:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.04, valid_loss=-2.04]\n","Epoch 229:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.88, valid_loss=-2.04]\n","Epoch 230:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.97, valid_loss=-2.04]\n","Epoch 231:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.89, valid_loss=-2.04]\n","Epoch 232:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.93, valid_loss=-2.04]\n","Epoch 233:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.01, valid_loss=-2.04]\n","Epoch 234:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.95, valid_loss=-2.04]\n","Epoch 235:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-2.08, valid_loss=-2.04]\n","Epoch 236:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.02, valid_loss=-2.04]\n","Epoch 237:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.84, valid_loss=-2.04]\n","Epoch 238:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.04, valid_loss=-2.04]\n","Epoch 239:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.88, valid_loss=-2.04]\n","Epoch 240:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.99, valid_loss=-2.04]\n","Epoch 241:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.08, valid_loss=-2.04]\n","Epoch 242:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.03, valid_loss=-2.04]\n","Epoch 243:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.95, valid_loss=-2.04]\n","Epoch 244:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.00, valid_loss=-2.04]\n","Epoch 245:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.95, valid_loss=-2.04]\n","Epoch 246:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.07, valid_loss=-2.04]\n","Epoch 247:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.00, valid_loss=-2.04]\n","Epoch 248:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.99, valid_loss=-2.04]\n","Epoch 249:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.56, train_loss_epoch=-2.15, valid_loss=-2.04]\n","Epoch 249: 100%|██████████| 4/4 [00:00\u003c00:00, 11.40it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.15, valid_loss=-2.04]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12811)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12811)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.05it/s]\u001b[A\n","Epoch 250:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.08, valid_loss=-2.05]\n","Epoch 251:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.84, valid_loss=-2.05]\n","Epoch 252:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.90, valid_loss=-2.05]\n","Epoch 253:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.02, valid_loss=-2.05]\n","Epoch 254:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.94, valid_loss=-2.05]\n","Epoch 255:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.96, valid_loss=-2.05]\n","Epoch 256:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-2.04, valid_loss=-2.05]\n","Epoch 257:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.06, valid_loss=-2.05]\n","Epoch 258:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.46, train_loss_epoch=-2.13, valid_loss=-2.05]\n","Epoch 259:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.83, valid_loss=-2.05]\n","Epoch 260:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.05, valid_loss=-2.05]\n","Epoch 261:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.98, valid_loss=-2.05]\n","Epoch 262:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.06, valid_loss=-2.05]\n","Epoch 263:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.79, valid_loss=-2.05]\n","Epoch 264:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.01, valid_loss=-2.05]\n","Epoch 265:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.55, train_loss_epoch=-2.16, valid_loss=-2.05]\n","Epoch 266:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-2.12, valid_loss=-2.05]\n","Epoch 267:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.99, valid_loss=-2.05]\n","Epoch 268:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.93, valid_loss=-2.05]\n","Epoch 269:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.09, valid_loss=-2.05]\n","Epoch 270:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.98, valid_loss=-2.05]\n","Epoch 271:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.97, valid_loss=-2.05]\n","Epoch 272:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.91, valid_loss=-2.05]\n","Epoch 273:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.96, valid_loss=-2.05]\n","Epoch 274:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.90, valid_loss=-2.05]\n","Epoch 274: 100%|██████████| 4/4 [00:00\u003c00:00, 11.31it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.90, valid_loss=-2.05]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12811)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12811)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.05it/s]\u001b[A\n","Epoch 275:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.87, valid_loss=-2.05]\n","Epoch 276:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.98, valid_loss=-2.05]\n","Epoch 277:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.99, valid_loss=-2.05]\n","Epoch 278:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.98, valid_loss=-2.05]\n","Epoch 279:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.95, valid_loss=-2.05]\n","Epoch 280:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.03, valid_loss=-2.05]\n","Epoch 281:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.06, valid_loss=-2.05]\n","Epoch 282:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.08, valid_loss=-2.05]\n","Epoch 283:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.07, valid_loss=-2.05]\n","Epoch 284:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.04, valid_loss=-2.05]\n","Epoch 285:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.00, valid_loss=-2.05]\n","Epoch 286:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-2.01, valid_loss=-2.05]\n","Epoch 287:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-2.11, valid_loss=-2.05]\n","Epoch 288:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.00, valid_loss=-2.05]\n","Epoch 289:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-2.10, valid_loss=-2.05]\n","Epoch 290:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.09, valid_loss=-2.05]\n","Epoch 291:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.07, valid_loss=-2.05]\n","Epoch 292:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.97, valid_loss=-2.05]\n","Epoch 293:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.99, valid_loss=-2.05]\n","Epoch 294:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-2.13, valid_loss=-2.05]\n","Epoch 295:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.41, train_loss_epoch=-2.09, valid_loss=-2.05]\n","Epoch 296:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.02, valid_loss=-2.05]\n","Epoch 297:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.03, valid_loss=-2.05]\n","Epoch 297: 100%|██████████| 4/4 [00:00\u003c00:00, 11.55it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.03, valid_loss=-2.05]\n","Epoch 298:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.02, valid_loss=-2.05]\n","Epoch 299:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.04, valid_loss=-2.05]\n","Epoch 299: 100%|██████████| 4/4 [00:00\u003c00:00, 11.38it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.04, valid_loss=-2.05]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12811)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12811)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.04it/s]\u001b[A\n","Epoch 300:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.98, valid_loss=-2.05]\n","Epoch 301:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.98, valid_loss=-2.05]\n","Epoch 302:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.98, valid_loss=-2.05]\n","Epoch 303:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.99, valid_loss=-2.05]\n","Epoch 304:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-2.10, valid_loss=-2.05]\n","Epoch 305:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.98, valid_loss=-2.05]\n","Epoch 306:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.00, valid_loss=-2.05]\n","Epoch 307:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.98, valid_loss=-2.05]\n","Epoch 308:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.85, valid_loss=-2.05]\n","Epoch 309:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.04, valid_loss=-2.05]\n","Epoch 310:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.01, valid_loss=-2.05]\n","Epoch 311:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.08, valid_loss=-2.05]\n","Epoch 312:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-2.09, valid_loss=-2.05]\n","Epoch 313:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-2.13, valid_loss=-2.05]\n","Epoch 314:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.10, valid_loss=-2.05]\n","Epoch 315:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.04, valid_loss=-2.05]\n","Epoch 316:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.07, valid_loss=-2.05]\n","Epoch 317:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.45, train_loss_epoch=-2.13, valid_loss=-2.05]\n","Epoch 318:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.95, valid_loss=-2.05]\n","Epoch 319:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-2.01, valid_loss=-2.05]\n","Epoch 320:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.02, valid_loss=-2.05]\n","Epoch 321:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.04, valid_loss=-2.05]\n","Epoch 322:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.06, valid_loss=-2.05]\n","Epoch 323:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.07, valid_loss=-2.05]\n","Epoch 324:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.07, valid_loss=-2.05]\n","Epoch 324: 100%|██████████| 4/4 [00:00\u003c00:00, 11.51it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-2.07, valid_loss=-2.05]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12811)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12811)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.03it/s]\u001b[A\n","Epoch 325:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-2.11, valid_loss=-2.04]\n","Epoch 326:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.02, valid_loss=-2.04]\n","Epoch 327:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.05, valid_loss=-2.04]\n","Epoch 328:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.98, valid_loss=-2.04]\n","Epoch 329:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.05, valid_loss=-2.04]\n","Epoch 330:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.07, valid_loss=-2.04]\n","Epoch 331:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.88, valid_loss=-2.04]\n","Epoch 332:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.93, valid_loss=-2.04]\n","Epoch 333:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.98, valid_loss=-2.04]\n","Epoch 334:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.08, valid_loss=-2.04]\n","Epoch 335:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-2.09, valid_loss=-2.04]\n","Epoch 336:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.42, train_loss_epoch=-2.12, valid_loss=-2.04]\n","Epoch 337:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.05, valid_loss=-2.04]\n","Epoch 338:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.95, valid_loss=-2.04]\n","Epoch 339:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.02, valid_loss=-2.04]\n","Epoch 340:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.97, valid_loss=-2.04]\n","Epoch 341:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.84, valid_loss=-2.04]\n","Epoch 342:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-2.11, valid_loss=-2.04]\n","Epoch 343:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.04, valid_loss=-2.04]\n","Epoch 344:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.04, valid_loss=-2.04]\n","Epoch 345:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.07, valid_loss=-2.04]\n","Epoch 346:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.98, valid_loss=-2.04]\n","Epoch 347:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.04, valid_loss=-2.04]\n","Epoch 348:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.01, valid_loss=-2.04]\n","Epoch 349:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.00, valid_loss=-2.04]\n","Epoch 349: 100%|██████████| 4/4 [00:00\u003c00:00, 11.35it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.00, valid_loss=-2.04]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12811)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=12811)\u001b[0m `Trainer.fit` stopped: `max_steps=1400.0` reached.\n","2024-11-12 18:50:00,910\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (168, 24, 1), 'n_pool_kernel_size': (8, 4, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=12811)\u001b[0m \n","\u001b[36m(_train_tune pid=12811)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.03it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12811)\u001b[0m \r                                                                      \u001b[A\rEpoch 349: 100%|██████████| 4/4 [00:02\u003c00:00,  1.65it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.00, valid_loss=-2.05]\rEpoch 349: 100%|██████████| 4/4 [00:02\u003c00:00,  1.65it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.00, valid_loss=-2.05]\rEpoch 349: 100%|██████████| 4/4 [00:02\u003c00:00,  1.65it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.00, valid_loss=-2.05]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=13559)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=13559)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=13559)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=13559)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=13559)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=13559)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=13559)\u001b[0m 2024-11-12 18:50:09.452626: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=13559)\u001b[0m 2024-11-12 18:50:09.473233: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=13559)\u001b[0m 2024-11-12 18:50:09.499212: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=13559)\u001b[0m 2024-11-12 18:50:09.507028: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=13559)\u001b[0m 2024-11-12 18:50:09.526347: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=13559)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=13559)\u001b[0m 2024-11-12 18:50:10.700791: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=13559)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=13559)\u001b[0m \n","\u001b[36m(_train_tune pid=13559)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=13559)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=13559)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=13559)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=13559)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=13559)\u001b[0m 3 | blocks       | ModuleList       | 2.6 M  | train\n","\u001b[36m(_train_tune pid=13559)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=13559)\u001b[0m 2.6 M     Trainable params\n","\u001b[36m(_train_tune pid=13559)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=13559)\u001b[0m 2.6 M     Total params\n","\u001b[36m(_train_tune pid=13559)\u001b[0m 10.495    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=13559)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=13559)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/2 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.55]\n","Epoch 1: 100%|██████████| 2/2 [00:00\u003c00:00,  7.22it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.55]\n","Epoch 2:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.50]\n","Epoch 3:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.58]\n","Epoch 4:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.50]\n","Epoch 5:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.56]\n","Epoch 6:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.55]\n","Epoch 7:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.52]\n","Epoch 8:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.60]\n","Epoch 9:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.61]\n","Epoch 10:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.65]\n","Epoch 11:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.69]\n","Epoch 12:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.66]\n","Epoch 13:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.74]\n","Epoch 14:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.73]\n","Epoch 15:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.74]\n","Epoch 16:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.81]\n","Epoch 17:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.69]\n","Epoch 18:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.76]\n","Epoch 19:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.85]\n","Epoch 20:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.81]\n","Epoch 21:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.79]\n","Epoch 22:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.78]\n","Epoch 23:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.88]\n","Epoch 24:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.86]\n","Epoch 25:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.86]\n","Epoch 26:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.83]\n","Epoch 27:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.86]\n","Epoch 28:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.84]\n","Epoch 29:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.81]\n","Epoch 30:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.90]\n","Epoch 31:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.93]\n","Epoch 32:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.89]\n","Epoch 33:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.90]\n","Epoch 34:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.91]\n","Epoch 35:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.86]\n","Epoch 36:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.87]\n","Epoch 37:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.89]\n","Epoch 37: 100%|██████████| 2/2 [00:00\u003c00:00,  8.41it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.91]\n","Epoch 38:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.91]\n","Epoch 39:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.93]\n","Epoch 40:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.86]\n","Epoch 41:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.94]\n","Epoch 42:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.90]\n","Epoch 43:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.90]\n","Epoch 44:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.92]\n","Epoch 45:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.92]\n","Epoch 46:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.88]\n","Epoch 47:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.94]\n","Epoch 48:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.93]\n","Epoch 49:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.90]\n","Epoch 49: 100%|██████████| 2/2 [00:00\u003c00:00,  8.29it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.90]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13559)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13559)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 11.38it/s]\u001b[A\n","Epoch 50:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.87, valid_loss=-1.90]\n","Epoch 51:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.93, valid_loss=-1.90]\n","Epoch 52:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.96, valid_loss=-1.90]\n","Epoch 53:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.93, valid_loss=-1.90]\n","Epoch 54:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.93, valid_loss=-1.90]\n","Epoch 55:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.94, valid_loss=-1.90]\n","Epoch 56:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.93, valid_loss=-1.90]\n","Epoch 57:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.93, valid_loss=-1.90]\n","Epoch 58:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.94, valid_loss=-1.90]\n","Epoch 59:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.92, valid_loss=-1.90]\n","Epoch 60:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.91, valid_loss=-1.90]\n","Epoch 61:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.01, valid_loss=-1.90]\n","Epoch 62:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.03, valid_loss=-1.90]\n","Epoch 63:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.96, valid_loss=-1.90]\n","Epoch 64:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.89, valid_loss=-1.90]\n","Epoch 65:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.03, valid_loss=-1.90]\n","Epoch 66:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.02, valid_loss=-1.90]\n","Epoch 67:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.95, valid_loss=-1.90]\n","Epoch 68:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.97, valid_loss=-1.90]\n","Epoch 69:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.96, valid_loss=-1.90]\n","Epoch 70:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.98, valid_loss=-1.90]\n","Epoch 71:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.93, valid_loss=-1.90]\n","Epoch 72:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.96, valid_loss=-1.90]\n","Epoch 72: 100%|██████████| 2/2 [00:00\u003c00:00,  8.30it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.96, valid_loss=-1.90]\n","Epoch 73:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.96, valid_loss=-1.90]\n","Epoch 74:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.98, valid_loss=-1.90]\n","Epoch 75:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.00, valid_loss=-1.90]\n","Epoch 76:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.91, valid_loss=-1.90]\n","Epoch 77:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.97, valid_loss=-1.90]\n","Epoch 78:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.99, valid_loss=-1.90]\n","Epoch 79:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.03, valid_loss=-1.90]\n","Epoch 80:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.03, valid_loss=-1.90]\n","Epoch 81:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.04, valid_loss=-1.90]\n","Epoch 82:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.99, valid_loss=-1.90]\n","Epoch 83:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.96, valid_loss=-1.90]\n","Epoch 84:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.03, valid_loss=-1.90]\n","Epoch 85:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.01, valid_loss=-1.90]\n","Epoch 86:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.02, valid_loss=-1.90]\n","Epoch 87:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.95, valid_loss=-1.90]\n","Epoch 88:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.00, valid_loss=-1.90]\n","Epoch 89:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.00, valid_loss=-1.90]\n","Epoch 90:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.97, valid_loss=-1.90]\n","Epoch 91:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.94, valid_loss=-1.90]\n","Epoch 92:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.96, valid_loss=-1.90]\n","Epoch 93:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.00, valid_loss=-1.90]\n","Epoch 94:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.02, valid_loss=-1.90]\n","Epoch 95:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.04, valid_loss=-1.90]\n","Epoch 96:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.05, valid_loss=-1.90]\n","Epoch 97:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.98, valid_loss=-1.90]\n","Epoch 98:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.97, valid_loss=-1.90]\n","Epoch 99:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.04, valid_loss=-1.90]\n","Epoch 99: 100%|██████████| 2/2 [00:00\u003c00:00,  8.51it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.04, valid_loss=-1.90]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13559)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13559)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 11.24it/s]\u001b[A\n","Epoch 100:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.02, valid_loss=-1.94]\n","Epoch 101:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.99, valid_loss=-1.94]\n","Epoch 102:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.98, valid_loss=-1.94]\n","Epoch 103:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.99, valid_loss=-1.94]\n","Epoch 104:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.00, valid_loss=-1.94]\n","Epoch 105:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.98, valid_loss=-1.94]\n","Epoch 106:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.00, valid_loss=-1.94]\n","Epoch 107:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.02, valid_loss=-1.94]\n","Epoch 108:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.05, valid_loss=-1.94]\n","Epoch 109:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.95, valid_loss=-1.94]\n","Epoch 110:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.95, valid_loss=-1.94]\n","Epoch 111:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.03, valid_loss=-1.94]\n","Epoch 112:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.97, valid_loss=-1.94]\n","Epoch 113:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.94, valid_loss=-1.94]\n","Epoch 114:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.96, valid_loss=-1.94]\n","Epoch 115:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.01, valid_loss=-1.94]\n","Epoch 116:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.99, valid_loss=-1.94]\n","Epoch 117:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.99, valid_loss=-1.94]\n","Epoch 118:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.97, valid_loss=-1.94]\n","Epoch 119:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.98, valid_loss=-1.94]\n","Epoch 119: 100%|██████████| 2/2 [00:00\u003c00:00,  8.37it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.98, valid_loss=-1.94]\n","Epoch 120:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.00, valid_loss=-1.94]\n","Epoch 121:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.04, valid_loss=-1.94]\n","Epoch 122:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.02, valid_loss=-1.94]\n","Epoch 123:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.02, valid_loss=-1.94]\n","Epoch 124:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.03, valid_loss=-1.94]\n","Epoch 125:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.08, valid_loss=-1.94]\n","Epoch 126:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.06, valid_loss=-1.94]\n","Epoch 127:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.02, valid_loss=-1.94]\n","Epoch 128:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.09, valid_loss=-1.94]\n","Epoch 129:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.04, valid_loss=-1.94]\n","Epoch 130:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.96, valid_loss=-1.94]\n","Epoch 131:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.01, valid_loss=-1.94]\n","Epoch 132:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.00, valid_loss=-1.94]\n","Epoch 133:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.05, valid_loss=-1.94]\n","Epoch 134:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.01, valid_loss=-1.94]\n","Epoch 135:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.00, valid_loss=-1.94]\n","Epoch 136:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.01, valid_loss=-1.94]\n","Epoch 137:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.01, valid_loss=-1.94]\n","Epoch 138:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.05, valid_loss=-1.94]\n","Epoch 139:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.98, valid_loss=-1.94]\n","Epoch 140:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.05, valid_loss=-1.94]\n","Epoch 141:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.06, valid_loss=-1.94]\n","Epoch 142:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.03, valid_loss=-1.94]\n","Epoch 143:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.04, valid_loss=-1.94]\n","Epoch 144:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.05, valid_loss=-1.94]\n","Epoch 145:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.99, valid_loss=-1.94]\n","Epoch 146:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.00, valid_loss=-1.94]\n","Epoch 147:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.98, valid_loss=-1.94]\n","Epoch 148:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.04, valid_loss=-1.94]\n","Epoch 149:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.02, valid_loss=-1.94]\n","Epoch 149: 100%|██████████| 2/2 [00:00\u003c00:00,  8.32it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.02, valid_loss=-1.94]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13559)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13559)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 11.57it/s]\u001b[A\n","Epoch 150:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.07, valid_loss=-1.93]\n","Epoch 151:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.06, valid_loss=-1.93]\n","Epoch 152:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.03, valid_loss=-1.93]\n","Epoch 153:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.02, valid_loss=-1.93]\n","Epoch 153: 100%|██████████| 2/2 [00:00\u003c00:00,  8.37it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.07, valid_loss=-1.93]\n","Epoch 154:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.07, valid_loss=-1.93]\n","Epoch 155:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.02, valid_loss=-1.93]\n","Epoch 156:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.05, valid_loss=-1.93]\n","Epoch 157:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.04, valid_loss=-1.93]\n","Epoch 158:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.01, valid_loss=-1.93]\n","Epoch 159:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.01, valid_loss=-1.93]\n","Epoch 160:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.03, valid_loss=-1.93]\n","Epoch 161:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.01, valid_loss=-1.93]\n","Epoch 162:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.09, valid_loss=-1.93]\n","Epoch 163:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.04, valid_loss=-1.93]\n","Epoch 164:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.05, valid_loss=-1.93]\n","Epoch 165:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.04, valid_loss=-1.93]\n","Epoch 166:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.02, valid_loss=-1.93]\n","Epoch 167:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.09, valid_loss=-1.93]\n","Epoch 168:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.05, valid_loss=-1.93]\n","Epoch 169:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.04, valid_loss=-1.93]\n","Epoch 170:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.02, valid_loss=-1.93]\n","Epoch 171:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.03, valid_loss=-1.93]\n","Epoch 172:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.03, valid_loss=-1.93]\n","Epoch 173:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.99, valid_loss=-1.93]\n","Epoch 174:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.05, valid_loss=-1.93]\n","Epoch 175:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.04, valid_loss=-1.93]\n","Epoch 176:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.06, valid_loss=-1.93]\n","Epoch 177:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.01, valid_loss=-1.93]\n","Epoch 177: 100%|██████████| 2/2 [00:00\u003c00:00,  8.50it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.03, valid_loss=-1.93]\n","Epoch 178:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.03, valid_loss=-1.93]\n","Epoch 179:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.07, valid_loss=-1.93]\n","Epoch 180:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.04, valid_loss=-1.93]\n","Epoch 181:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.03, valid_loss=-1.93]\n","Epoch 182:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.05, valid_loss=-1.93]\n","Epoch 183:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.01, valid_loss=-1.93]\n","Epoch 184:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.06, valid_loss=-1.93]\n","Epoch 185:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.04, valid_loss=-1.93]\n","Epoch 186:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.06, valid_loss=-1.93]\n","Epoch 187:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.07, valid_loss=-1.93]\n","Epoch 188:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.08, valid_loss=-1.93]\n","Epoch 189:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.07, valid_loss=-1.93]\n","Epoch 190:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.00, valid_loss=-1.93]\n","Epoch 191:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.02, valid_loss=-1.93]\n","Epoch 192:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.07, valid_loss=-1.93]\n","Epoch 193:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.11, valid_loss=-1.93]\n","Epoch 194:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.09, valid_loss=-1.93]\n","Epoch 195:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.03, valid_loss=-1.93]\n","Epoch 196:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.07, valid_loss=-1.93]\n","Epoch 197:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.09, valid_loss=-1.93]\n","Epoch 198:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.00, valid_loss=-1.93]\n","Epoch 199:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.99, valid_loss=-1.93]\n","Epoch 199: 100%|██████████| 2/2 [00:00\u003c00:00,  8.59it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.99, valid_loss=-1.93]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13559)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13559)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 11.38it/s]\u001b[A\n","Epoch 200:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.07, valid_loss=-1.96]\n","Epoch 201:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.08, valid_loss=-1.96]\n","Epoch 202:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.02, valid_loss=-1.96]\n","Epoch 203:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.04, valid_loss=-1.96]\n","Epoch 204:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.97, valid_loss=-1.96]\n","Epoch 204: 100%|██████████| 2/2 [00:00\u003c00:00,  8.35it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.97, valid_loss=-1.96]\n","Epoch 205:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.00, valid_loss=-1.96]\n","Epoch 206:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.06, valid_loss=-1.96]\n","Epoch 207:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.04, valid_loss=-1.96]\n","Epoch 208:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.05, valid_loss=-1.96]\n","Epoch 209:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.08, valid_loss=-1.96]\n","Epoch 210:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.08, valid_loss=-1.96]\n","Epoch 211:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.04, valid_loss=-1.96]\n","Epoch 212:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.07, valid_loss=-1.96]\n","Epoch 213:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.05, valid_loss=-1.96]\n","Epoch 214:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.07, valid_loss=-1.96]\n","Epoch 215:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.02, valid_loss=-1.96]\n","Epoch 216:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.06, valid_loss=-1.96]\n","Epoch 217:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.06, valid_loss=-1.96]\n","Epoch 218:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.10, valid_loss=-1.96]\n","Epoch 219:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.04, valid_loss=-1.96]\n","Epoch 220:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.08, valid_loss=-1.96]\n","Epoch 221:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.07, valid_loss=-1.96]\n","Epoch 222:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.03, valid_loss=-1.96]\n","Epoch 223:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.04, valid_loss=-1.96]\n","Epoch 224:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.13, valid_loss=-1.96]\n","Epoch 225:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.04, valid_loss=-1.96]\n","Epoch 226:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.06, valid_loss=-1.96]\n","Epoch 227:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.08, valid_loss=-1.96]\n","Epoch 228:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.01, valid_loss=-1.96]\n","Epoch 229:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.97, valid_loss=-1.96]\n","Epoch 230:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.10, valid_loss=-1.96]\n","Epoch 231:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.07, valid_loss=-1.96]\n","Epoch 232:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.09, valid_loss=-1.96]\n","Epoch 233:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.06, valid_loss=-1.96]\n","Epoch 234:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.10, valid_loss=-1.96]\n","Epoch 235:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.03, valid_loss=-1.96]\n","Epoch 236:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.05, valid_loss=-1.96]\n","Epoch 237:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.08, valid_loss=-1.96]\n","Epoch 238:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.00, valid_loss=-1.96]\n","Epoch 239:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.04, valid_loss=-1.96]\n","Epoch 240:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.09, valid_loss=-1.96]\n","Epoch 241:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.03, valid_loss=-1.96]\n","Epoch 242:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.10, valid_loss=-1.96]\n","Epoch 242: 100%|██████████| 2/2 [00:00\u003c00:00,  8.28it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.10, valid_loss=-1.96]\n","Epoch 243:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.07, valid_loss=-1.96]\n","Epoch 244:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.02, valid_loss=-1.96]\n","Epoch 245:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.09, valid_loss=-1.96]\n","Epoch 246:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.05, valid_loss=-1.96]\n","Epoch 247:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.13, valid_loss=-1.96]\n","Epoch 248:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.09, valid_loss=-1.96]\n","Epoch 249:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.03, valid_loss=-1.96]\n","Epoch 249: 100%|██████████| 2/2 [00:00\u003c00:00,  8.30it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.03, valid_loss=-1.96]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13559)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13559)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 11.48it/s]\u001b[A\n","Epoch 250:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.09, valid_loss=-1.97]\n","Epoch 251:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.06, valid_loss=-1.97]\n","Epoch 252:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 253:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 254:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.05, valid_loss=-1.97]\n","Epoch 255:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.04, valid_loss=-1.97]\n","Epoch 256:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.05, valid_loss=-1.97]\n","Epoch 257:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.05, valid_loss=-1.97]\n","Epoch 258:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.03, valid_loss=-1.97]\n","Epoch 259:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.02, valid_loss=-1.97]\n","Epoch 260:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.05, valid_loss=-1.97]\n","Epoch 261:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 262:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.09, valid_loss=-1.97]\n","Epoch 263:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.08, valid_loss=-1.97]\n","Epoch 264:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.02, valid_loss=-1.97]\n","Epoch 265:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.02, valid_loss=-1.97]\n","Epoch 266:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.05, valid_loss=-1.97]\n","Epoch 267:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.08, valid_loss=-1.97]\n","Epoch 268:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 269:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 270:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.06, valid_loss=-1.97]\n","Epoch 271:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 272:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.05, valid_loss=-1.97]\n","Epoch 273:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.05, valid_loss=-1.97]\n","Epoch 274:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.03, valid_loss=-1.97]\n","Epoch 275:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 276:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.04, valid_loss=-1.97]\n","Epoch 277:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.11, valid_loss=-1.97]\n","Epoch 278:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.05, valid_loss=-1.97]\n","Epoch 279:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.08, valid_loss=-1.97]\n","Epoch 280:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 281:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 282:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.98, valid_loss=-1.97]\n","Epoch 283:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 284:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.13, valid_loss=-1.97]\n","Epoch 285:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.05, valid_loss=-1.97]\n","Epoch 286:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.05, valid_loss=-1.97]\n","Epoch 287:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.06, valid_loss=-1.97]\n","Epoch 288:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 289:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.11, valid_loss=-1.97]\n","Epoch 290:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.12, valid_loss=-1.97]\n","Epoch 291:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.04, valid_loss=-1.97]\n","Epoch 292:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.10, valid_loss=-1.97]\n","Epoch 293:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.09, valid_loss=-1.97]\n","Epoch 294:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.09, valid_loss=-1.97]\n","Epoch 295:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.08, valid_loss=-1.97]\n","Epoch 296:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.03, valid_loss=-1.97]\n","Epoch 297:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.04, valid_loss=-1.97]\n","Epoch 298:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 299:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 299: 100%|██████████| 2/2 [00:00\u003c00:00,  8.33it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13559)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13559)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 11.45it/s]\u001b[A\n","Epoch 300:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.06, valid_loss=-1.95]\n","Epoch 301:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.09, valid_loss=-1.95]\n","Epoch 302:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.06, valid_loss=-1.95]\n","Epoch 303:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.04, valid_loss=-1.95]\n","Epoch 304:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.08, valid_loss=-1.95]\n","Epoch 305:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.08, valid_loss=-1.95]\n","Epoch 306:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.07, valid_loss=-1.95]\n","Epoch 307:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.07, valid_loss=-1.95]\n","Epoch 308:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.12, valid_loss=-1.95]\n","Epoch 309:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.07, valid_loss=-1.95]\n","Epoch 310:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.09, valid_loss=-1.95]\n","Epoch 311:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.07, valid_loss=-1.95]\n","Epoch 312:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.00, valid_loss=-1.95]\n","Epoch 313:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.07, valid_loss=-1.95]\n","Epoch 314:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.06, valid_loss=-1.95]\n","Epoch 315:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.10, valid_loss=-1.95]\n","Epoch 316:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.05, valid_loss=-1.95]\n","Epoch 317:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.06, valid_loss=-1.95]\n","Epoch 318:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.01, valid_loss=-1.95]\n","Epoch 319:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.10, valid_loss=-1.95]\n","Epoch 320:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.06, valid_loss=-1.95]\n","Epoch 321:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.09, valid_loss=-1.95]\n","Epoch 322:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.11, valid_loss=-1.95]\n","Epoch 323:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.00, valid_loss=-1.95]\n","Epoch 324:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.11, valid_loss=-1.95]\n","Epoch 325:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.04, valid_loss=-1.95]\n","Epoch 326:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.11, valid_loss=-1.95]\n","Epoch 327:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.03, valid_loss=-1.95]\n","Epoch 328:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.05, valid_loss=-1.95]\n","Epoch 329:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.07, valid_loss=-1.95]\n","Epoch 330:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.06, valid_loss=-1.95]\n","Epoch 331:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.03, valid_loss=-1.95]\n","Epoch 332:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.07, valid_loss=-1.95]\n","Epoch 333:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.08, valid_loss=-1.95]\n","Epoch 334:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.04, valid_loss=-1.95]\n","Epoch 335:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.08, valid_loss=-1.95]\n","Epoch 335: 100%|██████████| 2/2 [00:00\u003c00:00,  8.28it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.08, valid_loss=-1.95]\n","Epoch 336:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.08, valid_loss=-1.95]\n","Epoch 337:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.07, valid_loss=-1.95]\n","Epoch 338:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.12, valid_loss=-1.95]\n","Epoch 339:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.09, valid_loss=-1.95]\n","Epoch 340:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.09, valid_loss=-1.95]\n","Epoch 341:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.04, valid_loss=-1.95]\n","Epoch 342:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.06, valid_loss=-1.95]\n","Epoch 343:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.06, valid_loss=-1.95]\n","Epoch 344:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.10, valid_loss=-1.95]\n","Epoch 345:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.07, valid_loss=-1.95]\n","Epoch 346:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.07, valid_loss=-1.95]\n","Epoch 347:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.14, valid_loss=-1.95]\n","Epoch 348:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.06, valid_loss=-1.95]\n","Epoch 349:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.11, valid_loss=-1.95]\n","Epoch 349: 100%|██████████| 2/2 [00:00\u003c00:00,  8.54it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.11, valid_loss=-1.95]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13559)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13559)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 11.31it/s]\u001b[A\n","Epoch 350:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.08, valid_loss=-1.95]\n","Epoch 351:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.10, valid_loss=-1.95]\n","Epoch 352:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.10, valid_loss=-1.95]\n","Epoch 353:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.00, valid_loss=-1.95]\n","Epoch 354:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.03, valid_loss=-1.95]\n","Epoch 355:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.02, valid_loss=-1.95]\n","Epoch 356:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.06, valid_loss=-1.95]\n","Epoch 357:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.09, valid_loss=-1.95]\n","Epoch 358:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.06, valid_loss=-1.95]\n","Epoch 359:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.08, valid_loss=-1.95]\n","Epoch 360:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.99, valid_loss=-1.95]\n","Epoch 361:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.07, valid_loss=-1.95]\n","Epoch 362:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.07, valid_loss=-1.95]\n","Epoch 363:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.06, valid_loss=-1.95]\n","Epoch 364:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.09, valid_loss=-1.95]\n","Epoch 365:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.05, valid_loss=-1.95]\n","Epoch 366:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.14, valid_loss=-1.95]\n","Epoch 367:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.10, valid_loss=-1.95]\n","Epoch 368:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.06, valid_loss=-1.95]\n","Epoch 369:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.05, valid_loss=-1.95]\n","Epoch 370:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.14, valid_loss=-1.95]\n","Epoch 371:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.04, valid_loss=-1.95]\n","Epoch 372:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.10, valid_loss=-1.95]\n","Epoch 373:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.11, valid_loss=-1.95]\n","Epoch 374:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.10, valid_loss=-1.95]\n","Epoch 375:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.11, valid_loss=-1.95]\n","Epoch 376:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.09, valid_loss=-1.95]\n","Epoch 377:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.09, valid_loss=-1.95]\n","Epoch 378:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.04, valid_loss=-1.95]\n","Epoch 379:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.11, valid_loss=-1.95]\n","Epoch 380:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.08, valid_loss=-1.95]\n","Epoch 381:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.14, valid_loss=-1.95]\n","Epoch 382:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.05, valid_loss=-1.95]\n","Epoch 383:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.11, valid_loss=-1.95]\n","Epoch 384:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.10, valid_loss=-1.95]\n","Epoch 385:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.06, valid_loss=-1.95]\n","Epoch 386:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.11, valid_loss=-1.95]\n","Epoch 387:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.14, valid_loss=-1.95]\n","Epoch 388:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.07, valid_loss=-1.95]\n","Epoch 389:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.01, valid_loss=-1.95]\n","Epoch 390:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.05, valid_loss=-1.95]\n","Epoch 391:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.07, valid_loss=-1.95]\n","Epoch 392:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.11, valid_loss=-1.95]\n","Epoch 393:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.07, valid_loss=-1.95]\n","Epoch 394:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.09, valid_loss=-1.95]\n","Epoch 395:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.14, valid_loss=-1.95]\n","Epoch 396:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.05, valid_loss=-1.95]\n","Epoch 397:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.10, valid_loss=-1.95]\n","Epoch 398:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.09, valid_loss=-1.95]\n","Epoch 399:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.09, valid_loss=-1.95]\n","Epoch 399: 100%|██████████| 2/2 [00:00\u003c00:00,  8.17it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.09, valid_loss=-1.95]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13559)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13559)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 11.32it/s]\u001b[A\n","Epoch 400:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.03, valid_loss=-1.97]\n","Epoch 401:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 402:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.10, valid_loss=-1.97]\n","Epoch 403:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.10, valid_loss=-1.97]\n","Epoch 404:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.06, valid_loss=-1.97]\n","Epoch 405:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.10, valid_loss=-1.97]\n","Epoch 406:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.05, valid_loss=-1.97]\n","Epoch 407:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 408:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.06, valid_loss=-1.97]\n","Epoch 409:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.11, valid_loss=-1.97]\n","Epoch 410:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.09, valid_loss=-1.97]\n","Epoch 411:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.13, valid_loss=-1.97]\n","Epoch 412:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.05, valid_loss=-1.97]\n","Epoch 413:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.06, valid_loss=-1.97]\n","Epoch 414:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.99, valid_loss=-1.97]\n","Epoch 415:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 416:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 417:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.02, valid_loss=-1.97]\n","Epoch 418:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.05, valid_loss=-1.97]\n","Epoch 419:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.04, valid_loss=-1.97]\n","Epoch 420:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.10, valid_loss=-1.97]\n","Epoch 421:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.12, valid_loss=-1.97]\n","Epoch 422:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.08, valid_loss=-1.97]\n","Epoch 423:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.06, valid_loss=-1.97]\n","Epoch 424:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.09, valid_loss=-1.97]\n","Epoch 425:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.13, valid_loss=-1.97]\n","Epoch 426:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.05, valid_loss=-1.97]\n","Epoch 427:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 428:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 429:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.12, valid_loss=-1.97]\n","Epoch 430:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.06, valid_loss=-1.97]\n","Epoch 431:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.11, valid_loss=-1.97]\n","Epoch 432:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.05, valid_loss=-1.97]\n","Epoch 433:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.04, valid_loss=-1.97]\n","Epoch 434:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.10, valid_loss=-1.97]\n","Epoch 435:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.09, valid_loss=-1.97]\n","Epoch 436:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 437:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 438:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.11, valid_loss=-1.97]\n","Epoch 439:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.08, valid_loss=-1.97]\n","Epoch 440:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.13, valid_loss=-1.97]\n","Epoch 441:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.12, valid_loss=-1.97]\n","Epoch 442:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.06, valid_loss=-1.97]\n","Epoch 443:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.06, valid_loss=-1.97]\n","Epoch 444:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.11, valid_loss=-1.97]\n","Epoch 444: 100%|██████████| 2/2 [00:00\u003c00:00,  8.39it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.11, valid_loss=-1.97]\n","Epoch 445:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 446:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.06, valid_loss=-1.97]\n","Epoch 447:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.08, valid_loss=-1.97]\n","Epoch 448:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.08, valid_loss=-1.97]\n","Epoch 449:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.11, valid_loss=-1.97]\n","Epoch 449: 100%|██████████| 2/2 [00:00\u003c00:00,  8.42it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.11, valid_loss=-1.97]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13559)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13559)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 11.11it/s]\u001b[A\n","Epoch 450:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.10, valid_loss=-1.96]\n","Epoch 451:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.10, valid_loss=-1.96]\n","Epoch 452:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.11, valid_loss=-1.96]\n","Epoch 453:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.09, valid_loss=-1.96]\n","Epoch 454:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.15, valid_loss=-1.96]\n","Epoch 455:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.10, valid_loss=-1.96]\n","Epoch 456:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.15, valid_loss=-1.96]\n","Epoch 457:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.14, valid_loss=-1.96]\n","Epoch 458:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.06, valid_loss=-1.96]\n","Epoch 459:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.11, valid_loss=-1.96]\n","Epoch 460:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.02, valid_loss=-1.96]\n","Epoch 461:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.11, valid_loss=-1.96]\n","Epoch 462:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.08, valid_loss=-1.96]\n","Epoch 463:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.06, valid_loss=-1.96]\n","Epoch 464:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.05, valid_loss=-1.96]\n","Epoch 465:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.13, valid_loss=-1.96]\n","Epoch 466:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.13, valid_loss=-1.96]\n","Epoch 467:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.13, valid_loss=-1.96]\n","Epoch 468:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.09, valid_loss=-1.96]\n","Epoch 469:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.06, valid_loss=-1.96]\n","Epoch 470:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.06, valid_loss=-1.96]\n","Epoch 471:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.00, valid_loss=-1.96]\n","Epoch 471: 100%|██████████| 2/2 [00:00\u003c00:00,  8.45it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.15, valid_loss=-1.96]\n","Epoch 472:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.15, valid_loss=-1.96]\n","Epoch 473:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.11, valid_loss=-1.96]\n","Epoch 474:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.09, valid_loss=-1.96]\n","Epoch 475:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.09, valid_loss=-1.96]\n","Epoch 476:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.09, valid_loss=-1.96]\n","Epoch 477:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.07, valid_loss=-1.96]\n","Epoch 478:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.09, valid_loss=-1.96]\n","Epoch 479:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.02, valid_loss=-1.96]\n","Epoch 480:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.08, valid_loss=-1.96]\n","Epoch 481:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.08, valid_loss=-1.96]\n","Epoch 482:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.09, valid_loss=-1.96]\n","Epoch 483:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.14, valid_loss=-1.96]\n","Epoch 484:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.06, valid_loss=-1.96]\n","Epoch 485:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.01, valid_loss=-1.96]\n","Epoch 486:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.10, valid_loss=-1.96]\n","Epoch 487:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.11, valid_loss=-1.96]\n","Epoch 488:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.14, valid_loss=-1.96]\n","Epoch 489:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.11, valid_loss=-1.96]\n","Epoch 490:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.09, valid_loss=-1.96]\n","Epoch 491:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.09, valid_loss=-1.96]\n","Epoch 492:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.09, valid_loss=-1.96]\n","Epoch 493:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.08, valid_loss=-1.96]\n","Epoch 494:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.07, valid_loss=-1.96]\n","Epoch 495:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.06, valid_loss=-1.96]\n","Epoch 496:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.13, valid_loss=-1.96]\n","Epoch 497:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.06, valid_loss=-1.96]\n","Epoch 498:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.15, valid_loss=-1.96]\n","Epoch 499:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.10, valid_loss=-1.96]\n","Epoch 499: 100%|██████████| 2/2 [00:00\u003c00:00,  8.46it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.10, valid_loss=-1.96]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13559)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13559)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 11.25it/s]\u001b[A\n","Epoch 500:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.13, valid_loss=-1.97]\n","Epoch 501:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 502:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.11, valid_loss=-1.97]\n","Epoch 503:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.09, valid_loss=-1.97]\n","Epoch 504:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 505:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.11, valid_loss=-1.97]\n","Epoch 506:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.06, valid_loss=-1.97]\n","Epoch 507:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 508:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.08, valid_loss=-1.97]\n","Epoch 509:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.02, valid_loss=-1.97]\n","Epoch 510:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.09, valid_loss=-1.97]\n","Epoch 511:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 512:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.15, valid_loss=-1.97]\n","Epoch 513:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.08, valid_loss=-1.97]\n","Epoch 514:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 515:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.14, valid_loss=-1.97]\n","Epoch 516:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.06, valid_loss=-1.97]\n","Epoch 517:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.14, valid_loss=-1.97]\n","Epoch 518:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 519:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.01, valid_loss=-1.97]\n","Epoch 520:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.09, valid_loss=-1.97]\n","Epoch 521:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.11, valid_loss=-1.97]\n","Epoch 522:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.12, valid_loss=-1.97]\n","Epoch 523:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.08, valid_loss=-1.97]\n","Epoch 524:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.05, valid_loss=-1.97]\n","Epoch 525:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.04, valid_loss=-1.97]\n","Epoch 526:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.11, valid_loss=-1.97]\n","Epoch 527:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 528:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.06, valid_loss=-1.97]\n","Epoch 529:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.04, valid_loss=-1.97]\n","Epoch 530:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.09, valid_loss=-1.97]\n","Epoch 531:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.06, valid_loss=-1.97]\n","Epoch 532:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.08, valid_loss=-1.97]\n","Epoch 533:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.12, valid_loss=-1.97]\n","Epoch 534:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.10, valid_loss=-1.97]\n","Epoch 535:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.14, valid_loss=-1.97]\n","Epoch 536:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.13, valid_loss=-1.97]\n","Epoch 537:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.10, valid_loss=-1.97]\n","Epoch 538:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.08, valid_loss=-1.97]\n","Epoch 539:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.08, valid_loss=-1.97]\n","Epoch 540:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.14, valid_loss=-1.97]\n","Epoch 541:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.10, valid_loss=-1.97]\n","Epoch 542:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.11, valid_loss=-1.97]\n","Epoch 543:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.12, valid_loss=-1.97]\n","Epoch 544:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 544: 100%|██████████| 2/2 [00:00\u003c00:00,  8.40it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.09, valid_loss=-1.97]\n","Epoch 545:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.09, valid_loss=-1.97]\n","Epoch 546:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.13, valid_loss=-1.97]\n","Epoch 547:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 547: 100%|██████████| 2/2 [00:00\u003c00:00,  8.31it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 548:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.15, valid_loss=-1.97]\n","Epoch 549:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.11, valid_loss=-1.97]\n","Epoch 549: 100%|██████████| 2/2 [00:00\u003c00:00,  8.46it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.11, valid_loss=-1.97]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13559)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 18:52:29,366\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (168, 24, 1), 'n_pool_kernel_size': (8, 4, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=13559)\u001b[0m `Trainer.fit` stopped: `max_steps=1100.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=13559)\u001b[0m \n","\u001b[36m(_train_tune pid=13559)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 11.40it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13559)\u001b[0m \r                                                                      \u001b[A\rEpoch 549: 100%|██████████| 2/2 [00:00\u003c00:00,  3.62it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.11, valid_loss=-1.97]\rEpoch 549: 100%|██████████| 2/2 [00:00\u003c00:00,  3.60it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.08, valid_loss=-1.97]\rEpoch 549: 100%|██████████| 2/2 [00:00\u003c00:00,  3.59it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.08, valid_loss=-1.97]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=14236)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=14236)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=14236)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=14236)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=14236)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=14236)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=14236)\u001b[0m 2024-11-12 18:52:37.274202: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=14236)\u001b[0m 2024-11-12 18:52:37.296931: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=14236)\u001b[0m 2024-11-12 18:52:37.323896: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=14236)\u001b[0m 2024-11-12 18:52:37.332373: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=14236)\u001b[0m 2024-11-12 18:52:37.351744: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=14236)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=14236)\u001b[0m 2024-11-12 18:52:38.588475: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=14236)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=14236)\u001b[0m \n","\u001b[36m(_train_tune pid=14236)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=14236)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=14236)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=14236)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=14236)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=14236)\u001b[0m 3 | blocks       | ModuleList       | 2.5 M  | train\n","\u001b[36m(_train_tune pid=14236)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=14236)\u001b[0m 2.5 M     Trainable params\n","\u001b[36m(_train_tune pid=14236)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=14236)\u001b[0m 2.5 M     Total params\n","\u001b[36m(_train_tune pid=14236)\u001b[0m 9.956     Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=14236)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=14236)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/7 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.07]\n","Epoch 2:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-1.79]\n","Epoch 3:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-1.84]\n","Epoch 4:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.85]\n","Epoch 5:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.91]\n","Epoch 6:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.86]\n","Epoch 7:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.93]\n","Epoch 8:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.89]\n","Epoch 9:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-1.96]\n","Epoch 10:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.93]\n","Epoch 11:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-1.96]\n","Epoch 12:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.91]\n","Epoch 13:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.91]\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.92]\n","\u001b[36m(_train_tune pid=14236)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14236)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 27.82it/s]\u001b[A\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.92, valid_loss=-1.95]\n","Epoch 15:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.91, valid_loss=-1.95]\n","Epoch 16:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-2.02, valid_loss=-1.95]\n","Epoch 16: 100%|██████████| 7/7 [00:00\u003c00:00, 31.40it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-2.02, valid_loss=-1.95]\n","Epoch 17:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.94, valid_loss=-1.95]\n","Epoch 18:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.98, valid_loss=-1.95]\n","Epoch 19:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.50, train_loss_epoch=-2.07, valid_loss=-1.95]\n","Epoch 20:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.93, valid_loss=-1.95]\n","Epoch 21:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.00, valid_loss=-1.95]\n","Epoch 22:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.99, valid_loss=-1.95]\n","Epoch 23:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.98, valid_loss=-1.95]\n","Epoch 24:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.99, valid_loss=-1.95]\n","Epoch 25:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.00, valid_loss=-1.95]\n","Epoch 26:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-2.02, valid_loss=-1.95]\n","Epoch 27:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.01, valid_loss=-1.95]\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.96, valid_loss=-1.95]\n","\u001b[36m(_train_tune pid=14236)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14236)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 30.87it/s]\u001b[A\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.96, valid_loss=-1.96]\n","Epoch 29:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.90, valid_loss=-1.96]\n","Epoch 30:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-2.02, valid_loss=-1.96]\n","Epoch 31:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.95, valid_loss=-1.96]\n","Epoch 32:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.97, valid_loss=-1.96]\n","Epoch 33:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.02, valid_loss=-1.96]\n","Epoch 34:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.97, valid_loss=-1.96]\n","Epoch 35:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.02, valid_loss=-1.96]\n","Epoch 36:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.98, valid_loss=-1.96]\n","Epoch 37:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.61, train_loss_epoch=-2.10, valid_loss=-1.96]\n","Epoch 38:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.99, valid_loss=-1.96]\n","Epoch 39:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.98, valid_loss=-1.96]\n","Epoch 40:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.02, valid_loss=-1.96]\n","Epoch 41:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.45, train_loss_epoch=-2.06, valid_loss=-1.96]\n","Epoch 42:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-2.03, valid_loss=-1.96]\n","\u001b[36m(_train_tune pid=14236)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14236)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 28.02it/s]\u001b[A\n","Epoch 43:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.98, valid_loss=-1.97]\n","Epoch 44:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.02, valid_loss=-1.97]\n","Epoch 45:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.01, valid_loss=-1.97]\n","Epoch 46:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.03, valid_loss=-1.97]\n","Epoch 47:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.06, valid_loss=-1.97]\n","Epoch 48:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.98, valid_loss=-1.97]\n","Epoch 49:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.05, valid_loss=-1.97]\n","Epoch 50:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.98, valid_loss=-1.97]\n","Epoch 51:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.02, valid_loss=-1.97]\n","Epoch 52:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.01, valid_loss=-1.97]\n","Epoch 53:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.99, valid_loss=-1.97]\n","Epoch 54:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.00, valid_loss=-1.97]\n","Epoch 55:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-2.01, valid_loss=-1.97]\n","Epoch 56:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.03, valid_loss=-1.97]\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.92, valid_loss=-1.97]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14236)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 30.49it/s]\u001b[A\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.92, valid_loss=-1.97]\n","Epoch 58:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-2.04, valid_loss=-1.97]\n","Epoch 59:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.00, valid_loss=-1.97]\n","Epoch 60:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.00, valid_loss=-1.97]\n","Epoch 61:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-2.01, valid_loss=-1.97]\n","Epoch 62:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.04, valid_loss=-1.97]\n","Epoch 63:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.04, valid_loss=-1.97]\n","Epoch 64:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.99, valid_loss=-1.97]\n","Epoch 65:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.02, valid_loss=-1.97]\n","Epoch 66:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-2.02, valid_loss=-1.97]\n","Epoch 67:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 68:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.04, valid_loss=-1.97]\n","Epoch 69:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-2.00, valid_loss=-1.97]\n","Epoch 70:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.99, valid_loss=-1.97]\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.05, train_loss_epoch=-1.89, valid_loss=-1.97]\n","\u001b[36m(_train_tune pid=14236)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14236)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 27.53it/s]\u001b[A\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.89, valid_loss=-2.00]\n","Epoch 72:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-2.00, valid_loss=-2.00]\n","Epoch 73:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.08, valid_loss=-2.00]\n","Epoch 74:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.42, train_loss_epoch=-2.13, valid_loss=-2.00]\n","Epoch 75:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.05, valid_loss=-2.00]\n","Epoch 76:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.10, valid_loss=-2.00]\n","Epoch 77:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-2.03, valid_loss=-2.00]\n","Epoch 78:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-2.09, valid_loss=-2.00]\n","Epoch 79:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.09, valid_loss=-2.00]\n","Epoch 80:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.08, valid_loss=-2.00]\n","Epoch 81:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-2.03, valid_loss=-2.00]\n","Epoch 82:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.02, valid_loss=-2.00]\n","Epoch 83:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.96, valid_loss=-2.00]\n","Epoch 84:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.05, valid_loss=-2.00]\n","Epoch 85:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.06, valid_loss=-2.00]\n","\u001b[36m(_train_tune pid=14236)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14236)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.81it/s]\u001b[A\n","Epoch 85:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.06, valid_loss=-1.99]\n","Epoch 86:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.07, valid_loss=-1.99]\n","Epoch 87:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.70, train_loss_epoch=-2.13, valid_loss=-1.99]\n","Epoch 88:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.36, train_loss_epoch=-2.09, valid_loss=-1.99]\n","Epoch 89:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.06, valid_loss=-1.99]\n","Epoch 90:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.09, valid_loss=-1.99]\n","Epoch 91:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.12, valid_loss=-1.99]\n","Epoch 92:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-2.04, valid_loss=-1.99]\n","Epoch 93:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.06, valid_loss=-1.99]\n","Epoch 94:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.10, valid_loss=-1.99]\n","Epoch 95:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.99, valid_loss=-1.99]\n","Epoch 96:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-2.10, valid_loss=-1.99]\n","Epoch 97:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.08, valid_loss=-1.99]\n","Epoch 98:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-2.09, valid_loss=-1.99]\n","Epoch 99:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.05, valid_loss=-1.99]\n","Epoch 99: 100%|██████████| 7/7 [00:00\u003c00:00, 29.52it/s, v_num=0, train_loss_step=-2.41, train_loss_epoch=-2.05, valid_loss=-1.99]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14236)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 27.21it/s]\u001b[A\n","Epoch 99: 100%|██████████| 7/7 [00:00\u003c00:00, 13.14it/s, v_num=0, train_loss_step=-2.41, train_loss_epoch=-2.05, valid_loss=-2.00]\n","Epoch 100:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.41, train_loss_epoch=-2.13, valid_loss=-2.00]\n","Epoch 101:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.09, valid_loss=-2.00]\n","Epoch 102:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.04, valid_loss=-2.00]\n","Epoch 103:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.10, valid_loss=-2.00]\n","Epoch 104:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.51, train_loss_epoch=-2.16, valid_loss=-2.00]\n","Epoch 105:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-2.03, valid_loss=-2.00]\n","Epoch 106:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-2.03, valid_loss=-2.00]\n","Epoch 107:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.06, valid_loss=-2.00]\n","Epoch 108:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.41, train_loss_epoch=-2.10, valid_loss=-2.00]\n","Epoch 109:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.09, valid_loss=-2.00]\n","Epoch 110:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.98, valid_loss=-2.00]\n","Epoch 111:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-2.11, valid_loss=-2.00]\n","Epoch 112:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.08, valid_loss=-2.00]\n","Epoch 113:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.12, valid_loss=-2.00]\n","Epoch 114:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.53, train_loss_epoch=-2.14, valid_loss=-2.00]\n","\u001b[36m(_train_tune pid=14236)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14236)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.21it/s]\u001b[A\n","Epoch 114:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.14, valid_loss=-2.01]\n","Epoch 115:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.08, valid_loss=-2.01]\n","Epoch 116:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-2.04, valid_loss=-2.01]\n","Epoch 117:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.44, train_loss_epoch=-2.14, valid_loss=-2.01]\n","Epoch 118:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-2.04, valid_loss=-2.01]\n","Epoch 119:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-2.10, valid_loss=-2.01]\n","Epoch 120:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.09, valid_loss=-2.01]\n","Epoch 121:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.09, valid_loss=-2.01]\n","Epoch 122:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.09, valid_loss=-2.01]\n","Epoch 123:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.13, valid_loss=-2.01]\n","Epoch 124:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.14, valid_loss=-2.01]\n","Epoch 125:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.36, train_loss_epoch=-2.12, valid_loss=-2.01]\n","Epoch 126:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.11, valid_loss=-2.01]\n","Epoch 127:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-2.01, valid_loss=-2.01]\n","Epoch 128:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.09, valid_loss=-2.01]\n","\u001b[36m(_train_tune pid=14236)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14236)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.39it/s]\u001b[A\n","Epoch 128:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.09, valid_loss=-2.01]\n","Epoch 129:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.10, valid_loss=-2.01]\n","Epoch 130:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.06, valid_loss=-2.01]\n","Epoch 131:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.11, valid_loss=-2.01]\n","Epoch 132:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.05, valid_loss=-2.01]\n","Epoch 133:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-2.06, valid_loss=-2.01]\n","Epoch 134:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.70, train_loss_epoch=-2.20, valid_loss=-2.01]\n","Epoch 135:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-2.05, valid_loss=-2.01]\n","Epoch 136:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.52, train_loss_epoch=-2.14, valid_loss=-2.01]\n","Epoch 137:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.08, valid_loss=-2.01]\n","Epoch 138:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-2.04, valid_loss=-2.01]\n","Epoch 139:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.10, valid_loss=-2.01]\n","Epoch 140:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-2.03, valid_loss=-2.01]\n","Epoch 141:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.13, valid_loss=-2.01]\n","Epoch 142:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.11, valid_loss=-2.01]\n","\u001b[36m(_train_tune pid=14236)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14236)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.14it/s]\u001b[A\n","Epoch 143:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.08, valid_loss=-2.00]\n","Epoch 144:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.13, valid_loss=-2.00]\n","Epoch 145:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.45, train_loss_epoch=-2.16, valid_loss=-2.00]\n","Epoch 146:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.07, valid_loss=-2.00]\n","Epoch 147:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.57, train_loss_epoch=-2.18, valid_loss=-2.00]\n","Epoch 148:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-2.15, valid_loss=-2.00]\n","Epoch 149:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.14, valid_loss=-2.00]\n","Epoch 150:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-2.09, valid_loss=-2.00]\n","Epoch 151:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.09, valid_loss=-2.00]\n","Epoch 152:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.65, train_loss_epoch=-2.19, valid_loss=-2.00]\n","Epoch 153:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-2.05, valid_loss=-2.00]\n","Epoch 154:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-2.10, valid_loss=-2.00]\n","Epoch 155:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.14, valid_loss=-2.00]\n","Epoch 156:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-2.07, valid_loss=-2.00]\n","Epoch 157:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.56, train_loss_epoch=-2.18, valid_loss=-2.00]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14236)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 29.48it/s]\u001b[A\n","Epoch 157:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.18, valid_loss=-2.00]\n","Epoch 158:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-2.08, valid_loss=-2.00]\n","Epoch 159:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-2.07, valid_loss=-2.00]\n","Epoch 160:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.11, valid_loss=-2.00]\n","Epoch 161:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-2.11, valid_loss=-2.00]\n","Epoch 162:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.12, valid_loss=-2.00]\n","Epoch 163:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.71, train_loss_epoch=-2.22, valid_loss=-2.00]\n","Epoch 164:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-2.02, valid_loss=-2.00]\n","Epoch 165:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-2.08, valid_loss=-2.00]\n","Epoch 166:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-2.16, valid_loss=-2.00]\n","Epoch 167:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.08, valid_loss=-2.00]\n","Epoch 168:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.12, valid_loss=-2.00]\n","Epoch 169:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.11, valid_loss=-2.00]\n","Epoch 170:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.63, train_loss_epoch=-2.20, valid_loss=-2.00]\n","Epoch 171:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-2.13, valid_loss=-2.00]\n","\u001b[36m(_train_tune pid=14236)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14236)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 30.01it/s]\u001b[A\n","Epoch 171:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.13, valid_loss=-1.99]\n","Epoch 172:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-2.08, valid_loss=-1.99]\n","Epoch 173:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-2.06, valid_loss=-1.99]\n","Epoch 174:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.11, valid_loss=-1.99]\n","Epoch 175:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.36, train_loss_epoch=-2.16, valid_loss=-1.99]\n","Epoch 176:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.11, valid_loss=-1.99]\n","Epoch 177:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.14, valid_loss=-1.99]\n","Epoch 178:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.12, valid_loss=-1.99]\n","Epoch 179:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-2.10, valid_loss=-1.99]\n","Epoch 180:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.59, train_loss_epoch=-2.22, valid_loss=-1.99]\n","Epoch 181:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-2.05, valid_loss=-1.99]\n","Epoch 182:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.16, valid_loss=-1.99]\n","Epoch 183:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.15, valid_loss=-1.99]\n","Epoch 184:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.53, train_loss_epoch=-2.19, valid_loss=-1.99]\n","Epoch 185:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.13, valid_loss=-1.99]\n","\u001b[36m(_train_tune pid=14236)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14236)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 27.36it/s]\u001b[A\n","Epoch 185:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.13, valid_loss=-1.98]\n","Epoch 186:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-2.10, valid_loss=-1.98]\n","Epoch 187:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.12, valid_loss=-1.98]\n","Epoch 188:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-2.01, valid_loss=-1.98]\n","Epoch 189:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.17, valid_loss=-1.98]\n","Epoch 190:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.14, valid_loss=-1.98]\n","Epoch 191:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.16, valid_loss=-1.98]\n","Epoch 192:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.15, valid_loss=-1.98]\n","Epoch 193:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.78, train_loss_epoch=-2.26, valid_loss=-1.98]\n","Epoch 194:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.14, valid_loss=-1.98]\n","Epoch 195:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.15, valid_loss=-1.98]\n","Epoch 196:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-2.11, valid_loss=-1.98]\n","Epoch 197:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.15, valid_loss=-1.98]\n","Epoch 198:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.48, train_loss_epoch=-2.19, valid_loss=-1.98]\n","Epoch 199:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-2.18, valid_loss=-1.98]\n","Epoch 199: 100%|██████████| 7/7 [00:00\u003c00:00, 31.38it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-2.18, valid_loss=-1.98]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14236)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14236)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.89it/s]\u001b[A\n","Epoch 200:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-2.14, valid_loss=-1.99]\n","Epoch 201:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-2.10, valid_loss=-1.99]\n","Epoch 202:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.57, train_loss_epoch=-2.23, valid_loss=-1.99]\n","Epoch 203:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.58, train_loss_epoch=-2.18, valid_loss=-1.99]\n","Epoch 204:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.64, train_loss_epoch=-2.21, valid_loss=-1.99]\n","Epoch 205:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.13, valid_loss=-1.99]\n","Epoch 206:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-2.06, valid_loss=-1.99]\n","Epoch 207:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.13, valid_loss=-1.99]\n","Epoch 208:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.16, valid_loss=-1.99]\n","Epoch 209:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-2.10, valid_loss=-1.99]\n","Epoch 210:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-2.10, valid_loss=-1.99]\n","Epoch 211:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.36, train_loss_epoch=-2.18, valid_loss=-1.99]\n","Epoch 212:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.44, train_loss_epoch=-2.20, valid_loss=-1.99]\n","Epoch 213:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.14, valid_loss=-1.99]\n","Epoch 214:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.49, train_loss_epoch=-2.19, valid_loss=-1.99]\n","\u001b[36m(_train_tune pid=14236)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=14236)\u001b[0m `Trainer.fit` stopped: `max_steps=1500.0` reached.\n","2024-11-12 18:53:34,055\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (168, 24, 1), 'n_pool_kernel_size': (8, 4, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","2024-11-12 18:53:34,080\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/root/ray_results/_train_tune_2024-11-12_18-17-38' in 0.0211s.\n","2024-11-12 18:53:34,084\tERROR tune.py:1037 -- Trials did not complete: [_train_tune_5c12a5b4, _train_tune_8be5387b, _train_tune_a6b09eb8]\n","INFO:lightning_fabric.utilities.seed:Seed set to 42\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n","INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=14236)\u001b[0m \n","\u001b[36m(_train_tune pid=14236)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 27.31it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14236)\u001b[0m \r                                                                      \u001b[A\rEpoch 214:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.19, valid_loss=-1.99]\n","\u001b[36m(_train_tune pid=14236)\u001b[0m \rEpoch 214:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.16, valid_loss=-1.99]\rEpoch 214:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.16, valid_loss=-1.99]\n","\n"]},{"name":"stderr","output_type":"stream","text":["INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","INFO:pytorch_lightning.callbacks.model_summary:\n","  | Name         | Type             | Params | Mode \n","----------------------------------------------------------\n","0 | loss         | DistributionLoss | 3      | eval \n","1 | padder_train | ConstantPad1d    | 0      | train\n","2 | scaler       | TemporalNorm     | 0      | train\n","3 | blocks       | ModuleList       | 2.6 M  | train\n","----------------------------------------------------------\n","2.6 M     Trainable params\n","3         Non-trainable params\n","2.6 M     Total params\n","10.495    Total estimated model params size (MB)\n","33        Modules in train mode\n","1         Modules in eval mode\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a0841882e3644e83b0b9338e3c7967b8","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8dd837a8f0734abc8ebfc87c82d3cffb","version_major":2,"version_minor":0},"text/plain":["Training: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8ad12b724270422da267ebabbc7046c4","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"133b3a032dc4467faaf37cae323d1b60","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e15663f1698442169424b76519cb6e63","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0af9da5b53174169938c2cc6e629e9b6","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e8591c4625cd4dcf88319e5fa71f85c6","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"60013615264341369e868129f5b9d2ff","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9dd6e1bb21ad473b929ecd227b57533d","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5fcfc219ba4a41b7903f3bb7c6695e35","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a311102bec65498991c2cb4ad25a2e2f","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4cf2eb7aea01459483a741756bbc414b","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5356976ede224eb1930dd1564d9c15b7","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c001853d36eb4298a85c587064c8e212","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f497658f45fb4df78ecf72cb1bd29d13","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ae514459a628443f80cd6fdbe4f27041","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_steps=1400.0` reached.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n","INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6e2c10b24d3849249b8c82f6d24248fe","version_major":2,"version_minor":0},"text/plain":["Predicting: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["+--------------------------------------------------------------------+\n","| Configuration for experiment     _train_tune_2024-11-12_18-54-39   |\n","+--------------------------------------------------------------------+\n","| Search algorithm                 SearchGenerator                   |\n","| Scheduler                        FIFOScheduler                     |\n","| Number of trials                 30                                |\n","+--------------------------------------------------------------------+\n","\n","View detailed results here: /root/ray_results/_train_tune_2024-11-12_18-54-39\n","To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-11-12_18-17-38_352630_2326/artifacts/2024-11-12_18-54-39/_train_tune_2024-11-12_18-54-39/driver_artifacts`\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=14842)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=14842)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=14842)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=14842)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=14842)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=14842)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=14842)\u001b[0m 2024-11-12 18:54:46.822824: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=14842)\u001b[0m 2024-11-12 18:54:46.842528: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=14842)\u001b[0m 2024-11-12 18:54:46.867602: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=14842)\u001b[0m 2024-11-12 18:54:46.875235: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=14842)\u001b[0m 2024-11-12 18:54:46.893082: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=14842)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=14842)\u001b[0m 2024-11-12 18:54:48.102327: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=14842)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=14842)\u001b[0m \n","\u001b[36m(_train_tune pid=14842)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=14842)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=14842)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=14842)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=14842)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=14842)\u001b[0m 3 | blocks       | ModuleList       | 3.7 M  | train\n","\u001b[36m(_train_tune pid=14842)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=14842)\u001b[0m 3.7 M     Trainable params\n","\u001b[36m(_train_tune pid=14842)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=14842)\u001b[0m 3.7 M     Total params\n","\u001b[36m(_train_tune pid=14842)\u001b[0m 14.636    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=14842)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=14842)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","                                                                           \n","Epoch 0:   0%|          | 0/4 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.36, train_loss_epoch=-1.44]\n","Epoch 2:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.56]\n","Epoch 3:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.56]\n","Epoch 4:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.43]\n","Epoch 5:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.58]\n","Epoch 6:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.36, train_loss_epoch=-1.50]\n","Epoch 7:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.51]\n","Epoch 8:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.56]\n","Epoch 9:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.74]\n","Epoch 10:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.57]\n","Epoch 11:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-1.70]\n","Epoch 12:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.58]\n","Epoch 13:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.36, train_loss_epoch=-1.65]\n","Epoch 14:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.61]\n","Epoch 15:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.65]\n","Epoch 16:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.64]\n","Epoch 17:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.82]\n","Epoch 18:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-1.85]\n","Epoch 19:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.78]\n","Epoch 20:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.74]\n","Epoch 21:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-1.86]\n","Epoch 22:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.82]\n","Epoch 23:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.76]\n","Epoch 24:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.67]\n","Epoch 24: 100%|██████████| 4/4 [00:00\u003c00:00, 11.86it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-1.67]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14842)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14842)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.98it/s]\u001b[A\n","Epoch 25:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-1.88, valid_loss=-1.77]\n","Epoch 26:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.86, valid_loss=-1.77]\n","Epoch 27:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.73, valid_loss=-1.77]\n","Epoch 28:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.81, valid_loss=-1.77]\n","Epoch 29:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.63, valid_loss=-1.77]\n","Epoch 30:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.85, valid_loss=-1.77]\n","Epoch 31:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.83, valid_loss=-1.77]\n","Epoch 32:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.74, valid_loss=-1.77]\n","Epoch 33:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.78, valid_loss=-1.77]\n","Epoch 34:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.78, valid_loss=-1.77]\n","Epoch 35:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.84, valid_loss=-1.77]\n","Epoch 36:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.85, valid_loss=-1.77]\n","Epoch 37:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.94, valid_loss=-1.77]\n","Epoch 38:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-1.94, valid_loss=-1.77]\n","Epoch 39:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.93, valid_loss=-1.77]\n","Epoch 40:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.75, valid_loss=-1.77]\n","Epoch 41:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.68, valid_loss=-1.77]\n","Epoch 42:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.71, valid_loss=-1.77]\n","Epoch 43:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.92, valid_loss=-1.77]\n","Epoch 44:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.86, valid_loss=-1.77]\n","Epoch 45:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.87, valid_loss=-1.77]\n","Epoch 45: 100%|██████████| 4/4 [00:00\u003c00:00, 12.17it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.87, valid_loss=-1.77]\n","Epoch 46:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.88, valid_loss=-1.77]\n","Epoch 47:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.83, valid_loss=-1.77]\n","Epoch 48:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.95, valid_loss=-1.77]\n","Epoch 49:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.76, valid_loss=-1.77]\n","Epoch 49: 100%|██████████| 4/4 [00:00\u003c00:00, 11.67it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.76, valid_loss=-1.77]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14842)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.98it/s]\u001b[A\n","Epoch 50:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.89, valid_loss=-1.85]\n","Epoch 51:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.90, valid_loss=-1.85]\n","Epoch 52:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.88, valid_loss=-1.85]\n","Epoch 53:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.85, valid_loss=-1.85]\n","Epoch 54:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-1.95, valid_loss=-1.85]\n","Epoch 55:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.81, valid_loss=-1.85]\n","Epoch 56:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.94, valid_loss=-1.85]\n","Epoch 57:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-1.93, valid_loss=-1.85]\n","Epoch 58:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.83, valid_loss=-1.85]\n","Epoch 59:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.00, valid_loss=-1.85]\n","Epoch 60:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.85, valid_loss=-1.85]\n","Epoch 61:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.91, valid_loss=-1.85]\n","Epoch 62:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.80, valid_loss=-1.85]\n","Epoch 63:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.90, valid_loss=-1.85]\n","Epoch 64:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.87, valid_loss=-1.85]\n","Epoch 65:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.89, valid_loss=-1.85]\n","Epoch 66:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.86, valid_loss=-1.85]\n","Epoch 67:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.83, valid_loss=-1.85]\n","Epoch 68:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.90, valid_loss=-1.85]\n","Epoch 69:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.93, valid_loss=-1.85]\n","Epoch 70:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.86, valid_loss=-1.85]\n","Epoch 71:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.89, valid_loss=-1.85]\n","Epoch 72:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.936, train_loss_epoch=-1.67, valid_loss=-1.85]\n","Epoch 73:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.96, valid_loss=-1.85]\n","Epoch 74:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.93, valid_loss=-1.85]\n","Epoch 74: 100%|██████████| 4/4 [00:00\u003c00:00, 12.02it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.93, valid_loss=-1.85]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14842)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14842)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.98it/s]\u001b[A\n","Epoch 75:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.80, valid_loss=-1.91]\n","Epoch 76:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.97, valid_loss=-1.91]\n","Epoch 77:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.91, valid_loss=-1.91]\n","Epoch 78:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.89, valid_loss=-1.91]\n","Epoch 79:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.93, valid_loss=-1.91]\n","Epoch 80:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.93, valid_loss=-1.91]\n","Epoch 81:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.96, valid_loss=-1.91]\n","Epoch 82:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.84, valid_loss=-1.91]\n","Epoch 83:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.92, valid_loss=-1.91]\n","Epoch 84:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.985, train_loss_epoch=-1.67, valid_loss=-1.91]\n","Epoch 85:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.89, valid_loss=-1.91]\n","Epoch 86:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.94, valid_loss=-1.91]\n","Epoch 87:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.90, valid_loss=-1.91]\n","Epoch 88:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-1.96, valid_loss=-1.91]\n","Epoch 89:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.88, valid_loss=-1.91]\n","Epoch 90:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.83, valid_loss=-1.91]\n","Epoch 91:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.84, valid_loss=-1.91]\n","Epoch 92:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-2.08, valid_loss=-1.91]\n","Epoch 93:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.88, valid_loss=-1.91]\n","Epoch 94:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-2.09, valid_loss=-1.91]\n","Epoch 95:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.04, valid_loss=-1.91]\n","Epoch 96:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.86, valid_loss=-1.91]\n","Epoch 97:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.00, valid_loss=-1.91]\n","Epoch 98:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.04, valid_loss=-1.91]\n","Epoch 99:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.93, valid_loss=-1.91]\n","Epoch 99: 100%|██████████| 4/4 [00:00\u003c00:00, 12.16it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.93, valid_loss=-1.91]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14842)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14842)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.97it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14842)\u001b[0m \n","Epoch 100:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.97, valid_loss=-1.94]\n","Epoch 101:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.04, valid_loss=-1.94]\n","Epoch 102:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.90, valid_loss=-1.94]\n","Epoch 103:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.98, valid_loss=-1.94]\n","Epoch 104:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.92, valid_loss=-1.94]\n","Epoch 105:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-1.97, valid_loss=-1.94]\n","Epoch 106:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-1.99, valid_loss=-1.94]\n","Epoch 106: 100%|██████████| 4/4 [00:00\u003c00:00, 12.36it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.99, valid_loss=-1.94]\n","Epoch 107:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.88, valid_loss=-1.94]\n","Epoch 108:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.93, valid_loss=-1.94]\n","Epoch 109:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.91, valid_loss=-1.94]\n","Epoch 110:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.98, valid_loss=-1.94]\n","Epoch 111:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.90, valid_loss=-1.94]\n","Epoch 112:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.87, valid_loss=-1.94]\n","Epoch 113:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-2.04, valid_loss=-1.94]\n","Epoch 114:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-2.05, valid_loss=-1.94]\n","Epoch 115:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-1.97, valid_loss=-1.94]\n","Epoch 116:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.96, valid_loss=-1.94]\n","Epoch 117:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.86, valid_loss=-1.94]\n","Epoch 117: 100%|██████████| 4/4 [00:00\u003c00:00, 12.39it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.86, valid_loss=-1.94]\n","Epoch 118:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.82, valid_loss=-1.94]\n","Epoch 119:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.42, train_loss_epoch=-2.10, valid_loss=-1.94]\n","Epoch 120:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.84, valid_loss=-1.94]\n","Epoch 121:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.00, valid_loss=-1.94]\n","Epoch 122:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.96, valid_loss=-1.94]\n","Epoch 123:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.92, valid_loss=-1.94]\n","Epoch 124:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.01, valid_loss=-1.94]\n","Epoch 124: 100%|██████████| 4/4 [00:00\u003c00:00, 12.11it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.01, valid_loss=-1.94]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14842)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.97it/s]\u001b[A\n","Epoch 125:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.98, valid_loss=-1.94]\n","Epoch 126:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-2.03, valid_loss=-1.94]\n","Epoch 127:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.93, valid_loss=-1.94]\n","Epoch 128:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.03, valid_loss=-1.94]\n","Epoch 129:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.93, valid_loss=-1.94]\n","Epoch 130:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.92, valid_loss=-1.94]\n","Epoch 131:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.93, valid_loss=-1.94]\n","Epoch 132:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.93, valid_loss=-1.94]\n","Epoch 133:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.94, valid_loss=-1.94]\n","Epoch 134:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.88, valid_loss=-1.94]\n","Epoch 135:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.99, valid_loss=-1.94]\n","Epoch 136:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.41, train_loss_epoch=-2.05, valid_loss=-1.94]\n","Epoch 137:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.79, valid_loss=-1.94]\n","Epoch 138:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.07, valid_loss=-1.94]\n","Epoch 139:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.96, valid_loss=-1.94]\n","Epoch 140:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.94, valid_loss=-1.94]\n","Epoch 141:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.98, valid_loss=-1.94]\n","Epoch 142:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.85, valid_loss=-1.94]\n","Epoch 143:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-2.05, valid_loss=-1.94]\n","Epoch 144:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.01, valid_loss=-1.94]\n","Epoch 145:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.93, valid_loss=-1.94]\n","Epoch 146:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.98, valid_loss=-1.94]\n","Epoch 147:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.95, valid_loss=-1.94]\n","Epoch 148:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.05, valid_loss=-1.94]\n","Epoch 149:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.87, valid_loss=-1.94]\n","Epoch 149: 100%|██████████| 4/4 [00:00\u003c00:00, 12.43it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.87, valid_loss=-1.94]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14842)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14842)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.97it/s]\u001b[A\n","Epoch 150:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.92, valid_loss=-1.96]\n","Epoch 151:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.01, valid_loss=-1.96]\n","Epoch 152:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.92, valid_loss=-1.96]\n","Epoch 153:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.95, valid_loss=-1.96]\n","Epoch 154:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.56, train_loss_epoch=-2.12, valid_loss=-1.96]\n","Epoch 155:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.80, valid_loss=-1.96]\n","Epoch 156:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.81, valid_loss=-1.96]\n","Epoch 157:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.91, valid_loss=-1.96]\n","Epoch 158:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.86, valid_loss=-1.96]\n","Epoch 159:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-2.04, valid_loss=-1.96]\n","Epoch 160:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.94, valid_loss=-1.96]\n","Epoch 161:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.44, train_loss_epoch=-2.08, valid_loss=-1.96]\n","Epoch 162:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.47, train_loss_epoch=-2.04, valid_loss=-1.96]\n","Epoch 163:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-2.01, valid_loss=-1.96]\n","Epoch 164:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-2.03, valid_loss=-1.96]\n","Epoch 165:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.87, valid_loss=-1.96]\n","Epoch 166:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.96, valid_loss=-1.96]\n","Epoch 167:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.99, valid_loss=-1.96]\n","Epoch 168:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.94, valid_loss=-1.96]\n","Epoch 168: 100%|██████████| 4/4 [00:00\u003c00:00, 12.10it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.94, valid_loss=-1.96]\n","Epoch 169:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.44, train_loss_epoch=-2.07, valid_loss=-1.96]\n","Epoch 170:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.90, valid_loss=-1.96]\n","Epoch 171:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-1.99, valid_loss=-1.96]\n","Epoch 172:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.41, train_loss_epoch=-2.07, valid_loss=-1.96]\n","Epoch 173:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.94, valid_loss=-1.96]\n","Epoch 174:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.04, valid_loss=-1.96]\n","Epoch 174: 100%|██████████| 4/4 [00:00\u003c00:00, 11.94it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.04, valid_loss=-1.96]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14842)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14842)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.96it/s]\u001b[A\n","Epoch 175:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.00, valid_loss=-1.96]\n","Epoch 176:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.98, valid_loss=-1.96]\n","Epoch 177:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.95, valid_loss=-1.96]\n","Epoch 178:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.99, valid_loss=-1.96]\n","Epoch 179:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.98, valid_loss=-1.96]\n","Epoch 179: 100%|██████████| 4/4 [00:00\u003c00:00, 12.01it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.98, valid_loss=-1.96]\n","Epoch 180:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.00, valid_loss=-1.96]\n","Epoch 181:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-2.08, valid_loss=-1.96]\n","Epoch 182:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-2.10, valid_loss=-1.96]\n","Epoch 183:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.03, valid_loss=-1.96]\n","Epoch 184:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.90, valid_loss=-1.96]\n","Epoch 185:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.94, valid_loss=-1.96]\n","Epoch 186:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-2.09, valid_loss=-1.96]\n","Epoch 187:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.85, valid_loss=-1.96]\n","Epoch 188:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.96, valid_loss=-1.96]\n","Epoch 189:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.84, valid_loss=-1.96]\n","Epoch 190:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.91, valid_loss=-1.96]\n","Epoch 191:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.05, train_loss_epoch=-1.70, valid_loss=-1.96]\n","Epoch 192:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.00, valid_loss=-1.96]\n","Epoch 193:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.92, valid_loss=-1.96]\n","Epoch 194:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.98, valid_loss=-1.96]\n","Epoch 195:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.98, valid_loss=-1.96]\n","Epoch 196:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.53, train_loss_epoch=-2.09, valid_loss=-1.96]\n","Epoch 197:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.97, valid_loss=-1.96]\n","Epoch 198:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.94, valid_loss=-1.96]\n","Epoch 199:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.93, valid_loss=-1.96]\n","Epoch 199: 100%|██████████| 4/4 [00:00\u003c00:00, 11.86it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.93, valid_loss=-1.96]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14842)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14842)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.97it/s]\u001b[A\n","Epoch 200:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.99, valid_loss=-1.97]\n","Epoch 201:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.04, valid_loss=-1.97]\n","Epoch 202:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.95, valid_loss=-1.97]\n","Epoch 203:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.00, valid_loss=-1.97]\n","Epoch 204:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.84, valid_loss=-1.97]\n","Epoch 205:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-2.08, valid_loss=-1.97]\n","Epoch 206:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-2.01, valid_loss=-1.97]\n","Epoch 207:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.09, valid_loss=-1.97]\n","Epoch 208:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.90, valid_loss=-1.97]\n","Epoch 209:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.84, valid_loss=-1.97]\n","Epoch 210:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.97, valid_loss=-1.97]\n","Epoch 211:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.03, valid_loss=-1.97]\n","Epoch 212:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.92, valid_loss=-1.97]\n","Epoch 213:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.91, valid_loss=-1.97]\n","Epoch 214:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 215:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.00, valid_loss=-1.97]\n","Epoch 216:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.90, valid_loss=-1.97]\n","Epoch 217:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 218:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.88, valid_loss=-1.97]\n","Epoch 219:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.97, valid_loss=-1.97]\n","Epoch 220:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.86, valid_loss=-1.97]\n","Epoch 221:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.02, valid_loss=-1.97]\n","Epoch 222:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.99, valid_loss=-1.97]\n","Epoch 223:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 224:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.02, valid_loss=-1.97]\n","Epoch 224: 100%|██████████| 4/4 [00:00\u003c00:00, 11.88it/s, v_num=0, train_loss_step=-1.07, train_loss_epoch=-2.02, valid_loss=-1.97]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14842)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14842)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.97it/s]\u001b[A\n","Epoch 225:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.07, train_loss_epoch=-1.74, valid_loss=-1.97]\n","Epoch 226:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.93, valid_loss=-1.97]\n","Epoch 227:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.00, valid_loss=-1.97]\n","Epoch 228:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.04, valid_loss=-1.97]\n","Epoch 229:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.90, valid_loss=-1.97]\n","Epoch 230:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.92, valid_loss=-1.97]\n","Epoch 231:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.88, valid_loss=-1.97]\n","Epoch 232:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.86, valid_loss=-1.97]\n","Epoch 233:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.93, valid_loss=-1.97]\n","Epoch 234:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.98, valid_loss=-1.97]\n","Epoch 235:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-2.05, valid_loss=-1.97]\n","Epoch 236:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.97, valid_loss=-1.97]\n","Epoch 237:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.83, valid_loss=-1.97]\n","Epoch 238:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.95, valid_loss=-1.97]\n","Epoch 239:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.91, valid_loss=-1.97]\n","Epoch 240:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.97, valid_loss=-1.97]\n","Epoch 241:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.06, valid_loss=-1.97]\n","Epoch 242:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.96, valid_loss=-1.97]\n","Epoch 243:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.89, valid_loss=-1.97]\n","Epoch 244:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.05, valid_loss=-1.97]\n","Epoch 245:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.88, valid_loss=-1.97]\n","Epoch 246:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.03, valid_loss=-1.97]\n","Epoch 247:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.89, valid_loss=-1.97]\n","Epoch 248:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.97, valid_loss=-1.97]\n","Epoch 249:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.47, train_loss_epoch=-2.13, valid_loss=-1.97]\n","Epoch 249: 100%|██████████| 4/4 [00:00\u003c00:00, 12.05it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.13, valid_loss=-1.97]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14842)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14842)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.99it/s]\u001b[A\n","Epoch 250:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.05, valid_loss=-1.97]\n","Epoch 251:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.79, valid_loss=-1.97]\n","Epoch 252:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.88, valid_loss=-1.97]\n","Epoch 253:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.02, valid_loss=-1.97]\n","Epoch 254:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.86, valid_loss=-1.97]\n","Epoch 255:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.96, valid_loss=-1.97]\n","Epoch 256:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.40, train_loss_epoch=-2.07, valid_loss=-1.97]\n","Epoch 257:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.98, valid_loss=-1.97]\n","Epoch 258:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-2.06, valid_loss=-1.97]\n","Epoch 259:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.79, valid_loss=-1.97]\n","Epoch 260:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.02, valid_loss=-1.97]\n","Epoch 261:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.91, valid_loss=-1.97]\n","Epoch 262:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.06, valid_loss=-1.97]\n","Epoch 263:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.84, valid_loss=-1.97]\n","Epoch 264:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.98, valid_loss=-1.97]\n","Epoch 265:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.45, train_loss_epoch=-2.11, valid_loss=-1.97]\n","Epoch 266:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.03, valid_loss=-1.97]\n","Epoch 267:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.89, valid_loss=-1.97]\n","Epoch 268:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.88, valid_loss=-1.97]\n","Epoch 269:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.42, train_loss_epoch=-2.11, valid_loss=-1.97]\n","Epoch 270:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.91, valid_loss=-1.97]\n","Epoch 271:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.99, valid_loss=-1.97]\n","Epoch 272:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.96, valid_loss=-1.97]\n","Epoch 273:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.93, valid_loss=-1.97]\n","Epoch 274:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.87, valid_loss=-1.97]\n","Epoch 274: 100%|██████████| 4/4 [00:00\u003c00:00, 11.99it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.87, valid_loss=-1.97]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14842)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 18:56:46,348\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('MA(2,9)', 'MA(3,9)', 'MA(1,12)', 'MA(2,12)', 'MA(3,12)', 'MOM(9)', 'MOM(12)', 'RSI(7)', 'RSI(14)', 'EMA(3,9)', 'EMA(5,9)', 'EMA(5,12)', 'DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (24, 12, 1), 'n_pool_kernel_size': (2, 2, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=14842)\u001b[0m `Trainer.fit` stopped: `max_steps=1100.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=14842)\u001b[0m \n","\u001b[36m(_train_tune pid=14842)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.99it/s]\u001b[A\n","\u001b[36m(_train_tune pid=14842)\u001b[0m \r                                                                      \u001b[A\rEpoch 274: 100%|██████████| 4/4 [00:02\u003c00:00,  1.65it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.87, valid_loss=-1.98]\rEpoch 274: 100%|██████████| 4/4 [00:02\u003c00:00,  1.65it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.83, valid_loss=-1.98]\rEpoch 274: 100%|██████████| 4/4 [00:02\u003c00:00,  1.65it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.83, valid_loss=-1.98]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=15431)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=15431)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=15431)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=15431)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=15431)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=15431)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=15431)\u001b[0m 2024-11-12 18:56:54.353167: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=15431)\u001b[0m 2024-11-12 18:56:54.373017: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=15431)\u001b[0m 2024-11-12 18:56:54.398088: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=15431)\u001b[0m 2024-11-12 18:56:54.405669: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=15431)\u001b[0m 2024-11-12 18:56:54.423337: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=15431)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=15431)\u001b[0m 2024-11-12 18:56:55.593977: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=15431)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=15431)\u001b[0m \n","\u001b[36m(_train_tune pid=15431)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=15431)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=15431)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=15431)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=15431)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=15431)\u001b[0m 3 | blocks       | ModuleList       | 3.0 M  | train\n","\u001b[36m(_train_tune pid=15431)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=15431)\u001b[0m 3.0 M     Trainable params\n","\u001b[36m(_train_tune pid=15431)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=15431)\u001b[0m 3.0 M     Total params\n","\u001b[36m(_train_tune pid=15431)\u001b[0m 11.984    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=15431)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=15431)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 18:56:58,606\tERROR tune_controller.py:1331 -- Trial task failed for trial _train_tune_be5546d3\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n","    result = ray.get(future)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n","    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(OutOfMemoryError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=15431, ip=172.28.0.12, actor_id=89017ad21046e902a58625d801000000, repr=_train_tune)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n","    raise skipped from exception_cause(skipped)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n","    self._ret = self._target(*self._args, **self._kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in \u003clambda\u003e\n","    training_func=lambda: self._trainable_func(self.config),\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n","    output = fn()\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/util.py\", line 130, in inner\n","    return trainable(config, **fn_kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_auto.py\", line 214, in _train_tune\n","    _ = self._fit_model(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_auto.py\", line 362, in _fit_model\n","    model = model.fit(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_windows.py\", line 661, in fit\n","    return self._fit(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_model.py\", line 356, in _fit\n","    trainer.fit(model, datamodule=datamodule)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 538, in fit\n","    call._call_and_handle_interrupt(\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 47, in _call_and_handle_interrupt\n","    return trainer_fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 574, in _fit_impl\n","    self._run(model, ckpt_path=ckpt_path)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 981, in _run\n","    results = self._run_stage()\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1023, in _run_stage\n","    self._run_sanity_check()\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1052, in _run_sanity_check\n","    val_loop.run()\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py\", line 178, in _decorator\n","    return loop_run(self, *args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 135, in run\n","    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 396, in _evaluation_step\n","    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 319, in _call_strategy_hook\n","    output = fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py\", line 411, in validation_step\n","    return self.lightning_module.validation_step(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_windows.py\", line 534, in validation_step\n","    valid_loss_batch = self._compute_valid_loss(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_windows.py\", line 461, in _compute_valid_loss\n","    _, sample_mean, quants = self.loss.sample(distr_args=distr_args)\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/losses/pytorch.py\", line 2002, in sample\n","    samples = distr.sample(sample_shape=(num_samples,))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributions/distribution.py\", line 167, in sample\n","    return self.rsample(sample_shape)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributions/studentT.py\", line 90, in rsample\n","    Y = X * torch.rsqrt(Z / self.df)\n","torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.60 GiB. GPU 0 has a total capacity of 39.56 GiB of which 8.50 GiB is free. Process 27231 has 512.00 MiB memory in use. Process 276176 has 30.55 GiB memory in use. Of the allocated memory 29.93 GiB is allocated by PyTorch, and 134.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial _train_tune_be5546d3 errored after 0 iterations at 2024-11-12 18:56:58. Total running time: 2min 19s\n","Error file: /tmp/ray/session_2024-11-12_18-17-38_352630_2326/artifacts/2024-11-12_18-54-39/_train_tune_2024-11-12_18-54-39/driver_artifacts/_train_tune_be5546d3_2_batch_size=256,h=20,hist_exog_list=MA_2_9_MA_3_9_MA_1_12_MA_2_12_MA_3_12_MOM_9_MOM_12_RSI_7_RSI_14_EMA_3_9__2024-11-12_18-54-46/error.txt\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=15539)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=15539)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=15539)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=15539)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=15539)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=15539)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=15539)\u001b[0m 2024-11-12 18:57:06.389371: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=15539)\u001b[0m 2024-11-12 18:57:06.408356: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=15539)\u001b[0m 2024-11-12 18:57:06.433136: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=15539)\u001b[0m 2024-11-12 18:57:06.440662: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=15539)\u001b[0m 2024-11-12 18:57:06.457771: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=15539)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=15539)\u001b[0m 2024-11-12 18:57:07.626111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=15539)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=15539)\u001b[0m \n","\u001b[36m(_train_tune pid=15539)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=15539)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=15539)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=15539)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=15539)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=15539)\u001b[0m 3 | blocks       | ModuleList       | 3.9 M  | train\n","\u001b[36m(_train_tune pid=15539)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=15539)\u001b[0m 3.9 M     Trainable params\n","\u001b[36m(_train_tune pid=15539)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=15539)\u001b[0m 3.9 M     Total params\n","\u001b[36m(_train_tune pid=15539)\u001b[0m 15.583    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=15539)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=15539)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/13 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.26]\n","Epoch 2:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.33]\n","Epoch 3:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.35]\n","Epoch 4:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.35]\n","Epoch 5:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.40]\n","Epoch 6:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.38]\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.08, train_loss_epoch=-1.38]\n","\u001b[36m(_train_tune pid=15539)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=15539)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:02\u003c00:00,  6.05it/s]\u001b[A\n","Epoch 7:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.38, valid_loss=-1.38]\n","Epoch 8:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.41, valid_loss=-1.38]\n","Epoch 9:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.01, train_loss_epoch=-1.41, valid_loss=-1.38]\n","Epoch 10:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.41, valid_loss=-1.38]\n","Epoch 11:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.49, valid_loss=-1.38]\n","Epoch 12:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.47, valid_loss=-1.38]\n","Epoch 13:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.46, valid_loss=-1.38]\n","Epoch 14:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.45, valid_loss=-1.38]\n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.45, valid_loss=-1.38]\n","\u001b[36m(_train_tune pid=15539)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=15539)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:02\u003c00:00,  6.40it/s]\u001b[A\n","Epoch 15:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.45, valid_loss=-1.44]\n","Epoch 16:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.55, valid_loss=-1.44]\n","Epoch 16: 100%|██████████| 13/13 [00:00\u003c00:00, 33.56it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.55, valid_loss=-1.44]\n","Epoch 17:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.57, valid_loss=-1.44]\n","Epoch 18:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.55, valid_loss=-1.44]\n","Epoch 19:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.55, valid_loss=-1.44]\n","Epoch 20:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.60, valid_loss=-1.44]\n","Epoch 21:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.55, valid_loss=-1.44]\n","Epoch 22:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.59, valid_loss=-1.44]\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.57, valid_loss=-1.44]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=15539)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:02\u003c00:00,  6.37it/s]\u001b[A\n","Epoch 23:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.57, valid_loss=-1.56]\n","Epoch 24:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-1.67, valid_loss=-1.56]\n","Epoch 25:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.65, valid_loss=-1.56]\n","Epoch 26:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.62, valid_loss=-1.56]\n","Epoch 27:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.62, valid_loss=-1.56]\n","Epoch 28:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.943, train_loss_epoch=-1.58, valid_loss=-1.56]\n","Epoch 29:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.64, valid_loss=-1.56]\n","Epoch 30:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.66, valid_loss=-1.56]\n","\u001b[36m(_train_tune pid=15539)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=15539)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:02\u003c00:00,  6.37it/s]\u001b[A\n","Epoch 30:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.66, valid_loss=-1.62]\n","Epoch 31:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.64, valid_loss=-1.62]\n","Epoch 32:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.66, valid_loss=-1.62]\n","Epoch 33:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.64, valid_loss=-1.62]\n","Epoch 34:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.68, valid_loss=-1.62]\n","Epoch 35:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.68, valid_loss=-1.62]\n","Epoch 36:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-1.71, valid_loss=-1.62]\n","Epoch 37:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.67, valid_loss=-1.62]\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.66, valid_loss=-1.62]\n","\u001b[36m(_train_tune pid=15539)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=15539)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:02\u003c00:00,  6.42it/s]\u001b[A\n","Epoch 38:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.66, valid_loss=-1.68]\n","Epoch 39:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.74, valid_loss=-1.68]\n","Epoch 40:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.67, valid_loss=-1.68]\n","Epoch 41:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.71, valid_loss=-1.68]\n","Epoch 42:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.71, valid_loss=-1.68]\n","Epoch 43:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.75, valid_loss=-1.68]\n","Epoch 44:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.71, valid_loss=-1.68]\n","Epoch 45:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.70, valid_loss=-1.68]\n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.76, valid_loss=-1.68]\n","\u001b[36m(_train_tune pid=15539)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=15539)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:02\u003c00:00,  6.39it/s]\u001b[A\n","                                                                        \u001b[A\n","Epoch 46:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.76, valid_loss=-1.70]\n","Epoch 46: 100%|██████████| 13/13 [00:02\u003c00:00,  5.35it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.70, valid_loss=-1.70]\n","Epoch 47:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.70, valid_loss=-1.70]\n","Epoch 48:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.72, valid_loss=-1.70]\n","Epoch 49:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.73, valid_loss=-1.70]\n","Epoch 50:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.79, valid_loss=-1.70]\n","Epoch 51:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.69, valid_loss=-1.70]\n","Epoch 52:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-1.75, valid_loss=-1.70]\n","Epoch 53:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.70, valid_loss=-1.70]\n","\u001b[36m(_train_tune pid=15539)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 18:57:44,917\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('MA(2,9)', 'MA(3,9)', 'MA(1,12)', 'MA(2,12)', 'MA(3,12)', 'MOM(9)', 'MOM(12)', 'RSI(7)', 'RSI(14)', 'EMA(3,9)', 'EMA(5,9)', 'EMA(5,12)', 'DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (180, 60, 1), 'n_pool_kernel_size': (8, 4, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=15539)\u001b[0m `Trainer.fit` stopped: `max_steps=700.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=15539)\u001b[0m \n","\u001b[36m(_train_tune pid=15539)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 13/13 [00:02\u003c00:00,  6.40it/s]\u001b[A\n","\u001b[36m(_train_tune pid=15539)\u001b[0m \r                                                                        \u001b[A\rEpoch 53:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.70, valid_loss=-1.70]\rEpoch 53:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.69, valid_loss=-1.70]\rEpoch 53:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.69, valid_loss=-1.70]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=15796)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=15796)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=15796)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=15796)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=15796)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=15796)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=15796)\u001b[0m 2024-11-12 18:57:53.346085: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=15796)\u001b[0m 2024-11-12 18:57:53.368282: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=15796)\u001b[0m 2024-11-12 18:57:53.396989: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=15796)\u001b[0m 2024-11-12 18:57:53.405585: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=15796)\u001b[0m 2024-11-12 18:57:53.425199: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=15796)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=15796)\u001b[0m 2024-11-12 18:57:54.615004: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=15796)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=15796)\u001b[0m \n","\u001b[36m(_train_tune pid=15796)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=15796)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=15796)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=15796)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=15796)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=15796)\u001b[0m 3 | blocks       | ModuleList       | 4.9 M  | train\n","\u001b[36m(_train_tune pid=15796)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=15796)\u001b[0m 4.9 M     Trainable params\n","\u001b[36m(_train_tune pid=15796)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=15796)\u001b[0m 4.9 M     Total params\n","\u001b[36m(_train_tune pid=15796)\u001b[0m 19.429    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=15796)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=15796)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/13 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.19]\n","Epoch 2:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.47]\n","Epoch 3:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.54]\n","Epoch 4:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.56]\n","Epoch 5:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.60]\n","Epoch 6:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.64]\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.65]\n","\u001b[36m(_train_tune pid=15796)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=15796)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:02\u003c00:00,  6.42it/s]\u001b[A\n","Epoch 7:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.65, valid_loss=-1.63]\n","Epoch 8:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.67, valid_loss=-1.63]\n","Epoch 9:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.70, valid_loss=-1.63]\n","Epoch 10:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.73, valid_loss=-1.63]\n","Epoch 11:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-1.76, valid_loss=-1.63]\n","Epoch 12:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.73, valid_loss=-1.63]\n","Epoch 13:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.78, valid_loss=-1.63]\n","Epoch 14:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.74, valid_loss=-1.63]\n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.74, valid_loss=-1.63]\n","\u001b[36m(_train_tune pid=15796)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=15796)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:02\u003c00:00,  6.41it/s]\u001b[A\n","Epoch 15:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.74, valid_loss=-1.73]\n","Epoch 16:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.82, valid_loss=-1.73]\n","Epoch 17:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-1.83, valid_loss=-1.73]\n","Epoch 18:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.82, valid_loss=-1.73]\n","Epoch 19:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.81, valid_loss=-1.73]\n","Epoch 20:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-1.85, valid_loss=-1.73]\n","Epoch 21:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.85, valid_loss=-1.73]\n","Epoch 22:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.83, valid_loss=-1.73]\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.84, valid_loss=-1.73]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=15796)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:02\u003c00:00,  6.43it/s]\u001b[A\n","Epoch 23:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.84, valid_loss=-1.79]\n","Epoch 24:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.36, train_loss_epoch=-1.86, valid_loss=-1.79]\n","Epoch 25:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-1.86, valid_loss=-1.79]\n","Epoch 26:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.84, valid_loss=-1.79]\n","Epoch 27:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.85, valid_loss=-1.79]\n","Epoch 28:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.78, valid_loss=-1.79]\n","Epoch 29:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.87, valid_loss=-1.79]\n","Epoch 30:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.87, valid_loss=-1.79]\n","\u001b[36m(_train_tune pid=15796)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=15796)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:02\u003c00:00,  6.13it/s]\u001b[A\n","Epoch 30:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.87, valid_loss=-1.82]\n","Epoch 31:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.84, valid_loss=-1.82]\n","Epoch 32:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.87, valid_loss=-1.82]\n","Epoch 33:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.86, valid_loss=-1.82]\n","Epoch 33: 100%|██████████| 13/13 [00:00\u003c00:00, 35.99it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.89, valid_loss=-1.82]\n","Epoch 34:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.89, valid_loss=-1.82]\n","Epoch 35:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.86, valid_loss=-1.82]\n","Epoch 36:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-1.92, valid_loss=-1.82]\n","Epoch 37:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.87, valid_loss=-1.82]\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.87, valid_loss=-1.82]\n","\u001b[36m(_train_tune pid=15796)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=15796)\u001b[0m \n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=15796)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:02\u003c00:00,  6.37it/s]\u001b[A\n","Epoch 38:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.87, valid_loss=-1.84]\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.90, valid_loss=-1.84]         \n","Epoch 39:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.90, valid_loss=-1.84]\n","Epoch 40:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.85, valid_loss=-1.84]\n","Epoch 41:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.91, valid_loss=-1.84]\n","Epoch 42:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.89, valid_loss=-1.84]\n","Epoch 43:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.89, valid_loss=-1.84]\n","Epoch 44:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.89, valid_loss=-1.84]\n","Epoch 45:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.86, valid_loss=-1.84]\n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-1.91, valid_loss=-1.84]\n","\u001b[36m(_train_tune pid=15796)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=15796)\u001b[0m `Trainer.fit` stopped: `max_steps=600.0` reached.\n","2024-11-12 18:58:26,590\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('MA(2,9)', 'MA(3,9)', 'MA(1,12)', 'MA(2,12)', 'MA(3,12)', 'MOM(9)', 'MOM(12)', 'RSI(7)', 'RSI(14)', 'EMA(3,9)', 'EMA(5,9)', 'EMA(5,12)', 'DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (40, 20, 1), 'n_pool_kernel_size': (1, 1, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=15796)\u001b[0m \n","\u001b[36m(_train_tune pid=15796)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 13/13 [00:02\u003c00:00,  6.38it/s]\u001b[A\n","\u001b[36m(_train_tune pid=15796)\u001b[0m \r                                                                        \u001b[A\rEpoch 46:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.91, valid_loss=-1.85]\rEpoch 46:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.84, valid_loss=-1.85]\rEpoch 46:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.84, valid_loss=-1.85]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=16026)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=16026)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=16026)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=16026)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=16026)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=16026)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=16026)\u001b[0m 2024-11-12 18:58:34.316626: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=16026)\u001b[0m 2024-11-12 18:58:34.337302: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=16026)\u001b[0m 2024-11-12 18:58:34.362876: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=16026)\u001b[0m 2024-11-12 18:58:34.370629: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=16026)\u001b[0m 2024-11-12 18:58:34.388534: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=16026)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=16026)\u001b[0m 2024-11-12 18:58:35.578250: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=16026)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=16026)\u001b[0m \n","\u001b[36m(_train_tune pid=16026)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=16026)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=16026)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=16026)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=16026)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=16026)\u001b[0m 3 | blocks       | ModuleList       | 3.7 M  | train\n","\u001b[36m(_train_tune pid=16026)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=16026)\u001b[0m 3.7 M     Trainable params\n","\u001b[36m(_train_tune pid=16026)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=16026)\u001b[0m 3.7 M     Total params\n","\u001b[36m(_train_tune pid=16026)\u001b[0m 14.642    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=16026)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=16026)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/13 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.45]\n","Epoch 2:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.57]\n","Epoch 3:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.48]\n","Epoch 4:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.55]\n","Epoch 5:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.60]\n","Epoch 6:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.62]\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.65]\n","\u001b[36m(_train_tune pid=16026)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16026)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  6.85it/s]\u001b[A\n","Epoch 7:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.65, valid_loss=-1.62]\n","Epoch 8:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.65, valid_loss=-1.62]\n","Epoch 9:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.67, valid_loss=-1.62]\n","Epoch 10:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.68, valid_loss=-1.62]\n","Epoch 11:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-1.73, valid_loss=-1.62]\n","Epoch 12:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.74, valid_loss=-1.62]\n","Epoch 13:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.75, valid_loss=-1.62]\n","Epoch 14:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.69, valid_loss=-1.62]\n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.77, valid_loss=-1.62]\n","\u001b[36m(_train_tune pid=16026)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16026)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  6.91it/s]\u001b[A\n","Epoch 15:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.77, valid_loss=-1.74]\n","Epoch 16:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-1.79, valid_loss=-1.74]\n","Epoch 17:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-1.80, valid_loss=-1.74]\n","Epoch 18:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.79, valid_loss=-1.74]\n","Epoch 19:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.78, valid_loss=-1.74]\n","Epoch 20:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-1.86, valid_loss=-1.74]\n","Epoch 21:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.81, valid_loss=-1.74]\n","Epoch 22:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.84, valid_loss=-1.74]\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.84, valid_loss=-1.74]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16026)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.06it/s]\u001b[A\n","Epoch 23:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.84, valid_loss=-1.80]\n","Epoch 24:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-1.85, valid_loss=-1.80]\n","Epoch 24: 100%|██████████| 13/13 [00:00\u003c00:00, 37.36it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-1.89, valid_loss=-1.80]\n","Epoch 25:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-1.89, valid_loss=-1.80]\n","Epoch 26:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.83, valid_loss=-1.80]\n","Epoch 27:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.84, valid_loss=-1.80]\n","Epoch 28:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.08, train_loss_epoch=-1.77, valid_loss=-1.80]\n","Epoch 29:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.84, valid_loss=-1.80]\n","Epoch 30:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.84, valid_loss=-1.80]\n","\u001b[36m(_train_tune pid=16026)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16026)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.00it/s]\u001b[A\n","Epoch 30:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.84, valid_loss=-1.82]\n","Epoch 31:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.83, valid_loss=-1.82]\n","Epoch 32:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.79, valid_loss=-1.82]\n","Epoch 33:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.84, valid_loss=-1.82]\n","Epoch 34:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.88, valid_loss=-1.82]\n","Epoch 35:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.86, valid_loss=-1.82]\n","Epoch 36:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.42, train_loss_epoch=-1.88, valid_loss=-1.82]\n","Epoch 37:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.90, valid_loss=-1.82]\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.83, valid_loss=-1.82]\n","\u001b[36m(_train_tune pid=16026)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16026)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  6.99it/s]\u001b[A\n","Epoch 38:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.83, valid_loss=-1.85]\n","Epoch 39:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.91, valid_loss=-1.85]\n","Epoch 40:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.85, valid_loss=-1.85]\n","Epoch 41:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.88, valid_loss=-1.85]\n","Epoch 42:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.88, valid_loss=-1.85]\n","Epoch 43:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.86, valid_loss=-1.85]\n","Epoch 44:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.86, valid_loss=-1.85]\n","Epoch 45:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.91, valid_loss=-1.85]\n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-1.89, valid_loss=-1.85]\n","\u001b[36m(_train_tune pid=16026)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16026)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  6.90it/s]\u001b[A\n","Epoch 46:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.89, valid_loss=-1.86]\n","Epoch 47:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.91, valid_loss=-1.86]\n","Epoch 48:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.88, valid_loss=-1.86]\n","Epoch 49:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.92, valid_loss=-1.86]\n","Epoch 50:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.92, valid_loss=-1.86]\n","Epoch 51:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.93, valid_loss=-1.86]\n","Epoch 52:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-1.95, valid_loss=-1.86]\n","Epoch 52: 100%|██████████| 13/13 [00:00\u003c00:00, 36.37it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.95, valid_loss=-1.86]\n","Epoch 53:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.91, valid_loss=-1.86]\n","\u001b[36m(_train_tune pid=16026)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16026)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.06it/s]\u001b[A\n","Epoch 54:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.89, valid_loss=-1.87]\n","Epoch 55:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.84, valid_loss=-1.87]\n","Epoch 56:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-1.94, valid_loss=-1.87]\n","Epoch 57:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.93, valid_loss=-1.87]\n","Epoch 58:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-1.96, valid_loss=-1.87]\n","Epoch 59:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.90, valid_loss=-1.87]\n","Epoch 60:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.87, valid_loss=-1.87]\n","Epoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.90, valid_loss=-1.87]\n","\u001b[36m(_train_tune pid=16026)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16026)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  6.99it/s]\u001b[A\n","Epoch 61:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.90, valid_loss=-1.88]\n","Epoch 62:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.94, valid_loss=-1.88]\n","Epoch 63:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.92, valid_loss=-1.88]\n","Epoch 64:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.92, valid_loss=-1.88]\n","Epoch 65:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.96, valid_loss=-1.88]\n","Epoch 66:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.96, valid_loss=-1.88]\n","Epoch 67:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.90, valid_loss=-1.88]\n","Epoch 68:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.87, valid_loss=-1.88]\n","Epoch 69:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.84, valid_loss=-1.88]\n","\u001b[36m(_train_tune pid=16026)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16026)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  6.94it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16026)\u001b[0m \n","Epoch 69:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.84, valid_loss=-1.88]\n","Epoch 70:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.91, valid_loss=-1.88]\n","Epoch 71:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.92, valid_loss=-1.88]\n","Epoch 72:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-1.92, valid_loss=-1.88]\n","Epoch 73:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.95, valid_loss=-1.88]\n","Epoch 74:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.94, valid_loss=-1.88]\n","Epoch 75:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.92, valid_loss=-1.88]\n","Epoch 76:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.90, valid_loss=-1.88]\n","\u001b[36m(_train_tune pid=16026)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16026)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  6.97it/s]\u001b[A\n","Epoch 77:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.95, valid_loss=-1.90]\n","Epoch 78:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-1.94, valid_loss=-1.90]\n","Epoch 79:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.88, valid_loss=-1.90]\n","Epoch 80:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.93, valid_loss=-1.90]\n","Epoch 81:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-1.97, valid_loss=-1.90]\n","Epoch 82:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-1.98, valid_loss=-1.90]\n","Epoch 83:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.92, valid_loss=-1.90]\n","Epoch 84:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.93, valid_loss=-1.90]\n","\u001b[36m(_train_tune pid=16026)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16026)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  6.89it/s]\u001b[A\n","Epoch 84:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.93, valid_loss=-1.90]\n","Epoch 85:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.51, train_loss_epoch=-1.99, valid_loss=-1.90]\n","Epoch 86:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.89, valid_loss=-1.90]\n","Epoch 87:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.97, valid_loss=-1.90]\n","Epoch 88:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-1.91, valid_loss=-1.90]\n","Epoch 89:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.96, valid_loss=-1.90]\n","Epoch 90:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.51, train_loss_epoch=-1.98, valid_loss=-1.90]\n","Epoch 91:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-1.97, valid_loss=-1.90]\n","Epoch 92:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.91, valid_loss=-1.90]\n","\u001b[36m(_train_tune pid=16026)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16026)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  6.93it/s]\u001b[A\n","Epoch 92:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.91, valid_loss=-1.90]\n","Epoch 93:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.98, valid_loss=-1.90]\n","Epoch 94:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.95, valid_loss=-1.90]\n","Epoch 95:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.95, valid_loss=-1.90]\n","Epoch 96:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.92, valid_loss=-1.90]\n","Epoch 97:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-1.97, valid_loss=-1.90]\n","Epoch 98:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.95, valid_loss=-1.90]\n","Epoch 99:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-1.96, valid_loss=-1.90]\n","Epoch 99: 100%|██████████| 13/13 [00:00\u003c00:00, 37.84it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.96, valid_loss=-1.90]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16026)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  6.92it/s]\u001b[A\n","Epoch 100:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.92, valid_loss=-1.91]\n","Epoch 101:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-1.99, valid_loss=-1.91]\n","Epoch 102:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.91, valid_loss=-1.91]\n","Epoch 103:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.96, valid_loss=-1.91]\n","Epoch 104:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.99, valid_loss=-1.91]\n","Epoch 105:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.97, valid_loss=-1.91]\n","Epoch 106:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.95, valid_loss=-1.91]\n","Epoch 107:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.88, valid_loss=-1.91]\n","\u001b[36m(_train_tune pid=16026)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=16026)\u001b[0m `Trainer.fit` stopped: `max_steps=1400.0` reached.\n","2024-11-12 18:59:43,885\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('MA(2,9)', 'MA(3,9)', 'MA(1,12)', 'MA(2,12)', 'MA(3,12)', 'MOM(9)', 'MOM(12)', 'RSI(7)', 'RSI(14)', 'EMA(3,9)', 'EMA(5,9)', 'EMA(5,12)', 'DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (60, 8, 1), 'n_pool_kernel_size': (2, 2, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=16026)\u001b[0m \n","\u001b[36m(_train_tune pid=16026)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  6.96it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16026)\u001b[0m \r                                                                        \u001b[A\rEpoch 107:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.88, valid_loss=-1.91]\rEpoch 107:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.91, valid_loss=-1.91]\rEpoch 107:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.91, valid_loss=-1.91]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=16408)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=16408)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=16408)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=16408)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=16408)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=16408)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=16408)\u001b[0m 2024-11-12 18:59:52.352041: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=16408)\u001b[0m 2024-11-12 18:59:52.373052: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=16408)\u001b[0m 2024-11-12 18:59:52.399705: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=16408)\u001b[0m 2024-11-12 18:59:52.407874: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=16408)\u001b[0m 2024-11-12 18:59:52.426363: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=16408)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=16408)\u001b[0m 2024-11-12 18:59:53.575102: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=16408)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=16408)\u001b[0m \n","\u001b[36m(_train_tune pid=16408)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=16408)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=16408)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=16408)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=16408)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=16408)\u001b[0m 3 | blocks       | ModuleList       | 2.8 M  | train\n","\u001b[36m(_train_tune pid=16408)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=16408)\u001b[0m 2.8 M     Trainable params\n","\u001b[36m(_train_tune pid=16408)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=16408)\u001b[0m 2.8 M     Total params\n","\u001b[36m(_train_tune pid=16408)\u001b[0m 11.283    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=16408)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=16408)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 18:59:56,366\tERROR tune_controller.py:1331 -- Trial task failed for trial _train_tune_e2f48f06\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n","    result = ray.get(future)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n","    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(OutOfMemoryError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=16408, ip=172.28.0.12, actor_id=8218f5a7de379cb31cb54c7101000000, repr=_train_tune)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n","    raise skipped from exception_cause(skipped)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n","    self._ret = self._target(*self._args, **self._kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in \u003clambda\u003e\n","    training_func=lambda: self._trainable_func(self.config),\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n","    output = fn()\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/util.py\", line 130, in inner\n","    return trainable(config, **fn_kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_auto.py\", line 214, in _train_tune\n","    _ = self._fit_model(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_auto.py\", line 362, in _fit_model\n","    model = model.fit(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_windows.py\", line 661, in fit\n","    return self._fit(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_model.py\", line 356, in _fit\n","    trainer.fit(model, datamodule=datamodule)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 538, in fit\n","    call._call_and_handle_interrupt(\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 47, in _call_and_handle_interrupt\n","    return trainer_fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 574, in _fit_impl\n","    self._run(model, ckpt_path=ckpt_path)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 981, in _run\n","    results = self._run_stage()\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1023, in _run_stage\n","    self._run_sanity_check()\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1052, in _run_sanity_check\n","    val_loop.run()\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py\", line 178, in _decorator\n","    return loop_run(self, *args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 135, in run\n","    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 396, in _evaluation_step\n","    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 319, in _call_strategy_hook\n","    output = fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py\", line 411, in validation_step\n","    return self.lightning_module.validation_step(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_windows.py\", line 534, in validation_step\n","    valid_loss_batch = self._compute_valid_loss(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_windows.py\", line 461, in _compute_valid_loss\n","    _, sample_mean, quants = self.loss.sample(distr_args=distr_args)\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/losses/pytorch.py\", line 2002, in sample\n","    samples = distr.sample(sample_shape=(num_samples,))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributions/distribution.py\", line 167, in sample\n","    return self.rsample(sample_shape)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributions/studentT.py\", line 90, in rsample\n","    Y = X * torch.rsqrt(Z / self.df)\n","torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.60 GiB. GPU 0 has a total capacity of 39.56 GiB of which 8.78 GiB is free. Process 27231 has 512.00 MiB memory in use. Process 295328 has 30.27 GiB memory in use. Of the allocated memory 29.56 GiB is allocated by PyTorch, and 224.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial _train_tune_e2f48f06 errored after 0 iterations at 2024-11-12 18:59:56. Total running time: 5min 17s\n","Error file: /tmp/ray/session_2024-11-12_18-17-38_352630_2326/artifacts/2024-11-12_18-54-39/_train_tune_2024-11-12_18-54-39/driver_artifacts/_train_tune_e2f48f06_6_batch_size=256,h=20,hist_exog_list=MA_2_9_MA_3_9_MA_1_12_MA_2_12_MA_3_12_MOM_9_MOM_12_RSI_7_RSI_14_EMA_3_9__2024-11-12_18-58-33/error.txt\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=16516)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=16516)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=16516)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=16516)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=16516)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=16516)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=16516)\u001b[0m 2024-11-12 19:00:04.286119: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=16516)\u001b[0m 2024-11-12 19:00:04.304848: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=16516)\u001b[0m 2024-11-12 19:00:04.328779: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=16516)\u001b[0m 2024-11-12 19:00:04.339571: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=16516)\u001b[0m 2024-11-12 19:00:04.356512: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=16516)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=16516)\u001b[0m 2024-11-12 19:00:05.533249: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=16516)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=16516)\u001b[0m \n","\u001b[36m(_train_tune pid=16516)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=16516)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=16516)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=16516)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=16516)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=16516)\u001b[0m 3 | blocks       | ModuleList       | 2.9 M  | train\n","\u001b[36m(_train_tune pid=16516)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=16516)\u001b[0m 2.9 M     Trainable params\n","\u001b[36m(_train_tune pid=16516)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=16516)\u001b[0m 2.9 M     Total params\n","\u001b[36m(_train_tune pid=16516)\u001b[0m 11.718    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=16516)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=16516)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/4 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.570, train_loss_epoch=26.90]\n","Epoch 2:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.800, train_loss_epoch=10.10]\n","Epoch 3:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.70, train_loss_epoch=10.70]\n","Epoch 4:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.00, train_loss_epoch=10.60]\n","Epoch 5:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.10, train_loss_epoch=10.30]\n","Epoch 6:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.730, train_loss_epoch=9.770]\n","Epoch 7:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.220, train_loss_epoch=9.420]\n","Epoch 8:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.900, train_loss_epoch=9.840]\n","Epoch 9:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.810, train_loss_epoch=10.20]\n","Epoch 10:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.50, train_loss_epoch=10.40]\n","Epoch 11:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.940, train_loss_epoch=9.890]\n","Epoch 11: 100%|██████████| 4/4 [00:00\u003c00:00, 15.66it/s, v_num=0, train_loss_step=10.60, train_loss_epoch=10.30]\n","Epoch 12:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.60, train_loss_epoch=10.30]\n","Epoch 13:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.30, train_loss_epoch=10.40]\n","Epoch 14:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.430, train_loss_epoch=10.10]\n","Epoch 15:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.630, train_loss_epoch=9.910]\n","Epoch 16:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.530, train_loss_epoch=9.770]\n","Epoch 17:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.170, train_loss_epoch=9.390]\n","Epoch 18:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.860, train_loss_epoch=9.440]\n","Epoch 19:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.450, train_loss_epoch=9.270]\n","Epoch 20:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.70, train_loss_epoch=10.40]\n","Epoch 21:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.140, train_loss_epoch=9.460]\n","Epoch 22:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.540, train_loss_epoch=9.230]\n","Epoch 23:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.690, train_loss_epoch=9.090]\n","Epoch 24:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.60, train_loss_epoch=9.710]\n","Epoch 24: 100%|██████████| 4/4 [00:00\u003c00:00, 15.42it/s, v_num=0, train_loss_step=9.900, train_loss_epoch=9.710]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16516)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16516)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 16.03it/s]\u001b[A\n","Epoch 25:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.900, train_loss_epoch=10.10, valid_loss=10.00]\n","Epoch 26:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.30, train_loss_epoch=10.30, valid_loss=10.00]\n","Epoch 27:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.50, train_loss_epoch=10.30, valid_loss=10.00]\n","Epoch 28:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.80, train_loss_epoch=10.30, valid_loss=10.00]\n","Epoch 29:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.920, train_loss_epoch=10.00, valid_loss=10.00]\n","Epoch 30:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.510, train_loss_epoch=9.560, valid_loss=10.00]\n","Epoch 30: 100%|██████████| 4/4 [00:00\u003c00:00, 15.38it/s, v_num=0, train_loss_step=8.620, train_loss_epoch=9.010, valid_loss=10.00]\n","Epoch 31:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.620, train_loss_epoch=9.010, valid_loss=10.00]\n","Epoch 32:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.40, train_loss_epoch=9.260, valid_loss=10.00]\n","Epoch 33:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.130, train_loss_epoch=8.880, valid_loss=10.00]\n","Epoch 34:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.270, train_loss_epoch=9.150, valid_loss=10.00]\n","Epoch 35:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.180, train_loss_epoch=9.760, valid_loss=10.00]\n","Epoch 36:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.810, train_loss_epoch=10.10, valid_loss=10.00]\n","Epoch 37:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.760, train_loss_epoch=10.10, valid_loss=10.00]\n","Epoch 38:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.770, train_loss_epoch=10.10, valid_loss=10.00]\n","Epoch 39:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.50, train_loss_epoch=10.10, valid_loss=10.00]\n","Epoch 40:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.00, train_loss_epoch=10.00, valid_loss=10.00]\n","Epoch 41:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.10, train_loss_epoch=9.830, valid_loss=10.00]\n","Epoch 42:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.910, train_loss_epoch=9.200, valid_loss=10.00]\n","Epoch 43:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.530, train_loss_epoch=8.910, valid_loss=10.00]\n","Epoch 43: 100%|██████████| 4/4 [00:00\u003c00:00, 15.06it/s, v_num=0, train_loss_step=8.710, train_loss_epoch=9.060, valid_loss=10.00]\n","Epoch 44:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.710, train_loss_epoch=9.060, valid_loss=10.00]\n","Epoch 45:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.490, train_loss_epoch=8.750, valid_loss=10.00]\n","Epoch 46:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.740, train_loss_epoch=8.260, valid_loss=10.00]\n","Epoch 47:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.840, train_loss_epoch=8.380, valid_loss=10.00]\n","Epoch 48:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.570, train_loss_epoch=8.380, valid_loss=10.00]\n","Epoch 49:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.140, train_loss_epoch=8.940, valid_loss=10.00]\n","Epoch 49: 100%|██████████| 4/4 [00:00\u003c00:00, 15.21it/s, v_num=0, train_loss_step=8.520, train_loss_epoch=8.940, valid_loss=10.00]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16516)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.43it/s]\u001b[A\n","Epoch 50:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.520, train_loss_epoch=8.870, valid_loss=8.370]\n","Epoch 51:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.660, train_loss_epoch=8.840, valid_loss=8.370]\n","Epoch 52:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.480, train_loss_epoch=8.540, valid_loss=8.370]\n","Epoch 53:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.950, train_loss_epoch=8.220, valid_loss=8.370]\n","Epoch 54:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.170, train_loss_epoch=7.970, valid_loss=8.370]\n","Epoch 55:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.880, train_loss_epoch=8.340, valid_loss=8.370]\n","Epoch 56:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.380, train_loss_epoch=8.320, valid_loss=8.370]\n","Epoch 57:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.830, train_loss_epoch=8.220, valid_loss=8.370]\n","Epoch 57: 100%|██████████| 4/4 [00:00\u003c00:00, 15.44it/s, v_num=0, train_loss_step=7.770, train_loss_epoch=8.010, valid_loss=8.370]\n","Epoch 58:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.770, train_loss_epoch=8.010, valid_loss=8.370]\n","Epoch 59:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.510, train_loss_epoch=7.680, valid_loss=8.370]\n","Epoch 60:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.150, train_loss_epoch=7.260, valid_loss=8.370]\n","Epoch 61:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.880, train_loss_epoch=8.030, valid_loss=8.370]\n","Epoch 62:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.940, train_loss_epoch=8.120, valid_loss=8.370]\n","Epoch 63:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.620, train_loss_epoch=7.990, valid_loss=8.370]\n","Epoch 64:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.900, train_loss_epoch=8.040, valid_loss=8.370]\n","Epoch 65:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.490, train_loss_epoch=8.220, valid_loss=8.370]\n","Epoch 66:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.150, train_loss_epoch=7.600, valid_loss=8.370]\n","Epoch 67:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.780, train_loss_epoch=7.510, valid_loss=8.370]\n","Epoch 68:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.660, train_loss_epoch=7.490, valid_loss=8.370]\n","Epoch 69:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.190, train_loss_epoch=7.490, valid_loss=8.370]\n","Epoch 70:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.760, train_loss_epoch=8.580, valid_loss=8.370]\n","Epoch 71:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.70, train_loss_epoch=9.230, valid_loss=8.370]\n","Epoch 72:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.600, train_loss_epoch=8.820, valid_loss=8.370]\n","Epoch 73:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.890, train_loss_epoch=9.080, valid_loss=8.370]\n","Epoch 74:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.960, train_loss_epoch=8.710, valid_loss=8.370]\n","Epoch 74: 100%|██████████| 4/4 [00:00\u003c00:00, 15.17it/s, v_num=0, train_loss_step=8.470, train_loss_epoch=8.710, valid_loss=8.370]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16516)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16516)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.72it/s]\u001b[A\n","Epoch 75:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.470, train_loss_epoch=8.420, valid_loss=8.060]\n","Epoch 76:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.100, train_loss_epoch=8.200, valid_loss=8.060]\n","Epoch 77:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.220, train_loss_epoch=8.090, valid_loss=8.060]\n","Epoch 78:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.630, train_loss_epoch=7.820, valid_loss=8.060]\n","Epoch 79:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.390, train_loss_epoch=7.610, valid_loss=8.060]\n","Epoch 80:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.360, train_loss_epoch=7.900, valid_loss=8.060]\n","Epoch 81:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.20, train_loss_epoch=8.750, valid_loss=8.060]\n","Epoch 82:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.380, train_loss_epoch=8.870, valid_loss=8.060]\n","Epoch 83:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.980, train_loss_epoch=9.240, valid_loss=8.060]\n","Epoch 84:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.40, train_loss_epoch=9.500, valid_loss=8.060]\n","Epoch 85:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.110, train_loss_epoch=9.150, valid_loss=8.060]\n","Epoch 86:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.670, train_loss_epoch=9.010, valid_loss=8.060]\n","Epoch 87:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.570, train_loss_epoch=8.920, valid_loss=8.060]\n","Epoch 88:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.640, train_loss_epoch=8.790, valid_loss=8.060]\n","Epoch 89:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.320, train_loss_epoch=8.590, valid_loss=8.060]\n","Epoch 90:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.530, train_loss_epoch=8.290, valid_loss=8.060]\n","Epoch 91:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.710, train_loss_epoch=7.980, valid_loss=8.060]\n","Epoch 92:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.890, train_loss_epoch=7.760, valid_loss=8.060]\n","Epoch 93:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.490, train_loss_epoch=7.500, valid_loss=8.060]\n","Epoch 94:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.220, train_loss_epoch=7.480, valid_loss=8.060]\n","Epoch 95:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.540, train_loss_epoch=7.380, valid_loss=8.060]\n","Epoch 96:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.750, train_loss_epoch=7.420, valid_loss=8.060]\n","Epoch 97:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.450, train_loss_epoch=7.470, valid_loss=8.060]\n","Epoch 98:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.830, train_loss_epoch=7.160, valid_loss=8.060]\n","Epoch 99:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.380, train_loss_epoch=6.940, valid_loss=8.060]\n","Epoch 99: 100%|██████████| 4/4 [00:00\u003c00:00, 15.37it/s, v_num=0, train_loss_step=7.500, train_loss_epoch=6.940, valid_loss=8.060]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16516)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16516)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 16.20it/s]\u001b[A\n","Epoch 100:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.500, train_loss_epoch=7.390, valid_loss=7.620]\n","Epoch 101:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.460, train_loss_epoch=7.770, valid_loss=7.620]\n","Epoch 102:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.670, train_loss_epoch=7.870, valid_loss=7.620]\n","Epoch 103:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.810, train_loss_epoch=7.920, valid_loss=7.620]\n","Epoch 104:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.380, train_loss_epoch=8.060, valid_loss=7.620]\n","Epoch 105:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.480, train_loss_epoch=7.360, valid_loss=7.620]\n","Epoch 105: 100%|██████████| 4/4 [00:00\u003c00:00, 14.85it/s, v_num=0, train_loss_step=8.200, train_loss_epoch=7.220, valid_loss=7.620]\n","Epoch 106:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.200, train_loss_epoch=7.220, valid_loss=7.620]\n","Epoch 107:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.060, train_loss_epoch=6.410, valid_loss=7.620]\n","Epoch 108:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.480, train_loss_epoch=6.350, valid_loss=7.620]\n","Epoch 109:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.120, train_loss_epoch=6.050, valid_loss=7.620]\n","Epoch 110:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.800, train_loss_epoch=6.600, valid_loss=7.620]\n","Epoch 111:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.140, train_loss_epoch=6.520, valid_loss=7.620]\n","Epoch 112:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.040, train_loss_epoch=6.930, valid_loss=7.620]\n","Epoch 113:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.290, train_loss_epoch=7.390, valid_loss=7.620]\n","Epoch 114:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.240, train_loss_epoch=7.530, valid_loss=7.620]\n","Epoch 115:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.540, train_loss_epoch=7.200, valid_loss=7.620]\n","Epoch 116:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.810, train_loss_epoch=7.010, valid_loss=7.620]\n","Epoch 117:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.090, train_loss_epoch=6.840, valid_loss=7.620]\n","Epoch 118:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.790, train_loss_epoch=6.960, valid_loss=7.620]\n","Epoch 119:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.190, train_loss_epoch=6.310, valid_loss=7.620]\n","Epoch 120:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.550, train_loss_epoch=6.400, valid_loss=7.620]\n","Epoch 121:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.560, train_loss_epoch=6.080, valid_loss=7.620]\n","Epoch 122:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.470, train_loss_epoch=6.730, valid_loss=7.620]\n","Epoch 123:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.150, train_loss_epoch=6.880, valid_loss=7.620]\n","Epoch 124:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.840, train_loss_epoch=6.640, valid_loss=7.620]\n","Epoch 124: 100%|██████████| 4/4 [00:00\u003c00:00, 15.03it/s, v_num=0, train_loss_step=5.900, train_loss_epoch=6.640, valid_loss=7.620]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16516)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 16.26it/s]\u001b[A\n","Epoch 125:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.900, train_loss_epoch=6.130, valid_loss=5.800]\n","Epoch 126:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.980, train_loss_epoch=6.050, valid_loss=5.800]\n","Epoch 127:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.510, train_loss_epoch=6.340, valid_loss=5.800]\n","Epoch 128:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.980, train_loss_epoch=6.310, valid_loss=5.800]\n","Epoch 129:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.840, train_loss_epoch=6.500, valid_loss=5.800]\n","Epoch 130:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.910, train_loss_epoch=6.820, valid_loss=5.800]\n","Epoch 131:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.960, train_loss_epoch=7.170, valid_loss=5.800]\n","Epoch 132:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.230, train_loss_epoch=7.340, valid_loss=5.800]\n","Epoch 133:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.500, train_loss_epoch=7.370, valid_loss=5.800]\n","Epoch 134:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.820, train_loss_epoch=7.360, valid_loss=5.800]\n","Epoch 135:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.460, train_loss_epoch=6.810, valid_loss=5.800]\n","Epoch 136:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.570, train_loss_epoch=6.550, valid_loss=5.800]\n","Epoch 137:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.300, train_loss_epoch=6.580, valid_loss=5.800]\n","Epoch 138:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.440, train_loss_epoch=5.910, valid_loss=5.800]\n","Epoch 139:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.410, train_loss_epoch=6.450, valid_loss=5.800]\n","Epoch 140:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.600, train_loss_epoch=6.610, valid_loss=5.800]\n","Epoch 141:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.110, train_loss_epoch=6.570, valid_loss=5.800]\n","Epoch 142:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.610, train_loss_epoch=6.200, valid_loss=5.800]\n","Epoch 143:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.050, train_loss_epoch=6.700, valid_loss=5.800]\n","Epoch 144:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.940, train_loss_epoch=7.140, valid_loss=5.800]\n","Epoch 145:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.960, train_loss_epoch=8.150, valid_loss=5.800]\n","Epoch 146:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.750, train_loss_epoch=7.700, valid_loss=5.800]\n","Epoch 147:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.300, train_loss_epoch=7.680, valid_loss=5.800]\n","Epoch 148:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.260, train_loss_epoch=7.540, valid_loss=5.800]\n","Epoch 149:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.960, train_loss_epoch=7.760, valid_loss=5.800]\n","Epoch 149: 100%|██████████| 4/4 [00:00\u003c00:00, 15.20it/s, v_num=0, train_loss_step=7.460, train_loss_epoch=7.760, valid_loss=5.800]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 19:00:49,666\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('MA(2,9)', 'MA(3,9)', 'MA(1,12)', 'MA(2,12)', 'MA(3,12)', 'MOM(9)', 'MOM(12)', 'RSI(7)', 'RSI(14)', 'EMA(3,9)', 'EMA(5,9)', 'EMA(5,12)', 'DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (168, 24, 1), 'n_pool_kernel_size': (4, 4, 4), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=16516)\u001b[0m `Trainer.fit` stopped: `max_steps=600.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=16516)\u001b[0m \n","\u001b[36m(_train_tune pid=16516)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 16.19it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16516)\u001b[0m \r                                                                      \u001b[A\rEpoch 149: 100%|██████████| 4/4 [00:00\u003c00:00,  6.84it/s, v_num=0, train_loss_step=7.460, train_loss_epoch=7.760, valid_loss=7.200]\rEpoch 149: 100%|██████████| 4/4 [00:00\u003c00:00,  6.83it/s, v_num=0, train_loss_step=7.460, train_loss_epoch=7.480, valid_loss=7.200]\rEpoch 149: 100%|██████████| 4/4 [00:00\u003c00:00,  6.81it/s, v_num=0, train_loss_step=7.460, train_loss_epoch=7.480, valid_loss=7.200]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=16797)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=16797)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=16797)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=16797)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=16797)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=16797)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=16797)\u001b[0m 2024-11-12 19:00:57.276937: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=16797)\u001b[0m 2024-11-12 19:00:57.297530: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=16797)\u001b[0m 2024-11-12 19:00:57.323076: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=16797)\u001b[0m 2024-11-12 19:00:57.330889: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=16797)\u001b[0m 2024-11-12 19:00:57.348717: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=16797)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=16797)\u001b[0m 2024-11-12 19:00:58.520209: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=16797)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=16797)\u001b[0m \n","\u001b[36m(_train_tune pid=16797)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=16797)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=16797)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=16797)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=16797)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=16797)\u001b[0m 3 | blocks       | ModuleList       | 2.7 M  | train\n","\u001b[36m(_train_tune pid=16797)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=16797)\u001b[0m 2.7 M     Trainable params\n","\u001b[36m(_train_tune pid=16797)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=16797)\u001b[0m 2.7 M     Total params\n","\u001b[36m(_train_tune pid=16797)\u001b[0m 10.927    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=16797)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=16797)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/4 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.0677, train_loss_epoch=-0.0514]\n","Epoch 2:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-0.543]\n","Epoch 3:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.811, train_loss_epoch=-0.522]\n","Epoch 4:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.227, train_loss_epoch=-0.401]\n","Epoch 5:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-0.802]\n","Epoch 6:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.443, train_loss_epoch=-0.577]\n","Epoch 7:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.592, train_loss_epoch=-0.732]\n","Epoch 8:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.426, train_loss_epoch=-0.703]\n","Epoch 9:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.06, train_loss_epoch=-0.881]\n","Epoch 10:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.458, train_loss_epoch=-0.755]\n","Epoch 11:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.05]\n","Epoch 12:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.717, train_loss_epoch=-0.928]\n","Epoch 13:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.76, train_loss_epoch=-0.876]\n","Epoch 14:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.645, train_loss_epoch=-0.961]\n","Epoch 15:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.726, train_loss_epoch=-0.971]\n","Epoch 16:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.669, train_loss_epoch=-0.958]\n","Epoch 17:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.02]\n","Epoch 18:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.17]\n","Epoch 19:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.23]\n","Epoch 20:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.01, train_loss_epoch=-1.06]\n","Epoch 21:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.13]\n","Epoch 22:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.895, train_loss_epoch=-1.01]\n","Epoch 23:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.18]\n","Epoch 24:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.07, train_loss_epoch=-1.13]\n","Epoch 24: 100%|██████████| 4/4 [00:00\u003c00:00, 13.03it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.13]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16797)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.75it/s]\u001b[A\n","Epoch 25:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.27, valid_loss=-1.11]\n","Epoch 26:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.27, valid_loss=-1.11]\n","Epoch 27:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.36, train_loss_epoch=-1.19, valid_loss=-1.11]\n","Epoch 28:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.36, train_loss_epoch=-1.30, valid_loss=-1.11]\n","Epoch 29:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.355, train_loss_epoch=-0.96, valid_loss=-1.11]\n","Epoch 30:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.35, valid_loss=-1.11]\n","Epoch 31:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.26, valid_loss=-1.11]\n","Epoch 32:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.32, valid_loss=-1.11]\n","Epoch 33:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.32, valid_loss=-1.11]\n","Epoch 34:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.33, valid_loss=-1.11]\n","Epoch 35:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.40, valid_loss=-1.11]\n","Epoch 36:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.46, valid_loss=-1.11]\n","Epoch 37:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.36, valid_loss=-1.11]\n","Epoch 38:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.47, valid_loss=-1.11]\n","Epoch 39:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.38, valid_loss=-1.11]\n","Epoch 40:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.36, valid_loss=-1.11]\n","Epoch 41:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.871, train_loss_epoch=-1.25, valid_loss=-1.11]\n","Epoch 42:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.734, train_loss_epoch=-1.23, valid_loss=-1.11]\n","Epoch 43:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.49, valid_loss=-1.11]\n","Epoch 44:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.47, valid_loss=-1.11]\n","Epoch 45:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.49, valid_loss=-1.11]\n","Epoch 46:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.49, valid_loss=-1.11]\n","Epoch 47:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.44, valid_loss=-1.11]\n","Epoch 48:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.50, valid_loss=-1.11]\n","Epoch 49:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.40, valid_loss=-1.11]\n","Epoch 49: 100%|██████████| 4/4 [00:00\u003c00:00, 12.21it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.40, valid_loss=-1.11]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16797)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16797)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.74it/s]\u001b[A\n","Epoch 50:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.44, valid_loss=-1.45]\n","Epoch 51:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.44, valid_loss=-1.45]\n","Epoch 52:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.57, valid_loss=-1.45]\n","Epoch 53:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.45, valid_loss=-1.45]\n","Epoch 54:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-1.66, valid_loss=-1.45]\n","Epoch 55:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.50, valid_loss=-1.45]\n","Epoch 56:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.56, valid_loss=-1.45]\n","Epoch 57:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.57, valid_loss=-1.45]\n","Epoch 58:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.46, valid_loss=-1.45]\n","Epoch 59:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.61, valid_loss=-1.45]\n","Epoch 60:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.52, valid_loss=-1.45]\n","Epoch 61:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.54, valid_loss=-1.45]\n","Epoch 62:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.48, valid_loss=-1.45]\n","Epoch 63:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.55, valid_loss=-1.45]\n","Epoch 64:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.57, valid_loss=-1.45]\n","Epoch 65:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.49, valid_loss=-1.45]\n","Epoch 66:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.50, valid_loss=-1.45]\n","Epoch 67:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.47, valid_loss=-1.45]\n","Epoch 68:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.59, valid_loss=-1.45]\n","Epoch 69:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.58, valid_loss=-1.45]\n","Epoch 70:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.53, valid_loss=-1.45]\n","Epoch 71:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.49, valid_loss=-1.45]\n","Epoch 72:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.611, train_loss_epoch=-1.28, valid_loss=-1.45]\n","Epoch 73:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.56, valid_loss=-1.45]\n","Epoch 74:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.63, valid_loss=-1.45]\n","Epoch 74: 100%|██████████| 4/4 [00:00\u003c00:00, 12.19it/s, v_num=0, train_loss_step=-0.935, train_loss_epoch=-1.63, valid_loss=-1.45]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16797)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16797)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.73it/s]\u001b[A\n","Epoch 75:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.935, train_loss_epoch=-1.42, valid_loss=-1.54]\n","Epoch 76:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.58, valid_loss=-1.54]\n","Epoch 77:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.59, valid_loss=-1.54]\n","Epoch 78:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.46, valid_loss=-1.54]\n","Epoch 79:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.69, valid_loss=-1.54]\n","Epoch 80:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.56, valid_loss=-1.54]\n","Epoch 81:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.68, valid_loss=-1.54]\n","Epoch 82:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.52, valid_loss=-1.54]\n","Epoch 83:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.62, valid_loss=-1.54]\n","Epoch 84:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.647, train_loss_epoch=-1.39, valid_loss=-1.54]\n","Epoch 85:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.53, valid_loss=-1.54]\n","Epoch 86:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.59, valid_loss=-1.54]\n","Epoch 87:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.60, valid_loss=-1.54]\n","Epoch 88:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.77, valid_loss=-1.54]\n","Epoch 89:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.56, valid_loss=-1.54]\n","Epoch 90:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.47, valid_loss=-1.54]\n","Epoch 91:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.58, valid_loss=-1.54]\n","Epoch 92:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.75, valid_loss=-1.54]\n","Epoch 93:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.59, valid_loss=-1.54]\n","Epoch 94:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.73, valid_loss=-1.54]\n","Epoch 95:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.74, valid_loss=-1.54]\n","Epoch 96:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.61, valid_loss=-1.54]\n","Epoch 97:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.66, valid_loss=-1.54]\n","Epoch 98:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.75, valid_loss=-1.54]\n","Epoch 99:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.59, valid_loss=-1.54]\n","Epoch 99: 100%|██████████| 4/4 [00:00\u003c00:00, 12.51it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.59, valid_loss=-1.54]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16797)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16797)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.75it/s]\u001b[A\n","Epoch 100:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.59, valid_loss=-1.59]\n","Epoch 101:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.69, valid_loss=-1.59]\n","Epoch 102:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.57, valid_loss=-1.59]\n","Epoch 103:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.66, valid_loss=-1.59]\n","Epoch 104:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.55, valid_loss=-1.59]\n","Epoch 105:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.72, valid_loss=-1.59]\n","Epoch 106:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.60, valid_loss=-1.59]\n","Epoch 107:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.61, valid_loss=-1.59]\n","Epoch 108:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.65, valid_loss=-1.59]\n","Epoch 109:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.58, valid_loss=-1.59]\n","Epoch 110:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.71, valid_loss=-1.59]\n","Epoch 111:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.60, valid_loss=-1.59]\n","Epoch 112:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.56, valid_loss=-1.59]\n","Epoch 113:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-1.77, valid_loss=-1.59]\n","Epoch 114:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.74, valid_loss=-1.59]\n","Epoch 115:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.73, valid_loss=-1.59]\n","Epoch 116:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.61, valid_loss=-1.59]\n","Epoch 117:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.62, valid_loss=-1.59]\n","Epoch 118:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.935, train_loss_epoch=-1.50, valid_loss=-1.59]\n","Epoch 119:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-1.80, valid_loss=-1.59]\n","Epoch 120:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.59, valid_loss=-1.59]\n","Epoch 121:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.70, valid_loss=-1.59]\n","Epoch 122:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.66, valid_loss=-1.59]\n","Epoch 123:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.54, valid_loss=-1.59]\n","Epoch 124:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.67, valid_loss=-1.59]\n","Epoch 124: 100%|██████████| 4/4 [00:00\u003c00:00, 12.56it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.67, valid_loss=-1.59]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16797)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16797)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.73it/s]\u001b[A\n","Epoch 125:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.67, valid_loss=-1.64]\n","Epoch 126:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.71, valid_loss=-1.64]\n","Epoch 127:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.58, valid_loss=-1.64]\n","Epoch 128:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.77, valid_loss=-1.64]\n","Epoch 129:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.59, valid_loss=-1.64]\n","Epoch 130:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.58, valid_loss=-1.64]\n","Epoch 131:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.67, valid_loss=-1.64]\n","Epoch 132:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.67, valid_loss=-1.64]\n","Epoch 133:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.62, valid_loss=-1.64]\n","Epoch 134:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.65, valid_loss=-1.64]\n","Epoch 135:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.56, valid_loss=-1.64]\n","Epoch 136:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-1.82, valid_loss=-1.64]\n","Epoch 137:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.56, valid_loss=-1.64]\n","Epoch 138:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.72, valid_loss=-1.64]\n","Epoch 139:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.65, valid_loss=-1.64]\n","Epoch 140:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.66, valid_loss=-1.64]\n","Epoch 141:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.68, valid_loss=-1.64]\n","Epoch 142:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.45, valid_loss=-1.64]\n","Epoch 143:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.76, valid_loss=-1.64]\n","Epoch 144:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.69, valid_loss=-1.64]\n","Epoch 145:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.65, valid_loss=-1.64]\n","Epoch 146:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.69, valid_loss=-1.64]\n","Epoch 147:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.65, valid_loss=-1.64]\n","Epoch 148:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.65, valid_loss=-1.64]\n","Epoch 149:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.52, valid_loss=-1.64]\n","Epoch 149: 100%|██████████| 4/4 [00:00\u003c00:00, 12.64it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.52, valid_loss=-1.64]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16797)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16797)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.73it/s]\u001b[A\n","Epoch 150:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.68, valid_loss=-1.64]\n","Epoch 151:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.67, valid_loss=-1.64]\n","Epoch 152:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.59, valid_loss=-1.64]\n","Epoch 153:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.62, valid_loss=-1.64]\n","Epoch 154:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-1.79, valid_loss=-1.64]\n","Epoch 155:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.962, train_loss_epoch=-1.48, valid_loss=-1.64]\n","Epoch 156:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.61, valid_loss=-1.64]\n","Epoch 157:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.70, valid_loss=-1.64]\n","Epoch 158:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.55, valid_loss=-1.64]\n","Epoch 159:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.75, valid_loss=-1.64]\n","Epoch 160:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.60, valid_loss=-1.64]\n","Epoch 161:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-1.79, valid_loss=-1.64]\n","Epoch 162:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-1.84, valid_loss=-1.64]\n","Epoch 163:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.70, valid_loss=-1.64]\n","Epoch 164:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.69, valid_loss=-1.64]\n","Epoch 165:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.03, train_loss_epoch=-1.52, valid_loss=-1.64]\n","Epoch 166:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.60, valid_loss=-1.64]\n","Epoch 167:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.59, valid_loss=-1.64]\n","Epoch 167:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.61, valid_loss=-1.64]        \n","Epoch 168:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.61, valid_loss=-1.64]\n","Epoch 169:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-1.81, valid_loss=-1.64]\n","Epoch 170:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.59, valid_loss=-1.64]\n","Epoch 171:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.69, valid_loss=-1.64]\n","Epoch 172:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-1.79, valid_loss=-1.64]\n","Epoch 173:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.52, valid_loss=-1.64]\n","Epoch 174:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.71, valid_loss=-1.64]\n","Epoch 174: 100%|██████████| 4/4 [00:00\u003c00:00, 12.10it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.71, valid_loss=-1.64]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16797)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16797)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.75it/s]\u001b[A\n","Epoch 175:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.78, valid_loss=-1.66]\n","Epoch 176:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.68, valid_loss=-1.66]\n","Epoch 177:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.76, valid_loss=-1.66]\n","Epoch 178:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.72, valid_loss=-1.66]\n","Epoch 179:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.64, valid_loss=-1.66]\n","Epoch 180:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.63, valid_loss=-1.66]\n","Epoch 180: 100%|██████████| 4/4 [00:00\u003c00:00, 12.10it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.79, valid_loss=-1.66]\n","Epoch 181:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.79, valid_loss=-1.66]\n","Epoch 182:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.65, valid_loss=-1.66]\n","Epoch 183:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.71, valid_loss=-1.66]\n","Epoch 184:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.58, valid_loss=-1.66]\n","Epoch 185:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.64, valid_loss=-1.66]\n","Epoch 186:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.74, valid_loss=-1.66]\n","Epoch 187:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.57, valid_loss=-1.66]\n","Epoch 188:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.60, valid_loss=-1.66]\n","Epoch 189:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.83, train_loss_epoch=-1.48, valid_loss=-1.66]\n","Epoch 190:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.69, valid_loss=-1.66]\n","Epoch 191:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.05, train_loss_epoch=-1.54, valid_loss=-1.66]\n","Epoch 192:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.73, valid_loss=-1.66]\n","Epoch 193:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.63, valid_loss=-1.66]\n","Epoch 194:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.75, valid_loss=-1.66]\n","Epoch 195:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.71, valid_loss=-1.66]\n","Epoch 196:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-1.85, valid_loss=-1.66]\n","Epoch 197:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.65, valid_loss=-1.66]\n","Epoch 198:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.70, valid_loss=-1.66]\n","Epoch 199:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.64, valid_loss=-1.66]\n","Epoch 199: 100%|██████████| 4/4 [00:00\u003c00:00, 12.68it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.64, valid_loss=-1.66]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16797)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16797)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.74it/s]\u001b[A\n","Epoch 200:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.66, valid_loss=-1.66]\n","Epoch 201:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.68, valid_loss=-1.66]\n","Epoch 202:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.63, valid_loss=-1.66]\n","Epoch 203:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.67, valid_loss=-1.66]\n","Epoch 204:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.58, valid_loss=-1.66]\n","Epoch 205:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.77, valid_loss=-1.66]\n","Epoch 206:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.81, valid_loss=-1.66]\n","Epoch 207:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-1.82, valid_loss=-1.66]\n","Epoch 208:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.57, valid_loss=-1.66]\n","Epoch 209:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.854, train_loss_epoch=-1.42, valid_loss=-1.66]\n","Epoch 210:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.74, valid_loss=-1.66]\n","Epoch 211:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.64, valid_loss=-1.66]\n","Epoch 212:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.64, valid_loss=-1.66]\n","Epoch 213:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.56, valid_loss=-1.66]\n","Epoch 214:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.74, valid_loss=-1.66]\n","Epoch 215:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.72, valid_loss=-1.66]\n","Epoch 216:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.64, valid_loss=-1.66]\n","Epoch 217:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.80, valid_loss=-1.66]\n","Epoch 218:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.56, valid_loss=-1.66]\n","Epoch 219:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.60, valid_loss=-1.66]\n","Epoch 220:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.941, train_loss_epoch=-1.47, valid_loss=-1.66]\n","Epoch 221:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.68, valid_loss=-1.66]\n","Epoch 221: 100%|██████████| 4/4 [00:00\u003c00:00, 12.34it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.65, valid_loss=-1.66]\n","Epoch 222:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.65, valid_loss=-1.66]\n","Epoch 223:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.76, valid_loss=-1.66]\n","Epoch 224:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.63, valid_loss=-1.66]\n","Epoch 224: 100%|██████████| 4/4 [00:00\u003c00:00, 12.16it/s, v_num=0, train_loss_step=-0.761, train_loss_epoch=-1.63, valid_loss=-1.66]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16797)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16797)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.73it/s]\u001b[A\n","Epoch 225:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.761, train_loss_epoch=-1.47, valid_loss=-1.66]\n","Epoch 226:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.69, valid_loss=-1.66]\n","Epoch 227:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.73, valid_loss=-1.66]\n","Epoch 228:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.68, valid_loss=-1.66]\n","Epoch 229:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.03, train_loss_epoch=-1.52, valid_loss=-1.66]\n","Epoch 230:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.59, valid_loss=-1.66]\n","Epoch 231:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.09, train_loss_epoch=-1.51, valid_loss=-1.66]\n","Epoch 232:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.09, train_loss_epoch=-1.59, valid_loss=-1.66]\n","Epoch 233:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.75, valid_loss=-1.66]\n","Epoch 234:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.69, valid_loss=-1.66]\n","Epoch 235:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.74, valid_loss=-1.66]\n","Epoch 236:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.62, valid_loss=-1.66]\n","Epoch 237:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.55, valid_loss=-1.66]\n","Epoch 238:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.68, valid_loss=-1.66]\n","Epoch 239:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.54, valid_loss=-1.66]\n","Epoch 240:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.65, valid_loss=-1.66]\n","Epoch 241:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.78, valid_loss=-1.66]\n","Epoch 242:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.61, valid_loss=-1.66]\n","Epoch 243:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.62, valid_loss=-1.66]\n","Epoch 244:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.70, valid_loss=-1.66]\n","Epoch 245:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.62, valid_loss=-1.66]\n","Epoch 245: 100%|██████████| 4/4 [00:00\u003c00:00, 12.18it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.62, valid_loss=-1.66]\n","Epoch 246:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.65, valid_loss=-1.66]\n","Epoch 247:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.61, valid_loss=-1.66]\n","Epoch 248:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.64, valid_loss=-1.66]\n","Epoch 249:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-1.81, valid_loss=-1.66]\n","Epoch 249: 100%|██████████| 4/4 [00:00\u003c00:00, 12.63it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.81, valid_loss=-1.66]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 19:02:46,415\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('MA(2,9)', 'MA(3,9)', 'MA(1,12)', 'MA(2,12)', 'MA(3,12)', 'MOM(9)', 'MOM(12)', 'RSI(7)', 'RSI(14)', 'EMA(3,9)', 'EMA(5,9)', 'EMA(5,12)', 'DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (1, 1, 1), 'n_pool_kernel_size': (16, 8, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=16797)\u001b[0m `Trainer.fit` stopped: `max_steps=1000.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=16797)\u001b[0m \n","\u001b[36m(_train_tune pid=16797)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.73it/s]\u001b[A\n","\u001b[36m(_train_tune pid=16797)\u001b[0m \r                                                                      \u001b[A\rEpoch 249: 100%|██████████| 4/4 [00:02\u003c00:00,  1.48it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.81, valid_loss=-1.65]\rEpoch 249: 100%|██████████| 4/4 [00:02\u003c00:00,  1.48it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.74, valid_loss=-1.65]\rEpoch 249: 100%|██████████| 4/4 [00:02\u003c00:00,  1.48it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.74, valid_loss=-1.65]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=17345)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=17345)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=17345)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=17345)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=17345)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=17345)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=17345)\u001b[0m 2024-11-12 19:02:54.445855: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=17345)\u001b[0m 2024-11-12 19:02:54.466458: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=17345)\u001b[0m 2024-11-12 19:02:54.492289: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=17345)\u001b[0m 2024-11-12 19:02:54.500070: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=17345)\u001b[0m 2024-11-12 19:02:54.518591: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=17345)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=17345)\u001b[0m 2024-11-12 19:02:55.711684: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=17345)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=17345)\u001b[0m \n","\u001b[36m(_train_tune pid=17345)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=17345)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=17345)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=17345)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=17345)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=17345)\u001b[0m 3 | blocks       | ModuleList       | 3.1 M  | train\n","\u001b[36m(_train_tune pid=17345)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=17345)\u001b[0m 3.1 M     Trainable params\n","\u001b[36m(_train_tune pid=17345)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=17345)\u001b[0m 3.1 M     Total params\n","\u001b[36m(_train_tune pid=17345)\u001b[0m 12.424    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=17345)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=17345)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/4 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.825, train_loss_epoch=-0.755]\n","Epoch 2:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-0.939]\n","Epoch 3:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.12]\n","Epoch 4:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.22]\n","Epoch 5:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.35]\n","Epoch 6:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.21]\n","Epoch 7:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.24]\n","Epoch 7: 100%|██████████| 4/4 [00:00\u003c00:00, 10.94it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.24]\n","Epoch 8:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.24]\n","Epoch 9:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.46]\n","Epoch 10:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.27]\n","Epoch 11:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.51]\n","Epoch 12:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.05, train_loss_epoch=-1.32]\n","Epoch 13:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.34]\n","Epoch 14:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.35]\n","Epoch 15:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.04, train_loss_epoch=-1.35]\n","Epoch 16:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.34]\n","Epoch 17:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.50]\n","Epoch 18:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.54]\n","Epoch 19:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.46]\n","Epoch 20:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.37]\n","Epoch 21:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.54]\n","Epoch 21: 100%|██████████| 4/4 [00:00\u003c00:00, 10.68it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.54]\n","Epoch 22:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.40]\n","Epoch 23:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.44]\n","Epoch 24:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.38]\n","Epoch 24: 100%|██████████| 4/4 [00:00\u003c00:00, 10.81it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.38]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=17345)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=17345)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.85it/s]\u001b[A\n","Epoch 25:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.56, valid_loss=-1.45]\n","Epoch 26:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.55, valid_loss=-1.45]\n","Epoch 27:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.47, valid_loss=-1.45]\n","Epoch 28:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.49, valid_loss=-1.45]\n","Epoch 29:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.736, train_loss_epoch=-1.31, valid_loss=-1.45]\n","Epoch 30:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.51, valid_loss=-1.45]\n","Epoch 31:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.57, valid_loss=-1.45]\n","Epoch 32:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.50, valid_loss=-1.45]\n","Epoch 33:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.47, valid_loss=-1.45]\n","Epoch 34:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.48, valid_loss=-1.45]\n","Epoch 35:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.51, valid_loss=-1.45]\n","Epoch 36:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.58, valid_loss=-1.45]\n","Epoch 37:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.64, valid_loss=-1.45]\n","Epoch 38:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.69, valid_loss=-1.45]\n","Epoch 39:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.61, valid_loss=-1.45]\n","Epoch 40:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.59, valid_loss=-1.45]\n","Epoch 41:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.56, valid_loss=-1.45]\n","Epoch 42:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.51, valid_loss=-1.45]\n","Epoch 43:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.60, valid_loss=-1.45]\n","Epoch 44:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.63, valid_loss=-1.45]\n","Epoch 45:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.67, valid_loss=-1.45]\n","Epoch 46:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.57, valid_loss=-1.45]\n","Epoch 47:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.65, valid_loss=-1.45]\n","Epoch 48:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.74, valid_loss=-1.45]\n","Epoch 49:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.58, valid_loss=-1.45]\n","Epoch 49: 100%|██████████| 4/4 [00:00\u003c00:00, 11.10it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.58, valid_loss=-1.45]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=17345)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=17345)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.85it/s]\u001b[A\n","Epoch 50:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.71, valid_loss=-1.67]\n","Epoch 51:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.71, valid_loss=-1.67]\n","Epoch 52:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.74, valid_loss=-1.67]\n","Epoch 53:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.70, valid_loss=-1.67]\n","Epoch 54:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-1.82, valid_loss=-1.67]\n","Epoch 55:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.65, valid_loss=-1.67]\n","Epoch 56:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.70, valid_loss=-1.67]\n","Epoch 57:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.78, valid_loss=-1.67]\n","Epoch 58:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.66, valid_loss=-1.67]\n","Epoch 59:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.78, valid_loss=-1.67]\n","Epoch 60:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.74, valid_loss=-1.67]\n","Epoch 61:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.76, valid_loss=-1.67]\n","Epoch 62:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.67, valid_loss=-1.67]\n","Epoch 63:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.77, valid_loss=-1.67]\n","Epoch 64:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.75, valid_loss=-1.67]\n","Epoch 65:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.66, valid_loss=-1.67]\n","Epoch 66:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.73, valid_loss=-1.67]\n","Epoch 67:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.72, valid_loss=-1.67]\n","Epoch 67: 100%|██████████| 4/4 [00:00\u003c00:00, 10.85it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.72, valid_loss=-1.67]\n","Epoch 68:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.78, valid_loss=-1.67]\n","Epoch 69:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.77, valid_loss=-1.67]\n","Epoch 69: 100%|██████████| 4/4 [00:00\u003c00:00, 10.88it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.77, valid_loss=-1.67]\n","Epoch 70:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.67, valid_loss=-1.67]\n","Epoch 71:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.69, valid_loss=-1.67]\n","Epoch 71: 100%|██████████| 4/4 [00:00\u003c00:00, 10.96it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.69, valid_loss=-1.67]\n","Epoch 72:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.967, train_loss_epoch=-1.61, valid_loss=-1.67]\n","Epoch 73:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.75, valid_loss=-1.67]\n","Epoch 74:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.78, valid_loss=-1.67]\n","Epoch 74: 100%|██████████| 4/4 [00:00\u003c00:00, 10.96it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.78, valid_loss=-1.67]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=17345)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=17345)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.86it/s]\u001b[A\n","Epoch 75:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.63, valid_loss=-1.74]\n","Epoch 76:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.78, valid_loss=-1.74]\n","Epoch 77:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.79, valid_loss=-1.74]\n","Epoch 78:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.72, valid_loss=-1.74]\n","Epoch 79:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.84, valid_loss=-1.74]\n","Epoch 80:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.79, valid_loss=-1.74]\n","Epoch 81:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.85, valid_loss=-1.74]\n","Epoch 82:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.67, valid_loss=-1.74]\n","Epoch 83:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.77, valid_loss=-1.74]\n","Epoch 84:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.958, train_loss_epoch=-1.60, valid_loss=-1.74]\n","Epoch 85:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.78, valid_loss=-1.74]\n","Epoch 86:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.80, valid_loss=-1.74]\n","Epoch 87:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.83, valid_loss=-1.74]\n","Epoch 88:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.89, valid_loss=-1.74]\n","Epoch 89:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.77, valid_loss=-1.74]\n","Epoch 90:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.67, valid_loss=-1.74]\n","Epoch 91:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.72, valid_loss=-1.74]\n","Epoch 92:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-1.90, valid_loss=-1.74]\n","Epoch 93:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.77, valid_loss=-1.74]\n","Epoch 94:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-1.91, valid_loss=-1.74]\n","Epoch 95:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.92, valid_loss=-1.74]\n","Epoch 96:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.74, valid_loss=-1.74]\n","Epoch 97:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.87, valid_loss=-1.74]\n","Epoch 98:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-1.89, valid_loss=-1.74]\n","Epoch 99:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.79, valid_loss=-1.74]\n","Epoch 99: 100%|██████████| 4/4 [00:00\u003c00:00, 10.99it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.79, valid_loss=-1.74]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=17345)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=17345)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.86it/s]\u001b[A\n","Epoch 100:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.80, valid_loss=-1.79]\n","Epoch 101:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.91, valid_loss=-1.79]\n","Epoch 102:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.79, valid_loss=-1.79]\n","Epoch 103:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.84, valid_loss=-1.79]\n","Epoch 104:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.82, valid_loss=-1.79]\n","Epoch 105:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-1.86, valid_loss=-1.79]\n","Epoch 106:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-1.89, valid_loss=-1.79]\n","Epoch 107:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.80, valid_loss=-1.79]\n","Epoch 108:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.86, valid_loss=-1.79]\n","Epoch 109:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.77, valid_loss=-1.79]\n","Epoch 110:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.86, valid_loss=-1.79]\n","Epoch 111:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.84, valid_loss=-1.79]\n","Epoch 112:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.80, valid_loss=-1.79]\n","Epoch 113:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-1.96, valid_loss=-1.79]\n","Epoch 114:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-1.97, valid_loss=-1.79]\n","Epoch 114: 100%|██████████| 4/4 [00:00\u003c00:00, 10.78it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.88, valid_loss=-1.79]\n","Epoch 115:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.88, valid_loss=-1.79]\n","Epoch 116:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.80, valid_loss=-1.79]\n","Epoch 117:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.74, valid_loss=-1.79]\n","Epoch 118:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.71, valid_loss=-1.79]\n","Epoch 119:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-1.94, valid_loss=-1.79]\n","Epoch 120:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.73, valid_loss=-1.79]\n","Epoch 121:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.85, valid_loss=-1.79]\n","Epoch 122:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.86, valid_loss=-1.79]\n","Epoch 123:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.83, valid_loss=-1.79]\n","Epoch 124:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.90, valid_loss=-1.79]\n","Epoch 124: 100%|██████████| 4/4 [00:00\u003c00:00, 11.02it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.90, valid_loss=-1.79]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=17345)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=17345)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.86it/s]\u001b[A\n","Epoch 125:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.89, valid_loss=-1.84]\n","Epoch 126:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-1.89, valid_loss=-1.84]\n","Epoch 127:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.85, valid_loss=-1.84]\n","Epoch 128:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.85, valid_loss=-1.84]\n","Epoch 129:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.81, valid_loss=-1.84]\n","Epoch 130:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.85, valid_loss=-1.84]\n","Epoch 131:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.83, valid_loss=-1.84]\n","Epoch 132:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.81, valid_loss=-1.84]\n","Epoch 133:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.85, valid_loss=-1.84]\n","Epoch 134:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.86, valid_loss=-1.84]\n","Epoch 135:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.80, valid_loss=-1.84]\n","Epoch 136:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.40, train_loss_epoch=-2.02, valid_loss=-1.84]\n","Epoch 137:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.78, valid_loss=-1.84]\n","Epoch 138:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-2.00, valid_loss=-1.84]\n","Epoch 139:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.88, valid_loss=-1.84]\n","Epoch 140:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.83, valid_loss=-1.84]\n","Epoch 141:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.92, valid_loss=-1.84]\n","Epoch 142:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.72, valid_loss=-1.84]\n","Epoch 143:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.91, valid_loss=-1.84]\n","Epoch 143: 100%|██████████| 4/4 [00:00\u003c00:00, 10.74it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.91, valid_loss=-1.84]\n","Epoch 144:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.91, valid_loss=-1.84]\n","Epoch 145:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.86, valid_loss=-1.84]\n","Epoch 146:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.88, valid_loss=-1.84]\n","Epoch 147:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.83, valid_loss=-1.84]\n","Epoch 148:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.91, valid_loss=-1.84]\n","Epoch 149:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.80, valid_loss=-1.84]\n","Epoch 149: 100%|██████████| 4/4 [00:00\u003c00:00, 10.68it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.80, valid_loss=-1.84]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=17345)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=17345)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.83it/s]\u001b[A\n","Epoch 150:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.86, valid_loss=-1.86]\n","Epoch 151:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.84, valid_loss=-1.86]\n","Epoch 152:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.79, valid_loss=-1.86]\n","Epoch 153:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.86, valid_loss=-1.86]\n","Epoch 154:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-1.98, valid_loss=-1.86]\n","Epoch 155:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.75, valid_loss=-1.86]\n","Epoch 156:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.82, valid_loss=-1.86]\n","Epoch 157:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.92, valid_loss=-1.86]\n","Epoch 158:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.81, valid_loss=-1.86]\n","Epoch 159:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-1.97, valid_loss=-1.86]\n","Epoch 160:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.81, valid_loss=-1.86]\n","Epoch 161:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.44, train_loss_epoch=-2.00, valid_loss=-1.86]\n","Epoch 162:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.40, train_loss_epoch=-2.02, valid_loss=-1.86]\n","Epoch 163:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-1.98, valid_loss=-1.86]\n","Epoch 164:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-1.93, valid_loss=-1.86]\n","Epoch 165:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.73, valid_loss=-1.86]\n","Epoch 166:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.84, valid_loss=-1.86]\n","Epoch 167:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.91, valid_loss=-1.86]\n","Epoch 168:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.80, valid_loss=-1.86]\n","Epoch 169:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-2.00, valid_loss=-1.86]\n","Epoch 170:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.77, valid_loss=-1.86]\n","Epoch 171:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.89, valid_loss=-1.86]\n","Epoch 172:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-1.98, valid_loss=-1.86]\n","Epoch 173:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.80, valid_loss=-1.86]\n","Epoch 174:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.92, valid_loss=-1.86]\n","Epoch 174: 100%|██████████| 4/4 [00:00\u003c00:00, 10.75it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.92, valid_loss=-1.86]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=17345)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=17345)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.84it/s]\u001b[A\n","Epoch 175:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.91, valid_loss=-1.87]\n","Epoch 176:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.89, valid_loss=-1.87]\n","Epoch 177:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.90, valid_loss=-1.87]\n","Epoch 178:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.88, valid_loss=-1.87]\n","Epoch 179:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.88, valid_loss=-1.87]\n","Epoch 180:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.87, valid_loss=-1.87]\n","Epoch 181:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-1.95, valid_loss=-1.87]\n","Epoch 182:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.97, valid_loss=-1.87]\n","Epoch 183:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.93, valid_loss=-1.87]\n","Epoch 184:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.84, valid_loss=-1.87]\n","Epoch 185:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.81, valid_loss=-1.87]\n","Epoch 186:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-1.96, valid_loss=-1.87]\n","Epoch 187:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.84, valid_loss=-1.87]\n","Epoch 188:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.85, valid_loss=-1.87]\n","Epoch 189:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.72, valid_loss=-1.87]\n","Epoch 190:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.88, valid_loss=-1.87]\n","Epoch 191:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.08, train_loss_epoch=-1.72, valid_loss=-1.87]\n","Epoch 192:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.94, valid_loss=-1.87]\n","Epoch 193:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.84, valid_loss=-1.87]\n","Epoch 194:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.89, valid_loss=-1.87]\n","Epoch 195:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.88, valid_loss=-1.87]\n","Epoch 196:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.57, train_loss_epoch=-2.05, valid_loss=-1.87]\n","Epoch 197:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.86, valid_loss=-1.87]\n","Epoch 198:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.89, valid_loss=-1.87]\n","Epoch 199:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.82, valid_loss=-1.87]\n","Epoch 199: 100%|██████████| 4/4 [00:00\u003c00:00, 10.58it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.82, valid_loss=-1.87]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=17345)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=17345)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.85it/s]\u001b[A\n","\u001b[36m(_train_tune pid=17345)\u001b[0m \n","Epoch 200:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.86, valid_loss=-1.88]\n","Epoch 201:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.89, valid_loss=-1.88]\n","Epoch 201: 100%|██████████| 4/4 [00:00\u003c00:00, 11.02it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.96, valid_loss=-1.88]\n","Epoch 202:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.96, valid_loss=-1.88]\n","Epoch 203:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.85, valid_loss=-1.88]\n","Epoch 204:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.79, valid_loss=-1.88]\n","Epoch 205:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-1.97, valid_loss=-1.88]\n","Epoch 206:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-1.99, valid_loss=-1.88]\n","Epoch 207:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.00, valid_loss=-1.88]\n","Epoch 208:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.88, valid_loss=-1.88]\n","Epoch 209:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.71, valid_loss=-1.88]\n","Epoch 210:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.91, valid_loss=-1.88]\n","Epoch 211:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.86, valid_loss=-1.88]\n","Epoch 212:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.82, valid_loss=-1.88]\n","Epoch 213:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.78, valid_loss=-1.88]\n","Epoch 214:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.89, valid_loss=-1.88]\n","Epoch 215:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.94, valid_loss=-1.88]\n","Epoch 216:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.81, valid_loss=-1.88]\n","Epoch 217:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-1.96, valid_loss=-1.88]\n","Epoch 218:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.79, valid_loss=-1.88]\n","Epoch 219:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.85, valid_loss=-1.88]\n","Epoch 220:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.71, valid_loss=-1.88]\n","Epoch 221:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.90, valid_loss=-1.88]\n","Epoch 222:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.85, valid_loss=-1.88]\n","Epoch 223:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-2.02, valid_loss=-1.88]\n","Epoch 224:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.87, valid_loss=-1.88]\n","Epoch 224: 100%|██████████| 4/4 [00:00\u003c00:00, 10.85it/s, v_num=0, train_loss_step=-1.06, train_loss_epoch=-1.87, valid_loss=-1.88]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=17345)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=17345)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.85it/s]\u001b[A\n","Epoch 225:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.06, train_loss_epoch=-1.68, valid_loss=-1.90]\n","Epoch 226:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.91, valid_loss=-1.90]\n","Epoch 227:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-1.98, valid_loss=-1.90]\n","Epoch 228:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.89, valid_loss=-1.90]\n","Epoch 229:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.77, valid_loss=-1.90]\n","Epoch 230:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.87, valid_loss=-1.90]\n","Epoch 231:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.77, valid_loss=-1.90]\n","Epoch 232:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.83, valid_loss=-1.90]\n","Epoch 233:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.93, valid_loss=-1.90]\n","Epoch 234:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.85, valid_loss=-1.90]\n","Epoch 235:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.01, valid_loss=-1.90]\n","Epoch 236:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.91, valid_loss=-1.90]\n","Epoch 237:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.74, valid_loss=-1.90]\n","Epoch 238:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.91, valid_loss=-1.90]\n","Epoch 239:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.79, valid_loss=-1.90]\n","Epoch 240:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.90, valid_loss=-1.90]\n","Epoch 241:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.99, valid_loss=-1.90]\n","Epoch 242:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.92, valid_loss=-1.90]\n","Epoch 243:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.84, valid_loss=-1.90]\n","Epoch 244:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.86, valid_loss=-1.90]\n","Epoch 245:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.81, valid_loss=-1.90]\n","Epoch 246:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.93, valid_loss=-1.90]\n","Epoch 247:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.83, valid_loss=-1.90]\n","Epoch 248:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.86, valid_loss=-1.90]\n","Epoch 249:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.44, train_loss_epoch=-2.02, valid_loss=-1.90]\n","Epoch 249: 100%|██████████| 4/4 [00:00\u003c00:00, 11.05it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.02, valid_loss=-1.90]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=17345)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=17345)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.84it/s]\u001b[A\n","Epoch 250:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.95, valid_loss=-1.90]\n","Epoch 251:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.80, valid_loss=-1.90]\n","Epoch 252:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.83, valid_loss=-1.90]\n","Epoch 253:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.89, valid_loss=-1.90]\n","Epoch 254:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.80, valid_loss=-1.90]\n","Epoch 255:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.85, valid_loss=-1.90]\n","Epoch 256:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-1.98, valid_loss=-1.90]\n","Epoch 257:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.95, valid_loss=-1.90]\n","Epoch 258:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-2.04, valid_loss=-1.90]\n","Epoch 259:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.75, valid_loss=-1.90]\n","Epoch 260:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.97, valid_loss=-1.90]\n","Epoch 261:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.86, valid_loss=-1.90]\n","Epoch 262:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.02, valid_loss=-1.90]\n","Epoch 263:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.72, valid_loss=-1.90]\n","Epoch 264:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.90, valid_loss=-1.90]\n","Epoch 265:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.46, train_loss_epoch=-2.07, valid_loss=-1.90]\n","Epoch 266:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-1.99, valid_loss=-1.90]\n","Epoch 267:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.83, valid_loss=-1.90]\n","Epoch 268:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.83, valid_loss=-1.90]\n","Epoch 269:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.00, valid_loss=-1.90]\n","Epoch 270:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.84, valid_loss=-1.90]\n","Epoch 271:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.90, valid_loss=-1.90]\n","Epoch 272:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.84, valid_loss=-1.90]\n","Epoch 273:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.87, valid_loss=-1.90]\n","Epoch 274:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.82, valid_loss=-1.90]\n","Epoch 274: 100%|██████████| 4/4 [00:00\u003c00:00, 10.68it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.82, valid_loss=-1.90]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=17345)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=17345)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.86it/s]\u001b[A\n","Epoch 275:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.79, valid_loss=-1.91]\n","Epoch 276:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.91, valid_loss=-1.91]\n","Epoch 277:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.91, valid_loss=-1.91]\n","Epoch 278:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.90, valid_loss=-1.91]\n","Epoch 279:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.82, valid_loss=-1.91]\n","Epoch 280:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.98, valid_loss=-1.91]\n","Epoch 281:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.96, valid_loss=-1.91]\n","Epoch 282:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.94, valid_loss=-1.91]\n","Epoch 283:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.98, valid_loss=-1.91]\n","Epoch 284:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.93, valid_loss=-1.91]\n","Epoch 285:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.90, valid_loss=-1.91]\n","Epoch 286:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.89, valid_loss=-1.91]\n","Epoch 287:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-2.05, valid_loss=-1.91]\n","Epoch 288:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.98, valid_loss=-1.91]\n","Epoch 289:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-1.98, valid_loss=-1.91]\n","Epoch 290:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.95, valid_loss=-1.91]\n","Epoch 291:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.96, valid_loss=-1.91]\n","Epoch 292:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.83, valid_loss=-1.91]\n","Epoch 293:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.90, valid_loss=-1.91]\n","Epoch 294:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.99, valid_loss=-1.91]\n","Epoch 295:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.03, valid_loss=-1.91]\n","Epoch 296:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.97, valid_loss=-1.91]\n","Epoch 297:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.94, valid_loss=-1.91]\n","Epoch 298:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.92, valid_loss=-1.91]\n","Epoch 299:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.92, valid_loss=-1.91]\n","Epoch 299: 100%|██████████| 4/4 [00:00\u003c00:00, 10.98it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.92, valid_loss=-1.91]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=17345)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=17345)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.85it/s]\u001b[A\n","Epoch 300:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.94, valid_loss=-1.92]\n","Epoch 301:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.93, valid_loss=-1.92]\n","Epoch 302:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.89, valid_loss=-1.92]\n","Epoch 303:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.91, valid_loss=-1.92]\n","Epoch 304:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-1.99, valid_loss=-1.92]\n","Epoch 305:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.95, valid_loss=-1.92]\n","Epoch 306:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.96, valid_loss=-1.92]\n","Epoch 307:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.91, valid_loss=-1.92]\n","Epoch 308:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.70, valid_loss=-1.92]\n","Epoch 309:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.01, valid_loss=-1.92]\n","Epoch 310:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.89, valid_loss=-1.92]\n","Epoch 311:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-1.99, valid_loss=-1.92]\n","Epoch 312:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-2.02, valid_loss=-1.92]\n","Epoch 313:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-1.98, valid_loss=-1.92]\n","Epoch 314:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-1.98, valid_loss=-1.92]\n","Epoch 315:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.95, valid_loss=-1.92]\n","Epoch 316:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.96, valid_loss=-1.92]\n","Epoch 317:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-2.04, valid_loss=-1.92]\n","Epoch 318:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.85, valid_loss=-1.92]\n","Epoch 319:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.89, valid_loss=-1.92]\n","Epoch 320:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.98, valid_loss=-1.92]\n","Epoch 320: 100%|██████████| 4/4 [00:00\u003c00:00,  9.31it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.94, valid_loss=-1.92]\n","Epoch 321:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.94, valid_loss=-1.92]\n","Epoch 322:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.93, valid_loss=-1.92]\n","Epoch 323:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.98, valid_loss=-1.92]\n","Epoch 324:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.97, valid_loss=-1.92]\n","Epoch 324: 100%|██████████| 4/4 [00:00\u003c00:00, 10.52it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-1.97, valid_loss=-1.92]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=17345)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 19:05:29,397\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('MA(2,9)', 'MA(3,9)', 'MA(1,12)', 'MA(2,12)', 'MA(3,12)', 'MOM(9)', 'MOM(12)', 'RSI(7)', 'RSI(14)', 'EMA(3,9)', 'EMA(5,9)', 'EMA(5,12)', 'DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (24, 12, 1), 'n_pool_kernel_size': (4, 4, 4), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=17345)\u001b[0m `Trainer.fit` stopped: `max_steps=1300.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=17345)\u001b[0m \n","\u001b[36m(_train_tune pid=17345)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.85it/s]\u001b[A\n","\u001b[36m(_train_tune pid=17345)\u001b[0m \r                                                                      \u001b[A\rEpoch 324: 100%|██████████| 4/4 [00:02\u003c00:00,  1.51it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-1.97, valid_loss=-1.92]\rEpoch 324: 100%|██████████| 4/4 [00:02\u003c00:00,  1.51it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-2.03, valid_loss=-1.92]\rEpoch 324: 100%|██████████| 4/4 [00:02\u003c00:00,  1.51it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-2.03, valid_loss=-1.92]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=18081)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=18081)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=18081)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=18081)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=18081)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=18081)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=18081)\u001b[0m 2024-11-12 19:05:37.391545: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=18081)\u001b[0m 2024-11-12 19:05:37.412627: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=18081)\u001b[0m 2024-11-12 19:05:37.439537: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=18081)\u001b[0m 2024-11-12 19:05:37.449199: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=18081)\u001b[0m 2024-11-12 19:05:37.468831: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=18081)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=18081)\u001b[0m 2024-11-12 19:05:38.667358: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=18081)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=18081)\u001b[0m \n","\u001b[36m(_train_tune pid=18081)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=18081)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=18081)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=18081)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=18081)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=18081)\u001b[0m 3 | blocks       | ModuleList       | 2.7 M  | train\n","\u001b[36m(_train_tune pid=18081)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=18081)\u001b[0m 2.7 M     Trainable params\n","\u001b[36m(_train_tune pid=18081)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=18081)\u001b[0m 2.7 M     Total params\n","\u001b[36m(_train_tune pid=18081)\u001b[0m 10.888    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=18081)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=18081)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/13 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=16.40, train_loss_epoch=14.70]\n","Epoch 2:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.56e+5, train_loss_epoch=5.82e+4]\n","Epoch 3:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=17.30, train_loss_epoch=21.10]\n","Epoch 4:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.50, train_loss_epoch=13.50]\n","Epoch 5:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.10, train_loss_epoch=11.70]\n","Epoch 6:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.20, train_loss_epoch=28.10]\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.20, train_loss_epoch=12.50]\n","\u001b[36m(_train_tune pid=18081)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18081)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.29it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18081)\u001b[0m \n","Epoch 7:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=12.90, train_loss_epoch=12.50, valid_loss=13.40]\n","Epoch 8:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.50, train_loss_epoch=13.30, valid_loss=13.40]\n","Epoch 9:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.70, train_loss_epoch=13.10, valid_loss=13.40]\n","Epoch 10:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=14.20, train_loss_epoch=13.20, valid_loss=13.40]\n","Epoch 11:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.80, train_loss_epoch=14.20, valid_loss=13.40]\n","Epoch 12:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.80, train_loss_epoch=12.90, valid_loss=13.40]\n","Epoch 13:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.00, train_loss_epoch=12.70, valid_loss=13.40]\n","Epoch 14:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.50, train_loss_epoch=12.30, valid_loss=13.40]\n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.10, train_loss_epoch=12.60, valid_loss=13.40]\n","\u001b[36m(_train_tune pid=18081)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18081)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.28it/s]\u001b[A\n","Epoch 15:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=12.10, train_loss_epoch=12.60, valid_loss=584.0]\n","Epoch 16:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=14.20, train_loss_epoch=12.10, valid_loss=584.0]\n","Epoch 17:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.90, train_loss_epoch=13.70, valid_loss=584.0]\n","Epoch 18:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.40, train_loss_epoch=12.50, valid_loss=584.0]\n","Epoch 19:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=16.00, train_loss_epoch=12.70, valid_loss=584.0]\n","Epoch 20:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.30, train_loss_epoch=11.80, valid_loss=584.0]\n","Epoch 21:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.10, train_loss_epoch=11.50, valid_loss=584.0]\n","Epoch 22:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.70, train_loss_epoch=10.80, valid_loss=584.0]\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.00, train_loss_epoch=11.20, valid_loss=584.0]\n","\u001b[36m(_train_tune pid=18081)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18081)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.19it/s]\u001b[A\n","Epoch 23:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=11.20, train_loss_epoch=11.20, valid_loss=11.10]\n","Epoch 24:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.10, train_loss_epoch=10.90, valid_loss=11.10]\n","Epoch 25:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.40, train_loss_epoch=10.90, valid_loss=11.10]\n","Epoch 26:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.10, train_loss_epoch=11.20, valid_loss=11.10]\n","Epoch 27:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.20, train_loss_epoch=11.50, valid_loss=11.10]\n","Epoch 28:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.00, train_loss_epoch=11.20, valid_loss=11.10]\n","Epoch 29:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.30, train_loss_epoch=11.20, valid_loss=11.10]\n","Epoch 30:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.70, train_loss_epoch=10.80, valid_loss=11.10]\n","\u001b[36m(_train_tune pid=18081)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18081)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.37it/s]\u001b[A\n","Epoch 30:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=11.40, train_loss_epoch=10.80, valid_loss=11.20]\n","Epoch 31:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.80, train_loss_epoch=11.80, valid_loss=11.20]\n","Epoch 32:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.60, train_loss_epoch=11.50, valid_loss=11.20]\n","Epoch 33:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.90, train_loss_epoch=11.60, valid_loss=11.20]\n","Epoch 34:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.60, train_loss_epoch=13.40, valid_loss=11.20]\n","Epoch 35:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.90, train_loss_epoch=11.30, valid_loss=11.20]\n","Epoch 36:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.00, train_loss_epoch=11.50, valid_loss=11.20]\n","Epoch 37:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.80, train_loss_epoch=11.10, valid_loss=11.20]\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.50, train_loss_epoch=10.80, valid_loss=11.20]\n","\u001b[36m(_train_tune pid=18081)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18081)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.31it/s]\u001b[A\n","Epoch 38:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=10.80, train_loss_epoch=10.80, valid_loss=10.50]\n","Epoch 39:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.850, train_loss_epoch=10.60, valid_loss=10.50]\n","Epoch 40:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.60, train_loss_epoch=10.40, valid_loss=10.50]\n","Epoch 41:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.990, train_loss_epoch=10.30, valid_loss=10.50]\n","Epoch 42:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.40, train_loss_epoch=10.40, valid_loss=10.50]\n","Epoch 43:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.50, train_loss_epoch=10.90, valid_loss=10.50]\n","Epoch 44:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.260, train_loss_epoch=10.20, valid_loss=10.50]\n","Epoch 45:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.550, train_loss_epoch=9.680, valid_loss=10.50]\n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.850, train_loss_epoch=9.810, valid_loss=10.50]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18081)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.32it/s]\u001b[A\n","Epoch 46:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=9.890, train_loss_epoch=9.810, valid_loss=9.560]\n","Epoch 47:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.830, train_loss_epoch=9.230, valid_loss=9.560]\n","Epoch 48:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.40, train_loss_epoch=9.940, valid_loss=9.560]\n","Epoch 49:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.90, train_loss_epoch=11.20, valid_loss=9.560]\n","Epoch 50:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.40, train_loss_epoch=11.00, valid_loss=9.560]\n","Epoch 51:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.530, train_loss_epoch=10.10, valid_loss=9.560]\n","Epoch 52:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.40, train_loss_epoch=9.790, valid_loss=9.560]\n","Epoch 53:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.80, train_loss_epoch=10.60, valid_loss=9.560]\n","\u001b[36m(_train_tune pid=18081)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18081)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.27it/s]\u001b[A\n","Epoch 54:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.30, train_loss_epoch=10.60, valid_loss=10.30]\n","Epoch 55:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.770, train_loss_epoch=10.10, valid_loss=10.30]\n","Epoch 56:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.60, train_loss_epoch=10.20, valid_loss=10.30]\n","Epoch 57:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.080, train_loss_epoch=9.240, valid_loss=10.30]\n","Epoch 58:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.990, train_loss_epoch=8.720, valid_loss=10.30]\n","Epoch 59:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.700, train_loss_epoch=8.790, valid_loss=10.30]\n","Epoch 60:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.450, train_loss_epoch=8.500, valid_loss=10.30]\n","Epoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.490, train_loss_epoch=9.320, valid_loss=10.30]\n","\u001b[36m(_train_tune pid=18081)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18081)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.14it/s]\u001b[A\n","Epoch 61:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=9.220, train_loss_epoch=9.320, valid_loss=9.120]\n","Epoch 62:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.780, train_loss_epoch=9.340, valid_loss=9.120]\n","Epoch 63:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.910, train_loss_epoch=9.500, valid_loss=9.120]\n","Epoch 64:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.50, train_loss_epoch=10.30, valid_loss=9.120]\n","Epoch 65:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.20, train_loss_epoch=9.810, valid_loss=9.120]\n","Epoch 66:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.10, train_loss_epoch=9.130, valid_loss=9.120]\n","Epoch 67:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.50, train_loss_epoch=10.40, valid_loss=9.120]\n","Epoch 68:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.20, train_loss_epoch=10.40, valid_loss=9.120]\n","Epoch 69:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.470, train_loss_epoch=9.740, valid_loss=9.120]\n","\u001b[36m(_train_tune pid=18081)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18081)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.17it/s]\u001b[A\n","Epoch 69:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=9.740, train_loss_epoch=9.740, valid_loss=9.720]\n","Epoch 70:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.40, train_loss_epoch=9.630, valid_loss=9.720]\n","Epoch 71:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.270, train_loss_epoch=9.170, valid_loss=9.720]\n","Epoch 72:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.540, train_loss_epoch=9.080, valid_loss=9.720]\n","Epoch 73:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.460, train_loss_epoch=8.160, valid_loss=9.720]\n","Epoch 74:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.760, train_loss_epoch=8.150, valid_loss=9.720]\n","Epoch 75:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.940, train_loss_epoch=8.390, valid_loss=9.720]\n","Epoch 76:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.750, train_loss_epoch=7.990, valid_loss=9.720]\n","\u001b[36m(_train_tune pid=18081)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18081)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.33it/s]\u001b[A\n","Epoch 77:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.295, train_loss_epoch=2.180, valid_loss=0.775]\n","Epoch 78:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.121, train_loss_epoch=0.170, valid_loss=0.775]\n","Epoch 79:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.658, train_loss_epoch=-0.484, valid_loss=0.775]\n","Epoch 80:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.51, train_loss_epoch=-0.743, valid_loss=0.775]\n","Epoch 81:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.989, train_loss_epoch=-0.856, valid_loss=0.775]\n","Epoch 82:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.756, train_loss_epoch=-0.752, valid_loss=0.775]\n","Epoch 83:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.931, train_loss_epoch=-0.668, valid_loss=0.775]\n","Epoch 84:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.417, train_loss_epoch=-0.515, valid_loss=0.775]\n","\u001b[36m(_train_tune pid=18081)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18081)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.37it/s]\u001b[A\n","Epoch 84:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-0.417, train_loss_epoch=-0.515, valid_loss=-0.11]\n","Epoch 85:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-0.521, valid_loss=-0.11]\n","Epoch 86:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.845, train_loss_epoch=-0.868, valid_loss=-0.11]\n","Epoch 87:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.00, train_loss_epoch=-0.877, valid_loss=-0.11]\n","Epoch 88:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.02, valid_loss=-0.11]\n","Epoch 89:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.07, train_loss_epoch=-0.991, valid_loss=-0.11]\n","Epoch 90:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.02, valid_loss=-0.11]\n","Epoch 91:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-0.814, valid_loss=-0.11]\n","Epoch 92:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.08, train_loss_epoch=-1.08, valid_loss=-0.11]\n","\u001b[36m(_train_tune pid=18081)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18081)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.33it/s]\u001b[A\n","Epoch 92:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.08, valid_loss=-0.877]\n","Epoch 93:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.12, valid_loss=-0.877]\n","Epoch 94:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.13, valid_loss=-0.877]\n","Epoch 95:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.13, valid_loss=-0.877]\n","Epoch 96:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.13, valid_loss=-0.877]\n","Epoch 97:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.13, valid_loss=-0.877]\n","Epoch 98:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.14, valid_loss=-0.877]\n","Epoch 99:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.13, valid_loss=-0.877]\n","Epoch 99: 100%|██████████| 13/13 [00:00\u003c00:00, 37.07it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.13, valid_loss=-0.877]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18081)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.37it/s]\u001b[A\n","Epoch 100:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.14, valid_loss=-0.869]\n","Epoch 101:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.15, valid_loss=-0.869]\n","Epoch 102:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.08, train_loss_epoch=-1.13, valid_loss=-0.869]\n","Epoch 103:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.804, train_loss_epoch=-1.11, valid_loss=-0.869]\n","Epoch 104:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.14, valid_loss=-0.869]\n","Epoch 105:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.16, valid_loss=-0.869]\n","Epoch 106:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-0.562, valid_loss=-0.869]\n","Epoch 107:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.725, train_loss_epoch=-1.11, valid_loss=-0.869]\n","\u001b[36m(_train_tune pid=18081)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 19:06:42,345\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('MA(2,9)', 'MA(3,9)', 'MA(1,12)', 'MA(2,12)', 'MA(3,12)', 'MOM(9)', 'MOM(12)', 'RSI(7)', 'RSI(14)', 'EMA(3,9)', 'EMA(5,9)', 'EMA(5,12)', 'DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (40, 20, 1), 'n_pool_kernel_size': (2, 2, 2), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=18081)\u001b[0m `Trainer.fit` stopped: `max_steps=1400.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=18081)\u001b[0m \n","\u001b[36m(_train_tune pid=18081)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.35it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18081)\u001b[0m \r                                                                        \u001b[A\rEpoch 107:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.11, valid_loss=-0.904] \rEpoch 107:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.16, valid_loss=-0.904]\rEpoch 107:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.16, valid_loss=-0.904]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=18446)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=18446)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=18446)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=18446)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=18446)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=18446)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=18446)\u001b[0m 2024-11-12 19:06:50.500631: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=18446)\u001b[0m 2024-11-12 19:06:50.521023: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=18446)\u001b[0m 2024-11-12 19:06:50.546612: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=18446)\u001b[0m 2024-11-12 19:06:50.554476: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=18446)\u001b[0m 2024-11-12 19:06:50.572519: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=18446)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=18446)\u001b[0m 2024-11-12 19:06:51.735211: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=18446)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=18446)\u001b[0m \n","\u001b[36m(_train_tune pid=18446)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=18446)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=18446)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=18446)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=18446)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=18446)\u001b[0m 3 | blocks       | ModuleList       | 3.2 M  | train\n","\u001b[36m(_train_tune pid=18446)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=18446)\u001b[0m 3.2 M     Trainable params\n","\u001b[36m(_train_tune pid=18446)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=18446)\u001b[0m 3.2 M     Total params\n","\u001b[36m(_train_tune pid=18446)\u001b[0m 12.956    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=18446)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=18446)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/7 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.43]\n","Epoch 2:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-1.62]\n","Epoch 3:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.65]\n","Epoch 4:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.74]\n","Epoch 5:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.80]\n","Epoch 6:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.73]\n","Epoch 7:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.77]\n","Epoch 8:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.78]\n","Epoch 9:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.83]\n","Epoch 10:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.72]\n","Epoch 11:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-1.90]\n","Epoch 12:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.82]\n","Epoch 13:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.80]\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.84]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18446)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18446)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 27.61it/s]\u001b[A\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.84, valid_loss=-1.83]\n","Epoch 15:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.79, valid_loss=-1.83]\n","Epoch 16:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-1.95, valid_loss=-1.83]\n","Epoch 17:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.82, valid_loss=-1.83]\n","Epoch 18:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.85, valid_loss=-1.83]\n","Epoch 19:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.36, train_loss_epoch=-1.95, valid_loss=-1.83]\n","Epoch 20:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.87, valid_loss=-1.83]\n","Epoch 21:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.92, valid_loss=-1.83]\n","Epoch 22:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.92, valid_loss=-1.83]\n","Epoch 23:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.88, valid_loss=-1.83]\n","Epoch 24:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.86, valid_loss=-1.83]\n","Epoch 25:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.98, valid_loss=-1.83]\n","Epoch 26:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-1.94, valid_loss=-1.83]\n","Epoch 27:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.90, valid_loss=-1.83]\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.97, valid_loss=-1.83]\n","\u001b[36m(_train_tune pid=18446)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18446)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.75it/s]\u001b[A\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.97, valid_loss=-1.85]\n","Epoch 29:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.84, valid_loss=-1.85]\n","Epoch 30:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-1.95, valid_loss=-1.85]\n","Epoch 31:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.88, valid_loss=-1.85]\n","Epoch 32:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.88, valid_loss=-1.85]\n","Epoch 33:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.97, valid_loss=-1.85]\n","Epoch 34:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.86, valid_loss=-1.85]\n","Epoch 35:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-1.92, valid_loss=-1.85]\n","Epoch 35: 100%|██████████| 7/7 [00:00\u003c00:00, 32.32it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.91, valid_loss=-1.85]\n","Epoch 36:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.91, valid_loss=-1.85]\n","Epoch 37:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.57, train_loss_epoch=-2.03, valid_loss=-1.85]\n","Epoch 38:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.94, valid_loss=-1.85]\n","Epoch 39:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.94, valid_loss=-1.85]\n","Epoch 40:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.02, valid_loss=-1.85]\n","Epoch 41:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-2.06, valid_loss=-1.85]\n","Epoch 42:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-1.92, valid_loss=-1.85]\n","\u001b[36m(_train_tune pid=18446)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18446)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.30it/s]\u001b[A\n","Epoch 43:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.95, valid_loss=-1.87]\n","Epoch 44:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.97, valid_loss=-1.87]\n","Epoch 45:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.96, valid_loss=-1.87]\n","Epoch 46:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.96, valid_loss=-1.87]\n","Epoch 47:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-1.97, valid_loss=-1.87]\n","Epoch 48:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.95, valid_loss=-1.87]\n","Epoch 49:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.00, valid_loss=-1.87]\n","Epoch 50:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.91, valid_loss=-1.87]\n","Epoch 51:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.95, valid_loss=-1.87]\n","Epoch 52:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.92, valid_loss=-1.87]\n","Epoch 53:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.88, valid_loss=-1.87]\n","Epoch 54:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.00, valid_loss=-1.87]\n","Epoch 55:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.03, valid_loss=-1.87]\n","Epoch 56:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-2.05, valid_loss=-1.87]\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.90, valid_loss=-1.87]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18446)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18446)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 27.47it/s]\u001b[A\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.90, valid_loss=-1.92]\n","Epoch 58:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.40, train_loss_epoch=-2.04, valid_loss=-1.92]\n","Epoch 59:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-2.01, valid_loss=-1.92]\n","Epoch 60:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.00, valid_loss=-1.92]\n","Epoch 61:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-2.04, valid_loss=-1.92]\n","Epoch 62:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.01, valid_loss=-1.92]\n","Epoch 63:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.03, valid_loss=-1.92]\n","Epoch 64:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-2.00, valid_loss=-1.92]\n","Epoch 65:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.99, valid_loss=-1.92]\n","Epoch 66:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-2.00, valid_loss=-1.92]\n","Epoch 66: 100%|██████████| 7/7 [00:00\u003c00:00, 32.03it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.00, valid_loss=-1.92]\n","Epoch 67:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.05, valid_loss=-1.92]\n","Epoch 68:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.08, valid_loss=-1.92]\n","Epoch 69:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.96, valid_loss=-1.92]\n","Epoch 70:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-2.08, valid_loss=-1.92]\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.09, train_loss_epoch=-1.89, valid_loss=-1.92]\n","\u001b[36m(_train_tune pid=18446)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18446)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.26it/s]\u001b[A\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-1.89, valid_loss=-1.94]\n","Epoch 72:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.04, valid_loss=-1.94]\n","Epoch 73:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.01, valid_loss=-1.94]\n","Epoch 74:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-2.06, valid_loss=-1.94]\n","Epoch 75:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.07, valid_loss=-1.94]\n","Epoch 76:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.00, valid_loss=-1.94]\n","Epoch 77:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-2.01, valid_loss=-1.94]\n","Epoch 78:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-2.07, valid_loss=-1.94]\n","Epoch 79:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.04, valid_loss=-1.94]\n","Epoch 80:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-2.13, valid_loss=-1.94]\n","Epoch 81:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.97, valid_loss=-1.94]\n","Epoch 82:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.97, valid_loss=-1.94]\n","Epoch 83:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.94, valid_loss=-1.94]\n","Epoch 84:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.03, valid_loss=-1.94]\n","Epoch 85:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.06, valid_loss=-1.94]\n","\u001b[36m(_train_tune pid=18446)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18446)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 30.50it/s]\u001b[A\n","Epoch 86:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.04, valid_loss=-1.93]\n","Epoch 87:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.63, train_loss_epoch=-2.16, valid_loss=-1.93]\n","Epoch 88:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.48, train_loss_epoch=-2.09, valid_loss=-1.93]\n","Epoch 88: 100%|██████████| 7/7 [00:00\u003c00:00, 30.69it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.03, valid_loss=-1.93]\n","Epoch 89:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.03, valid_loss=-1.93]\n","Epoch 90:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.01, valid_loss=-1.93]\n","Epoch 91:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.04, valid_loss=-1.93]\n","Epoch 92:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.02, valid_loss=-1.93]\n","Epoch 93:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.02, valid_loss=-1.93]\n","Epoch 94:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.11, valid_loss=-1.93]\n","Epoch 95:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-2.08, valid_loss=-1.93]\n","Epoch 96:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.48, train_loss_epoch=-2.11, valid_loss=-1.93]\n","Epoch 97:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-2.02, valid_loss=-1.93]\n","Epoch 98:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.42, train_loss_epoch=-2.14, valid_loss=-1.93]\n","Epoch 99:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-2.09, valid_loss=-1.93]\n","Epoch 99: 100%|██████████| 7/7 [00:00\u003c00:00, 30.34it/s, v_num=0, train_loss_step=-2.41, train_loss_epoch=-2.09, valid_loss=-1.93]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18446)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18446)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 27.60it/s]\u001b[A\n","Epoch 100:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.41, train_loss_epoch=-2.10, valid_loss=-1.92]\n","Epoch 101:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.06, valid_loss=-1.92]\n","Epoch 102:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.99, valid_loss=-1.92]\n","Epoch 103:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.09, valid_loss=-1.92]\n","Epoch 104:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.44, train_loss_epoch=-2.12, valid_loss=-1.92]\n","Epoch 105:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-2.05, valid_loss=-1.92]\n","Epoch 106:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-2.06, valid_loss=-1.92]\n","Epoch 107:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.08, valid_loss=-1.92]\n","Epoch 108:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.49, train_loss_epoch=-2.18, valid_loss=-1.92]\n","Epoch 109:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.13, valid_loss=-1.92]\n","Epoch 110:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.98, valid_loss=-1.92]\n","Epoch 111:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.42, train_loss_epoch=-2.18, valid_loss=-1.92]\n","Epoch 112:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.13, valid_loss=-1.92]\n","Epoch 113:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.01, valid_loss=-1.92]\n","Epoch 114:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.55, train_loss_epoch=-2.22, valid_loss=-1.92]\n","\u001b[36m(_train_tune pid=18446)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18446)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 27.66it/s]\u001b[A\n","Epoch 114:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.22, valid_loss=-1.93]\n","Epoch 115:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.10, valid_loss=-1.93]\n","Epoch 116:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-2.02, valid_loss=-1.93]\n","Epoch 117:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.57, train_loss_epoch=-2.18, valid_loss=-1.93]\n","Epoch 118:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-2.07, valid_loss=-1.93]\n","Epoch 119:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.12, valid_loss=-1.93]\n","Epoch 120:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.09, valid_loss=-1.93]\n","Epoch 121:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.14, valid_loss=-1.93]\n","Epoch 122:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.12, valid_loss=-1.93]\n","Epoch 123:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.16, valid_loss=-1.93]\n","Epoch 124:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.14, valid_loss=-1.93]\n","Epoch 125:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-2.11, valid_loss=-1.93]\n","Epoch 126:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.14, valid_loss=-1.93]\n","Epoch 127:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-2.01, valid_loss=-1.93]\n","Epoch 128:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.14, valid_loss=-1.93]\n","\u001b[36m(_train_tune pid=18446)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18446)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.32it/s]\u001b[A\n","Epoch 128:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.14, valid_loss=-1.94]\n","Epoch 129:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.15, valid_loss=-1.94]\n","Epoch 130:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.14, valid_loss=-1.94]\n","Epoch 131:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.11, valid_loss=-1.94]\n","Epoch 132:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.16, valid_loss=-1.94]\n","Epoch 133:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.12, valid_loss=-1.94]\n","Epoch 134:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.73, train_loss_epoch=-2.18, valid_loss=-1.94]\n","Epoch 135:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-2.07, valid_loss=-1.94]\n","Epoch 136:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.58, train_loss_epoch=-2.24, valid_loss=-1.94]\n","Epoch 137:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-2.15, valid_loss=-1.94]\n","Epoch 138:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-2.05, valid_loss=-1.94]\n","Epoch 139:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.18, valid_loss=-1.94]\n","Epoch 140:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-2.08, valid_loss=-1.94]\n","Epoch 141:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-2.15, valid_loss=-1.94]\n","Epoch 142:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.17, valid_loss=-1.94]\n","\u001b[36m(_train_tune pid=18446)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18446)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.82it/s]\u001b[A\n","Epoch 143:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.11, valid_loss=-1.94]\n","Epoch 144:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.16, valid_loss=-1.94]\n","Epoch 145:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-2.17, valid_loss=-1.94]\n","Epoch 146:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.12, valid_loss=-1.94]\n","Epoch 147:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.57, train_loss_epoch=-2.23, valid_loss=-1.94]\n","Epoch 148:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.50, train_loss_epoch=-2.15, valid_loss=-1.94]\n","Epoch 149:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.16, valid_loss=-1.94]\n","Epoch 150:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-2.05, valid_loss=-1.94]\n","Epoch 151:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-2.10, valid_loss=-1.94]\n","Epoch 152:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.75, train_loss_epoch=-2.26, valid_loss=-1.94]\n","Epoch 153:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-2.03, valid_loss=-1.94]\n","Epoch 154:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-2.09, valid_loss=-1.94]\n","Epoch 155:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-2.15, valid_loss=-1.94]\n","Epoch 156:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-2.08, valid_loss=-1.94]\n","Epoch 157:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.53, train_loss_epoch=-2.17, valid_loss=-1.94]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18446)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 19:07:32,939\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('MA(2,9)', 'MA(3,9)', 'MA(1,12)', 'MA(2,12)', 'MA(3,12)', 'MOM(9)', 'MOM(12)', 'RSI(7)', 'RSI(14)', 'EMA(3,9)', 'EMA(5,9)', 'EMA(5,12)', 'DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (40, 20, 1), 'n_pool_kernel_size': (2, 2, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=18446)\u001b[0m `Trainer.fit` stopped: `max_steps=1100.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=18446)\u001b[0m \n","\u001b[36m(_train_tune pid=18446)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 29.97it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18446)\u001b[0m \r                                                                      \u001b[A\rEpoch 157:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.17, valid_loss=-1.92]\rEpoch 157:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.15, valid_loss=-1.92]\rEpoch 157:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.15, valid_loss=-1.92]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=18717)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=18717)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=18717)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=18717)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=18717)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=18717)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=18717)\u001b[0m 2024-11-12 19:07:41.421996: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=18717)\u001b[0m 2024-11-12 19:07:41.442645: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=18717)\u001b[0m 2024-11-12 19:07:41.469042: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=18717)\u001b[0m 2024-11-12 19:07:41.477149: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=18717)\u001b[0m 2024-11-12 19:07:41.495938: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=18717)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=18717)\u001b[0m 2024-11-12 19:07:42.662725: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=18717)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=18717)\u001b[0m \n","\u001b[36m(_train_tune pid=18717)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=18717)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=18717)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=18717)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=18717)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=18717)\u001b[0m 3 | blocks       | ModuleList       | 3.1 M  | train\n","\u001b[36m(_train_tune pid=18717)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=18717)\u001b[0m 3.1 M     Trainable params\n","\u001b[36m(_train_tune pid=18717)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=18717)\u001b[0m 3.1 M     Total params\n","\u001b[36m(_train_tune pid=18717)\u001b[0m 12.289    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=18717)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=18717)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/7 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.410, train_loss_epoch=17.60]\n","Epoch 2:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.370, train_loss_epoch=6.810]\n","Epoch 3:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.910, train_loss_epoch=6.110]\n","Epoch 4:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.730, train_loss_epoch=5.580]\n","Epoch 5:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.660, train_loss_epoch=5.700]\n","Epoch 6:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.960, train_loss_epoch=5.750]\n","Epoch 7:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.820, train_loss_epoch=5.300]\n","Epoch 8:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.800, train_loss_epoch=5.330]\n","Epoch 9:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.340, train_loss_epoch=5.030]\n","Epoch 10:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.590, train_loss_epoch=5.070]\n","Epoch 11:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.970, train_loss_epoch=4.940]\n","Epoch 12:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.250, train_loss_epoch=4.930]\n","Epoch 13:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.890, train_loss_epoch=4.820]\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.640, train_loss_epoch=4.600]\n","\u001b[36m(_train_tune pid=18717)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18717)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.28it/s]\u001b[A\n","Epoch 14:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=4.620, train_loss_epoch=4.600, valid_loss=4.510]\n","Epoch 15:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.680, train_loss_epoch=4.670, valid_loss=4.510]\n","Epoch 16:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.910, train_loss_epoch=4.770, valid_loss=4.510]\n","Epoch 17:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.010, train_loss_epoch=4.730, valid_loss=4.510]\n","Epoch 18:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.000, train_loss_epoch=4.730, valid_loss=4.510]\n","Epoch 19:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.250, train_loss_epoch=4.660, valid_loss=4.510]\n","Epoch 20:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.320, train_loss_epoch=4.680, valid_loss=4.510]\n","Epoch 20: 100%|██████████| 7/7 [00:00\u003c00:00, 26.67it/s, v_num=0, train_loss_step=4.770, train_loss_epoch=4.680, valid_loss=4.510]\n","Epoch 21:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.770, train_loss_epoch=4.560, valid_loss=4.510]\n","Epoch 22:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.100, train_loss_epoch=4.670, valid_loss=4.510]\n","Epoch 23:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.860, train_loss_epoch=4.310, valid_loss=4.510]\n","Epoch 24:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.570, train_loss_epoch=4.170, valid_loss=4.510]\n","Epoch 25:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.150, train_loss_epoch=4.200, valid_loss=4.510]\n","Epoch 26:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.260, train_loss_epoch=4.170, valid_loss=4.510]\n","Epoch 27:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.470, train_loss_epoch=4.170, valid_loss=4.510]\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.040, train_loss_epoch=4.200, valid_loss=4.510]\n","\u001b[36m(_train_tune pid=18717)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18717)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.26it/s]\u001b[A\n","Epoch 28:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=4.370, train_loss_epoch=4.200, valid_loss=3.910]\n","Epoch 29:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.270, train_loss_epoch=4.110, valid_loss=3.910]\n","Epoch 30:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.120, train_loss_epoch=3.920, valid_loss=3.910]\n","Epoch 31:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.730, train_loss_epoch=3.980, valid_loss=3.910]\n","Epoch 32:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.470, train_loss_epoch=4.070, valid_loss=3.910]\n","Epoch 33:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.430, train_loss_epoch=3.800, valid_loss=3.910]\n","Epoch 34:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.370, train_loss_epoch=3.980, valid_loss=3.910]\n","Epoch 35:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.090, train_loss_epoch=3.850, valid_loss=3.910]\n","Epoch 36:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.860, train_loss_epoch=3.940, valid_loss=3.910]\n","Epoch 37:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.600, train_loss_epoch=4.070, valid_loss=3.910]\n","Epoch 38:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.690, train_loss_epoch=3.870, valid_loss=3.910]\n","Epoch 39:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.630, train_loss_epoch=4.000, valid_loss=3.910]\n","Epoch 40:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.430, train_loss_epoch=3.830, valid_loss=3.910]\n","Epoch 41:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.640, train_loss_epoch=4.030, valid_loss=3.910]\n","Epoch 42:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.670, train_loss_epoch=3.920, valid_loss=3.910]\n","\u001b[36m(_train_tune pid=18717)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18717)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.32it/s]\u001b[A\n","Epoch 43:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.920, train_loss_epoch=3.990, valid_loss=3.750]\n","Epoch 44:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.860, train_loss_epoch=3.940, valid_loss=3.750]\n","Epoch 45:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.410, train_loss_epoch=3.660, valid_loss=3.750]\n","Epoch 46:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.990, train_loss_epoch=3.790, valid_loss=3.750]\n","Epoch 47:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.640, train_loss_epoch=3.810, valid_loss=3.750]\n","Epoch 48:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.640, train_loss_epoch=3.960, valid_loss=3.750]\n","Epoch 49:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.450, train_loss_epoch=3.770, valid_loss=3.750]\n","Epoch 50:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.640, train_loss_epoch=3.890, valid_loss=3.750]\n","Epoch 51:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.440, train_loss_epoch=3.850, valid_loss=3.750]\n","Epoch 52:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.080, train_loss_epoch=4.100, valid_loss=3.750]\n","Epoch 53:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.270, train_loss_epoch=3.850, valid_loss=3.750]\n","Epoch 54:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.030, train_loss_epoch=3.750, valid_loss=3.750]\n","Epoch 55:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.800, train_loss_epoch=3.740, valid_loss=3.750]\n","Epoch 56:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.340, train_loss_epoch=3.590, valid_loss=3.750]\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.480, train_loss_epoch=3.590, valid_loss=3.750]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18717)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.30it/s]\u001b[A\n","Epoch 57:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=3.670, train_loss_epoch=3.590, valid_loss=3.510]\n","Epoch 58:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.080, train_loss_epoch=3.760, valid_loss=3.510]\n","Epoch 59:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.850, train_loss_epoch=3.750, valid_loss=3.510]\n","Epoch 60:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.980, train_loss_epoch=3.530, valid_loss=3.510]\n","Epoch 61:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.730, train_loss_epoch=3.640, valid_loss=3.510]\n","Epoch 62:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.410, train_loss_epoch=3.560, valid_loss=3.510]\n","Epoch 63:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.370, train_loss_epoch=3.550, valid_loss=3.510]\n","Epoch 64:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.360, train_loss_epoch=3.640, valid_loss=3.510]\n","Epoch 65:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.250, train_loss_epoch=3.460, valid_loss=3.510]\n","Epoch 66:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.720, train_loss_epoch=3.640, valid_loss=3.510]\n","Epoch 67:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.460, train_loss_epoch=3.640, valid_loss=3.510]\n","Epoch 68:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.620, train_loss_epoch=3.460, valid_loss=3.510]\n","Epoch 69:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.660, train_loss_epoch=3.490, valid_loss=3.510]\n","Epoch 70:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.570, train_loss_epoch=3.710, valid_loss=3.510]\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.770, train_loss_epoch=3.440, valid_loss=3.510]\n","\u001b[36m(_train_tune pid=18717)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18717)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.31it/s]\u001b[A\n","Epoch 71:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=3.400, train_loss_epoch=3.440, valid_loss=3.230]\n","Epoch 72:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.740, train_loss_epoch=3.610, valid_loss=3.230]\n","Epoch 73:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.320, train_loss_epoch=3.430, valid_loss=3.230]\n","Epoch 74:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.030, train_loss_epoch=3.400, valid_loss=3.230]\n","Epoch 75:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.130, train_loss_epoch=3.360, valid_loss=3.230]\n","Epoch 76:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.570, train_loss_epoch=3.400, valid_loss=3.230]\n","Epoch 77:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.150, train_loss_epoch=3.450, valid_loss=3.230]\n","Epoch 78:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.420, train_loss_epoch=3.230, valid_loss=3.230]\n","Epoch 79:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.000, train_loss_epoch=3.510, valid_loss=3.230]\n","Epoch 80:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.170, train_loss_epoch=3.560, valid_loss=3.230]\n","Epoch 81:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.060, train_loss_epoch=3.320, valid_loss=3.230]\n","Epoch 82:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.390, train_loss_epoch=3.460, valid_loss=3.230]\n","Epoch 83:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.210, train_loss_epoch=3.560, valid_loss=3.230]\n","Epoch 84:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.290, train_loss_epoch=3.410, valid_loss=3.230]\n","Epoch 85:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.910, train_loss_epoch=3.370, valid_loss=3.230]\n","\u001b[36m(_train_tune pid=18717)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18717)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.34it/s]\u001b[A\n","Epoch 85:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=3.530, train_loss_epoch=3.370, valid_loss=3.380]\n","Epoch 86:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.030, train_loss_epoch=3.400, valid_loss=3.380]\n","Epoch 87:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.700, train_loss_epoch=3.670, valid_loss=3.380]\n","Epoch 88:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.970, train_loss_epoch=3.380, valid_loss=3.380]\n","Epoch 89:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.700, train_loss_epoch=3.390, valid_loss=3.380]\n","Epoch 90:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.740, train_loss_epoch=3.710, valid_loss=3.380]\n","Epoch 91:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.790, train_loss_epoch=3.380, valid_loss=3.380]\n","Epoch 92:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.300, train_loss_epoch=3.310, valid_loss=3.380]\n","Epoch 93:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.280, train_loss_epoch=3.290, valid_loss=3.380]\n","Epoch 94:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.010, train_loss_epoch=3.260, valid_loss=3.380]\n","Epoch 95:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.100, train_loss_epoch=3.250, valid_loss=3.380]\n","Epoch 96:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.520, train_loss_epoch=3.260, valid_loss=3.380]\n","Epoch 97:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.920, train_loss_epoch=3.010, valid_loss=3.380]\n","Epoch 98:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.540, train_loss_epoch=3.110, valid_loss=3.380]\n","Epoch 99:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.830, train_loss_epoch=3.110, valid_loss=3.380]\n","Epoch 99: 100%|██████████| 7/7 [00:00\u003c00:00, 25.51it/s, v_num=0, train_loss_step=2.460, train_loss_epoch=3.110, valid_loss=3.380]\n","\u001b[36m(_train_tune pid=18717)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18717)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18717)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.32it/s]\u001b[A\n","Epoch 100:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.460, train_loss_epoch=3.090, valid_loss=3.120]\n","Epoch 101:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.810, train_loss_epoch=3.170, valid_loss=3.120]\n","Epoch 102:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.390, train_loss_epoch=3.800, valid_loss=3.120]\n","Epoch 103:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.880, train_loss_epoch=3.200, valid_loss=3.120]\n","Epoch 104:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.590, train_loss_epoch=3.090, valid_loss=3.120]\n","Epoch 105:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.970, train_loss_epoch=3.120, valid_loss=3.120]\n","Epoch 106:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.570, train_loss_epoch=3.210, valid_loss=3.120]\n","Epoch 107:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.270, train_loss_epoch=2.990, valid_loss=3.120]\n","Epoch 108:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.610, train_loss_epoch=3.040, valid_loss=3.120]\n","Epoch 109:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.290, train_loss_epoch=3.010, valid_loss=3.120]\n","Epoch 110:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.310, train_loss_epoch=3.110, valid_loss=3.120]\n","Epoch 111:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.840, train_loss_epoch=2.910, valid_loss=3.120]\n","Epoch 112:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.750, train_loss_epoch=3.150, valid_loss=3.120]\n","Epoch 113:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.430, train_loss_epoch=3.090, valid_loss=3.120]\n","Epoch 114:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.910, train_loss_epoch=3.200, valid_loss=3.120]\n","\u001b[36m(_train_tune pid=18717)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18717)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.34it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18717)\u001b[0m \n","Epoch 114:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=3.120, train_loss_epoch=3.200, valid_loss=2.960]\n","Epoch 115:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.610, train_loss_epoch=3.320, valid_loss=2.960]\n","Epoch 116:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.780, train_loss_epoch=3.410, valid_loss=2.960]\n","Epoch 117:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.670, train_loss_epoch=3.080, valid_loss=2.960]\n","Epoch 118:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.600, train_loss_epoch=3.150, valid_loss=2.960]\n","Epoch 119:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.870, train_loss_epoch=2.840, valid_loss=2.960]\n","Epoch 120:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.240, train_loss_epoch=2.920, valid_loss=2.960]\n","Epoch 121:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.730, train_loss_epoch=3.130, valid_loss=2.960]\n","Epoch 122:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.010, train_loss_epoch=3.120, valid_loss=2.960]\n","Epoch 123:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.230, train_loss_epoch=2.900, valid_loss=2.960]\n","Epoch 124:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.690, train_loss_epoch=2.820, valid_loss=2.960]\n","Epoch 125:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.330, train_loss_epoch=2.910, valid_loss=2.960]\n","Epoch 126:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.800, train_loss_epoch=2.940, valid_loss=2.960]\n","Epoch 127:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.320, train_loss_epoch=2.960, valid_loss=2.960]\n","Epoch 128:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.080, train_loss_epoch=3.230, valid_loss=2.960]\n","\u001b[36m(_train_tune pid=18717)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18717)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.29it/s]\u001b[A\n","Epoch 128:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=2.620, train_loss_epoch=3.230, valid_loss=2.730]\n","Epoch 129:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.060, train_loss_epoch=2.750, valid_loss=2.730]\n","Epoch 129: 100%|██████████| 7/7 [00:00\u003c00:00, 25.01it/s, v_num=0, train_loss_step=2.710, train_loss_epoch=2.800, valid_loss=2.730]\n","Epoch 130:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.710, train_loss_epoch=2.800, valid_loss=2.730]\n","Epoch 131:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.240, train_loss_epoch=2.840, valid_loss=2.730]\n","Epoch 132:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.940, train_loss_epoch=2.990, valid_loss=2.730]\n","Epoch 133:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.810, train_loss_epoch=2.800, valid_loss=2.730]\n","Epoch 134:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.640, train_loss_epoch=2.680, valid_loss=2.730]\n","Epoch 135:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.300, train_loss_epoch=3.010, valid_loss=2.730]\n","Epoch 136:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.750, train_loss_epoch=2.650, valid_loss=2.730]\n","Epoch 137:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.470, train_loss_epoch=2.750, valid_loss=2.730]\n","Epoch 138:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.910, train_loss_epoch=2.830, valid_loss=2.730]\n","Epoch 139:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.840, train_loss_epoch=2.650, valid_loss=2.730]\n","Epoch 140:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.870, train_loss_epoch=2.790, valid_loss=2.730]\n","Epoch 141:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.520, train_loss_epoch=2.870, valid_loss=2.730]\n","Epoch 142:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.940, train_loss_epoch=2.680, valid_loss=2.730]\n","\u001b[36m(_train_tune pid=18717)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18717)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.30it/s]\u001b[A\n","Epoch 143:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.500, train_loss_epoch=3.180, valid_loss=2.660]\n","Epoch 144:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.610, train_loss_epoch=2.970, valid_loss=2.660]\n","Epoch 145:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.690, train_loss_epoch=3.010, valid_loss=2.660]\n","Epoch 146:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.840, train_loss_epoch=2.970, valid_loss=2.660]\n","Epoch 147:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.970, train_loss_epoch=2.860, valid_loss=2.660]\n","Epoch 148:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.850, train_loss_epoch=2.660, valid_loss=2.660]\n","Epoch 149:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.560, train_loss_epoch=2.910, valid_loss=2.660]\n","Epoch 150:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.460, train_loss_epoch=2.740, valid_loss=2.660]\n","Epoch 151:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.680, train_loss_epoch=2.680, valid_loss=2.660]\n","Epoch 152:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.910, train_loss_epoch=2.880, valid_loss=2.660]\n","Epoch 153:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.470, train_loss_epoch=2.990, valid_loss=2.660]\n","Epoch 154:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.530, train_loss_epoch=2.860, valid_loss=2.660]\n","Epoch 155:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.190, train_loss_epoch=2.660, valid_loss=2.660]\n","Epoch 156:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.950, train_loss_epoch=2.710, valid_loss=2.660]\n","Epoch 157:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.760, train_loss_epoch=2.570, valid_loss=2.660]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18717)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18717)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.35it/s]\u001b[A\n","Epoch 157:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=2.630, train_loss_epoch=2.570, valid_loss=2.550]\n","Epoch 158:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.720, train_loss_epoch=2.700, valid_loss=2.550]\n","Epoch 159:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.160, train_loss_epoch=2.620, valid_loss=2.550]\n","Epoch 160:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.030, train_loss_epoch=2.570, valid_loss=2.550]\n","Epoch 161:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.040, train_loss_epoch=2.790, valid_loss=2.550]\n","Epoch 162:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.650, train_loss_epoch=2.760, valid_loss=2.550]\n","Epoch 163:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.780, train_loss_epoch=2.570, valid_loss=2.550]\n","Epoch 164:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.390, train_loss_epoch=2.880, valid_loss=2.550]\n","Epoch 165:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.510, train_loss_epoch=2.690, valid_loss=2.550]\n","Epoch 166:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.530, train_loss_epoch=2.950, valid_loss=2.550]\n","Epoch 167:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.850, train_loss_epoch=2.600, valid_loss=2.550]\n","Epoch 168:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.510, train_loss_epoch=2.770, valid_loss=2.550]\n","Epoch 169:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.220, train_loss_epoch=2.800, valid_loss=2.550]\n","Epoch 170:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.610, train_loss_epoch=2.670, valid_loss=2.550]\n","Epoch 171:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.650, train_loss_epoch=2.610, valid_loss=2.550]\n","\u001b[36m(_train_tune pid=18717)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18717)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18717)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.30it/s]\u001b[A\n","Epoch 171:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=2.670, train_loss_epoch=2.610, valid_loss=2.480]\n","Epoch 172:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.770, train_loss_epoch=2.630, valid_loss=2.480]\n","Epoch 173:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.380, train_loss_epoch=2.740, valid_loss=2.480]\n","Epoch 174:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.140, train_loss_epoch=2.710, valid_loss=2.480]\n","Epoch 175:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.750, train_loss_epoch=2.670, valid_loss=2.480]\n","Epoch 176:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.890, train_loss_epoch=2.560, valid_loss=2.480]\n","Epoch 177:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.870, train_loss_epoch=2.570, valid_loss=2.480]\n","Epoch 178:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.360, train_loss_epoch=2.740, valid_loss=2.480]\n","Epoch 178: 100%|██████████| 7/7 [00:00\u003c00:00, 25.75it/s, v_num=0, train_loss_step=2.310, train_loss_epoch=2.740, valid_loss=2.480]\n","Epoch 179:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.310, train_loss_epoch=2.570, valid_loss=2.480]\n","Epoch 180:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.280, train_loss_epoch=2.430, valid_loss=2.480]\n","Epoch 181:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.890, train_loss_epoch=2.640, valid_loss=2.480]\n","Epoch 182:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.140, train_loss_epoch=2.400, valid_loss=2.480]\n","Epoch 183:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.200, train_loss_epoch=2.650, valid_loss=2.480]\n","Epoch 184:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.070, train_loss_epoch=2.590, valid_loss=2.480]\n","Epoch 185:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.760, train_loss_epoch=2.520, valid_loss=2.480]\n","\u001b[36m(_train_tune pid=18717)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 19:09:03,597\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('MA(2,9)', 'MA(3,9)', 'MA(1,12)', 'MA(2,12)', 'MA(3,12)', 'MOM(9)', 'MOM(12)', 'RSI(7)', 'RSI(14)', 'EMA(3,9)', 'EMA(5,9)', 'EMA(5,12)', 'DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (1, 1, 1), 'n_pool_kernel_size': (1, 1, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=18717)\u001b[0m `Trainer.fit` stopped: `max_steps=1300.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=18717)\u001b[0m \n","\u001b[36m(_train_tune pid=18717)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.30it/s]\u001b[A\n","\u001b[36m(_train_tune pid=18717)\u001b[0m \r                                                                      \u001b[A\rEpoch 185:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=2.590, train_loss_epoch=2.520, valid_loss=2.490]\rEpoch 185:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=2.590, train_loss_epoch=2.660, valid_loss=2.490]\rEpoch 185:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=2.590, train_loss_epoch=2.660, valid_loss=2.490]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=19156)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=19156)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=19156)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=19156)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=19156)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=19156)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=19156)\u001b[0m 2024-11-12 19:09:11.545461: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=19156)\u001b[0m 2024-11-12 19:09:11.565570: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=19156)\u001b[0m 2024-11-12 19:09:11.590338: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=19156)\u001b[0m 2024-11-12 19:09:11.597931: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=19156)\u001b[0m 2024-11-12 19:09:11.615511: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=19156)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=19156)\u001b[0m 2024-11-12 19:09:12.785390: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=19156)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=19156)\u001b[0m \n","\u001b[36m(_train_tune pid=19156)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=19156)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=19156)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=19156)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=19156)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=19156)\u001b[0m 3 | blocks       | ModuleList       | 3.1 M  | train\n","\u001b[36m(_train_tune pid=19156)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=19156)\u001b[0m 3.1 M     Trainable params\n","\u001b[36m(_train_tune pid=19156)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=19156)\u001b[0m 3.1 M     Total params\n","\u001b[36m(_train_tune pid=19156)\u001b[0m 12.424    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=19156)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=19156)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/2 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.52]\n","Epoch 2:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.40]\n","Epoch 3:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.47]\n","Epoch 4:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.51]\n","Epoch 5:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.46]\n","Epoch 6:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.46]\n","Epoch 7:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.53]\n","Epoch 8:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.56]\n","Epoch 9:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.57]\n","Epoch 10:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.57]\n","Epoch 11:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.52]\n","Epoch 12:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.61]\n","Epoch 13:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.52]\n","Epoch 14:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.68]\n","Epoch 15:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.61]\n","Epoch 16:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.64]\n","Epoch 17:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.65]\n","Epoch 18:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.64]\n","Epoch 19:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.66]\n","Epoch 20:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.72]\n","Epoch 21:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.68]\n","Epoch 22:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.64]\n","Epoch 23:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.67]\n","Epoch 24:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.71]\n","Epoch 25:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.64]\n","Epoch 26:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.69]\n","Epoch 27:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.79]\n","Epoch 28:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.71]\n","Epoch 29:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.75]\n","Epoch 30:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.70]\n","Epoch 31:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.72]\n","Epoch 32:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.80]\n","Epoch 33:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.81]\n","Epoch 34:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.74]\n","Epoch 35:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.76]\n","Epoch 36:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.79]\n","Epoch 37:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.71]\n","Epoch 38:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.83]\n","Epoch 39:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.75]\n","Epoch 40:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.84]\n","Epoch 41:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.76]\n","Epoch 42:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.77]\n","Epoch 43:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.78]\n","Epoch 44:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.84]\n","Epoch 45:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.89]\n","Epoch 46:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.80]\n","Epoch 47:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.68]\n","Epoch 48:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.81]\n","Epoch 49:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.76]\n","Epoch 49: 100%|██████████| 2/2 [00:00\u003c00:00,  6.93it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.76]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=19156)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=19156)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 10.97it/s]\u001b[A\n","Epoch 50:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.80, valid_loss=-1.76]\n","Epoch 51:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.80, valid_loss=-1.76]\n","Epoch 52:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.85, valid_loss=-1.76]\n","Epoch 53:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.74, valid_loss=-1.76]\n","Epoch 54:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.82, valid_loss=-1.76]\n","Epoch 55:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.91, valid_loss=-1.76]\n","Epoch 56:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.83, valid_loss=-1.76]\n","Epoch 56: 100%|██████████| 2/2 [00:00\u003c00:00,  7.06it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.82, valid_loss=-1.76]\n","Epoch 57:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.82, valid_loss=-1.76]\n","Epoch 58:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.92, valid_loss=-1.76]\n","Epoch 59:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.86, valid_loss=-1.76]\n","Epoch 60:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.88, valid_loss=-1.76]\n","Epoch 61:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.90, valid_loss=-1.76]\n","Epoch 62:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.67, valid_loss=-1.76]\n","Epoch 63:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.77, valid_loss=-1.76]\n","Epoch 64:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.85, valid_loss=-1.76]\n","Epoch 65:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.89, valid_loss=-1.76]\n","Epoch 66:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.86, valid_loss=-1.76]\n","Epoch 66: 100%|██████████| 2/2 [00:00\u003c00:00,  7.58it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.80, valid_loss=-1.76]\n","Epoch 67:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.80, valid_loss=-1.76]\n","Epoch 68:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.85, valid_loss=-1.76]\n","Epoch 69:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.86, valid_loss=-1.76]\n","Epoch 70:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.85, valid_loss=-1.76]\n","Epoch 71:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.91, valid_loss=-1.76]\n","Epoch 71: 100%|██████████| 2/2 [00:00\u003c00:00,  6.93it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.87, valid_loss=-1.76]\n","Epoch 72:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.87, valid_loss=-1.76]\n","Epoch 73:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.78, valid_loss=-1.76]\n","Epoch 74:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.89, valid_loss=-1.76]\n","Epoch 75:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.90, valid_loss=-1.76]\n","Epoch 76:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.89, valid_loss=-1.76]\n","Epoch 77:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.85, valid_loss=-1.76]\n","Epoch 78:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.95, valid_loss=-1.76]\n","Epoch 79:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.88, valid_loss=-1.76]\n","Epoch 80:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.86, valid_loss=-1.76]\n","Epoch 81:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.93, valid_loss=-1.76]\n","Epoch 82:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.90, valid_loss=-1.76]\n","Epoch 83:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.85, valid_loss=-1.76]\n","Epoch 84:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.89, valid_loss=-1.76]\n","Epoch 85:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.92, valid_loss=-1.76]\n","Epoch 86:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.92, valid_loss=-1.76]\n","Epoch 87:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.91, valid_loss=-1.76]\n","Epoch 88:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.93, valid_loss=-1.76]\n","Epoch 89:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.88, valid_loss=-1.76]\n","Epoch 90:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.94, valid_loss=-1.76]\n","Epoch 91:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.94, valid_loss=-1.76]\n","Epoch 92:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.86, valid_loss=-1.76]\n","Epoch 93:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.86, valid_loss=-1.76]\n","Epoch 94:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.91, valid_loss=-1.76]\n","Epoch 95:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.90, valid_loss=-1.76]\n","Epoch 96:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.92, valid_loss=-1.76]\n","Epoch 97:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.87, valid_loss=-1.76]\n","Epoch 98:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.97, valid_loss=-1.76]\n","Epoch 99:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.92, valid_loss=-1.76]\n","Epoch 99: 100%|██████████| 2/2 [00:00\u003c00:00,  7.50it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.92, valid_loss=-1.76]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=19156)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=19156)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 11.17it/s]\n","\u001b[36m(_train_tune pid=19156)\u001b[0m \u001b[A\n","\u001b[36m(_train_tune pid=19156)\u001b[0m \n","Epoch 100:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.01, valid_loss=-1.82]\n","Epoch 101:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.96, valid_loss=-1.82]\n","Epoch 102:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.86, valid_loss=-1.82]\n","Epoch 103:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.94, valid_loss=-1.82]\n","Epoch 104:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.87, valid_loss=-1.82]\n","Epoch 105:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.02, valid_loss=-1.82]\n","Epoch 106:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.00, valid_loss=-1.82]\n","Epoch 107:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.96, valid_loss=-1.82]\n","Epoch 108:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.98, valid_loss=-1.82]\n","Epoch 109:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.85, valid_loss=-1.82]\n","Epoch 110:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.96, valid_loss=-1.82]\n","Epoch 111:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.92, valid_loss=-1.82]\n","Epoch 112:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.92, valid_loss=-1.82]\n","Epoch 113:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.90, valid_loss=-1.82]\n","Epoch 114:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.94, valid_loss=-1.82]\n","Epoch 115:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.94, valid_loss=-1.82]\n","Epoch 116:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.95, valid_loss=-1.82]\n","Epoch 117:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.05, valid_loss=-1.82]\n","Epoch 118:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.94, valid_loss=-1.82]\n","Epoch 118: 100%|██████████| 2/2 [00:00\u003c00:00,  6.19it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.90, valid_loss=-1.82]\n","Epoch 119:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.90, valid_loss=-1.82]\n","Epoch 120:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.99, valid_loss=-1.82]\n","Epoch 121:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.95, valid_loss=-1.82]\n","Epoch 122:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.00, valid_loss=-1.82]\n","Epoch 123:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.98, valid_loss=-1.82]\n","Epoch 124:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.96, valid_loss=-1.82]\n","Epoch 125:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.96, valid_loss=-1.82]\n","Epoch 126:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.91, valid_loss=-1.82]\n","Epoch 127:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.99, valid_loss=-1.82]\n","Epoch 128:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.93, valid_loss=-1.82]\n","Epoch 129:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.85, valid_loss=-1.82]\n","Epoch 130:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.94, valid_loss=-1.82]\n","Epoch 131:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.00, valid_loss=-1.82]\n","Epoch 132:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.85, valid_loss=-1.82]\n","Epoch 133:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.03, valid_loss=-1.82]\n","Epoch 134:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.92, valid_loss=-1.82]\n","Epoch 135:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.98, valid_loss=-1.82]\n","Epoch 136:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.00, valid_loss=-1.82]\n","Epoch 137:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.93, valid_loss=-1.82]\n","Epoch 138:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.02, valid_loss=-1.82]\n","Epoch 139:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.86, valid_loss=-1.82]\n","Epoch 140:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.99, valid_loss=-1.82]\n","Epoch 141:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.95, valid_loss=-1.82]\n","Epoch 142:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.99, valid_loss=-1.82]\n","Epoch 143:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.89, valid_loss=-1.82]\n","Epoch 144:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.96, valid_loss=-1.82]\n","Epoch 145:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.97, valid_loss=-1.82]\n","Epoch 146:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.95, valid_loss=-1.82]\n","Epoch 147:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.97, valid_loss=-1.82]\n","Epoch 148:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.07, valid_loss=-1.82]\n","Epoch 149:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.94, valid_loss=-1.82]\n","Epoch 149: 100%|██████████| 2/2 [00:00\u003c00:00,  7.75it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.94, valid_loss=-1.82]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=19156)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=19156)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 11.12it/s]\u001b[A\n","Epoch 150:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.95, valid_loss=-1.85]\n","Epoch 151:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.91, valid_loss=-1.85]\n","Epoch 152:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.97, valid_loss=-1.85]\n","Epoch 153:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.93, valid_loss=-1.85]\n","Epoch 154:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.96, valid_loss=-1.85]\n","Epoch 155:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.94, valid_loss=-1.85]\n","Epoch 156:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.03, valid_loss=-1.85]\n","Epoch 157:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.01, valid_loss=-1.85]\n","Epoch 158:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.01, valid_loss=-1.85]\n","Epoch 159:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.99, valid_loss=-1.85]\n","Epoch 160:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.97, valid_loss=-1.85]\n","Epoch 161:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.87, valid_loss=-1.85]\n","Epoch 162:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.94, valid_loss=-1.85]\n","Epoch 163:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.97, valid_loss=-1.85]\n","Epoch 164:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.94, valid_loss=-1.85]\n","Epoch 165:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.91, valid_loss=-1.85]\n","Epoch 166:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.98, valid_loss=-1.85]\n","Epoch 167:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.95, valid_loss=-1.85]\n","Epoch 168:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.94, valid_loss=-1.85]\n","Epoch 169:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.04, valid_loss=-1.85]\n","Epoch 170:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.98, valid_loss=-1.85]\n","Epoch 171:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.00, valid_loss=-1.85]\n","Epoch 172:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.99, valid_loss=-1.85]\n","Epoch 173:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.92, valid_loss=-1.85]\n","Epoch 174:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.99, valid_loss=-1.85]\n","Epoch 175:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.00, valid_loss=-1.85]\n","Epoch 176:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.96, valid_loss=-1.85]\n","Epoch 177:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.95, valid_loss=-1.85]\n","Epoch 178:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.92, valid_loss=-1.85]\n","Epoch 179:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.93, valid_loss=-1.85]\n","Epoch 180:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.90, valid_loss=-1.85]\n","Epoch 181:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.99, valid_loss=-1.85]\n","Epoch 182:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.97, valid_loss=-1.85]\n","Epoch 183:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.05, valid_loss=-1.85]\n","Epoch 184:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.96, valid_loss=-1.85]\n","Epoch 185:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.92, valid_loss=-1.85]\n","Epoch 186:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.02, valid_loss=-1.85]\n","Epoch 187:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.01, valid_loss=-1.85]\n","Epoch 188:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.06, valid_loss=-1.85]\n","Epoch 189:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.00, valid_loss=-1.85]\n","Epoch 190:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.98, valid_loss=-1.85]\n","Epoch 191:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.00, valid_loss=-1.85]\n","Epoch 192:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.98, valid_loss=-1.85]\n","Epoch 193:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.96, valid_loss=-1.85]\n","Epoch 194:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.00, valid_loss=-1.85]\n","Epoch 195:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.99, valid_loss=-1.85]\n","Epoch 195: 100%|██████████| 2/2 [00:00\u003c00:00,  7.67it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.98, valid_loss=-1.85]\n","Epoch 196:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.98, valid_loss=-1.85]\n","Epoch 197:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.96, valid_loss=-1.85]\n","Epoch 198:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.03, valid_loss=-1.85]\n","Epoch 199:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.90, valid_loss=-1.85]\n","Epoch 199: 100%|██████████| 2/2 [00:00\u003c00:00,  7.60it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.90, valid_loss=-1.85]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=19156)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=19156)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 11.00it/s]\u001b[A\n","Epoch 200:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.01, valid_loss=-1.87]\n","Epoch 201:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.03, valid_loss=-1.87]\n","Epoch 202:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.94, valid_loss=-1.87]\n","Epoch 203:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.02, valid_loss=-1.87]\n","Epoch 204:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.07, valid_loss=-1.87]\n","Epoch 205:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.05, valid_loss=-1.87]\n","Epoch 206:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.08, valid_loss=-1.87]\n","Epoch 207:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.99, valid_loss=-1.87]\n","Epoch 208:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.06, valid_loss=-1.87]\n","Epoch 209:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.03, valid_loss=-1.87]\n","Epoch 210:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.99, valid_loss=-1.87]\n","Epoch 211:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.00, valid_loss=-1.87]\n","Epoch 212:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.00, valid_loss=-1.87]\n","Epoch 213:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.03, valid_loss=-1.87]\n","Epoch 214:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.03, valid_loss=-1.87]\n","Epoch 214: 100%|██████████| 2/2 [00:00\u003c00:00,  7.66it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.05, valid_loss=-1.87]\n","Epoch 215:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.05, valid_loss=-1.87]\n","Epoch 216:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.02, valid_loss=-1.87]\n","Epoch 217:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.06, valid_loss=-1.87]\n","Epoch 218:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.03, valid_loss=-1.87]\n","Epoch 219:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.00, valid_loss=-1.87]\n","Epoch 220:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.04, valid_loss=-1.87]\n","Epoch 221:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.94, valid_loss=-1.87]\n","Epoch 222:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.05, valid_loss=-1.87]\n","Epoch 223:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.10, valid_loss=-1.87]\n","Epoch 224:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.91, valid_loss=-1.87]\n","Epoch 225:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.07, valid_loss=-1.87]\n","Epoch 226:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.98, valid_loss=-1.87]\n","Epoch 227:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.09, valid_loss=-1.87]\n","Epoch 228:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.98, valid_loss=-1.87]\n","Epoch 229:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.99, valid_loss=-1.87]\n","Epoch 230:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.94, valid_loss=-1.87]\n","Epoch 231:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.09, valid_loss=-1.87]\n","Epoch 232:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.03, valid_loss=-1.87]\n","Epoch 233:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.02, valid_loss=-1.87]\n","Epoch 234:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.05, valid_loss=-1.87]\n","Epoch 234: 100%|██████████| 2/2 [00:00\u003c00:00,  7.65it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.96, valid_loss=-1.87]\n","Epoch 235:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.96, valid_loss=-1.87]\n","Epoch 236:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.00, valid_loss=-1.87]\n","Epoch 237:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.97, valid_loss=-1.87]\n","Epoch 238:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.03, valid_loss=-1.87]\n","Epoch 239:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.98, valid_loss=-1.87]\n","Epoch 240:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.98, valid_loss=-1.87]\n","Epoch 241:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.93, valid_loss=-1.87]\n","Epoch 242:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.04, valid_loss=-1.87]\n","Epoch 243:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.02, valid_loss=-1.87]\n","Epoch 244:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.99, valid_loss=-1.87]\n","Epoch 245:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.97, valid_loss=-1.87]\n","Epoch 246:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.01, valid_loss=-1.87]\n","Epoch 247:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.04, valid_loss=-1.87]\n","Epoch 248:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.01, valid_loss=-1.87]\n","Epoch 249:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.03, valid_loss=-1.87]\n","Epoch 249: 100%|██████████| 2/2 [00:00\u003c00:00,  7.78it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.03, valid_loss=-1.87]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=19156)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=19156)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 10.83it/s]\u001b[A\n","Epoch 250:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.07, valid_loss=-1.87]\n","Epoch 251:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.06, valid_loss=-1.87]\n","Epoch 252:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.94, valid_loss=-1.87]\n","Epoch 253:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.97, valid_loss=-1.87]\n","Epoch 254:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.02, valid_loss=-1.87]\n","Epoch 255:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.03, valid_loss=-1.87]\n","Epoch 256:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.00, valid_loss=-1.87]\n","Epoch 257:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.03, valid_loss=-1.87]\n","Epoch 258:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.01, valid_loss=-1.87]\n","Epoch 259:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.88, valid_loss=-1.87]\n","Epoch 260:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.07, valid_loss=-1.87]\n","Epoch 261:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.04, valid_loss=-1.87]\n","Epoch 262:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.08, valid_loss=-1.87]\n","Epoch 263:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.04, valid_loss=-1.87]\n","Epoch 264:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.01, valid_loss=-1.87]\n","Epoch 265:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.04, valid_loss=-1.87]\n","Epoch 266:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.99, valid_loss=-1.87]\n","Epoch 267:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.05, valid_loss=-1.87]\n","Epoch 268:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.96, valid_loss=-1.87]\n","Epoch 269:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.02, valid_loss=-1.87]\n","Epoch 270:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.03, valid_loss=-1.87]\n","Epoch 271:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.06, valid_loss=-1.87]\n","Epoch 272:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.96, valid_loss=-1.87]\n","Epoch 273:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.96, valid_loss=-1.87]\n","Epoch 274:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.03, valid_loss=-1.87]\n","Epoch 275:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.99, valid_loss=-1.87]\n","Epoch 276:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.00, valid_loss=-1.87]\n","Epoch 277:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.99, valid_loss=-1.87]\n","Epoch 278:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.05, valid_loss=-1.87]\n","Epoch 279:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.04, valid_loss=-1.87]\n","Epoch 280:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.98, valid_loss=-1.87]\n","Epoch 281:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.08, valid_loss=-1.87]\n","Epoch 282:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.03, valid_loss=-1.87]\n","Epoch 283:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.97, valid_loss=-1.87]\n","Epoch 284:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.98, valid_loss=-1.87]\n","Epoch 285:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.97, valid_loss=-1.87]\n","Epoch 286:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.00, valid_loss=-1.87]\n","Epoch 287:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.05, valid_loss=-1.87]\n","Epoch 288:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.01, valid_loss=-1.87]\n","Epoch 289:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.97, valid_loss=-1.87]\n","Epoch 290:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.02, valid_loss=-1.87]\n","Epoch 291:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.07, valid_loss=-1.87]\n","Epoch 292:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.06, valid_loss=-1.87]\n","Epoch 293:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.96, valid_loss=-1.87]\n","Epoch 294:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.03, valid_loss=-1.87]\n","Epoch 295:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.95, valid_loss=-1.87]\n","Epoch 296:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.99, valid_loss=-1.87]\n","Epoch 297:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.07, valid_loss=-1.87]\n","Epoch 298:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.08, valid_loss=-1.87]\n","Epoch 299:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.03, valid_loss=-1.87]\n","Epoch 299: 100%|██████████| 2/2 [00:00\u003c00:00,  7.83it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.03, valid_loss=-1.87]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 19:10:40,519\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('MA(2,9)', 'MA(3,9)', 'MA(1,12)', 'MA(2,12)', 'MA(3,12)', 'MOM(9)', 'MOM(12)', 'RSI(7)', 'RSI(14)', 'EMA(3,9)', 'EMA(5,9)', 'EMA(5,12)', 'DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (180, 60, 1), 'n_pool_kernel_size': (4, 4, 4), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=19156)\u001b[0m \n","\u001b[36m(_train_tune pid=19156)\u001b[0m \rValidation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=19156)\u001b[0m \rValidation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=19156)\u001b[0m `Trainer.fit` stopped: `max_steps=600.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=19156)\u001b[0m \n","\u001b[36m(_train_tune pid=19156)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 10.84it/s]\u001b[A\n","\u001b[36m(_train_tune pid=19156)\u001b[0m \r                                                                      \u001b[A\rEpoch 299: 100%|██████████| 2/2 [00:00\u003c00:00,  3.33it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.03, valid_loss=-1.86]\rEpoch 299: 100%|██████████| 2/2 [00:00\u003c00:00,  3.31it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.98, valid_loss=-1.86]\rEpoch 299: 100%|██████████| 2/2 [00:00\u003c00:00,  3.31it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.98, valid_loss=-1.86]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=19619)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=19619)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=19619)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=19619)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=19619)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=19619)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=19619)\u001b[0m 2024-11-12 19:10:48.469062: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=19619)\u001b[0m 2024-11-12 19:10:48.488939: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=19619)\u001b[0m 2024-11-12 19:10:48.513662: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=19619)\u001b[0m 2024-11-12 19:10:48.521339: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=19619)\u001b[0m 2024-11-12 19:10:48.538827: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=19619)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=19619)\u001b[0m 2024-11-12 19:10:49.703598: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=19619)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=19619)\u001b[0m \n","\u001b[36m(_train_tune pid=19619)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=19619)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=19619)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=19619)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=19619)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=19619)\u001b[0m 3 | blocks       | ModuleList       | 3.0 M  | train\n","\u001b[36m(_train_tune pid=19619)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=19619)\u001b[0m 3.0 M     Trainable params\n","\u001b[36m(_train_tune pid=19619)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=19619)\u001b[0m 3.0 M     Total params\n","\u001b[36m(_train_tune pid=19619)\u001b[0m 12.178    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=19619)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=19619)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 19:10:51,903\tERROR tune_controller.py:1331 -- Trial task failed for trial _train_tune_65a60f7c\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n","    result = ray.get(future)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n","    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(OutOfMemoryError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=19619, ip=172.28.0.12, actor_id=2c5cad7074d17894840d53e501000000, repr=_train_tune)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n","    raise skipped from exception_cause(skipped)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n","    self._ret = self._target(*self._args, **self._kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in \u003clambda\u003e\n","    training_func=lambda: self._trainable_func(self.config),\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n","    output = fn()\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/util.py\", line 130, in inner\n","    return trainable(config, **fn_kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_auto.py\", line 214, in _train_tune\n","    _ = self._fit_model(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_auto.py\", line 362, in _fit_model\n","    model = model.fit(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_windows.py\", line 661, in fit\n","    return self._fit(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_model.py\", line 356, in _fit\n","    trainer.fit(model, datamodule=datamodule)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 538, in fit\n","    call._call_and_handle_interrupt(\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 47, in _call_and_handle_interrupt\n","    return trainer_fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 574, in _fit_impl\n","    self._run(model, ckpt_path=ckpt_path)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 981, in _run\n","    results = self._run_stage()\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1023, in _run_stage\n","    self._run_sanity_check()\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1052, in _run_sanity_check\n","    val_loop.run()\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py\", line 178, in _decorator\n","    return loop_run(self, *args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 135, in run\n","    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 396, in _evaluation_step\n","    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 319, in _call_strategy_hook\n","    output = fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py\", line 411, in validation_step\n","    return self.lightning_module.validation_step(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_windows.py\", line 534, in validation_step\n","    valid_loss_batch = self._compute_valid_loss(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_windows.py\", line 461, in _compute_valid_loss\n","    _, sample_mean, quants = self.loss.sample(distr_args=distr_args)\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/losses/pytorch.py\", line 2002, in sample\n","    samples = distr.sample(sample_shape=(num_samples,))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributions/distribution.py\", line 167, in sample\n","    return self.rsample(sample_shape)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributions/studentT.py\", line 90, in rsample\n","    Y = X * torch.rsqrt(Z / self.df)\n","torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.60 GiB. GPU 0 has a total capacity of 39.56 GiB of which 8.51 GiB is free. Process 27231 has 512.00 MiB memory in use. Process 362297 has 30.54 GiB memory in use. Of the allocated memory 29.92 GiB is allocated by PyTorch, and 137.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial _train_tune_65a60f7c errored after 0 iterations at 2024-11-12 19:10:51. Total running time: 16min 12s\n","Error file: /tmp/ray/session_2024-11-12_18-17-38_352630_2326/artifacts/2024-11-12_18-54-39/_train_tune_2024-11-12_18-54-39/driver_artifacts/_train_tune_65a60f7c_14_batch_size=256,h=20,hist_exog_list=MA_2_9_MA_3_9_MA_1_12_MA_2_12_MA_3_12_MOM_9_MOM_12_RSI_7_RSI_14_EMA_3_9_2024-11-12_19-09-10/error.txt\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=19723)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=19723)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=19723)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=19723)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=19723)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=19723)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=19723)\u001b[0m 2024-11-12 19:10:59.449016: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=19723)\u001b[0m 2024-11-12 19:10:59.469792: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=19723)\u001b[0m 2024-11-12 19:10:59.496492: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=19723)\u001b[0m 2024-11-12 19:10:59.504665: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=19723)\u001b[0m 2024-11-12 19:10:59.522882: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=19723)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=19723)\u001b[0m 2024-11-12 19:11:00.691107: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=19723)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=19723)\u001b[0m \n","\u001b[36m(_train_tune pid=19723)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=19723)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=19723)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=19723)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=19723)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=19723)\u001b[0m 3 | blocks       | ModuleList       | 3.4 M  | train\n","\u001b[36m(_train_tune pid=19723)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=19723)\u001b[0m 3.4 M     Trainable params\n","\u001b[36m(_train_tune pid=19723)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=19723)\u001b[0m 3.4 M     Total params\n","\u001b[36m(_train_tune pid=19723)\u001b[0m 13.469    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=19723)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=19723)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/4 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.10, train_loss_epoch=11.40]\n","Epoch 2:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.60, train_loss_epoch=6.01e+4]\n","Epoch 3:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=15.70, train_loss_epoch=15.30]\n","Epoch 4:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=29.20, train_loss_epoch=28.90]\n","Epoch 5:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=30.90, train_loss_epoch=30.40]\n","Epoch 6:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=19.90, train_loss_epoch=24.70]\n","Epoch 7:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=18.90, train_loss_epoch=19.70]\n","Epoch 8:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=20.10, train_loss_epoch=20.40]\n","Epoch 9:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=19.80, train_loss_epoch=20.40]\n","Epoch 10:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=21.30, train_loss_epoch=21.00]\n","Epoch 11:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=20.80, train_loss_epoch=20.70]\n","Epoch 12:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=26.50, train_loss_epoch=26.10]\n","Epoch 13:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=38.20, train_loss_epoch=36.00]\n","Epoch 14:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=39.00, train_loss_epoch=38.20]\n","Epoch 15:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=41.80, train_loss_epoch=7.35e+7]\n","Epoch 16:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=41.80, train_loss_epoch=1.65e+6]\n","Epoch 17:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=32.50, train_loss_epoch=36.40]\n","Epoch 18:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=31.80, train_loss_epoch=33.80]\n","Epoch 19:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=31.10, train_loss_epoch=32.70]\n","Epoch 20:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=46.00, train_loss_epoch=37.10]\n","Epoch 21:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=32.90, train_loss_epoch=34.00]\n","Epoch 22:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=17.70, train_loss_epoch=28.10]\n","Epoch 23:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.40, train_loss_epoch=12.60]\n","Epoch 24:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.80, train_loss_epoch=14.00]\n","Epoch 24: 100%|██████████| 4/4 [00:00\u003c00:00, 13.98it/s, v_num=0, train_loss_step=12.80, train_loss_epoch=14.00]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=19723)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.54it/s]\u001b[A\n","Epoch 25:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.80, train_loss_epoch=18.10, valid_loss=12.90]\n","Epoch 26:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.60, train_loss_epoch=13.20, valid_loss=12.90]\n","Epoch 27:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=14.90, train_loss_epoch=13.50, valid_loss=12.90]\n","Epoch 28:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.90, train_loss_epoch=13.30, valid_loss=12.90]\n","Epoch 29:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.50, train_loss_epoch=12.80, valid_loss=12.90]\n","Epoch 30:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.80, train_loss_epoch=12.80, valid_loss=12.90]\n","Epoch 31:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.70, train_loss_epoch=13.00, valid_loss=12.90]\n","Epoch 32:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=15.00, train_loss_epoch=13.70, valid_loss=12.90]\n","Epoch 33:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.70, train_loss_epoch=13.30, valid_loss=12.90]\n","Epoch 34:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.50, train_loss_epoch=13.40, valid_loss=12.90]\n","Epoch 35:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.20, train_loss_epoch=12.40, valid_loss=12.90]\n","Epoch 36:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.00, train_loss_epoch=12.40, valid_loss=12.90]\n","Epoch 37:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.60, train_loss_epoch=13.30, valid_loss=12.90]\n","Epoch 38:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.40, train_loss_epoch=13.70, valid_loss=12.90]\n","Epoch 39:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=14.30, train_loss_epoch=13.90, valid_loss=12.90]\n","Epoch 40:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.70, train_loss_epoch=14.60, valid_loss=12.90]\n","Epoch 41:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.40, train_loss_epoch=1.08e+3, valid_loss=12.90]\n","Epoch 42:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.50, train_loss_epoch=13.40, valid_loss=12.90]\n","Epoch 43:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.70, train_loss_epoch=13.70, valid_loss=12.90]\n","Epoch 44:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.20, train_loss_epoch=13.60, valid_loss=12.90]\n","Epoch 45:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.70, train_loss_epoch=13.30, valid_loss=12.90]\n","Epoch 46:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.90, train_loss_epoch=12.70, valid_loss=12.90]\n","Epoch 47:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.80, train_loss_epoch=12.70, valid_loss=12.90]\n","Epoch 48:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=15.30, train_loss_epoch=14.60, valid_loss=12.90]\n","Epoch 49:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.90, train_loss_epoch=14.40, valid_loss=12.90]\n","Epoch 49: 100%|██████████| 4/4 [00:00\u003c00:00, 14.00it/s, v_num=0, train_loss_step=15.10, train_loss_epoch=14.40, valid_loss=12.90]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=19723)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.86it/s]\u001b[A\n","Epoch 50:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=15.10, train_loss_epoch=15.20, valid_loss=14.10]\n","Epoch 51:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=14.80, train_loss_epoch=13.90, valid_loss=14.10]\n","Epoch 52:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.80, train_loss_epoch=12.80, valid_loss=14.10]\n","Epoch 53:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.90, train_loss_epoch=12.80, valid_loss=14.10]\n","Epoch 54:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.80, train_loss_epoch=12.30, valid_loss=14.10]\n","Epoch 55:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.90, train_loss_epoch=12.40, valid_loss=14.10]\n","Epoch 56:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.20, train_loss_epoch=11.50, valid_loss=14.10]\n","Epoch 57:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.50, train_loss_epoch=12.40, valid_loss=14.10]\n","Epoch 58:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.70, train_loss_epoch=11.90, valid_loss=14.10]\n","Epoch 59:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.10, train_loss_epoch=11.90, valid_loss=14.10]\n","Epoch 60:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.20, train_loss_epoch=11.70, valid_loss=14.10]\n","Epoch 61:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.20, train_loss_epoch=12.00, valid_loss=14.10]\n","Epoch 62:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.30, train_loss_epoch=12.20, valid_loss=14.10]\n","Epoch 63:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.80, train_loss_epoch=12.10, valid_loss=14.10]\n","Epoch 64:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.50, train_loss_epoch=11.80, valid_loss=14.10]\n","Epoch 65:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.80, train_loss_epoch=12.00, valid_loss=14.10]\n","Epoch 66:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.80, train_loss_epoch=10.80, valid_loss=14.10]\n","Epoch 67:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.40, train_loss_epoch=11.50, valid_loss=14.10]\n","Epoch 68:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.70, train_loss_epoch=11.20, valid_loss=14.10]\n","Epoch 69:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.80, train_loss_epoch=13.00, valid_loss=14.10]\n","Epoch 70:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.60, train_loss_epoch=12.10, valid_loss=14.10]\n","Epoch 70: 100%|██████████| 4/4 [00:00\u003c00:00, 14.34it/s, v_num=0, train_loss_step=12.60, train_loss_epoch=12.10, valid_loss=14.10]\n","Epoch 71:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=14.60, train_loss_epoch=13.10, valid_loss=14.10]\n","Epoch 72:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.00, train_loss_epoch=12.40, valid_loss=14.10]\n","Epoch 73:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.90, train_loss_epoch=12.30, valid_loss=14.10]\n","Epoch 74:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.30, train_loss_epoch=12.10, valid_loss=14.10]\n","Epoch 74: 100%|██████████| 4/4 [00:00\u003c00:00, 14.40it/s, v_num=0, train_loss_step=11.60, train_loss_epoch=12.10, valid_loss=14.10]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=19723)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=19723)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.50it/s]\u001b[A\n","Epoch 75:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.60, train_loss_epoch=11.70, valid_loss=11.70]\n","Epoch 76:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.70, train_loss_epoch=11.80, valid_loss=11.70]\n","Epoch 77:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.60, train_loss_epoch=11.70, valid_loss=11.70]\n","Epoch 78:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.80, train_loss_epoch=11.60, valid_loss=11.70]\n","Epoch 79:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.30, train_loss_epoch=11.50, valid_loss=11.70]\n","Epoch 80:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.60, train_loss_epoch=12.30, valid_loss=11.70]\n","Epoch 81:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.70, train_loss_epoch=12.30, valid_loss=11.70]\n","Epoch 82:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.90, train_loss_epoch=11.50, valid_loss=11.70]\n","Epoch 83:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.00, train_loss_epoch=12.10, valid_loss=11.70]\n","Epoch 84:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.30, train_loss_epoch=12.30, valid_loss=11.70]\n","Epoch 85:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.80, train_loss_epoch=11.90, valid_loss=11.70]\n","Epoch 86:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.20, train_loss_epoch=11.70, valid_loss=11.70]\n","Epoch 87:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.10, train_loss_epoch=11.60, valid_loss=11.70]\n","Epoch 88:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.80, train_loss_epoch=11.80, valid_loss=11.70]\n","Epoch 89:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.90, train_loss_epoch=12.00, valid_loss=11.70]\n","Epoch 90:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.30, train_loss_epoch=12.00, valid_loss=11.70]\n","Epoch 91:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.00, train_loss_epoch=12.80, valid_loss=11.70]\n","Epoch 92:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.10, train_loss_epoch=11.50, valid_loss=11.70]\n","Epoch 93:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.70, train_loss_epoch=11.60, valid_loss=11.70]\n","Epoch 94:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.20, train_loss_epoch=11.50, valid_loss=11.70]\n","Epoch 95:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.00, train_loss_epoch=11.30, valid_loss=11.70]\n","Epoch 96:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.20, train_loss_epoch=11.10, valid_loss=11.70]\n","Epoch 97:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.10, train_loss_epoch=11.10, valid_loss=11.70]\n","Epoch 98:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.20, train_loss_epoch=11.10, valid_loss=11.70]\n","Epoch 99:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.90, train_loss_epoch=11.40, valid_loss=11.70]\n","Epoch 99: 100%|██████████| 4/4 [00:00\u003c00:00, 13.76it/s, v_num=0, train_loss_step=10.90, train_loss_epoch=11.40, valid_loss=11.70]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=19723)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=19723)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.29it/s]\u001b[A\n","Epoch 100:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.90, train_loss_epoch=11.10, valid_loss=11.00]\n","Epoch 101:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.50, train_loss_epoch=11.00, valid_loss=11.00]\n","Epoch 102:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.30, train_loss_epoch=10.80, valid_loss=11.00]\n","Epoch 103:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.50, train_loss_epoch=10.70, valid_loss=11.00]\n","Epoch 104:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.30, train_loss_epoch=10.90, valid_loss=11.00]\n","Epoch 105:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.70, train_loss_epoch=10.40, valid_loss=11.00]\n","Epoch 106:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.00, train_loss_epoch=11.40, valid_loss=11.00]\n","Epoch 107:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.90, train_loss_epoch=11.10, valid_loss=11.00]\n","Epoch 108:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.30, train_loss_epoch=11.60, valid_loss=11.00]\n","Epoch 109:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.00, train_loss_epoch=11.40, valid_loss=11.00]\n","Epoch 110:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.40, train_loss_epoch=11.40, valid_loss=11.00]\n","Epoch 111:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.50, train_loss_epoch=11.00, valid_loss=11.00]\n","Epoch 112:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.80, train_loss_epoch=13.70, valid_loss=11.00]\n","Epoch 113:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.10, train_loss_epoch=11.60, valid_loss=11.00]\n","Epoch 114:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.80, train_loss_epoch=11.20, valid_loss=11.00]\n","Epoch 115:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.30, train_loss_epoch=10.90, valid_loss=11.00]\n","Epoch 116:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.40, train_loss_epoch=10.70, valid_loss=11.00]\n","Epoch 117:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.940, train_loss_epoch=10.70, valid_loss=11.00]\n","Epoch 118:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.70, train_loss_epoch=11.60, valid_loss=11.00]\n","Epoch 119:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.60, train_loss_epoch=11.20, valid_loss=11.00]\n","Epoch 120:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.80, train_loss_epoch=11.40, valid_loss=11.00]\n","Epoch 121:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.10, train_loss_epoch=11.00, valid_loss=11.00]\n","Epoch 122:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.40, train_loss_epoch=12.50, valid_loss=11.00]\n","Epoch 123:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.80, train_loss_epoch=11.20, valid_loss=11.00]\n","Epoch 124:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.40, train_loss_epoch=11.20, valid_loss=11.00]\n","Epoch 124: 100%|██████████| 4/4 [00:00\u003c00:00, 13.96it/s, v_num=0, train_loss_step=10.70, train_loss_epoch=11.20, valid_loss=11.00]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=19723)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=19723)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.59it/s]\u001b[A\n","Epoch 125:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.70, train_loss_epoch=11.00, valid_loss=11.30]\n","Epoch 126:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.70, train_loss_epoch=11.50, valid_loss=11.30]\n","Epoch 127:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.60, train_loss_epoch=12.10, valid_loss=11.30]\n","Epoch 128:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.30, train_loss_epoch=11.70, valid_loss=11.30]\n","Epoch 129:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.70, train_loss_epoch=11.50, valid_loss=11.30]\n","Epoch 130:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.30, train_loss_epoch=11.50, valid_loss=11.30]\n","Epoch 131:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.80, train_loss_epoch=11.30, valid_loss=11.30]\n","Epoch 132:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.20, train_loss_epoch=11.70, valid_loss=11.30]\n","Epoch 133:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.40, train_loss_epoch=11.80, valid_loss=11.30]\n","Epoch 134:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.50, train_loss_epoch=11.70, valid_loss=11.30]\n","Epoch 135:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.20, train_loss_epoch=11.50, valid_loss=11.30]\n","Epoch 136:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.80, train_loss_epoch=11.30, valid_loss=11.30]\n","Epoch 137:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.90, train_loss_epoch=11.90, valid_loss=11.30]\n","Epoch 138:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.50, train_loss_epoch=11.20, valid_loss=11.30]\n","Epoch 139:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.80, train_loss_epoch=11.10, valid_loss=11.30]\n","Epoch 140:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.80, train_loss_epoch=10.90, valid_loss=11.30]\n","Epoch 141:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.980, train_loss_epoch=10.80, valid_loss=11.30]\n","Epoch 142:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.60, train_loss_epoch=10.90, valid_loss=11.30]\n","Epoch 143:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.60, train_loss_epoch=11.00, valid_loss=11.30]\n","Epoch 144:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.00, train_loss_epoch=10.80, valid_loss=11.30]\n","Epoch 145:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.00, train_loss_epoch=11.20, valid_loss=11.30]\n","Epoch 146:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.80, train_loss_epoch=10.80, valid_loss=11.30]\n","Epoch 147:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.90, train_loss_epoch=11.10, valid_loss=11.30]\n","Epoch 148:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.80, train_loss_epoch=11.20, valid_loss=11.30]\n","Epoch 149:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.90, train_loss_epoch=11.60, valid_loss=11.30]\n","Epoch 149: 100%|██████████| 4/4 [00:00\u003c00:00, 13.76it/s, v_num=0, train_loss_step=11.40, train_loss_epoch=11.60, valid_loss=11.30]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=19723)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=19723)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.82it/s]\u001b[A\n","Epoch 150:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.40, train_loss_epoch=11.40, valid_loss=11.20]\n","Epoch 151:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.20, train_loss_epoch=11.60, valid_loss=11.20]\n","Epoch 152:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.90, train_loss_epoch=11.20, valid_loss=11.20]\n","Epoch 153:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.70, train_loss_epoch=11.10, valid_loss=11.20]\n","Epoch 154:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.00, train_loss_epoch=11.20, valid_loss=11.20]\n","Epoch 155:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.70, train_loss_epoch=11.00, valid_loss=11.20]\n","Epoch 156:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.20, train_loss_epoch=10.70, valid_loss=11.20]\n","Epoch 157:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.50, train_loss_epoch=10.70, valid_loss=11.20]\n","Epoch 158:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.00, train_loss_epoch=11.10, valid_loss=11.20]\n","Epoch 159:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.10, train_loss_epoch=10.70, valid_loss=11.20]\n","Epoch 160:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.30, train_loss_epoch=10.60, valid_loss=11.20]\n","Epoch 161:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.910, train_loss_epoch=10.40, valid_loss=11.20]\n","Epoch 162:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.90, train_loss_epoch=10.60, valid_loss=11.20]\n","Epoch 163:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.920, train_loss_epoch=10.30, valid_loss=11.20]\n","Epoch 164:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.80, train_loss_epoch=11.10, valid_loss=11.20]\n","Epoch 165:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.30, train_loss_epoch=10.80, valid_loss=11.20]\n","Epoch 166:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.20, train_loss_epoch=10.60, valid_loss=11.20]\n","Epoch 166: 100%|██████████| 4/4 [00:00\u003c00:00, 14.17it/s, v_num=0, train_loss_step=10.90, train_loss_epoch=10.60, valid_loss=11.20]\n","Epoch 167:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.90, train_loss_epoch=10.60, valid_loss=11.20]\n","Epoch 168:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.50, train_loss_epoch=11.10, valid_loss=11.20]\n","Epoch 169:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.30, train_loss_epoch=11.50, valid_loss=11.20]\n","Epoch 170:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.830, train_loss_epoch=10.90, valid_loss=11.20]\n","Epoch 171:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.730, train_loss_epoch=10.70, valid_loss=11.20]\n","Epoch 172:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.20, train_loss_epoch=10.60, valid_loss=11.20]\n","Epoch 173:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.40, train_loss_epoch=10.70, valid_loss=11.20]\n","Epoch 174:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.20, train_loss_epoch=11.00, valid_loss=11.20]\n","Epoch 174: 100%|██████████| 4/4 [00:00\u003c00:00, 13.71it/s, v_num=0, train_loss_step=10.90, train_loss_epoch=11.00, valid_loss=11.20]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=19723)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=19723)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.80it/s]\u001b[A\n","Epoch 175:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.90, train_loss_epoch=11.30, valid_loss=11.30]\n","Epoch 176:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.50, train_loss_epoch=11.50, valid_loss=11.30]\n","Epoch 177:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.90, train_loss_epoch=11.60, valid_loss=11.30]\n","Epoch 178:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.80, train_loss_epoch=11.60, valid_loss=11.30]\n","Epoch 179:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.40, train_loss_epoch=11.10, valid_loss=11.30]\n","Epoch 180:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.40, train_loss_epoch=11.30, valid_loss=11.30]\n","Epoch 181:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.40, train_loss_epoch=10.80, valid_loss=11.30]\n","Epoch 182:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.80, train_loss_epoch=10.70, valid_loss=11.30]\n","Epoch 183:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.40, train_loss_epoch=11.10, valid_loss=11.30]\n","Epoch 184:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.80, train_loss_epoch=11.10, valid_loss=11.30]\n","Epoch 185:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.20, train_loss_epoch=11.20, valid_loss=11.30]\n","Epoch 186:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.00, train_loss_epoch=11.10, valid_loss=11.30]\n","Epoch 187:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.40, train_loss_epoch=11.20, valid_loss=11.30]\n","Epoch 188:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.90, train_loss_epoch=10.80, valid_loss=11.30]\n","Epoch 189:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.940, train_loss_epoch=10.70, valid_loss=11.30]\n","Epoch 190:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.20, train_loss_epoch=11.30, valid_loss=11.30]\n","Epoch 191:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.80, train_loss_epoch=11.10, valid_loss=11.30]\n","Epoch 192:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.30, train_loss_epoch=11.00, valid_loss=11.30]\n","Epoch 193:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.00, train_loss_epoch=11.00, valid_loss=11.30]\n","Epoch 194:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.60, train_loss_epoch=10.70, valid_loss=11.30]\n","Epoch 195:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.40, train_loss_epoch=10.90, valid_loss=11.30]\n","Epoch 196:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.20, train_loss_epoch=10.60, valid_loss=11.30]\n","Epoch 197:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.30, train_loss_epoch=11.00, valid_loss=11.30]\n","Epoch 198:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.50, train_loss_epoch=10.90, valid_loss=11.30]\n","Epoch 199:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.00, train_loss_epoch=11.00, valid_loss=11.30]\n","Epoch 199: 100%|██████████| 4/4 [00:00\u003c00:00, 13.48it/s, v_num=0, train_loss_step=10.20, train_loss_epoch=11.00, valid_loss=11.30]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=19723)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.73it/s]\u001b[A\n","Epoch 200:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.20, train_loss_epoch=10.80, valid_loss=10.60]\n","Epoch 201:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.870, train_loss_epoch=10.50, valid_loss=10.60]\n","Epoch 202:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.930, train_loss_epoch=10.40, valid_loss=10.60]\n","Epoch 203:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.970, train_loss_epoch=10.30, valid_loss=10.60]\n","Epoch 204:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.830, train_loss_epoch=10.20, valid_loss=10.60]\n","Epoch 205:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.30, train_loss_epoch=10.20, valid_loss=10.60]\n","Epoch 206:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.680, train_loss_epoch=10.00, valid_loss=10.60]\n","Epoch 207:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.50, train_loss_epoch=10.30, valid_loss=10.60]\n","Epoch 208:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.640, train_loss_epoch=10.40, valid_loss=10.60]\n","Epoch 209:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.40, train_loss_epoch=10.50, valid_loss=10.60]\n","Epoch 210:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.20, train_loss_epoch=10.40, valid_loss=10.60]\n","Epoch 211:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.750, train_loss_epoch=9.860, valid_loss=10.60]\n","Epoch 211: 100%|██████████| 4/4 [00:00\u003c00:00, 13.93it/s, v_num=0, train_loss_step=10.40, train_loss_epoch=10.00, valid_loss=10.60]\n","Epoch 212:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.40, train_loss_epoch=10.00, valid_loss=10.60]\n","Epoch 213:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.430, train_loss_epoch=9.690, valid_loss=10.60]\n","Epoch 214:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.180, train_loss_epoch=9.720, valid_loss=10.60]\n","Epoch 215:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.750, train_loss_epoch=9.790, valid_loss=10.60]\n","Epoch 216:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.40, train_loss_epoch=10.10, valid_loss=10.60]\n","Epoch 217:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.910, train_loss_epoch=10.10, valid_loss=10.60]\n","Epoch 218:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.60, train_loss_epoch=10.20, valid_loss=10.60]\n","Epoch 219:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.960, train_loss_epoch=10.00, valid_loss=10.60]\n","Epoch 220:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.250, train_loss_epoch=9.700, valid_loss=10.60]\n","Epoch 221:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.320, train_loss_epoch=9.920, valid_loss=10.60]\n","Epoch 222:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.40, train_loss_epoch=10.60, valid_loss=10.60]\n","Epoch 223:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.20, train_loss_epoch=10.60, valid_loss=10.60]\n","Epoch 224:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.360, train_loss_epoch=9.790, valid_loss=10.60]\n","Epoch 224: 100%|██████████| 4/4 [00:00\u003c00:00, 14.02it/s, v_num=0, train_loss_step=9.610, train_loss_epoch=9.790, valid_loss=10.60]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=19723)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.85it/s]\u001b[A\n","Epoch 225:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.610, train_loss_epoch=10.10, valid_loss=9.920]\n","Epoch 226:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.990, train_loss_epoch=10.10, valid_loss=9.920]\n","Epoch 227:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.280, train_loss_epoch=10.10, valid_loss=9.920]\n","Epoch 228:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.790, train_loss_epoch=10.10, valid_loss=9.920]\n","Epoch 229:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.10, train_loss_epoch=10.20, valid_loss=9.920]\n","Epoch 230:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.10, train_loss_epoch=10.10, valid_loss=9.920]\n","Epoch 231:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.780, train_loss_epoch=9.940, valid_loss=9.920]\n","Epoch 232:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.20, train_loss_epoch=10.70, valid_loss=9.920]\n","Epoch 233:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.480, train_loss_epoch=9.730, valid_loss=9.920]\n","Epoch 234:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.490, train_loss_epoch=9.570, valid_loss=9.920]\n","Epoch 235:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.160, train_loss_epoch=9.610, valid_loss=9.920]\n","Epoch 236:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.40, train_loss_epoch=10.10, valid_loss=9.920]\n","Epoch 237:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.10, train_loss_epoch=9.840, valid_loss=9.920]\n","Epoch 238:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.920, train_loss_epoch=9.750, valid_loss=9.920]\n","Epoch 239:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.320, train_loss_epoch=9.520, valid_loss=9.920]\n","Epoch 240:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.020, train_loss_epoch=9.300, valid_loss=9.920]\n","Epoch 241:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.40, train_loss_epoch=9.850, valid_loss=9.920]\n","Epoch 242:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.190, train_loss_epoch=9.530, valid_loss=9.920]\n","Epoch 243:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.770, train_loss_epoch=9.780, valid_loss=9.920]\n","Epoch 244:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.940, train_loss_epoch=9.390, valid_loss=9.920]\n","Epoch 245:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.810, train_loss_epoch=9.750, valid_loss=9.920]\n","Epoch 246:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.30, train_loss_epoch=10.10, valid_loss=9.920]\n","Epoch 247:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.800, train_loss_epoch=9.540, valid_loss=9.920]\n","Epoch 248:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.390, train_loss_epoch=9.140, valid_loss=9.920]\n","Epoch 249:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.850, train_loss_epoch=9.480, valid_loss=9.920]\n","Epoch 249: 100%|██████████| 4/4 [00:00\u003c00:00, 13.98it/s, v_num=0, train_loss_step=10.70, train_loss_epoch=9.480, valid_loss=9.920]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=19723)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=19723)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.64it/s]\u001b[A\n","Epoch 250:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.70, train_loss_epoch=9.620, valid_loss=9.550]\n","Epoch 251:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.00, train_loss_epoch=9.860, valid_loss=9.550]\n","Epoch 252:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.10, train_loss_epoch=10.30, valid_loss=9.550]\n","Epoch 253:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.660, train_loss_epoch=10.10, valid_loss=9.550]\n","Epoch 254:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.600, train_loss_epoch=10.00, valid_loss=9.550]\n","Epoch 255:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.730, train_loss_epoch=9.950, valid_loss=9.550]\n","Epoch 256:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.50, train_loss_epoch=10.60, valid_loss=9.550]\n","Epoch 257:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.80, train_loss_epoch=10.50, valid_loss=9.550]\n","Epoch 258:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.720, train_loss_epoch=10.20, valid_loss=9.550]\n","Epoch 259:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.790, train_loss_epoch=10.20, valid_loss=9.550]\n","Epoch 260:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.590, train_loss_epoch=10.00, valid_loss=9.550]\n","Epoch 261:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.00, train_loss_epoch=10.10, valid_loss=9.550]\n","Epoch 261: 100%|██████████| 4/4 [00:00\u003c00:00, 13.88it/s, v_num=0, train_loss_step=11.00, train_loss_epoch=10.00, valid_loss=9.550]\n","Epoch 262:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.00, train_loss_epoch=10.00, valid_loss=9.550]\n","Epoch 263:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.90, train_loss_epoch=10.70, valid_loss=9.550]\n","Epoch 264:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.770, train_loss_epoch=10.40, valid_loss=9.550]\n","Epoch 265:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.70, train_loss_epoch=10.70, valid_loss=9.550]\n","Epoch 266:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.590, train_loss_epoch=10.30, valid_loss=9.550]\n","Epoch 267:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.990, train_loss_epoch=10.40, valid_loss=9.550]\n","Epoch 268:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.850, train_loss_epoch=10.30, valid_loss=9.550]\n","Epoch 269:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.900, train_loss_epoch=10.20, valid_loss=9.550]\n","Epoch 270:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.080, train_loss_epoch=9.820, valid_loss=9.550]\n","Epoch 271:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.70, train_loss_epoch=10.20, valid_loss=9.550]\n","Epoch 272:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.720, train_loss_epoch=9.790, valid_loss=9.550]\n","Epoch 273:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.910, train_loss_epoch=9.720, valid_loss=9.550]\n","Epoch 274:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.00, train_loss_epoch=9.670, valid_loss=9.550]\n","Epoch 274: 100%|██████████| 4/4 [00:00\u003c00:00, 13.61it/s, v_num=0, train_loss_step=9.820, train_loss_epoch=9.670, valid_loss=9.550]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=19723)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 19:12:26,650\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('MA(2,9)', 'MA(3,9)', 'MA(1,12)', 'MA(2,12)', 'MA(3,12)', 'MOM(9)', 'MOM(12)', 'RSI(7)', 'RSI(14)', 'EMA(3,9)', 'EMA(5,9)', 'EMA(5,12)', 'DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (24, 12, 1), 'n_pool_kernel_size': (2, 2, 2), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=19723)\u001b[0m `Trainer.fit` stopped: `max_steps=1100.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=19723)\u001b[0m \n","\u001b[36m(_train_tune pid=19723)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 15.29it/s]\u001b[A\n","\u001b[36m(_train_tune pid=19723)\u001b[0m \r                                                                      \u001b[A\rEpoch 274: 100%|██████████| 4/4 [00:00\u003c00:00,  6.09it/s, v_num=0, train_loss_step=9.820, train_loss_epoch=9.670, valid_loss=9.150]\rEpoch 274: 100%|██████████| 4/4 [00:00\u003c00:00,  6.08it/s, v_num=0, train_loss_step=9.820, train_loss_epoch=9.540, valid_loss=9.150]\rEpoch 274: 100%|██████████| 4/4 [00:00\u003c00:00,  6.07it/s, v_num=0, train_loss_step=9.820, train_loss_epoch=9.540, valid_loss=9.150]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=20179)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=20179)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=20179)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=20179)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=20179)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=20179)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=20179)\u001b[0m 2024-11-12 19:12:34.467402: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=20179)\u001b[0m 2024-11-12 19:12:34.488213: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=20179)\u001b[0m 2024-11-12 19:12:34.515805: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=20179)\u001b[0m 2024-11-12 19:12:34.524028: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=20179)\u001b[0m 2024-11-12 19:12:34.542782: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=20179)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=20179)\u001b[0m 2024-11-12 19:12:35.812437: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=20179)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=20179)\u001b[0m \n","\u001b[36m(_train_tune pid=20179)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=20179)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=20179)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=20179)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=20179)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=20179)\u001b[0m 3 | blocks       | ModuleList       | 3.0 M  | train\n","\u001b[36m(_train_tune pid=20179)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=20179)\u001b[0m 3.0 M     Trainable params\n","\u001b[36m(_train_tune pid=20179)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=20179)\u001b[0m 3.0 M     Total params\n","\u001b[36m(_train_tune pid=20179)\u001b[0m 12.055    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=20179)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=20179)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/13 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.31]\n","Epoch 2:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-1.57]\n","Epoch 3:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.59]\n","Epoch 4:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.61]\n","Epoch 5:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.67]\n","Epoch 6:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.67]\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.68]\n","\u001b[36m(_train_tune pid=20179)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=20179)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 42.18it/s]\u001b[A\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.68, valid_loss=-1.64]\n","Epoch 8:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.65, valid_loss=-1.64]\n","Epoch 9:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.65, valid_loss=-1.64]\n","Epoch 10:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.70, valid_loss=-1.64]\n","Epoch 11:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-1.76, valid_loss=-1.64]\n","Epoch 12:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.70, valid_loss=-1.64]\n","Epoch 13:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.76, valid_loss=-1.64]\n","Epoch 14:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.72, valid_loss=-1.64]\n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.73, valid_loss=-1.64]\n","\u001b[36m(_train_tune pid=20179)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=20179)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 44.03it/s]\u001b[A\n","\u001b[36m(_train_tune pid=20179)\u001b[0m \n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.73, valid_loss=-1.66]\n","Epoch 16:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.77, valid_loss=-1.66]\n","Epoch 17:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-1.81, valid_loss=-1.66]\n","Epoch 18:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.80, valid_loss=-1.66]\n","Epoch 19:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.74, valid_loss=-1.66]\n","Epoch 20:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-1.83, valid_loss=-1.66]\n","Epoch 21:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.81, valid_loss=-1.66]\n","Epoch 22:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.82, valid_loss=-1.66]\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.79, valid_loss=-1.66]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=20179)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 48.06it/s]\u001b[A\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.79, valid_loss=-1.66]\n","Epoch 24:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-1.87, valid_loss=-1.66]\n","Epoch 25:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-1.85, valid_loss=-1.66]\n","Epoch 26:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.84, valid_loss=-1.66]\n","Epoch 27:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.82, valid_loss=-1.66]\n","Epoch 28:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.829, train_loss_epoch=-1.76, valid_loss=-1.66]\n","Epoch 29:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.85, valid_loss=-1.66]\n","Epoch 30:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-1.87, valid_loss=-1.66]\n","\u001b[36m(_train_tune pid=20179)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=20179)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 44.41it/s]\u001b[A\n","Epoch 30:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.87, valid_loss=-1.64]\n","Epoch 31:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.86, valid_loss=-1.64]\n","Epoch 32:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.88, valid_loss=-1.64]\n","Epoch 33:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.87, valid_loss=-1.64]\n","Epoch 34:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.89, valid_loss=-1.64]\n","Epoch 34: 100%|██████████| 13/13 [00:00\u003c00:00, 42.13it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.89, valid_loss=-1.64]\n","Epoch 35:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.89, valid_loss=-1.64]\n","Epoch 36:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.47, train_loss_epoch=-1.94, valid_loss=-1.64]\n","Epoch 37:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.91, valid_loss=-1.64]\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.92, valid_loss=-1.64]\n","\u001b[36m(_train_tune pid=20179)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=20179)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 46.95it/s]\u001b[A\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.92, valid_loss=-1.62]\n","Epoch 39:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-1.94, valid_loss=-1.62]\n","Epoch 40:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.89, valid_loss=-1.62]\n","Epoch 41:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-1.96, valid_loss=-1.62]\n","Epoch 42:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.95, valid_loss=-1.62]\n","Epoch 43:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.96, valid_loss=-1.62]\n","Epoch 44:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.94, valid_loss=-1.62]\n","Epoch 45:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.96, valid_loss=-1.62]\n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.01, valid_loss=-1.62]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=20179)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 47.73it/s]\u001b[A\n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.01, valid_loss=-1.61]\n","Epoch 47:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.96, valid_loss=-1.61]\n","Epoch 48:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.00, valid_loss=-1.61]\n","Epoch 49:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.99, valid_loss=-1.61]\n","Epoch 50:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.01, valid_loss=-1.61]\n","Epoch 51:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.00, valid_loss=-1.61]\n","Epoch 52:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.51, train_loss_epoch=-2.05, valid_loss=-1.61]\n","Epoch 53:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-2.01, valid_loss=-1.61]\n","\u001b[36m(_train_tune pid=20179)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 19:12:57,559\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('MA(2,9)', 'MA(3,9)', 'MA(1,12)', 'MA(2,12)', 'MA(3,12)', 'MOM(9)', 'MOM(12)', 'RSI(7)', 'RSI(14)', 'EMA(3,9)', 'EMA(5,9)', 'EMA(5,12)', 'DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (168, 24, 1), 'n_pool_kernel_size': (1, 1, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=20179)\u001b[0m `Trainer.fit` stopped: `max_steps=700.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=20179)\u001b[0m \n","\u001b[36m(_train_tune pid=20179)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 45.14it/s]\u001b[A\n","\u001b[36m(_train_tune pid=20179)\u001b[0m \r                                                                        \u001b[A\rEpoch 53:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.01, valid_loss=-1.60]\rEpoch 53:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.00, valid_loss=-1.60]\rEpoch 53:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.00, valid_loss=-1.60]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=20368)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=20368)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=20368)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=20368)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=20368)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=20368)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=20368)\u001b[0m 2024-11-12 19:13:05.521238: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=20368)\u001b[0m 2024-11-12 19:13:05.541284: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=20368)\u001b[0m 2024-11-12 19:13:05.566190: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=20368)\u001b[0m 2024-11-12 19:13:05.574008: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=20368)\u001b[0m 2024-11-12 19:13:05.591878: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=20368)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=20368)\u001b[0m 2024-11-12 19:13:06.788300: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=20368)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=20368)\u001b[0m \n","\u001b[36m(_train_tune pid=20368)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=20368)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=20368)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=20368)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=20368)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=20368)\u001b[0m 3 | blocks       | ModuleList       | 5.5 M  | train\n","\u001b[36m(_train_tune pid=20368)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=20368)\u001b[0m 5.5 M     Trainable params\n","\u001b[36m(_train_tune pid=20368)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=20368)\u001b[0m 5.5 M     Total params\n","\u001b[36m(_train_tune pid=20368)\u001b[0m 21.893    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=20368)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=20368)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/2 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.720, train_loss_epoch=0.747]\n","Epoch 2:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.210, train_loss_epoch=5.440]\n","Epoch 3:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.480, train_loss_epoch=4.030]\n","Epoch 4:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.650, train_loss_epoch=3.390]\n","Epoch 5:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.660, train_loss_epoch=3.730]\n","Epoch 6:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.690, train_loss_epoch=2.810]\n","Epoch 7:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.430, train_loss_epoch=2.990]\n","Epoch 8:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.140, train_loss_epoch=3.110]\n","Epoch 8: 100%|██████████| 2/2 [00:00\u003c00:00,  7.07it/s, v_num=0, train_loss_step=3.530, train_loss_epoch=3.110]\n","Epoch 9:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.530, train_loss_epoch=3.110]\n","Epoch 10:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.570, train_loss_epoch=2.790]\n","Epoch 11:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.900, train_loss_epoch=2.390]\n","Epoch 12:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.210, train_loss_epoch=2.240]\n","Epoch 13:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.820, train_loss_epoch=2.070]\n","Epoch 14:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.160, train_loss_epoch=3.340]\n","Epoch 15:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.170, train_loss_epoch=2.180]\n","Epoch 16:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.090, train_loss_epoch=1.280]\n","Epoch 17:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.510, train_loss_epoch=2.000]\n","Epoch 18:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.130, train_loss_epoch=1.140]\n","Epoch 19:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.688, train_loss_epoch=0.862]\n","Epoch 20:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.164, train_loss_epoch=0.243]\n","Epoch 21:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.202, train_loss_epoch=-0.242]\n","Epoch 22:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.0906, train_loss_epoch=-0.25]\n","Epoch 23:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.631, train_loss_epoch=-0.524]\n","Epoch 24:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.08, train_loss_epoch=-0.933]\n","Epoch 25:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.03, train_loss_epoch=-0.953]\n","Epoch 26:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.95, train_loss_epoch=-1.04]\n","Epoch 27:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.07]\n","Epoch 28:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.02, train_loss_epoch=-0.941]\n","Epoch 28: 100%|██████████| 2/2 [00:00\u003c00:00,  6.98it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.13] \n","Epoch 29:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.13]\n","Epoch 30:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.27]\n","Epoch 31:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.34]\n","Epoch 32:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.28]\n","Epoch 33:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.29]\n","Epoch 34:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.37]\n","Epoch 35:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.35]\n","Epoch 36:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.37]\n","Epoch 37:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.40]\n","Epoch 38:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.34]\n","Epoch 39:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.38]\n","Epoch 40:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.24]\n","Epoch 41:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.44]\n","Epoch 42:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.32]\n","Epoch 43:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.41]\n","Epoch 44:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.42]\n","Epoch 45:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.35]\n","Epoch 46:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.33]\n","Epoch 47:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.34]\n","Epoch 48:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.29]\n","Epoch 49:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.27]\n","Epoch 49: 100%|██████████| 2/2 [00:00\u003c00:00,  6.88it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.27]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=20368)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=20368)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 10.21it/s]\u001b[A\n","Epoch 50:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.18, valid_loss=-1.08]\n","Epoch 51:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.22, valid_loss=-1.08]\n","Epoch 52:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.42, valid_loss=-1.08]\n","Epoch 53:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.36, valid_loss=-1.08]\n","Epoch 54:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.41, valid_loss=-1.08]\n","Epoch 55:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.40, valid_loss=-1.08]\n","Epoch 56:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.37, valid_loss=-1.08]\n","Epoch 57:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.36, train_loss_epoch=-1.43, valid_loss=-1.08]\n","Epoch 58:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.39, valid_loss=-1.08]\n","Epoch 59:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.44, valid_loss=-1.08]\n","Epoch 60:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.40, valid_loss=-1.08]\n","Epoch 61:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.52, valid_loss=-1.08]\n","Epoch 62:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.50, valid_loss=-1.08]\n","Epoch 63:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.38, valid_loss=-1.08]\n","Epoch 64:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.31, valid_loss=-1.08]\n","Epoch 65:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.48, valid_loss=-1.08]\n","Epoch 66:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.52, valid_loss=-1.08]\n","Epoch 67:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.40, valid_loss=-1.08]\n","Epoch 68:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.47, valid_loss=-1.08]\n","Epoch 69:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.42, valid_loss=-1.08]\n","Epoch 70:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.50, valid_loss=-1.08]\n","Epoch 71:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.49, valid_loss=-1.08]\n","Epoch 72:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.47, valid_loss=-1.08]\n","Epoch 73:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.34, valid_loss=-1.08]\n","Epoch 74:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.55, valid_loss=-1.08]\n","Epoch 75:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.45, valid_loss=-1.08]\n","Epoch 76:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.37, valid_loss=-1.08]\n","Epoch 77:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.45, valid_loss=-1.08]\n","Epoch 78:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.44, valid_loss=-1.08]\n","Epoch 79:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.51, valid_loss=-1.08]\n","Epoch 80:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.42, valid_loss=-1.08]\n","Epoch 81:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.54, valid_loss=-1.08]\n","Epoch 82:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.52, valid_loss=-1.08]\n","Epoch 83:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.48, valid_loss=-1.08]\n","Epoch 84:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.45, valid_loss=-1.08]\n","Epoch 85:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.54, valid_loss=-1.08]\n","Epoch 86:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.47, valid_loss=-1.08]\n","Epoch 87:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.38, valid_loss=-1.08]\n","Epoch 88:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.54, valid_loss=-1.08]\n","Epoch 89:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.47, valid_loss=-1.08]\n","Epoch 90:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.47, valid_loss=-1.08]\n","Epoch 91:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.44, valid_loss=-1.08]\n","Epoch 92:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.48, valid_loss=-1.08]\n","Epoch 93:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.47, valid_loss=-1.08]\n","Epoch 94:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.52, valid_loss=-1.08]\n","Epoch 95:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.52, valid_loss=-1.08]\n","Epoch 96:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.46, valid_loss=-1.08]\n","Epoch 97:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.42, valid_loss=-1.08]\n","Epoch 98:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.39, valid_loss=-1.08]\n","Epoch 99:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.53, valid_loss=-1.08]\n","Epoch 99: 100%|██████████| 2/2 [00:00\u003c00:00,  6.94it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.53, valid_loss=-1.08]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=20368)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=20368)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 10.22it/s]\u001b[A\n","Epoch 100:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.47, valid_loss=-1.43]\n","Epoch 101:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.46, valid_loss=-1.43]\n","Epoch 102:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.47, valid_loss=-1.43]\n","Epoch 103:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.48, valid_loss=-1.43]\n","Epoch 104:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.47, valid_loss=-1.43]\n","Epoch 105:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.47, valid_loss=-1.43]\n","Epoch 106:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.48, valid_loss=-1.43]\n","Epoch 107:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.46, valid_loss=-1.43]\n","Epoch 108:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.56, valid_loss=-1.43]\n","Epoch 109:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.43, valid_loss=-1.43]\n","Epoch 110:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.51, valid_loss=-1.43]\n","Epoch 111:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.51, valid_loss=-1.43]\n","Epoch 112:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.46, valid_loss=-1.43]\n","Epoch 113:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.37, valid_loss=-1.43]\n","Epoch 114:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.32, valid_loss=-1.43]\n","Epoch 115:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.48, valid_loss=-1.43]\n","Epoch 116:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.48, valid_loss=-1.43]\n","Epoch 117:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.42, valid_loss=-1.43]\n","Epoch 118:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.53, valid_loss=-1.43]\n","Epoch 119:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.46, valid_loss=-1.43]\n","Epoch 120:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.58, valid_loss=-1.43]\n","Epoch 121:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.54, valid_loss=-1.43]\n","Epoch 122:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.51, valid_loss=-1.43]\n","Epoch 123:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.44, valid_loss=-1.43]\n","Epoch 124:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.49, valid_loss=-1.43]\n","Epoch 125:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.56, valid_loss=-1.43]\n","Epoch 126:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.56, valid_loss=-1.43]\n","Epoch 127:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.52, valid_loss=-1.43]\n","Epoch 128:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.55, valid_loss=-1.43]\n","Epoch 129:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.46, valid_loss=-1.43]\n","Epoch 130:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.52, valid_loss=-1.43]\n","Epoch 131:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.54, valid_loss=-1.43]\n","Epoch 132:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.45, valid_loss=-1.43]\n","Epoch 133:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.59, valid_loss=-1.43]\n","Epoch 134:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.47, valid_loss=-1.43]\n","Epoch 135:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.46, valid_loss=-1.43]\n","Epoch 136:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.52, valid_loss=-1.43]\n","Epoch 137:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.54, valid_loss=-1.43]\n","Epoch 138:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.54, valid_loss=-1.43]\n","Epoch 139:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.49, valid_loss=-1.43]\n","Epoch 140:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.51, valid_loss=-1.43]\n","Epoch 141:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.58, valid_loss=-1.43]\n","Epoch 142:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.55, valid_loss=-1.43]\n","Epoch 143:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.56, valid_loss=-1.43]\n","Epoch 144:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.56, valid_loss=-1.43]\n","Epoch 145:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.45, valid_loss=-1.43]\n","Epoch 146:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.50, valid_loss=-1.43]\n","Epoch 147:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.49, valid_loss=-1.43]\n","Epoch 148:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.58, valid_loss=-1.43]\n","Epoch 149:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.58, valid_loss=-1.43]\n","Epoch 149: 100%|██████████| 2/2 [00:00\u003c00:00,  7.53it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.58, valid_loss=-1.43]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=20368)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=20368)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 10.33it/s]\u001b[A\n","Epoch 150:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.59, valid_loss=-1.49]\n","Epoch 151:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.62, valid_loss=-1.49]\n","Epoch 151: 100%|██████████| 2/2 [00:00\u003c00:00,  7.38it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.55, valid_loss=-1.49]\n","Epoch 152:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.55, valid_loss=-1.49]\n","Epoch 153:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.51, valid_loss=-1.49]\n","Epoch 154:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.57, valid_loss=-1.49]\n","Epoch 155:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.43, valid_loss=-1.49]\n","Epoch 156:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.59, valid_loss=-1.49]\n","Epoch 157:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.56, valid_loss=-1.49]\n","Epoch 158:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.58, valid_loss=-1.49]\n","Epoch 159:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.56, valid_loss=-1.49]\n","Epoch 160:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.58, valid_loss=-1.49]\n","Epoch 161:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.59, valid_loss=-1.49]\n","Epoch 162:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.58, valid_loss=-1.49]\n","Epoch 163:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.60, valid_loss=-1.49]\n","Epoch 164:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.60, valid_loss=-1.49]\n","Epoch 165:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.67, valid_loss=-1.49]\n","Epoch 166:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.54, valid_loss=-1.49]\n","Epoch 167:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.58, valid_loss=-1.49]\n","Epoch 168:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.68, valid_loss=-1.49]\n","Epoch 169:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.61, valid_loss=-1.49]\n","Epoch 170:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.50, valid_loss=-1.49]\n","Epoch 171:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.59, valid_loss=-1.49]\n","Epoch 172:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.55, valid_loss=-1.49]\n","Epoch 173:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.54, valid_loss=-1.49]\n","Epoch 174:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.61, valid_loss=-1.49]\n","Epoch 175:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.50, valid_loss=-1.49]\n","Epoch 176:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.62, valid_loss=-1.49]\n","Epoch 177:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.50, valid_loss=-1.49]\n","Epoch 178:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.58, valid_loss=-1.49]\n","Epoch 179:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.61, valid_loss=-1.49]\n","Epoch 180:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.55, valid_loss=-1.49]\n","Epoch 181:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.65, valid_loss=-1.49]\n","Epoch 182:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.58, valid_loss=-1.49]\n","Epoch 182: 100%|██████████| 2/2 [00:00\u003c00:00,  7.00it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.58, valid_loss=-1.49]\n","Epoch 183:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.48, valid_loss=-1.49]\n","Epoch 184:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.59, valid_loss=-1.49]\n","Epoch 185:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.61, valid_loss=-1.49]\n","Epoch 186:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.59, valid_loss=-1.49]\n","Epoch 187:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.65, valid_loss=-1.49]\n","Epoch 188:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.54, valid_loss=-1.49]\n","Epoch 189:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.67, valid_loss=-1.49]\n","Epoch 190:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.58, valid_loss=-1.49]\n","Epoch 191:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.59, valid_loss=-1.49]\n","Epoch 192:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.62, valid_loss=-1.49]\n","Epoch 193:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.66, valid_loss=-1.49]\n","Epoch 194:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.56, valid_loss=-1.49]\n","Epoch 195:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.60, valid_loss=-1.49]\n","Epoch 196:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.55, valid_loss=-1.49]\n","Epoch 197:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.60, valid_loss=-1.49]\n","Epoch 198:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.56, valid_loss=-1.49]\n","Epoch 199:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.57, valid_loss=-1.49]\n","Epoch 199: 100%|██████████| 2/2 [00:00\u003c00:00,  6.82it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.57, valid_loss=-1.49]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=20368)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=20368)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 10.27it/s]\u001b[A\n","Epoch 200:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.55, valid_loss=-1.52]\n","Epoch 201:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.72, valid_loss=-1.52]\n","Epoch 202:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.54, valid_loss=-1.52]\n","Epoch 203:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.63, valid_loss=-1.52]\n","Epoch 204:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.51, valid_loss=-1.52]\n","Epoch 205:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.52, valid_loss=-1.52]\n","Epoch 206:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.70, valid_loss=-1.52]\n","Epoch 207:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.65, valid_loss=-1.52]\n","Epoch 208:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.66, valid_loss=-1.52]\n","Epoch 208: 100%|██████████| 2/2 [00:00\u003c00:00,  6.94it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.66, valid_loss=-1.52]\n","Epoch 209:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.59, valid_loss=-1.52]\n","Epoch 210:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.64, valid_loss=-1.52]\n","Epoch 211:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.56, valid_loss=-1.52]\n","Epoch 212:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.59, valid_loss=-1.52]\n","Epoch 213:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.61, valid_loss=-1.52]\n","Epoch 214:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.63, valid_loss=-1.52]\n","Epoch 215:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.48, valid_loss=-1.52]\n","Epoch 216:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.62, valid_loss=-1.52]\n","Epoch 217:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.65, valid_loss=-1.52]\n","Epoch 218:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.69, valid_loss=-1.52]\n","Epoch 219:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.58, valid_loss=-1.52]\n","Epoch 220:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.61, valid_loss=-1.52]\n","Epoch 221:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.62, valid_loss=-1.52]\n","Epoch 222:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.59, valid_loss=-1.52]\n","Epoch 223:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.60, valid_loss=-1.52]\n","Epoch 224:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.59, valid_loss=-1.52]\n","Epoch 225:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.62, valid_loss=-1.52]\n","Epoch 226:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.59, valid_loss=-1.52]\n","Epoch 227:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.66, valid_loss=-1.52]\n","Epoch 228:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.61, valid_loss=-1.52]\n","Epoch 229:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.55, valid_loss=-1.52]\n","Epoch 230:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.63, valid_loss=-1.52]\n","Epoch 231:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.60, valid_loss=-1.52]\n","Epoch 232:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.66, valid_loss=-1.52]\n","Epoch 233:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.64, valid_loss=-1.52]\n","Epoch 234:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.64, valid_loss=-1.52]\n","Epoch 235:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.64, valid_loss=-1.52]\n","Epoch 236:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.63, valid_loss=-1.52]\n","Epoch 237:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.69, valid_loss=-1.52]\n","Epoch 238:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.56, valid_loss=-1.52]\n","Epoch 239:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.64, valid_loss=-1.52]\n","Epoch 240:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.65, valid_loss=-1.52]\n","Epoch 241:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.65, valid_loss=-1.52]\n","Epoch 242:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.65, valid_loss=-1.52]\n","Epoch 243:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.69, valid_loss=-1.52]\n","Epoch 244:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.64, valid_loss=-1.52]\n","Epoch 245:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.70, valid_loss=-1.52]\n","Epoch 246:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.63, valid_loss=-1.52]\n","Epoch 247:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.66, valid_loss=-1.52]\n","Epoch 248:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.68, valid_loss=-1.52]\n","Epoch 249:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.64, valid_loss=-1.52]\n","Epoch 249: 100%|██████████| 2/2 [00:00\u003c00:00,  6.99it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.64, valid_loss=-1.52]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=20368)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=20368)\u001b[0m `Trainer.fit` stopped: `max_steps=500.0` reached.\n","2024-11-12 19:14:23,989\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('MA(2,9)', 'MA(3,9)', 'MA(1,12)', 'MA(2,12)', 'MA(3,12)', 'MOM(9)', 'MOM(12)', 'RSI(7)', 'RSI(14)', 'EMA(3,9)', 'EMA(5,9)', 'EMA(5,12)', 'DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (60, 8, 1), 'n_pool_kernel_size': (1, 1, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=20368)\u001b[0m \n","\u001b[36m(_train_tune pid=20368)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 10.24it/s]\u001b[A\n","\u001b[36m(_train_tune pid=20368)\u001b[0m \r                                                                      \u001b[A\rEpoch 249: 100%|██████████| 2/2 [00:00\u003c00:00,  3.11it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.64, valid_loss=-1.58]\rEpoch 249: 100%|██████████| 2/2 [00:00\u003c00:00,  3.09it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.70, valid_loss=-1.58]\rEpoch 249: 100%|██████████| 2/2 [00:00\u003c00:00,  3.08it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.70, valid_loss=-1.58]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=20786)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=20786)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=20786)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=20786)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=20786)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=20786)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=20786)\u001b[0m 2024-11-12 19:14:32.490515: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=20786)\u001b[0m 2024-11-12 19:14:32.511926: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=20786)\u001b[0m 2024-11-12 19:14:32.539747: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=20786)\u001b[0m 2024-11-12 19:14:32.548473: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=20786)\u001b[0m 2024-11-12 19:14:32.568157: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=20786)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=20786)\u001b[0m 2024-11-12 19:14:33.740612: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=20786)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=20786)\u001b[0m \n","\u001b[36m(_train_tune pid=20786)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=20786)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=20786)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=20786)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=20786)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=20786)\u001b[0m 3 | blocks       | ModuleList       | 2.9 M  | train\n","\u001b[36m(_train_tune pid=20786)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=20786)\u001b[0m 2.9 M     Trainable params\n","\u001b[36m(_train_tune pid=20786)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=20786)\u001b[0m 2.9 M     Total params\n","\u001b[36m(_train_tune pid=20786)\u001b[0m 11.711    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=20786)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=20786)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/13 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.41]\n","Epoch 2:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.54]\n","Epoch 3:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.59]\n","Epoch 4:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.58]\n","Epoch 5:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.69]\n","Epoch 6:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.71]\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.81]\n","\u001b[36m(_train_tune pid=20786)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=20786)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.20it/s]\u001b[A\n","Epoch 7:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.81, valid_loss=-1.79]\n","Epoch 8:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.79, valid_loss=-1.79]\n","Epoch 9:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.80, valid_loss=-1.79]\n","Epoch 10:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.82, valid_loss=-1.79]\n","Epoch 11:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.47, train_loss_epoch=-1.88, valid_loss=-1.79]\n","Epoch 12:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.84, valid_loss=-1.79]\n","Epoch 13:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.83, valid_loss=-1.79]\n","Epoch 14:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.85, valid_loss=-1.79]\n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.84, valid_loss=-1.79]\n","\u001b[36m(_train_tune pid=20786)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=20786)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.21it/s]\u001b[A\n","Epoch 15:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.84, valid_loss=-1.86]\n","Epoch 16:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-1.97, valid_loss=-1.86]\n","Epoch 17:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.46, train_loss_epoch=-1.98, valid_loss=-1.86]\n","Epoch 18:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-1.92, valid_loss=-1.86]\n","Epoch 18: 100%|██████████| 13/13 [00:00\u003c00:00, 37.41it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.92, valid_loss=-1.86]\n","Epoch 19:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.90, valid_loss=-1.86]\n","Epoch 20:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.46, train_loss_epoch=-1.95, valid_loss=-1.86]\n","Epoch 21:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.92, valid_loss=-1.86]\n","Epoch 22:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.91, valid_loss=-1.86]\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.89, valid_loss=-1.86]\n","\u001b[36m(_train_tune pid=20786)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=20786)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.13it/s]\u001b[A\n","Epoch 23:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.89, valid_loss=-1.87]\n","Epoch 24:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.45, train_loss_epoch=-1.92, valid_loss=-1.87]\n","Epoch 25:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-1.90, valid_loss=-1.87]\n","Epoch 26:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.96, valid_loss=-1.87]\n","Epoch 27:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.92, valid_loss=-1.87]\n","Epoch 28:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.05, train_loss_epoch=-1.82, valid_loss=-1.87]\n","Epoch 29:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.97, valid_loss=-1.87]\n","Epoch 30:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-1.91, valid_loss=-1.87]\n","\u001b[36m(_train_tune pid=20786)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=20786)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.15it/s]\u001b[A\n","Epoch 31:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.90, valid_loss=-1.90]\n","Epoch 32:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-1.94, valid_loss=-1.90]\n","Epoch 33:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.85, valid_loss=-1.90]\n","Epoch 34:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-1.92, valid_loss=-1.90]\n","Epoch 35:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.87, valid_loss=-1.90]\n","Epoch 36:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-1.97, valid_loss=-1.90]\n","Epoch 37:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.92, valid_loss=-1.90]\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.91, valid_loss=-1.90]\n","\u001b[36m(_train_tune pid=20786)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=20786)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.13it/s]\u001b[A\n","Epoch 38:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.91, valid_loss=-1.90]\n","Epoch 39:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.92, valid_loss=-1.90]\n","Epoch 40:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.90, valid_loss=-1.90]\n","Epoch 41:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.94, valid_loss=-1.90]\n","Epoch 41: 100%|██████████| 13/13 [00:00\u003c00:00, 36.49it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.94, valid_loss=-1.90]\n","Epoch 42:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.93, valid_loss=-1.90]\n","Epoch 43:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.97, valid_loss=-1.90]\n","Epoch 44:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.94, valid_loss=-1.90]\n","Epoch 45:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.92, valid_loss=-1.90]\n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-1.99, valid_loss=-1.90]\n","\u001b[36m(_train_tune pid=20786)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=20786)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.14it/s]\u001b[A\n","                                                                        \u001b[A\n","Epoch 46:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.99, valid_loss=-1.92]\n","Epoch 47:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.91, valid_loss=-1.92]\n","Epoch 48:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-1.98, valid_loss=-1.92]\n","Epoch 49:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-1.96, valid_loss=-1.92]\n","Epoch 50:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.95, valid_loss=-1.92]\n","Epoch 51:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.95, valid_loss=-1.92]\n","Epoch 52:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.41, train_loss_epoch=-2.00, valid_loss=-1.92]\n","Epoch 53:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.96, valid_loss=-1.92]\n","\u001b[36m(_train_tune pid=20786)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=20786)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.11it/s]\u001b[A\n","Epoch 53:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.96, valid_loss=-1.93]\n","Epoch 54:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-1.97, valid_loss=-1.93]\n","Epoch 55:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.94, valid_loss=-1.93]\n","Epoch 56:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-1.97, valid_loss=-1.93]\n","Epoch 57:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.91, valid_loss=-1.93]\n","Epoch 58:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-1.98, valid_loss=-1.93]\n","Epoch 59:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.95, valid_loss=-1.93]\n","Epoch 60:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.96, valid_loss=-1.93]\n","Epoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.94, valid_loss=-1.93]\n","\u001b[36m(_train_tune pid=20786)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=20786)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.03it/s]\u001b[A\n","Epoch 61:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.94, valid_loss=-1.92]\n","Epoch 62:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.99, valid_loss=-1.92]\n","Epoch 63:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.94, valid_loss=-1.92]\n","Epoch 64:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.92, valid_loss=-1.92]\n","Epoch 65:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.93, valid_loss=-1.92]\n","Epoch 66:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.94, valid_loss=-1.92]\n","Epoch 67:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.95, valid_loss=-1.92]\n","Epoch 68:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.94, valid_loss=-1.92]\n","Epoch 69:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.89, valid_loss=-1.92]\n","\u001b[36m(_train_tune pid=20786)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=20786)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.00it/s]\u001b[A\n","Epoch 69:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.89, valid_loss=-1.93]\n","Epoch 70:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.98, valid_loss=-1.93]\n","Epoch 71:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.98, valid_loss=-1.93]\n","Epoch 72:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-1.99, valid_loss=-1.93]\n","Epoch 73:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.96, valid_loss=-1.93]\n","Epoch 74:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.97, valid_loss=-1.93]\n","Epoch 75:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.98, valid_loss=-1.93]\n","Epoch 76:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-2.00, valid_loss=-1.93]\n","\u001b[36m(_train_tune pid=20786)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=20786)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.15it/s]\u001b[A\n","Epoch 77:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.00, valid_loss=-1.94]\n","Epoch 78:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.96, valid_loss=-1.94]\n","Epoch 79:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.97, valid_loss=-1.94]\n","Epoch 80:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.94, valid_loss=-1.94]\n","Epoch 81:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.01, valid_loss=-1.94]\n","Epoch 82:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.41, train_loss_epoch=-2.01, valid_loss=-1.94]\n","Epoch 83:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.95, valid_loss=-1.94]\n","Epoch 84:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.97, valid_loss=-1.94]\n","\u001b[36m(_train_tune pid=20786)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=20786)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.14it/s]\u001b[A\n","Epoch 84:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.97, valid_loss=-1.94]\n","Epoch 85:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.50, train_loss_epoch=-2.05, valid_loss=-1.94]\n","Epoch 85: 100%|██████████| 13/13 [00:00\u003c00:00, 37.82it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-2.05, valid_loss=-1.94]\n","Epoch 86:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.97, valid_loss=-1.94]\n","Epoch 87:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.92, valid_loss=-1.94]\n","Epoch 88:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.50, train_loss_epoch=-2.00, valid_loss=-1.94]\n","Epoch 89:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.00, valid_loss=-1.94]\n","Epoch 90:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-2.03, valid_loss=-1.94]\n","Epoch 91:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.99, valid_loss=-1.94]\n","Epoch 92:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.96, valid_loss=-1.94]\n","\u001b[36m(_train_tune pid=20786)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=20786)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.21it/s]\u001b[A\n","Epoch 92:   0%|          | 0/13 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.96, valid_loss=-1.94]\n","Epoch 92: 100%|██████████| 13/13 [00:02\u003c00:00,  5.96it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.00, valid_loss=-1.94]\n","Epoch 93:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.00, valid_loss=-1.94]\n","Epoch 94:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.98, valid_loss=-1.94]\n","Epoch 95:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.03, valid_loss=-1.94]\n","Epoch 96:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.99, valid_loss=-1.94]\n","Epoch 97:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-1.97, valid_loss=-1.94]\n","Epoch 98:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.95, valid_loss=-1.94]\n","Epoch 99:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.01, valid_loss=-1.94]\n","Epoch 99: 100%|██████████| 13/13 [00:00\u003c00:00, 37.08it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-2.01, valid_loss=-1.94]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=20786)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.11it/s]\u001b[A\n","Epoch 100:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.93, valid_loss=-1.95]\n","Epoch 101:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-1.98, valid_loss=-1.95]\n","Epoch 102:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.93, valid_loss=-1.95]\n","Epoch 103:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.92, valid_loss=-1.95]\n","Epoch 104:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.00, valid_loss=-1.95]\n","Epoch 105:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-1.98, valid_loss=-1.95]\n","Epoch 106:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.94, valid_loss=-1.95]\n","Epoch 107:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.91, valid_loss=-1.95]\n","\u001b[36m(_train_tune pid=20786)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=20786)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.10it/s]\u001b[A\n","\u001b[36m(_train_tune pid=20786)\u001b[0m \n","Epoch 107:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.91, valid_loss=-1.95]\n","Epoch 108:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.00, valid_loss=-1.95]\n","Epoch 109:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.44, train_loss_epoch=-2.03, valid_loss=-1.95]\n","Epoch 110:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-2.02, valid_loss=-1.95]\n","Epoch 111:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.97, valid_loss=-1.95]\n","Epoch 112:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.91, valid_loss=-1.95]\n","Epoch 113:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.93, valid_loss=-1.95]\n","Epoch 114:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.44, train_loss_epoch=-2.03, valid_loss=-1.95]\n","Epoch 115:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.98, valid_loss=-1.95]\n","\u001b[36m(_train_tune pid=20786)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 19:15:44,809\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('MA(2,9)', 'MA(3,9)', 'MA(1,12)', 'MA(2,12)', 'MA(3,12)', 'MOM(9)', 'MOM(12)', 'RSI(7)', 'RSI(14)', 'EMA(3,9)', 'EMA(5,9)', 'EMA(5,12)', 'DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (24, 12, 1), 'n_pool_kernel_size': (16, 8, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=20786)\u001b[0m `Trainer.fit` stopped: `max_steps=1500.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=20786)\u001b[0m \n","\u001b[36m(_train_tune pid=20786)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 13/13 [00:01\u003c00:00,  7.09it/s]\u001b[A\n","\u001b[36m(_train_tune pid=20786)\u001b[0m \r                                                                        \u001b[A\rEpoch 115:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.98, valid_loss=-1.95]\rEpoch 115:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.98, valid_loss=-1.95]\rEpoch 115:   0%|          | 0/13 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.98, valid_loss=-1.95]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=21180)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=21180)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=21180)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=21180)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=21180)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=21180)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=21180)\u001b[0m 2024-11-12 19:15:52.535556: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=21180)\u001b[0m 2024-11-12 19:15:52.555557: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=21180)\u001b[0m 2024-11-12 19:15:52.580308: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=21180)\u001b[0m 2024-11-12 19:15:52.587974: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=21180)\u001b[0m 2024-11-12 19:15:52.605533: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=21180)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=21180)\u001b[0m 2024-11-12 19:15:53.767496: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=21180)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=21180)\u001b[0m \n","\u001b[36m(_train_tune pid=21180)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=21180)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=21180)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=21180)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=21180)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=21180)\u001b[0m 3 | blocks       | ModuleList       | 2.8 M  | train\n","\u001b[36m(_train_tune pid=21180)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=21180)\u001b[0m 2.8 M     Trainable params\n","\u001b[36m(_train_tune pid=21180)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=21180)\u001b[0m 2.8 M     Total params\n","\u001b[36m(_train_tune pid=21180)\u001b[0m 11.277    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=21180)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=21180)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/7 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.940, train_loss_epoch=10.60]\n","Epoch 2:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.850, train_loss_epoch=7.810]\n","Epoch 3:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.010, train_loss_epoch=7.120]\n","Epoch 4:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.540, train_loss_epoch=6.840]\n","Epoch 5:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.550, train_loss_epoch=7.660]\n","Epoch 6:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.000, train_loss_epoch=7.170]\n","Epoch 7:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.960, train_loss_epoch=6.190]\n","Epoch 8:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.240, train_loss_epoch=7.530]\n","Epoch 9:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.460, train_loss_epoch=7.380]\n","Epoch 10:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.040, train_loss_epoch=6.890]\n","Epoch 11:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.950, train_loss_epoch=6.970]\n","Epoch 12:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.650, train_loss_epoch=6.750]\n","Epoch 13:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.270, train_loss_epoch=8.000]\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.960, train_loss_epoch=8.060]\n","\u001b[36m(_train_tune pid=21180)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21180)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.66it/s]\u001b[A\n","Epoch 14:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=8.040, train_loss_epoch=8.060, valid_loss=7.960]\n","Epoch 15:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.020, train_loss_epoch=8.150, valid_loss=7.960]\n","Epoch 16:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.860, train_loss_epoch=7.860, valid_loss=7.960]\n","Epoch 17:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.930, train_loss_epoch=7.090, valid_loss=7.960]\n","Epoch 18:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.770, train_loss_epoch=6.290, valid_loss=7.960]\n","Epoch 19:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.250, train_loss_epoch=7.290, valid_loss=7.960]\n","Epoch 20:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.140, train_loss_epoch=7.560, valid_loss=7.960]\n","Epoch 21:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.080, train_loss_epoch=7.160, valid_loss=7.960]\n","Epoch 22:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.460, train_loss_epoch=6.760, valid_loss=7.960]\n","Epoch 23:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.820, train_loss_epoch=5.970, valid_loss=7.960]\n","Epoch 24:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.710, train_loss_epoch=5.400, valid_loss=7.960]\n","Epoch 25:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.310, train_loss_epoch=6.640, valid_loss=7.960]\n","Epoch 26:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.490, train_loss_epoch=7.340, valid_loss=7.960]\n","Epoch 27:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.820, train_loss_epoch=6.880, valid_loss=7.960]\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.090, train_loss_epoch=6.090, valid_loss=7.960]\n","\u001b[36m(_train_tune pid=21180)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21180)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.66it/s]\u001b[A\n","Epoch 28:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=7.020, train_loss_epoch=6.090, valid_loss=6.490]\n","Epoch 29:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.880, train_loss_epoch=6.670, valid_loss=6.490]\n","Epoch 30:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.150, train_loss_epoch=6.280, valid_loss=6.490]\n","Epoch 31:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.440, train_loss_epoch=5.620, valid_loss=6.490]\n","Epoch 32:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.490, train_loss_epoch=5.690, valid_loss=6.490]\n","Epoch 33:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.940, train_loss_epoch=6.240, valid_loss=6.490]\n","Epoch 34:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.390, train_loss_epoch=6.110, valid_loss=6.490]\n","Epoch 35:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.030, train_loss_epoch=5.250, valid_loss=6.490]\n","Epoch 36:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.810, train_loss_epoch=4.580, valid_loss=6.490]\n","Epoch 37:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.910, train_loss_epoch=4.940, valid_loss=6.490]\n","Epoch 38:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.960, train_loss_epoch=4.070, valid_loss=6.490]\n","Epoch 39:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.480, train_loss_epoch=4.670, valid_loss=6.490]\n","Epoch 40:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.750, train_loss_epoch=5.500, valid_loss=6.490]\n","Epoch 41:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.920, train_loss_epoch=6.260, valid_loss=6.490]\n","Epoch 42:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.550, train_loss_epoch=5.900, valid_loss=6.490]\n","\u001b[36m(_train_tune pid=21180)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21180)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.68it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21180)\u001b[0m \n","Epoch 43:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.740, train_loss_epoch=5.310, valid_loss=4.930]\n","Epoch 44:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.100, train_loss_epoch=4.300, valid_loss=4.930]\n","Epoch 45:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.950, train_loss_epoch=3.950, valid_loss=4.930]\n","Epoch 46:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.170, train_loss_epoch=4.340, valid_loss=4.930]\n","Epoch 47:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.910, train_loss_epoch=3.930, valid_loss=4.930]\n","Epoch 48:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.940, train_loss_epoch=4.400, valid_loss=4.930]\n","Epoch 49:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.330, train_loss_epoch=3.750, valid_loss=4.930]\n","Epoch 50:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.970, train_loss_epoch=3.690, valid_loss=4.930]\n","Epoch 51:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.220, train_loss_epoch=3.160, valid_loss=4.930]\n","Epoch 52:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.110, train_loss_epoch=3.670, valid_loss=4.930]\n","Epoch 53:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.220, train_loss_epoch=3.000, valid_loss=4.930]\n","Epoch 54:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.640, train_loss_epoch=2.620, valid_loss=4.930]\n","Epoch 55:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.180, train_loss_epoch=2.550, valid_loss=4.930]\n","Epoch 56:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.970, train_loss_epoch=2.500, valid_loss=4.930]\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.240, train_loss_epoch=2.470, valid_loss=4.930]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21180)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.72it/s]\u001b[A\n","Epoch 57:   0%|          | 0/7 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=2.340, train_loss_epoch=2.470, valid_loss=2.190]\n","Epoch 58:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.310, train_loss_epoch=2.620, valid_loss=2.190]\n","Epoch 59:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.780, train_loss_epoch=5.370, valid_loss=2.190]\n","Epoch 60:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.040, train_loss_epoch=3.470, valid_loss=2.190]\n","Epoch 61:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.610, train_loss_epoch=2.730, valid_loss=2.190]\n","Epoch 62:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.300, train_loss_epoch=2.500, valid_loss=2.190]\n","Epoch 63:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.530, train_loss_epoch=2.080, valid_loss=2.190]\n","Epoch 64:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.700, train_loss_epoch=1.920, valid_loss=2.190]\n","Epoch 65:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.780, train_loss_epoch=1.850, valid_loss=2.190]\n","Epoch 66:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.180, train_loss_epoch=1.810, valid_loss=2.190]\n","Epoch 67:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.898, train_loss_epoch=1.600, valid_loss=2.190]\n","Epoch 68:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.560, train_loss_epoch=1.470, valid_loss=2.190]\n","Epoch 69:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.570, train_loss_epoch=1.420, valid_loss=2.190]\n","Epoch 70:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.999, train_loss_epoch=1.330, valid_loss=2.190]\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.580, train_loss_epoch=1.480, valid_loss=2.190]\n","\u001b[36m(_train_tune pid=21180)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21180)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.70it/s]\u001b[A\n","Epoch 71:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=1.250, train_loss_epoch=1.480, valid_loss=1.200]\n","Epoch 72:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.579, train_loss_epoch=1.180, valid_loss=1.200]\n","Epoch 73:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.240, train_loss_epoch=1.180, valid_loss=1.200]\n","Epoch 74:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.200, train_loss_epoch=1.210, valid_loss=1.200]\n","Epoch 75:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.210, train_loss_epoch=1.190, valid_loss=1.200]\n","Epoch 76:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.010, train_loss_epoch=1.020, valid_loss=1.200]\n","Epoch 77:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.820, train_loss_epoch=1.120, valid_loss=1.200]\n","Epoch 77: 100%|██████████| 7/7 [00:00\u003c00:00, 27.06it/s, v_num=0, train_loss_step=1.230, train_loss_epoch=1.160, valid_loss=1.200]\n","Epoch 78:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.230, train_loss_epoch=1.160, valid_loss=1.200]\n","Epoch 79:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.411, train_loss_epoch=0.880, valid_loss=1.200]\n","Epoch 80:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.285, train_loss_epoch=0.929, valid_loss=1.200]\n","Epoch 81:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.980, train_loss_epoch=1.150, valid_loss=1.200]\n","Epoch 82:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.361, train_loss_epoch=0.885, valid_loss=1.200]\n","Epoch 83:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.943, train_loss_epoch=0.849, valid_loss=1.200]\n","Epoch 84:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.545, train_loss_epoch=0.745, valid_loss=1.200]\n","Epoch 85:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.678, train_loss_epoch=0.522, valid_loss=1.200]\n","\u001b[36m(_train_tune pid=21180)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21180)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.70it/s]\u001b[A\n","Epoch 86:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.531, train_loss_epoch=0.513, valid_loss=0.532]\n","Epoch 87:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.35, train_loss_epoch=0.334, valid_loss=0.532]\n","Epoch 88:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.0822, train_loss_epoch=0.419, valid_loss=0.532]\n","Epoch 89:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.341, train_loss_epoch=0.468, valid_loss=0.532]\n","Epoch 90:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.326, train_loss_epoch=0.282, valid_loss=0.532]\n","Epoch 91:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.274, train_loss_epoch=0.221, valid_loss=0.532]\n","Epoch 92:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.0979, train_loss_epoch=0.0659, valid_loss=0.532]\n","Epoch 93:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.314, train_loss_epoch=0.252, valid_loss=0.532]\n","Epoch 93: 100%|██████████| 7/7 [00:00\u003c00:00, 25.63it/s, v_num=0, train_loss_step=0.121, train_loss_epoch=0.252, valid_loss=0.532]\n","Epoch 94:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.121, train_loss_epoch=0.131, valid_loss=0.532]\n","Epoch 95:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.171, train_loss_epoch=0.0756, valid_loss=0.532]\n","Epoch 96:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.0119, train_loss_epoch=-0.0501, valid_loss=0.532]\n","Epoch 97:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.362, train_loss_epoch=0.135, valid_loss=0.532]\n","Epoch 98:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.226, train_loss_epoch=-0.0652, valid_loss=0.532]\n","Epoch 99:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.683, train_loss_epoch=0.109, valid_loss=0.532]\n","Epoch 99: 100%|██████████| 7/7 [00:00\u003c00:00, 26.38it/s, v_num=0, train_loss_step=-0.152, train_loss_epoch=0.109, valid_loss=0.532]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 19:16:36,799\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('MA(2,9)', 'MA(3,9)', 'MA(1,12)', 'MA(2,12)', 'MA(3,12)', 'MOM(9)', 'MOM(12)', 'RSI(7)', 'RSI(14)', 'EMA(3,9)', 'EMA(5,9)', 'EMA(5,12)', 'DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (24, 12, 1), 'n_pool_kernel_size': (2, 2, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=21180)\u001b[0m `Trainer.fit` stopped: `max_steps=700.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=21180)\u001b[0m \n","\u001b[36m(_train_tune pid=21180)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.71it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21180)\u001b[0m \r                                                                      \u001b[A\rEpoch 99: 100%|██████████| 7/7 [00:02\u003c00:00,  3.20it/s, v_num=0, train_loss_step=-0.152, train_loss_epoch=0.109, valid_loss=-0.0529]\rEpoch 99: 100%|██████████| 7/7 [00:02\u003c00:00,  3.20it/s, v_num=0, train_loss_step=-0.152, train_loss_epoch=-0.0969, valid_loss=-0.0529]\rEpoch 99: 100%|██████████| 7/7 [00:02\u003c00:00,  3.20it/s, v_num=0, train_loss_step=-0.152, train_loss_epoch=-0.0969, valid_loss=-0.0529]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=21455)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=21455)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=21455)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=21455)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=21455)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=21455)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=21455)\u001b[0m 2024-11-12 19:16:44.655474: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=21455)\u001b[0m 2024-11-12 19:16:44.675490: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=21455)\u001b[0m 2024-11-12 19:16:44.700241: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=21455)\u001b[0m 2024-11-12 19:16:44.707895: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=21455)\u001b[0m 2024-11-12 19:16:44.725691: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=21455)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=21455)\u001b[0m 2024-11-12 19:16:45.903905: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=21455)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=21455)\u001b[0m \n","\u001b[36m(_train_tune pid=21455)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=21455)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=21455)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=21455)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=21455)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=21455)\u001b[0m 3 | blocks       | ModuleList       | 3.7 M  | train\n","\u001b[36m(_train_tune pid=21455)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=21455)\u001b[0m 3.7 M     Trainable params\n","\u001b[36m(_train_tune pid=21455)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=21455)\u001b[0m 3.7 M     Total params\n","\u001b[36m(_train_tune pid=21455)\u001b[0m 14.882    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=21455)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=21455)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/7 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.20]\n","Epoch 2:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.27]\n","Epoch 3:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.23]\n","Epoch 4:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.21]\n","Epoch 5:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.38]\n","Epoch 6:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.19]\n","Epoch 7:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.712, train_loss_epoch=-1.16]\n","Epoch 8:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.30]\n","Epoch 9:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.44]\n","Epoch 10:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.26]\n","Epoch 11:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.43]\n","Epoch 11: 100%|██████████| 7/7 [00:00\u003c00:00, 30.27it/s, v_num=0, train_loss_step=-0.894, train_loss_epoch=-1.43]\n","Epoch 12:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.894, train_loss_epoch=-1.26]\n","Epoch 13:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.977, train_loss_epoch=-1.32]\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.94, train_loss_epoch=-1.34]\n","\u001b[36m(_train_tune pid=21455)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21455)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.53it/s]\u001b[A\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.34, valid_loss=-1.37]\n","Epoch 15:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.08, train_loss_epoch=-1.31, valid_loss=-1.37]\n","Epoch 16:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.49, valid_loss=-1.37]\n","Epoch 17:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.41, valid_loss=-1.37]\n","Epoch 18:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.47, valid_loss=-1.37]\n","Epoch 19:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.44, valid_loss=-1.37]\n","Epoch 20:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.07, train_loss_epoch=-1.37, valid_loss=-1.37]\n","Epoch 21:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.43, valid_loss=-1.37]\n","Epoch 22:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.43, valid_loss=-1.37]\n","Epoch 23:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.46, valid_loss=-1.37]\n","Epoch 24:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.38, valid_loss=-1.37]\n","Epoch 25:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.47, valid_loss=-1.37]\n","Epoch 26:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.50, valid_loss=-1.37]\n","Epoch 27:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.53, valid_loss=-1.37]\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.49, valid_loss=-1.37]\n","\u001b[36m(_train_tune pid=21455)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21455)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 28.59it/s]\u001b[A\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.49, valid_loss=-1.43]\n","Epoch 29:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.711, train_loss_epoch=-1.37, valid_loss=-1.43]\n","Epoch 30:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.59, valid_loss=-1.43]\n","Epoch 31:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.42, valid_loss=-1.43]\n","Epoch 32:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.51, valid_loss=-1.43]\n","Epoch 33:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.57, valid_loss=-1.43]\n","Epoch 34:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.52, valid_loss=-1.43]\n","Epoch 35:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.59, valid_loss=-1.43]\n","Epoch 36:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.51, valid_loss=-1.43]\n","Epoch 37:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-1.65, valid_loss=-1.43]\n","Epoch 38:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.56, valid_loss=-1.43]\n","Epoch 39:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.56, valid_loss=-1.43]\n","Epoch 40:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.58, valid_loss=-1.43]\n","Epoch 41:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.66, valid_loss=-1.43]\n","Epoch 42:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.55, valid_loss=-1.43]\n","\u001b[36m(_train_tune pid=21455)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21455)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 24.91it/s]\u001b[A\n","Epoch 43:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.57, valid_loss=-1.47]\n","Epoch 44:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.65, valid_loss=-1.47]\n","Epoch 45:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.55, valid_loss=-1.47]\n","Epoch 46:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.60, valid_loss=-1.47]\n","Epoch 47:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.64, valid_loss=-1.47]\n","Epoch 48:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.62, valid_loss=-1.47]\n","Epoch 49:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.56, valid_loss=-1.47]\n","Epoch 50:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.58, valid_loss=-1.47]\n","Epoch 51:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.64, valid_loss=-1.47]\n","Epoch 52:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.67, valid_loss=-1.47]\n","Epoch 53:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.54, valid_loss=-1.47]\n","Epoch 54:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.73, valid_loss=-1.47]\n","Epoch 55:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.74, valid_loss=-1.47]\n","Epoch 56:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.65, valid_loss=-1.47]\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.885, train_loss_epoch=-1.56, valid_loss=-1.47]\n","\u001b[36m(_train_tune pid=21455)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21455)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.98it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21455)\u001b[0m \n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.56, valid_loss=-1.58] \n","Epoch 58:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.68, valid_loss=-1.58]\n","Epoch 59:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.71, valid_loss=-1.58]\n","Epoch 60:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.68, valid_loss=-1.58]\n","Epoch 61:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.70, valid_loss=-1.58]\n","Epoch 62:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.78, valid_loss=-1.58]\n","Epoch 63:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.66, valid_loss=-1.58]\n","Epoch 64:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.77, valid_loss=-1.58]\n","Epoch 65:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.72, valid_loss=-1.58]\n","Epoch 66:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.71, valid_loss=-1.58]\n","Epoch 67:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.77, valid_loss=-1.58]\n","Epoch 68:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.74, valid_loss=-1.58]\n","Epoch 69:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.71, valid_loss=-1.58]\n","Epoch 70:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.77, valid_loss=-1.58]\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.852, train_loss_epoch=-1.60, valid_loss=-1.58]\n","\u001b[36m(_train_tune pid=21455)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21455)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.86it/s]\u001b[A\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.60, valid_loss=-1.65] \n","Epoch 72:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.74, valid_loss=-1.65]\n","Epoch 73:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.77, valid_loss=-1.65]\n","Epoch 74:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.81, valid_loss=-1.65]\n","Epoch 75:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.76, valid_loss=-1.65]\n","Epoch 76:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.73, valid_loss=-1.65]\n","Epoch 76: 100%|██████████| 7/7 [00:00\u003c00:00, 30.24it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.73, valid_loss=-1.65]\n","Epoch 77:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.73, valid_loss=-1.65]\n","Epoch 78:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.85, valid_loss=-1.65]\n","Epoch 79:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.78, valid_loss=-1.65]\n","Epoch 80:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.69, valid_loss=-1.65]\n","Epoch 81:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.68, valid_loss=-1.65]\n","Epoch 82:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.76, valid_loss=-1.65]\n","Epoch 83:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.906, train_loss_epoch=-1.64, valid_loss=-1.65]\n","Epoch 84:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.77, valid_loss=-1.65]\n","Epoch 85:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.78, valid_loss=-1.65]\n","\u001b[36m(_train_tune pid=21455)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21455)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.71it/s]\u001b[A\n","Epoch 86:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.76, valid_loss=-1.66]\n","Epoch 87:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-1.82, valid_loss=-1.66]\n","Epoch 88:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.79, valid_loss=-1.66]\n","Epoch 89:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.79, valid_loss=-1.66]\n","Epoch 90:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.85, valid_loss=-1.66]\n","Epoch 91:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.78, valid_loss=-1.66]\n","Epoch 92:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.73, valid_loss=-1.66]\n","Epoch 93:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.82, valid_loss=-1.66]\n","Epoch 94:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.80, valid_loss=-1.66]\n","Epoch 95:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.82, valid_loss=-1.66]\n","Epoch 96:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-1.88, valid_loss=-1.66]\n","Epoch 97:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.79, valid_loss=-1.66]\n","Epoch 98:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.82, valid_loss=-1.66]\n","Epoch 99:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.73, valid_loss=-1.66]\n","Epoch 99: 100%|██████████| 7/7 [00:00\u003c00:00, 31.91it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.73, valid_loss=-1.66]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21455)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 27.14it/s]\u001b[A\n","Epoch 100:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.83, valid_loss=-1.67]\n","Epoch 101:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.85, valid_loss=-1.67]\n","Epoch 102:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.78, valid_loss=-1.67]\n","Epoch 103:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.80, valid_loss=-1.67]\n","Epoch 104:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-1.81, valid_loss=-1.67]\n","Epoch 105:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.73, valid_loss=-1.67]\n","Epoch 106:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.71, valid_loss=-1.67]\n","Epoch 107:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.78, valid_loss=-1.67]\n","Epoch 108:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-1.84, valid_loss=-1.67]\n","Epoch 109:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.83, valid_loss=-1.67]\n","Epoch 110:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.68, valid_loss=-1.67]\n","Epoch 111:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-1.83, valid_loss=-1.67]\n","Epoch 112:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.84, valid_loss=-1.67]\n","Epoch 113:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.80, valid_loss=-1.67]\n","Epoch 114:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-1.91, valid_loss=-1.67]\n","\u001b[36m(_train_tune pid=21455)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21455)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 25.85it/s]\u001b[A\n","Epoch 114:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.91, valid_loss=-1.67]\n","Epoch 115:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.79, valid_loss=-1.67]\n","Epoch 115: 100%|██████████| 7/7 [00:00\u003c00:00, 31.65it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.76, valid_loss=-1.67]\n","Epoch 116:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.76, valid_loss=-1.67]\n","Epoch 117:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-1.91, valid_loss=-1.67]\n","Epoch 118:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.76, valid_loss=-1.67]\n","Epoch 119:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.83, valid_loss=-1.67]\n","Epoch 120:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.79, valid_loss=-1.67]\n","Epoch 121:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.75, valid_loss=-1.67]\n","Epoch 122:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.88, valid_loss=-1.67]\n","Epoch 123:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.89, valid_loss=-1.67]\n","Epoch 124:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.85, valid_loss=-1.67]\n","Epoch 125:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.93, valid_loss=-1.67]\n","Epoch 126:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.88, valid_loss=-1.67]\n","Epoch 127:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.06, train_loss_epoch=-1.75, valid_loss=-1.67]\n","Epoch 128:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.86, valid_loss=-1.67]\n","\u001b[36m(_train_tune pid=21455)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21455)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.40it/s]\u001b[A\n","Epoch 128:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.86, valid_loss=-1.68]\n","Epoch 129:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.85, valid_loss=-1.68]\n","Epoch 130:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.79, valid_loss=-1.68]\n","Epoch 131:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.84, valid_loss=-1.68]\n","Epoch 132:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.83, valid_loss=-1.68]\n","Epoch 133:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.80, valid_loss=-1.68]\n","Epoch 134:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.49, train_loss_epoch=-1.93, valid_loss=-1.68]\n","Epoch 135:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.36, train_loss_epoch=-1.76, valid_loss=-1.68]\n","Epoch 136:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-1.90, valid_loss=-1.68]\n","Epoch 137:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.84, valid_loss=-1.68]\n","Epoch 138:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.82, valid_loss=-1.68]\n","Epoch 139:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.81, valid_loss=-1.68]\n","Epoch 140:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.79, valid_loss=-1.68]\n","Epoch 141:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-1.87, valid_loss=-1.68]\n","Epoch 142:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.85, valid_loss=-1.68]\n","\u001b[36m(_train_tune pid=21455)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21455)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21455)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.62it/s]\u001b[A\n","Epoch 143:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.86, valid_loss=-1.69]\n","Epoch 144:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.79, valid_loss=-1.69]\n","Epoch 145:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-1.94, valid_loss=-1.69]\n","Epoch 146:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.81, valid_loss=-1.69]\n","Epoch 147:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-1.86, valid_loss=-1.69]\n","Epoch 148:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-1.91, valid_loss=-1.69]\n","Epoch 149:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.85, valid_loss=-1.69]\n","Epoch 150:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.72, valid_loss=-1.69]\n","Epoch 151:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.86, valid_loss=-1.69]\n","Epoch 152:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-1.94, valid_loss=-1.69]\n","Epoch 153:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.76, valid_loss=-1.69]\n","Epoch 154:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.78, valid_loss=-1.69]\n","Epoch 155:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.89, valid_loss=-1.69]\n","Epoch 156:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.79, valid_loss=-1.69]\n","Epoch 157:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-1.93, valid_loss=-1.69]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21455)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 27.10it/s]\u001b[A\n","Epoch 157:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.93, valid_loss=-1.68]\n","Epoch 158:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.82, valid_loss=-1.68]\n","Epoch 159:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.85, valid_loss=-1.68]\n","Epoch 160:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.87, valid_loss=-1.68]\n","Epoch 161:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.74, valid_loss=-1.68]\n","Epoch 162:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.80, valid_loss=-1.68]\n","Epoch 163:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-1.95, valid_loss=-1.68]\n","Epoch 164:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.77, valid_loss=-1.68]\n","Epoch 165:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.82, valid_loss=-1.68]\n","Epoch 166:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.94, valid_loss=-1.68]\n","Epoch 167:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.82, valid_loss=-1.68]\n","Epoch 168:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.90, valid_loss=-1.68]\n","Epoch 169:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.78, valid_loss=-1.68]\n","Epoch 170:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.45, train_loss_epoch=-1.91, valid_loss=-1.68]\n","Epoch 171:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.82, valid_loss=-1.68]\n","\u001b[36m(_train_tune pid=21455)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 19:17:31,417\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('MA(2,9)', 'MA(3,9)', 'MA(1,12)', 'MA(2,12)', 'MA(3,12)', 'MOM(9)', 'MOM(12)', 'RSI(7)', 'RSI(14)', 'EMA(3,9)', 'EMA(5,9)', 'EMA(5,12)', 'DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (40, 20, 1), 'n_pool_kernel_size': (16, 8, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=21455)\u001b[0m \n","\u001b[36m(_train_tune pid=21455)\u001b[0m \rValidation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21455)\u001b[0m \rValidation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=21455)\u001b[0m `Trainer.fit` stopped: `max_steps=1200.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=21455)\u001b[0m \n","\u001b[36m(_train_tune pid=21455)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 28.08it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21455)\u001b[0m \r                                                                      \u001b[A\rEpoch 171:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.82, valid_loss=-1.68]\rEpoch 171:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.84, valid_loss=-1.68]\rEpoch 171:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.84, valid_loss=-1.68]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=21747)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=21747)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=21747)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=21747)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=21747)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=21747)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=21747)\u001b[0m 2024-11-12 19:17:39.447980: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=21747)\u001b[0m 2024-11-12 19:17:39.468226: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=21747)\u001b[0m 2024-11-12 19:17:39.494544: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=21747)\u001b[0m 2024-11-12 19:17:39.502897: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=21747)\u001b[0m 2024-11-12 19:17:39.521085: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=21747)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=21747)\u001b[0m 2024-11-12 19:17:40.686231: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=21747)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=21747)\u001b[0m \n","\u001b[36m(_train_tune pid=21747)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=21747)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=21747)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=21747)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=21747)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=21747)\u001b[0m 3 | blocks       | ModuleList       | 2.7 M  | train\n","\u001b[36m(_train_tune pid=21747)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=21747)\u001b[0m 2.7 M     Trainable params\n","\u001b[36m(_train_tune pid=21747)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=21747)\u001b[0m 2.7 M     Total params\n","\u001b[36m(_train_tune pid=21747)\u001b[0m 10.927    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=21747)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=21747)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/7 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.310, train_loss_epoch=26.40]\n","Epoch 2:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.470, train_loss_epoch=7.060]\n","Epoch 3:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.140, train_loss_epoch=6.310]\n","Epoch 4:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.020, train_loss_epoch=6.080]\n","Epoch 5:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.690, train_loss_epoch=5.820]\n","Epoch 6:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.110, train_loss_epoch=5.780]\n","Epoch 7:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.420, train_loss_epoch=5.600]\n","Epoch 8:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.370, train_loss_epoch=5.750]\n","Epoch 9:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.110, train_loss_epoch=5.630]\n","Epoch 10:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.220, train_loss_epoch=5.780]\n","Epoch 11:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.480, train_loss_epoch=5.530]\n","Epoch 12:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.830, train_loss_epoch=5.650]\n","Epoch 13:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.630, train_loss_epoch=5.600]\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.630, train_loss_epoch=5.340]\n","\u001b[36m(_train_tune pid=21747)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21747)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 28.67it/s]\u001b[A\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.500, train_loss_epoch=5.340, valid_loss=5.370]\n","Epoch 15:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.590, train_loss_epoch=5.530, valid_loss=5.370]\n","Epoch 16:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.600, train_loss_epoch=5.560, valid_loss=5.370]\n","Epoch 17:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.640, train_loss_epoch=5.400, valid_loss=5.370]\n","Epoch 18:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.700, train_loss_epoch=5.390, valid_loss=5.370]\n","Epoch 19:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.940, train_loss_epoch=5.380, valid_loss=5.370]\n","Epoch 20:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.000, train_loss_epoch=5.410, valid_loss=5.370]\n","Epoch 21:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.680, train_loss_epoch=5.320, valid_loss=5.370]\n","Epoch 22:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.770, train_loss_epoch=5.540, valid_loss=5.370]\n","Epoch 23:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.870, train_loss_epoch=5.500, valid_loss=5.370]\n","Epoch 24:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.670, train_loss_epoch=5.140, valid_loss=5.370]\n","Epoch 25:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.140, train_loss_epoch=5.170, valid_loss=5.370]\n","Epoch 26:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.470, train_loss_epoch=5.140, valid_loss=5.370]\n","Epoch 27:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.850, train_loss_epoch=5.470, valid_loss=5.370]\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.440, train_loss_epoch=5.570, valid_loss=5.370]\n","\u001b[36m(_train_tune pid=21747)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21747)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 29.89it/s]\u001b[A\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.330, train_loss_epoch=5.570, valid_loss=5.250]\n","Epoch 29:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.720, train_loss_epoch=5.570, valid_loss=5.250]\n","Epoch 30:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.570, train_loss_epoch=5.270, valid_loss=5.250]\n","Epoch 31:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.170, train_loss_epoch=5.310, valid_loss=5.250]\n","Epoch 32:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.790, train_loss_epoch=5.370, valid_loss=5.250]\n","Epoch 33:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.860, train_loss_epoch=5.250, valid_loss=5.250]\n","Epoch 34:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.010, train_loss_epoch=6.090, valid_loss=5.250]\n","Epoch 35:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.180, train_loss_epoch=5.730, valid_loss=5.250]\n","Epoch 35: 100%|██████████| 7/7 [00:00\u003c00:00, 31.42it/s, v_num=0, train_loss_step=5.730, train_loss_epoch=5.770, valid_loss=5.250]\n","Epoch 36:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.730, train_loss_epoch=5.770, valid_loss=5.250]\n","Epoch 37:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.180, train_loss_epoch=5.780, valid_loss=5.250]\n","Epoch 38:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.670, train_loss_epoch=5.740, valid_loss=5.250]\n","Epoch 39:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.090, train_loss_epoch=5.580, valid_loss=5.250]\n","Epoch 40:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.990, train_loss_epoch=5.390, valid_loss=5.250]\n","Epoch 41:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.990, train_loss_epoch=5.500, valid_loss=5.250]\n","Epoch 42:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.250, train_loss_epoch=5.370, valid_loss=5.250]\n","\u001b[36m(_train_tune pid=21747)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21747)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 27.22it/s]\u001b[A\n","Epoch 42:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.400, train_loss_epoch=5.370, valid_loss=5.180]\n","Epoch 43:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.370, train_loss_epoch=5.350, valid_loss=5.180]\n","Epoch 44:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.510, train_loss_epoch=5.500, valid_loss=5.180]\n","Epoch 45:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.930, train_loss_epoch=5.180, valid_loss=5.180]\n","Epoch 46:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.590, train_loss_epoch=5.240, valid_loss=5.180]\n","Epoch 47:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.220, train_loss_epoch=5.310, valid_loss=5.180]\n","Epoch 48:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.790, train_loss_epoch=5.430, valid_loss=5.180]\n","Epoch 49:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.990, train_loss_epoch=5.340, valid_loss=5.180]\n","Epoch 50:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.110, train_loss_epoch=5.470, valid_loss=5.180]\n","Epoch 51:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.050, train_loss_epoch=5.450, valid_loss=5.180]\n","Epoch 52:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.530, train_loss_epoch=5.600, valid_loss=5.180]\n","Epoch 53:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.810, train_loss_epoch=5.490, valid_loss=5.180]\n","Epoch 54:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.810, train_loss_epoch=5.660, valid_loss=5.180]\n","Epoch 55:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.540, train_loss_epoch=5.590, valid_loss=5.180]\n","Epoch 55: 100%|██████████| 7/7 [00:00\u003c00:00, 31.99it/s, v_num=0, train_loss_step=5.120, train_loss_epoch=5.590, valid_loss=5.180]\n","Epoch 56:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.120, train_loss_epoch=5.320, valid_loss=5.180]\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.990, train_loss_epoch=5.230, valid_loss=5.180]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21747)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 26.78it/s]\u001b[A\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.200, train_loss_epoch=5.230, valid_loss=5.050]\n","Epoch 58:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.660, train_loss_epoch=5.220, valid_loss=5.050]\n","Epoch 59:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.030, train_loss_epoch=5.210, valid_loss=5.050]\n","Epoch 60:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.630, train_loss_epoch=5.090, valid_loss=5.050]\n","Epoch 61:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.160, train_loss_epoch=5.180, valid_loss=5.050]\n","Epoch 62:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.960, train_loss_epoch=5.080, valid_loss=5.050]\n","Epoch 63:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.950, train_loss_epoch=5.040, valid_loss=5.050]\n","Epoch 64:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.670, train_loss_epoch=5.100, valid_loss=5.050]\n","Epoch 65:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.740, train_loss_epoch=4.880, valid_loss=5.050]\n","Epoch 66:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.990, train_loss_epoch=5.110, valid_loss=5.050]\n","Epoch 67:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.730, train_loss_epoch=5.110, valid_loss=5.050]\n","Epoch 68:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.120, train_loss_epoch=4.890, valid_loss=5.050]\n","Epoch 69:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.400, train_loss_epoch=5.100, valid_loss=5.050]\n","Epoch 70:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.320, train_loss_epoch=5.640, valid_loss=5.050]\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.400, train_loss_epoch=5.400, valid_loss=5.050]\n","\u001b[36m(_train_tune pid=21747)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21747)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 29.88it/s]\u001b[A\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.360, train_loss_epoch=5.400, valid_loss=5.130]\n","Epoch 72:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.530, train_loss_epoch=5.570, valid_loss=5.130]\n","Epoch 73:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.350, train_loss_epoch=5.340, valid_loss=5.130]\n","Epoch 74:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.800, train_loss_epoch=5.110, valid_loss=5.130]\n","Epoch 75:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.870, train_loss_epoch=4.970, valid_loss=5.130]\n","Epoch 76:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.330, train_loss_epoch=5.120, valid_loss=5.130]\n","Epoch 77:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.550, train_loss_epoch=5.060, valid_loss=5.130]\n","Epoch 78:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.230, train_loss_epoch=4.940, valid_loss=5.130]\n","Epoch 79:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.490, train_loss_epoch=5.200, valid_loss=5.130]\n","Epoch 80:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.750, train_loss_epoch=5.270, valid_loss=5.130]\n","Epoch 81:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.300, train_loss_epoch=4.820, valid_loss=5.130]\n","Epoch 82:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.890, train_loss_epoch=4.990, valid_loss=5.130]\n","Epoch 83:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.550, train_loss_epoch=5.260, valid_loss=5.130]\n","Epoch 84:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.950, train_loss_epoch=5.060, valid_loss=5.130]\n","Epoch 85:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.500, train_loss_epoch=5.040, valid_loss=5.130]\n","\u001b[36m(_train_tune pid=21747)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21747)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 27.71it/s]\u001b[A\n","Epoch 86:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.710, train_loss_epoch=5.070, valid_loss=4.860]\n","Epoch 87:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.100, train_loss_epoch=5.140, valid_loss=4.860]\n","Epoch 88:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.470, train_loss_epoch=4.900, valid_loss=4.860]\n","Epoch 89:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.570, train_loss_epoch=4.870, valid_loss=4.860]\n","Epoch 90:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.160, train_loss_epoch=5.210, valid_loss=4.860]\n","Epoch 91:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.760, train_loss_epoch=5.010, valid_loss=4.860]\n","Epoch 92:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.900, train_loss_epoch=4.940, valid_loss=4.860]\n","Epoch 93:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.860, train_loss_epoch=4.940, valid_loss=4.860]\n","Epoch 94:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.580, train_loss_epoch=4.870, valid_loss=4.860]\n","Epoch 95:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.860, train_loss_epoch=4.820, valid_loss=4.860]\n","Epoch 96:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.140, train_loss_epoch=4.910, valid_loss=4.860]\n","Epoch 97:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.420, train_loss_epoch=4.690, valid_loss=4.860]\n","Epoch 98:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.160, train_loss_epoch=4.700, valid_loss=4.860]\n","Epoch 99:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.350, train_loss_epoch=4.760, valid_loss=4.860]\n","Epoch 99: 100%|██████████| 7/7 [00:00\u003c00:00, 30.65it/s, v_num=0, train_loss_step=4.200, train_loss_epoch=4.760, valid_loss=4.860]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21747)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 30.72it/s]\u001b[A\n","Epoch 100:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.200, train_loss_epoch=4.710, valid_loss=4.690]\n","Epoch 101:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.580, train_loss_epoch=4.770, valid_loss=4.690]\n","Epoch 102:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.470, train_loss_epoch=5.350, valid_loss=4.690]\n","Epoch 103:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.790, train_loss_epoch=5.030, valid_loss=4.690]\n","Epoch 104:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.570, train_loss_epoch=4.970, valid_loss=4.690]\n","Epoch 105:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.750, train_loss_epoch=4.960, valid_loss=4.690]\n","Epoch 106:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.430, train_loss_epoch=5.030, valid_loss=4.690]\n","Epoch 107:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.930, train_loss_epoch=4.690, valid_loss=4.690]\n","Epoch 108:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.410, train_loss_epoch=4.880, valid_loss=4.690]\n","Epoch 109:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.240, train_loss_epoch=4.950, valid_loss=4.690]\n","Epoch 110:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.060, train_loss_epoch=4.880, valid_loss=4.690]\n","Epoch 111:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.740, train_loss_epoch=4.710, valid_loss=4.690]\n","Epoch 112:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.410, train_loss_epoch=4.880, valid_loss=4.690]\n","Epoch 113:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.110, train_loss_epoch=4.870, valid_loss=4.690]\n","Epoch 114:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.420, train_loss_epoch=4.870, valid_loss=4.690]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21747)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 19:18:10,733\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('MA(2,9)', 'MA(3,9)', 'MA(1,12)', 'MA(2,12)', 'MA(3,12)', 'MOM(9)', 'MOM(12)', 'RSI(7)', 'RSI(14)', 'EMA(3,9)', 'EMA(5,9)', 'EMA(5,12)', 'DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (1, 1, 1), 'n_pool_kernel_size': (16, 8, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=21747)\u001b[0m `Trainer.fit` stopped: `max_steps=800.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=21747)\u001b[0m \n","\u001b[36m(_train_tune pid=21747)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 27.00it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21747)\u001b[0m \r                                                                      \u001b[A\rEpoch 114:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.720, train_loss_epoch=4.870, valid_loss=4.680]\rEpoch 114:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.720, train_loss_epoch=4.810, valid_loss=4.680]\rEpoch 114:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.720, train_loss_epoch=4.810, valid_loss=4.680]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=21972)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=21972)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=21972)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=21972)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=21972)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=21972)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=21972)\u001b[0m 2024-11-12 19:18:18.511203: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=21972)\u001b[0m 2024-11-12 19:18:18.531263: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=21972)\u001b[0m 2024-11-12 19:18:18.556005: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=21972)\u001b[0m 2024-11-12 19:18:18.563921: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=21972)\u001b[0m 2024-11-12 19:18:18.581455: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=21972)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=21972)\u001b[0m 2024-11-12 19:18:19.752797: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=21972)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=21972)\u001b[0m \n","\u001b[36m(_train_tune pid=21972)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=21972)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=21972)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=21972)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=21972)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=21972)\u001b[0m 3 | blocks       | ModuleList       | 3.7 M  | train\n","\u001b[36m(_train_tune pid=21972)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=21972)\u001b[0m 3.7 M     Trainable params\n","\u001b[36m(_train_tune pid=21972)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=21972)\u001b[0m 3.7 M     Total params\n","\u001b[36m(_train_tune pid=21972)\u001b[0m 14.882    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=21972)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=21972)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/7 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.47]\n","Epoch 2:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-1.71]\n","Epoch 3:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.75]\n","Epoch 4:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.75]\n","Epoch 5:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.82]\n","Epoch 6:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.78]\n","Epoch 7:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.86]\n","Epoch 8:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.82]\n","Epoch 9:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-1.92]\n","Epoch 10:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.83]\n","Epoch 11:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-1.96]\n","Epoch 12:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.87]\n","Epoch 13:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.89]\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.89]\n","\u001b[36m(_train_tune pid=21972)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21972)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.51it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21972)\u001b[0m \n","Epoch 14:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.89, valid_loss=-1.91]\n","Epoch 15:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.90, valid_loss=-1.91]\n","Epoch 16:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-2.01, valid_loss=-1.91]\n","Epoch 17:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.91, valid_loss=-1.91]\n","Epoch 18:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.96, valid_loss=-1.91]\n","Epoch 18: 100%|██████████| 7/7 [00:00\u003c00:00, 21.35it/s, v_num=0, train_loss_step=-2.48, train_loss_epoch=-2.03, valid_loss=-1.91]\n","Epoch 19:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.48, train_loss_epoch=-2.03, valid_loss=-1.91]\n","Epoch 20:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.93, valid_loss=-1.91]\n","Epoch 21:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.97, valid_loss=-1.91]\n","Epoch 22:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.96, valid_loss=-1.91]\n","Epoch 23:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.95, valid_loss=-1.91]\n","Epoch 24:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.97, valid_loss=-1.91]\n","Epoch 25:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.02, valid_loss=-1.91]\n","Epoch 26:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.01, valid_loss=-1.91]\n","Epoch 27:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.00, valid_loss=-1.91]\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.98, valid_loss=-1.91]\n","\u001b[36m(_train_tune pid=21972)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21972)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21972)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.52it/s]\u001b[A\n","Epoch 28:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.98, valid_loss=-1.98]\n","Epoch 29:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.89, valid_loss=-1.98]\n","Epoch 30:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-2.03, valid_loss=-1.98]\n","Epoch 31:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.96, valid_loss=-1.98]\n","Epoch 32:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.92, valid_loss=-1.98]\n","Epoch 33:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.00, valid_loss=-1.98]\n","Epoch 34:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.97, valid_loss=-1.98]\n","Epoch 35:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.02, valid_loss=-1.98]\n","Epoch 36:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.00, valid_loss=-1.98]\n","Epoch 37:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.59, train_loss_epoch=-2.09, valid_loss=-1.98]\n","Epoch 38:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.00, valid_loss=-1.98]\n","Epoch 39:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-2.01, valid_loss=-1.98]\n","Epoch 40:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.02, valid_loss=-1.98]\n","Epoch 41:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.54, train_loss_epoch=-2.09, valid_loss=-1.98]\n","Epoch 42:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.04, valid_loss=-1.98]\n","\u001b[36m(_train_tune pid=21972)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21972)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21972)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.51it/s]\u001b[A\n","Epoch 43:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.99, valid_loss=-1.99]\n","Epoch 44:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 45:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.06, valid_loss=-1.99]\n","Epoch 46:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 47:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-2.10, valid_loss=-1.99]\n","Epoch 48:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.99, valid_loss=-1.99]\n","Epoch 49:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.05, valid_loss=-1.99]\n","Epoch 50:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.99, valid_loss=-1.99]\n","Epoch 51:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.04, valid_loss=-1.99]\n","Epoch 52:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 53:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.99, valid_loss=-1.99]\n","Epoch 54:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 55:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-2.07, valid_loss=-1.99]\n","Epoch 56:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.05, valid_loss=-1.99]\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.95, valid_loss=-1.99]\n","\u001b[36m(_train_tune pid=21972)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21972)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.52it/s]\u001b[A\n","Epoch 57:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.95, valid_loss=-2.02]\n","Epoch 58:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.08, valid_loss=-2.02]\n","Epoch 59:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.03, valid_loss=-2.02]\n","Epoch 60:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.08, valid_loss=-2.02]\n","Epoch 61:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.01, valid_loss=-2.02]\n","Epoch 62:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.09, valid_loss=-2.02]\n","Epoch 63:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.06, valid_loss=-2.02]\n","Epoch 64:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-2.03, valid_loss=-2.02]\n","Epoch 65:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.05, valid_loss=-2.02]\n","Epoch 66:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.05, valid_loss=-2.02]\n","Epoch 67:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.09, valid_loss=-2.02]\n","Epoch 68:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.05, valid_loss=-2.02]\n","Epoch 69:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-2.01, valid_loss=-2.02]\n","Epoch 70:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.04, valid_loss=-2.02]\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.94, valid_loss=-2.02]\n","\u001b[36m(_train_tune pid=21972)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21972)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.54it/s]\u001b[A\n","Epoch 71:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.94, valid_loss=-2.02]\n","Epoch 72:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.06, valid_loss=-2.02]\n","Epoch 73:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.09, valid_loss=-2.02]\n","Epoch 74:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.44, train_loss_epoch=-2.13, valid_loss=-2.02]\n","Epoch 75:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.10, valid_loss=-2.02]\n","Epoch 76:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.08, valid_loss=-2.02]\n","Epoch 77:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.04, valid_loss=-2.02]\n","Epoch 78:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.41, train_loss_epoch=-2.12, valid_loss=-2.02]\n","Epoch 79:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.08, valid_loss=-2.02]\n","Epoch 80:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.09, valid_loss=-2.02]\n","Epoch 81:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-2.04, valid_loss=-2.02]\n","Epoch 82:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.08, valid_loss=-2.02]\n","Epoch 83:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.96, valid_loss=-2.02]\n","Epoch 84:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.04, valid_loss=-2.02]\n","Epoch 85:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.07, valid_loss=-2.02]\n","\u001b[36m(_train_tune pid=21972)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21972)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.51it/s]\u001b[A\n","Epoch 85:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.07, valid_loss=-2.04]\n","Epoch 86:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.08, valid_loss=-2.04]\n","Epoch 87:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.60, train_loss_epoch=-2.16, valid_loss=-2.04]\n","Epoch 88:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.13, valid_loss=-2.04]\n","Epoch 89:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.09, valid_loss=-2.04]\n","Epoch 90:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.10, valid_loss=-2.04]\n","Epoch 91:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-2.12, valid_loss=-2.04]\n","Epoch 92:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.06, valid_loss=-2.04]\n","Epoch 93:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.12, valid_loss=-2.04]\n","Epoch 94:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.10, valid_loss=-2.04]\n","Epoch 95:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-2.07, valid_loss=-2.04]\n","Epoch 96:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.46, train_loss_epoch=-2.14, valid_loss=-2.04]\n","Epoch 97:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.11, valid_loss=-2.04]\n","Epoch 98:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-2.13, valid_loss=-2.04]\n","Epoch 99:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.10, valid_loss=-2.04]\n","Epoch 99: 100%|██████████| 7/7 [00:00\u003c00:00, 20.35it/s, v_num=0, train_loss_step=-2.46, train_loss_epoch=-2.10, valid_loss=-2.04]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21972)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21972)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.51it/s]\u001b[A\n","Epoch 100:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.46, train_loss_epoch=-2.15, valid_loss=-2.04]\n","Epoch 101:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.12, valid_loss=-2.04]\n","Epoch 102:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-2.04, valid_loss=-2.04]\n","Epoch 103:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.10, valid_loss=-2.04]\n","Epoch 103: 100%|██████████| 7/7 [00:00\u003c00:00, 20.58it/s, v_num=0, train_loss_step=-2.48, train_loss_epoch=-2.17, valid_loss=-2.04]\n","Epoch 104:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.48, train_loss_epoch=-2.17, valid_loss=-2.04]\n","Epoch 105:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.08, valid_loss=-2.04]\n","Epoch 106:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-2.03, valid_loss=-2.04]\n","Epoch 107:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.12, valid_loss=-2.04]\n","Epoch 108:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-2.15, valid_loss=-2.04]\n","Epoch 108:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.09, valid_loss=-2.04]        \n","Epoch 109:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.09, valid_loss=-2.04]\n","Epoch 110:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-2.03, valid_loss=-2.04]\n","Epoch 111:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-2.15, valid_loss=-2.04]\n","Epoch 112:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.11, valid_loss=-2.04]\n","Epoch 113:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.12, valid_loss=-2.04]\n","Epoch 114:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.63, train_loss_epoch=-2.21, valid_loss=-2.04]\n","\u001b[36m(_train_tune pid=21972)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21972)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.55it/s]\u001b[A\n","Epoch 114:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.21, valid_loss=-2.04]\n","Epoch 115:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.07, valid_loss=-2.04]\n","Epoch 116:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-2.02, valid_loss=-2.04]\n","Epoch 117:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.47, train_loss_epoch=-2.17, valid_loss=-2.04]\n","Epoch 118:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-2.07, valid_loss=-2.04]\n","Epoch 119:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.13, valid_loss=-2.04]\n","Epoch 120:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.10, valid_loss=-2.04]\n","Epoch 121:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.09, valid_loss=-2.04]\n","Epoch 122:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.12, valid_loss=-2.04]\n","Epoch 123:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.13, valid_loss=-2.04]\n","Epoch 124:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.15, valid_loss=-2.04]\n","Epoch 125:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-2.15, valid_loss=-2.04]\n","Epoch 126:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.16, valid_loss=-2.04]\n","Epoch 127:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-2.04, valid_loss=-2.04]\n","Epoch 128:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.11, valid_loss=-2.04]\n","\u001b[36m(_train_tune pid=21972)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21972)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 19:19:24,557\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('MA(2,9)', 'MA(3,9)', 'MA(1,12)', 'MA(2,12)', 'MA(3,12)', 'MOM(9)', 'MOM(12)', 'RSI(7)', 'RSI(14)', 'EMA(3,9)', 'EMA(5,9)', 'EMA(5,12)', 'DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (168, 24, 1), 'n_pool_kernel_size': (16, 8, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=21972)\u001b[0m `Trainer.fit` stopped: `max_steps=900.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=21972)\u001b[0m \n","\u001b[36m(_train_tune pid=21972)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.54it/s]\u001b[A\n","\u001b[36m(_train_tune pid=21972)\u001b[0m \r                                                                      \u001b[A\rEpoch 128:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.11, valid_loss=-2.05]\rEpoch 128:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.14, valid_loss=-2.05]\rEpoch 128:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.14, valid_loss=-2.05]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=22342)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=22342)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=22342)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=22342)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=22342)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=22342)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=22342)\u001b[0m 2024-11-12 19:19:32.667907: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=22342)\u001b[0m 2024-11-12 19:19:32.687462: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=22342)\u001b[0m 2024-11-12 19:19:32.711770: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=22342)\u001b[0m 2024-11-12 19:19:32.719282: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=22342)\u001b[0m 2024-11-12 19:19:32.736762: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=22342)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=22342)\u001b[0m 2024-11-12 19:19:33.909373: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=22342)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=22342)\u001b[0m \n","\u001b[36m(_train_tune pid=22342)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=22342)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=22342)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=22342)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=22342)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=22342)\u001b[0m 3 | blocks       | ModuleList       | 2.8 M  | train\n","\u001b[36m(_train_tune pid=22342)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=22342)\u001b[0m 2.8 M     Trainable params\n","\u001b[36m(_train_tune pid=22342)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=22342)\u001b[0m 2.8 M     Total params\n","\u001b[36m(_train_tune pid=22342)\u001b[0m 11.011    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=22342)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=22342)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/4 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.370, train_loss_epoch=79.80]\n","Epoch 2:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.810, train_loss_epoch=9.050]\n","Epoch 3:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.940, train_loss_epoch=9.930]\n","Epoch 4:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.200, train_loss_epoch=9.850]\n","Epoch 5:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.630, train_loss_epoch=9.920]\n","Epoch 6:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.650, train_loss_epoch=9.600]\n","Epoch 7:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.470, train_loss_epoch=9.300]\n","Epoch 8:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.680, train_loss_epoch=9.890]\n","Epoch 9:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.410, train_loss_epoch=9.870]\n","Epoch 10:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.980, train_loss_epoch=9.830]\n","Epoch 11:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.070, train_loss_epoch=9.270]\n","Epoch 12:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.080, train_loss_epoch=8.860]\n","Epoch 13:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.960, train_loss_epoch=8.800]\n","Epoch 14:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.690, train_loss_epoch=8.560]\n","Epoch 15:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.600, train_loss_epoch=8.760]\n","Epoch 16:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.400, train_loss_epoch=9.110]\n","Epoch 17:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.270, train_loss_epoch=9.790]\n","Epoch 18:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.420, train_loss_epoch=10.10]\n","Epoch 19:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.310, train_loss_epoch=10.00]\n","Epoch 20:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=14.60, train_loss_epoch=11.30]\n","Epoch 21:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.450, train_loss_epoch=9.900]\n","Epoch 22:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.100, train_loss_epoch=9.120]\n","Epoch 23:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.590, train_loss_epoch=8.900]\n","Epoch 24:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.860, train_loss_epoch=9.330]\n","Epoch 24: 100%|██████████| 4/4 [00:00\u003c00:00, 11.41it/s, v_num=0, train_loss_step=12.50, train_loss_epoch=9.330]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22342)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22342)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.03it/s]\u001b[A\n","Epoch 25:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.50, train_loss_epoch=9.680, valid_loss=8.650]\n","Epoch 26:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.00, train_loss_epoch=9.500, valid_loss=8.650]\n","Epoch 27:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.60, train_loss_epoch=10.20, valid_loss=8.650]\n","Epoch 28:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.20, train_loss_epoch=10.50, valid_loss=8.650]\n","Epoch 29:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.50, train_loss_epoch=10.40, valid_loss=8.650]\n","Epoch 30:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.770, train_loss_epoch=10.10, valid_loss=8.650]\n","Epoch 31:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.750, train_loss_epoch=10.10, valid_loss=8.650]\n","Epoch 32:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.90, train_loss_epoch=10.50, valid_loss=8.650]\n","Epoch 33:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.00, train_loss_epoch=9.740, valid_loss=8.650]\n","Epoch 34:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.520, train_loss_epoch=9.000, valid_loss=8.650]\n","Epoch 35:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.510, train_loss_epoch=8.430, valid_loss=8.650]\n","Epoch 36:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.900, train_loss_epoch=8.910, valid_loss=8.650]\n","Epoch 37:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.710, train_loss_epoch=9.180, valid_loss=8.650]\n","Epoch 38:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.680, train_loss_epoch=9.120, valid_loss=8.650]\n","Epoch 39:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.400, train_loss_epoch=9.150, valid_loss=8.650]\n","Epoch 40:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.520, train_loss_epoch=8.590, valid_loss=8.650]\n","Epoch 41:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.890, train_loss_epoch=9.380, valid_loss=8.650]\n","Epoch 42:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.890, train_loss_epoch=9.340, valid_loss=8.650]\n","Epoch 43:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.420, train_loss_epoch=9.260, valid_loss=8.650]\n","Epoch 44:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.000, train_loss_epoch=9.330, valid_loss=8.650]\n","Epoch 45:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.260, train_loss_epoch=8.830, valid_loss=8.650]\n","Epoch 46:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.690, train_loss_epoch=8.580, valid_loss=8.650]\n","Epoch 47:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.660, train_loss_epoch=9.160, valid_loss=8.650]\n","Epoch 48:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.320, train_loss_epoch=9.360, valid_loss=8.650]\n","Epoch 49:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.760, train_loss_epoch=8.910, valid_loss=8.650]\n","Epoch 49: 100%|██████████| 4/4 [00:00\u003c00:00, 11.24it/s, v_num=0, train_loss_step=8.100, train_loss_epoch=8.910, valid_loss=8.650]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22342)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22342)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.03it/s]\u001b[A\n","Epoch 50:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.100, train_loss_epoch=8.720, valid_loss=8.340]\n","Epoch 51:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.50, train_loss_epoch=9.190, valid_loss=8.340]\n","Epoch 52:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.50, train_loss_epoch=9.380, valid_loss=8.340]\n","Epoch 53:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.440, train_loss_epoch=8.730, valid_loss=8.340]\n","Epoch 54:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.720, train_loss_epoch=8.280, valid_loss=8.340]\n","Epoch 55:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.910, train_loss_epoch=8.660, valid_loss=8.340]\n","Epoch 56:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.950, train_loss_epoch=8.560, valid_loss=8.340]\n","Epoch 57:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.170, train_loss_epoch=9.390, valid_loss=8.340]\n","Epoch 58:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.820, train_loss_epoch=9.810, valid_loss=8.340]\n","Epoch 59:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.00, train_loss_epoch=9.970, valid_loss=8.340]\n","Epoch 60:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.420, train_loss_epoch=9.890, valid_loss=8.340]\n","Epoch 61:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.480, train_loss_epoch=9.840, valid_loss=8.340]\n","Epoch 62:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.740, train_loss_epoch=9.860, valid_loss=8.340]\n","Epoch 63:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.530, train_loss_epoch=9.670, valid_loss=8.340]\n","Epoch 64:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.380, train_loss_epoch=9.570, valid_loss=8.340]\n","Epoch 65:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.910, train_loss_epoch=9.550, valid_loss=8.340]\n","Epoch 66:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.760, train_loss_epoch=9.010, valid_loss=8.340]\n","Epoch 67:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.600, train_loss_epoch=9.020, valid_loss=8.340]\n","Epoch 68:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.230, train_loss_epoch=8.670, valid_loss=8.340]\n","Epoch 69:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.820, train_loss_epoch=8.280, valid_loss=8.340]\n","Epoch 69: 100%|██████████| 4/4 [00:00\u003c00:00, 11.32it/s, v_num=0, train_loss_step=8.060, train_loss_epoch=7.910, valid_loss=8.340]\n","Epoch 70:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.060, train_loss_epoch=7.910, valid_loss=8.340]\n","Epoch 71:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.10, train_loss_epoch=8.540, valid_loss=8.340]\n","Epoch 72:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.030, train_loss_epoch=8.980, valid_loss=8.340]\n","Epoch 73:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.40, train_loss_epoch=9.540, valid_loss=8.340]\n","Epoch 74:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.570, train_loss_epoch=9.390, valid_loss=8.340]\n","Epoch 74: 100%|██████████| 4/4 [00:00\u003c00:00, 11.41it/s, v_num=0, train_loss_step=8.820, train_loss_epoch=9.390, valid_loss=8.340]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22342)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22342)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.03it/s]\u001b[A\n","Epoch 75:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.820, train_loss_epoch=9.060, valid_loss=8.680]\n","Epoch 76:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.260, train_loss_epoch=8.600, valid_loss=8.680]\n","Epoch 77:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.570, train_loss_epoch=8.420, valid_loss=8.680]\n","Epoch 78:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.160, train_loss_epoch=8.150, valid_loss=8.680]\n","Epoch 79:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.160, train_loss_epoch=7.690, valid_loss=8.680]\n","Epoch 80:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.480, train_loss_epoch=8.120, valid_loss=8.680]\n","Epoch 81:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.50, train_loss_epoch=9.010, valid_loss=8.680]\n","Epoch 82:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.340, train_loss_epoch=8.810, valid_loss=8.680]\n","Epoch 83:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.810, train_loss_epoch=9.210, valid_loss=8.680]\n","Epoch 84:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.500, train_loss_epoch=8.860, valid_loss=8.680]\n","Epoch 85:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.480, train_loss_epoch=8.900, valid_loss=8.680]\n","Epoch 85: 100%|██████████| 4/4 [00:00\u003c00:00, 11.58it/s, v_num=0, train_loss_step=8.170, train_loss_epoch=8.900, valid_loss=8.680]\n","Epoch 86:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.170, train_loss_epoch=8.470, valid_loss=8.680]\n","Epoch 87:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.860, train_loss_epoch=8.360, valid_loss=8.680]\n","Epoch 88:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.000, train_loss_epoch=8.660, valid_loss=8.680]\n","Epoch 89:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.180, train_loss_epoch=9.020, valid_loss=8.680]\n","Epoch 90:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.00, train_loss_epoch=9.570, valid_loss=8.680]\n","Epoch 91:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.540, train_loss_epoch=9.600, valid_loss=8.680]\n","Epoch 92:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.10, train_loss_epoch=9.680, valid_loss=8.680]\n","Epoch 93:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.710, train_loss_epoch=9.630, valid_loss=8.680]\n","Epoch 94:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.240, train_loss_epoch=9.400, valid_loss=8.680]\n","Epoch 95:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.250, train_loss_epoch=9.270, valid_loss=8.680]\n","Epoch 96:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.390, train_loss_epoch=9.210, valid_loss=8.680]\n","Epoch 97:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.970, train_loss_epoch=8.900, valid_loss=8.680]\n","Epoch 98:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.420, train_loss_epoch=8.590, valid_loss=8.680]\n","Epoch 99:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.360, train_loss_epoch=8.220, valid_loss=8.680]\n","Epoch 99: 100%|██████████| 4/4 [00:00\u003c00:00, 11.38it/s, v_num=0, train_loss_step=7.150, train_loss_epoch=8.220, valid_loss=8.680]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22342)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22342)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.02it/s]\u001b[A\n","Epoch 100:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.150, train_loss_epoch=7.600, valid_loss=7.420]\n","Epoch 101:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.600, train_loss_epoch=7.060, valid_loss=7.420]\n","Epoch 102:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.680, train_loss_epoch=7.030, valid_loss=7.420]\n","Epoch 103:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.100, train_loss_epoch=7.610, valid_loss=7.420]\n","Epoch 104:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.20, train_loss_epoch=8.550, valid_loss=7.420]\n","Epoch 105:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.640, train_loss_epoch=8.280, valid_loss=7.420]\n","Epoch 106:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.580, train_loss_epoch=8.370, valid_loss=7.420]\n","Epoch 107:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.140, train_loss_epoch=7.560, valid_loss=7.420]\n","Epoch 108:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.440, train_loss_epoch=7.280, valid_loss=7.420]\n","Epoch 109:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.310, train_loss_epoch=7.320, valid_loss=7.420]\n","Epoch 110:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.060, train_loss_epoch=7.230, valid_loss=7.420]\n","Epoch 111:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.080, train_loss_epoch=6.560, valid_loss=7.420]\n","Epoch 112:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.880, train_loss_epoch=7.340, valid_loss=7.420]\n","Epoch 113:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.310, train_loss_epoch=8.370, valid_loss=7.420]\n","Epoch 114:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.270, train_loss_epoch=8.550, valid_loss=7.420]\n","Epoch 115:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.580, train_loss_epoch=8.420, valid_loss=7.420]\n","Epoch 116:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.500, train_loss_epoch=8.680, valid_loss=7.420]\n","Epoch 117:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.440, train_loss_epoch=8.340, valid_loss=7.420]\n","Epoch 118:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.740, train_loss_epoch=8.860, valid_loss=7.420]\n","Epoch 119:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.210, train_loss_epoch=8.370, valid_loss=7.420]\n","Epoch 120:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.610, train_loss_epoch=8.350, valid_loss=7.420]\n","Epoch 121:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.120, train_loss_epoch=7.910, valid_loss=7.420]\n","Epoch 122:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.880, train_loss_epoch=7.640, valid_loss=7.420]\n","Epoch 123:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.350, train_loss_epoch=7.180, valid_loss=7.420]\n","Epoch 124:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.370, train_loss_epoch=7.670, valid_loss=7.420]\n","Epoch 124: 100%|██████████| 4/4 [00:00\u003c00:00, 11.45it/s, v_num=0, train_loss_step=7.250, train_loss_epoch=7.670, valid_loss=7.420]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22342)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22342)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.03it/s]\u001b[A\n","Epoch 125:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.250, train_loss_epoch=7.520, valid_loss=7.270]\n","Epoch 125: 100%|██████████| 4/4 [00:00\u003c00:00, 11.72it/s, v_num=0, train_loss_step=7.300, train_loss_epoch=7.520, valid_loss=7.270]\n","Epoch 126:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.300, train_loss_epoch=7.420, valid_loss=7.270]\n","Epoch 127:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.110, train_loss_epoch=7.290, valid_loss=7.270]\n","Epoch 128:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.370, train_loss_epoch=7.180, valid_loss=7.270]\n","Epoch 129:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.630, train_loss_epoch=8.080, valid_loss=7.270]\n","Epoch 130:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.640, train_loss_epoch=8.560, valid_loss=7.270]\n","Epoch 131:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.320, train_loss_epoch=8.560, valid_loss=7.270]\n","Epoch 132:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.530, train_loss_epoch=8.670, valid_loss=7.270]\n","Epoch 133:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.850, train_loss_epoch=8.680, valid_loss=7.270]\n","Epoch 134:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.320, train_loss_epoch=8.750, valid_loss=7.270]\n","Epoch 135:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.210, train_loss_epoch=8.380, valid_loss=7.270]\n","Epoch 136:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.260, train_loss_epoch=7.980, valid_loss=7.270]\n","Epoch 137:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.090, train_loss_epoch=8.310, valid_loss=7.270]\n","Epoch 138:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.720, train_loss_epoch=7.500, valid_loss=7.270]\n","Epoch 139:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.690, train_loss_epoch=7.260, valid_loss=7.270]\n","Epoch 140:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.460, train_loss_epoch=6.860, valid_loss=7.270]\n","Epoch 141:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.570, train_loss_epoch=6.510, valid_loss=7.270]\n","Epoch 142:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.000, train_loss_epoch=6.160, valid_loss=7.270]\n","Epoch 143:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.310, train_loss_epoch=6.070, valid_loss=7.270]\n","Epoch 144:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.810, train_loss_epoch=6.300, valid_loss=7.270]\n","Epoch 145:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.00, train_loss_epoch=9.410, valid_loss=7.270]\n","Epoch 146:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.140, train_loss_epoch=9.000, valid_loss=7.270]\n","Epoch 147:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.990, train_loss_epoch=9.250, valid_loss=7.270]\n","Epoch 148:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.060, train_loss_epoch=9.420, valid_loss=7.270]\n","Epoch 149:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.00, train_loss_epoch=9.630, valid_loss=7.270]\n","Epoch 149: 100%|██████████| 4/4 [00:00\u003c00:00, 11.60it/s, v_num=0, train_loss_step=9.610, train_loss_epoch=9.630, valid_loss=7.270]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22342)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.02it/s]\u001b[A\n","Epoch 150:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.610, train_loss_epoch=9.600, valid_loss=9.370]\n","Epoch 151:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.30, train_loss_epoch=9.760, valid_loss=9.370]\n","Epoch 152:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.270, train_loss_epoch=9.450, valid_loss=9.370]\n","Epoch 153:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.040, train_loss_epoch=9.350, valid_loss=9.370]\n","Epoch 154:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.20, train_loss_epoch=9.550, valid_loss=9.370]\n","Epoch 155:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.30, train_loss_epoch=9.430, valid_loss=9.370]\n","Epoch 156:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.230, train_loss_epoch=8.970, valid_loss=9.370]\n","Epoch 157:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.270, train_loss_epoch=8.610, valid_loss=9.370]\n","Epoch 158:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.330, train_loss_epoch=8.420, valid_loss=9.370]\n","Epoch 159:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.710, train_loss_epoch=7.590, valid_loss=9.370]\n","Epoch 160:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.180, train_loss_epoch=8.250, valid_loss=9.370]\n","Epoch 161:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.520, train_loss_epoch=8.670, valid_loss=9.370]\n","Epoch 162:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.800, train_loss_epoch=9.190, valid_loss=9.370]\n","Epoch 163:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.520, train_loss_epoch=8.980, valid_loss=9.370]\n","Epoch 164:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.910, train_loss_epoch=9.410, valid_loss=9.370]\n","Epoch 165:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.730, train_loss_epoch=9.220, valid_loss=9.370]\n","Epoch 166:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.870, train_loss_epoch=9.110, valid_loss=9.370]\n","Epoch 167:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.770, train_loss_epoch=9.330, valid_loss=9.370]\n","Epoch 168:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.150, train_loss_epoch=9.000, valid_loss=9.370]\n","Epoch 169:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.320, train_loss_epoch=8.610, valid_loss=9.370]\n","Epoch 170:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.040, train_loss_epoch=8.090, valid_loss=9.370]\n","Epoch 171:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.110, train_loss_epoch=8.050, valid_loss=9.370]\n","Epoch 172:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.300, train_loss_epoch=7.960, valid_loss=9.370]\n","Epoch 173:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.740, train_loss_epoch=7.840, valid_loss=9.370]\n","Epoch 174:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.710, train_loss_epoch=7.530, valid_loss=9.370]\n","Epoch 174: 100%|██████████| 4/4 [00:00\u003c00:00, 11.36it/s, v_num=0, train_loss_step=7.160, train_loss_epoch=7.530, valid_loss=9.370]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22342)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22342)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.02it/s]\u001b[A\n","Epoch 175:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.160, train_loss_epoch=7.650, valid_loss=7.450]\n","Epoch 176:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.170, train_loss_epoch=7.570, valid_loss=7.450]\n","Epoch 177:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.570, train_loss_epoch=7.480, valid_loss=7.450]\n","Epoch 178:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.200, train_loss_epoch=7.740, valid_loss=7.450]\n","Epoch 179:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.970, train_loss_epoch=7.640, valid_loss=7.450]\n","Epoch 180:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.510, train_loss_epoch=7.610, valid_loss=7.450]\n","Epoch 181:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.360, train_loss_epoch=8.510, valid_loss=7.450]\n","Epoch 182:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.160, train_loss_epoch=8.780, valid_loss=7.450]\n","Epoch 183:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.660, train_loss_epoch=8.830, valid_loss=7.450]\n","Epoch 184:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.740, train_loss_epoch=9.020, valid_loss=7.450]\n","Epoch 185:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.180, train_loss_epoch=9.110, valid_loss=7.450]\n","Epoch 186:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.050, train_loss_epoch=9.070, valid_loss=7.450]\n","Epoch 187:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.500, train_loss_epoch=9.160, valid_loss=7.450]\n","Epoch 188:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.040, train_loss_epoch=9.000, valid_loss=7.450]\n","Epoch 189:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.370, train_loss_epoch=8.930, valid_loss=7.450]\n","Epoch 190:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.680, train_loss_epoch=9.190, valid_loss=7.450]\n","Epoch 191:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.490, train_loss_epoch=8.800, valid_loss=7.450]\n","Epoch 192:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.920, train_loss_epoch=8.840, valid_loss=7.450]\n","Epoch 193:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.810, train_loss_epoch=8.750, valid_loss=7.450]\n","Epoch 194:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.300, train_loss_epoch=8.470, valid_loss=7.450]\n","Epoch 195:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.760, train_loss_epoch=8.350, valid_loss=7.450]\n","Epoch 196:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.020, train_loss_epoch=7.690, valid_loss=7.450]\n","Epoch 197:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.870, train_loss_epoch=7.770, valid_loss=7.450]\n","Epoch 198:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.160, train_loss_epoch=7.560, valid_loss=7.450]\n","Epoch 199:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.090, train_loss_epoch=7.340, valid_loss=7.450]\n","Epoch 199: 100%|██████████| 4/4 [00:00\u003c00:00, 11.22it/s, v_num=0, train_loss_step=6.670, train_loss_epoch=7.340, valid_loss=7.450]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22342)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22342)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.03it/s]\u001b[A\n","Epoch 200:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.670, train_loss_epoch=7.080, valid_loss=7.070]\n","Epoch 201:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.850, train_loss_epoch=6.940, valid_loss=7.070]\n","Epoch 202:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.680, train_loss_epoch=6.930, valid_loss=7.070]\n","Epoch 203:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.830, train_loss_epoch=6.980, valid_loss=7.070]\n","Epoch 204:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.950, train_loss_epoch=7.140, valid_loss=7.070]\n","Epoch 205:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.790, train_loss_epoch=6.770, valid_loss=7.070]\n","Epoch 206:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.610, train_loss_epoch=6.720, valid_loss=7.070]\n","Epoch 207:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.230, train_loss_epoch=7.900, valid_loss=7.070]\n","Epoch 208:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.610, train_loss_epoch=8.070, valid_loss=7.070]\n","Epoch 209:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.880, train_loss_epoch=8.260, valid_loss=7.070]\n","Epoch 210:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.360, train_loss_epoch=8.380, valid_loss=7.070]\n","Epoch 211:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.180, train_loss_epoch=8.300, valid_loss=7.070]\n","Epoch 212:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.110, train_loss_epoch=8.530, valid_loss=7.070]\n","Epoch 213:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.760, train_loss_epoch=8.130, valid_loss=7.070]\n","Epoch 214:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.340, train_loss_epoch=7.680, valid_loss=7.070]\n","Epoch 215:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.300, train_loss_epoch=7.820, valid_loss=7.070]\n","Epoch 216:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.000, train_loss_epoch=7.820, valid_loss=7.070]\n","Epoch 217:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.010, train_loss_epoch=7.440, valid_loss=7.070]\n","Epoch 218:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.570, train_loss_epoch=7.360, valid_loss=7.070]\n","Epoch 219:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.720, train_loss_epoch=6.860, valid_loss=7.070]\n","Epoch 220:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.720, train_loss_epoch=6.780, valid_loss=7.070]\n","Epoch 221:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.950, train_loss_epoch=6.580, valid_loss=7.070]\n","Epoch 222:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.350, train_loss_epoch=6.650, valid_loss=7.070]\n","Epoch 223:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.930, train_loss_epoch=6.970, valid_loss=7.070]\n","Epoch 224:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.180, train_loss_epoch=7.520, valid_loss=7.070]\n","Epoch 224: 100%|██████████| 4/4 [00:00\u003c00:00, 11.52it/s, v_num=0, train_loss_step=8.510, train_loss_epoch=7.520, valid_loss=7.070]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22342)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22342)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.03it/s]\u001b[A\n","Epoch 225:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.510, train_loss_epoch=8.430, valid_loss=8.440]\n","Epoch 226:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.590, train_loss_epoch=8.730, valid_loss=8.440]\n","Epoch 227:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.840, train_loss_epoch=8.560, valid_loss=8.440]\n","Epoch 228:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.600, train_loss_epoch=8.830, valid_loss=8.440]\n","Epoch 229:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.900, train_loss_epoch=8.900, valid_loss=8.440]\n","Epoch 230:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.070, train_loss_epoch=8.990, valid_loss=8.440]\n","Epoch 231:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.810, train_loss_epoch=8.870, valid_loss=8.440]\n","Epoch 232:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.90, train_loss_epoch=9.580, valid_loss=8.440]\n","Epoch 233:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.500, train_loss_epoch=8.730, valid_loss=8.440]\n","Epoch 234:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.690, train_loss_epoch=8.660, valid_loss=8.440]\n","Epoch 235:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.260, train_loss_epoch=8.560, valid_loss=8.440]\n","Epoch 236:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.10, train_loss_epoch=8.980, valid_loss=8.440]\n","Epoch 237:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.020, train_loss_epoch=8.490, valid_loss=8.440]\n","Epoch 238:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.260, train_loss_epoch=8.160, valid_loss=8.440]\n","Epoch 239:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.260, train_loss_epoch=7.870, valid_loss=8.440]\n","Epoch 240:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.250, train_loss_epoch=7.500, valid_loss=8.440]\n","Epoch 241:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.010, train_loss_epoch=7.520, valid_loss=8.440]\n","Epoch 242:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.590, train_loss_epoch=6.950, valid_loss=8.440]\n","Epoch 243:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.300, train_loss_epoch=7.120, valid_loss=8.440]\n","Epoch 244:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.080, train_loss_epoch=6.780, valid_loss=8.440]\n","Epoch 245:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.410, train_loss_epoch=7.330, valid_loss=8.440]\n","Epoch 246:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.560, train_loss_epoch=7.460, valid_loss=8.440]\n","Epoch 247:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.540, train_loss_epoch=6.980, valid_loss=8.440]\n","Epoch 248:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.190, train_loss_epoch=6.500, valid_loss=8.440]\n","Epoch 249:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.380, train_loss_epoch=7.740, valid_loss=8.440]\n","Epoch 249: 100%|██████████| 4/4 [00:00\u003c00:00, 11.54it/s, v_num=0, train_loss_step=9.630, train_loss_epoch=7.740, valid_loss=8.440]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22342)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22342)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.02it/s]\u001b[A\n","Epoch 250:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.630, train_loss_epoch=8.470, valid_loss=8.120]\n","Epoch 251:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.110, train_loss_epoch=8.340, valid_loss=8.120]\n","Epoch 252:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.540, train_loss_epoch=8.480, valid_loss=8.120]\n","Epoch 253:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.960, train_loss_epoch=8.320, valid_loss=8.120]\n","Epoch 254:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.430, train_loss_epoch=8.440, valid_loss=8.120]\n","Epoch 255:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.390, train_loss_epoch=8.400, valid_loss=8.120]\n","Epoch 256:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.380, train_loss_epoch=8.620, valid_loss=8.120]\n","Epoch 257:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.540, train_loss_epoch=8.390, valid_loss=8.120]\n","Epoch 258:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.380, train_loss_epoch=7.970, valid_loss=8.120]\n","Epoch 259:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.740, train_loss_epoch=7.940, valid_loss=8.120]\n","Epoch 260:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.540, train_loss_epoch=7.780, valid_loss=8.120]\n","Epoch 261:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.000, train_loss_epoch=7.790, valid_loss=8.120]\n","Epoch 262:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.120, train_loss_epoch=7.630, valid_loss=8.120]\n","Epoch 263:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.330, train_loss_epoch=7.460, valid_loss=8.120]\n","Epoch 264:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.290, train_loss_epoch=6.870, valid_loss=8.120]\n","Epoch 265:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.890, train_loss_epoch=6.870, valid_loss=8.120]\n","Epoch 266:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.650, train_loss_epoch=6.530, valid_loss=8.120]\n","Epoch 267:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.040, train_loss_epoch=6.230, valid_loss=8.120]\n","Epoch 268:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.460, train_loss_epoch=6.380, valid_loss=8.120]\n","Epoch 269:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.110, train_loss_epoch=7.090, valid_loss=8.120]\n","Epoch 270:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.690, train_loss_epoch=7.260, valid_loss=8.120]\n","Epoch 271:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.040, train_loss_epoch=7.560, valid_loss=8.120]\n","Epoch 272:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.130, train_loss_epoch=7.250, valid_loss=8.120]\n","Epoch 272: 100%|██████████| 4/4 [00:00\u003c00:00, 11.35it/s, v_num=0, train_loss_step=7.130, train_loss_epoch=7.250, valid_loss=8.120]\n","Epoch 273:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.030, train_loss_epoch=6.960, valid_loss=8.120]\n","Epoch 274:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.630, train_loss_epoch=6.620, valid_loss=8.120]\n","Epoch 274: 100%|██████████| 4/4 [00:00\u003c00:00, 11.50it/s, v_num=0, train_loss_step=6.860, train_loss_epoch=6.620, valid_loss=8.120]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22342)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=22342)\u001b[0m `Trainer.fit` stopped: `max_steps=1100.0` reached.\n","2024-11-12 19:21:36,429\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('MA(2,9)', 'MA(3,9)', 'MA(1,12)', 'MA(2,12)', 'MA(3,12)', 'MOM(9)', 'MOM(12)', 'RSI(7)', 'RSI(14)', 'EMA(3,9)', 'EMA(5,9)', 'EMA(5,12)', 'DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (40, 20, 1), 'n_pool_kernel_size': (4, 4, 4), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=22342)\u001b[0m \n","\u001b[36m(_train_tune pid=22342)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.01it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22342)\u001b[0m \r                                                                      \u001b[A\rEpoch 274: 100%|██████████| 4/4 [00:02\u003c00:00,  1.64it/s, v_num=0, train_loss_step=6.860, train_loss_epoch=6.620, valid_loss=6.160]\rEpoch 274: 100%|██████████| 4/4 [00:02\u003c00:00,  1.64it/s, v_num=0, train_loss_step=6.860, train_loss_epoch=6.350, valid_loss=6.160]\rEpoch 274: 100%|██████████| 4/4 [00:02\u003c00:00,  1.64it/s, v_num=0, train_loss_step=6.860, train_loss_epoch=6.350, valid_loss=6.160]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=22951)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=22951)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=22951)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=22951)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=22951)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=22951)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=22951)\u001b[0m 2024-11-12 19:21:44.560690: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=22951)\u001b[0m 2024-11-12 19:21:44.580690: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=22951)\u001b[0m 2024-11-12 19:21:44.606169: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=22951)\u001b[0m 2024-11-12 19:21:44.613889: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=22951)\u001b[0m 2024-11-12 19:21:44.631894: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=22951)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=22951)\u001b[0m 2024-11-12 19:21:45.804322: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=22951)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=22951)\u001b[0m \n","\u001b[36m(_train_tune pid=22951)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=22951)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=22951)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=22951)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=22951)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=22951)\u001b[0m 3 | blocks       | ModuleList       | 3.3 M  | train\n","\u001b[36m(_train_tune pid=22951)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=22951)\u001b[0m 3.3 M     Trainable params\n","\u001b[36m(_train_tune pid=22951)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=22951)\u001b[0m 3.3 M     Total params\n","\u001b[36m(_train_tune pid=22951)\u001b[0m 13.196    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=22951)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=22951)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/4 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.43]\n","Epoch 1: 100%|██████████| 4/4 [00:00\u003c00:00, 12.02it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.43]\n","Epoch 2:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.55]\n","Epoch 3:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.51]\n","Epoch 4:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.36]\n","Epoch 5:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.52]\n","Epoch 6:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.43]\n","Epoch 7:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.47]\n","Epoch 8:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.39]\n","Epoch 9:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.58]\n","Epoch 10:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.39]\n","Epoch 11:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.55]\n","Epoch 12:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.39]\n","Epoch 13:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.43]\n","Epoch 14:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.43]\n","Epoch 15:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.40]\n","Epoch 16:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.38]\n","Epoch 17:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.58]\n","Epoch 18:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.63]\n","Epoch 19:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.53]\n","Epoch 20:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.47]\n","Epoch 21:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.61]\n","Epoch 22:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.54]\n","Epoch 23:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.49]\n","Epoch 24:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.44]\n","Epoch 24: 100%|██████████| 4/4 [00:00\u003c00:00, 12.01it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.44]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22951)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.98it/s]\u001b[A\n","Epoch 25:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.63, valid_loss=-1.47]\n","Epoch 26:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.62, valid_loss=-1.47]\n","Epoch 27:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.51, valid_loss=-1.47]\n","Epoch 28:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.53, valid_loss=-1.47]\n","Epoch 29:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.769, train_loss_epoch=-1.33, valid_loss=-1.47]\n","Epoch 30:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.58, valid_loss=-1.47]\n","Epoch 31:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.55, valid_loss=-1.47]\n","Epoch 32:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.48, valid_loss=-1.47]\n","Epoch 33:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.50, valid_loss=-1.47]\n","Epoch 34:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.36, train_loss_epoch=-1.50, valid_loss=-1.47]\n","Epoch 35:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.55, valid_loss=-1.47]\n","Epoch 36:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.57, valid_loss=-1.47]\n","Epoch 37:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.61, valid_loss=-1.47]\n","Epoch 38:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.66, valid_loss=-1.47]\n","Epoch 39:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.62, valid_loss=-1.47]\n","Epoch 40:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.52, valid_loss=-1.47]\n","Epoch 41:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.05, train_loss_epoch=-1.43, valid_loss=-1.47]\n","Epoch 42:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.01, train_loss_epoch=-1.43, valid_loss=-1.47]\n","Epoch 43:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.63, valid_loss=-1.47]\n","Epoch 44:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.59, valid_loss=-1.47]\n","Epoch 45:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.61, valid_loss=-1.47]\n","Epoch 46:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.62, valid_loss=-1.47]\n","Epoch 47:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.59, valid_loss=-1.47]\n","Epoch 48:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.66, valid_loss=-1.47]\n","Epoch 48: 100%|██████████| 4/4 [00:00\u003c00:00, 11.97it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.66, valid_loss=-1.47]\n","Epoch 49:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.55, valid_loss=-1.47]\n","Epoch 49: 100%|██████████| 4/4 [00:00\u003c00:00, 11.89it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.55, valid_loss=-1.47]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22951)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  2.00it/s]\u001b[A\n","Epoch 50:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.65, valid_loss=-1.61]\n","Epoch 51:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.63, valid_loss=-1.61]\n","Epoch 52:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.68, valid_loss=-1.61]\n","Epoch 53:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.68, valid_loss=-1.61]\n","Epoch 54:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-1.77, valid_loss=-1.61]\n","Epoch 55:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.61, valid_loss=-1.61]\n","Epoch 56:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.67, valid_loss=-1.61]\n","Epoch 57:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.73, valid_loss=-1.61]\n","Epoch 58:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.60, valid_loss=-1.61]\n","Epoch 59:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.77, valid_loss=-1.61]\n","Epoch 60:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.64, valid_loss=-1.61]\n","Epoch 61:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.70, valid_loss=-1.61]\n","Epoch 62:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.60, valid_loss=-1.61]\n","Epoch 63:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.73, valid_loss=-1.61]\n","Epoch 64:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.67, valid_loss=-1.61]\n","Epoch 65:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.64, valid_loss=-1.61]\n","Epoch 66:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.65, valid_loss=-1.61]\n","Epoch 67:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.63, valid_loss=-1.61]\n","Epoch 68:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.72, valid_loss=-1.61]\n","Epoch 69:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.72, valid_loss=-1.61]\n","Epoch 70:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.66, valid_loss=-1.61]\n","Epoch 71:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.66, valid_loss=-1.61]\n","Epoch 72:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.831, train_loss_epoch=-1.48, valid_loss=-1.61]\n","Epoch 73:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.70, valid_loss=-1.61]\n","Epoch 74:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.71, valid_loss=-1.61]\n","Epoch 74: 100%|██████████| 4/4 [00:00\u003c00:00, 11.87it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.71, valid_loss=-1.61]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22951)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22951)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.99it/s]\u001b[A\n","Epoch 75:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.58, valid_loss=-1.71]\n","Epoch 76:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.73, valid_loss=-1.71]\n","Epoch 77:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.75, valid_loss=-1.71]\n","Epoch 78:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.66, valid_loss=-1.71]\n","Epoch 79:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.82, valid_loss=-1.71]\n","Epoch 80:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.74, valid_loss=-1.71]\n","Epoch 81:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.79, valid_loss=-1.71]\n","Epoch 82:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.68, valid_loss=-1.71]\n","Epoch 83:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.71, valid_loss=-1.71]\n","Epoch 84:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.816, train_loss_epoch=-1.49, valid_loss=-1.71]\n","Epoch 85:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.66, valid_loss=-1.71]\n","Epoch 86:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.72, valid_loss=-1.71]\n","Epoch 87:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.75, valid_loss=-1.71]\n","Epoch 88:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.82, valid_loss=-1.71]\n","Epoch 89:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.68, valid_loss=-1.71]\n","Epoch 90:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.63, valid_loss=-1.71]\n","Epoch 91:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.69, valid_loss=-1.71]\n","Epoch 92:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.83, valid_loss=-1.71]\n","Epoch 93:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.74, valid_loss=-1.71]\n","Epoch 94:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-1.86, valid_loss=-1.71]\n","Epoch 95:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.83, valid_loss=-1.71]\n","Epoch 96:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.70, valid_loss=-1.71]\n","Epoch 97:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.78, valid_loss=-1.71]\n","Epoch 98:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.85, valid_loss=-1.71]\n","Epoch 99:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.71, valid_loss=-1.71]\n","Epoch 99: 100%|██████████| 4/4 [00:00\u003c00:00, 11.39it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.71, valid_loss=-1.71]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22951)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22951)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.98it/s]\u001b[A\n","Epoch 100:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.76, valid_loss=-1.76]\n","Epoch 101:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.83, valid_loss=-1.76]\n","Epoch 102:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.74, valid_loss=-1.76]\n","Epoch 103:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.79, valid_loss=-1.76]\n","Epoch 104:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.71, valid_loss=-1.76]\n","Epoch 105:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.79, valid_loss=-1.76]\n","Epoch 106:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.82, valid_loss=-1.76]\n","Epoch 106: 100%|██████████| 4/4 [00:00\u003c00:00, 11.23it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.74, valid_loss=-1.76]\n","Epoch 107:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.74, valid_loss=-1.76]\n","Epoch 108:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.80, valid_loss=-1.76]\n","Epoch 109:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.75, valid_loss=-1.76]\n","Epoch 110:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.81, valid_loss=-1.76]\n","Epoch 111:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.76, valid_loss=-1.76]\n","Epoch 112:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.73, valid_loss=-1.76]\n","Epoch 113:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-1.86, valid_loss=-1.76]\n","Epoch 114:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-1.87, valid_loss=-1.76]\n","Epoch 115:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.81, valid_loss=-1.76]\n","Epoch 116:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.75, valid_loss=-1.76]\n","Epoch 117:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.72, valid_loss=-1.76]\n","Epoch 118:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.61, valid_loss=-1.76]\n","Epoch 119:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-1.89, valid_loss=-1.76]\n","Epoch 120:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.69, valid_loss=-1.76]\n","Epoch 121:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.81, valid_loss=-1.76]\n","Epoch 122:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.76, valid_loss=-1.76]\n","Epoch 123:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.78, valid_loss=-1.76]\n","Epoch 124:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.81, valid_loss=-1.76]\n","Epoch 124: 100%|██████████| 4/4 [00:00\u003c00:00, 11.89it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.81, valid_loss=-1.76]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22951)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22951)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.98it/s]\u001b[A\n","Epoch 124: 100%|██████████| 4/4 [00:02\u003c00:00,  1.65it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.80, valid_loss=-1.79]\n","Epoch 125:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.80, valid_loss=-1.79]\n","Epoch 126:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.87, valid_loss=-1.79]\n","Epoch 127:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.76, valid_loss=-1.79]\n","Epoch 128:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.85, valid_loss=-1.79]\n","Epoch 129:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.78, valid_loss=-1.79]\n","Epoch 130:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.77, valid_loss=-1.79]\n","Epoch 131:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.76, valid_loss=-1.79]\n","Epoch 132:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.80, valid_loss=-1.79]\n","Epoch 133:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.79, valid_loss=-1.79]\n","Epoch 134:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.75, valid_loss=-1.79]\n","Epoch 134: 100%|██████████| 4/4 [00:00\u003c00:00, 11.85it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.75, valid_loss=-1.79]\n","Epoch 135:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.78, valid_loss=-1.79]\n","Epoch 136:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-1.94, valid_loss=-1.79]\n","Epoch 137:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.70, valid_loss=-1.79]\n","Epoch 138:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.88, valid_loss=-1.79]\n","Epoch 139:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.79, valid_loss=-1.79]\n","Epoch 140:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.75, valid_loss=-1.79]\n","Epoch 141:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.82, valid_loss=-1.79]\n","Epoch 142:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.67, valid_loss=-1.79]\n","Epoch 143:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.86, valid_loss=-1.79]\n","Epoch 144:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.85, valid_loss=-1.79]\n","Epoch 145:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.81, valid_loss=-1.79]\n","Epoch 145: 100%|██████████| 4/4 [00:00\u003c00:00, 11.74it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.81, valid_loss=-1.79]\n","Epoch 146:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.81, valid_loss=-1.79]\n","Epoch 147:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.79, valid_loss=-1.79]\n","Epoch 148:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.86, valid_loss=-1.79]\n","Epoch 149:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.71, valid_loss=-1.79]\n","Epoch 149: 100%|██████████| 4/4 [00:00\u003c00:00, 11.69it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.71, valid_loss=-1.79]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22951)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22951)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.97it/s]\u001b[A\n","Epoch 150:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.82, valid_loss=-1.81]\n","Epoch 151:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.82, valid_loss=-1.81]\n","Epoch 152:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.75, valid_loss=-1.81]\n","Epoch 153:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.76, valid_loss=-1.81]\n","Epoch 154:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.36, train_loss_epoch=-1.96, valid_loss=-1.81]\n","Epoch 155:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.65, valid_loss=-1.81]\n","Epoch 156:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.72, valid_loss=-1.81]\n","Epoch 157:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.79, valid_loss=-1.81]\n","Epoch 158:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.72, valid_loss=-1.81]\n","Epoch 159:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-1.89, valid_loss=-1.81]\n","Epoch 160:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.80, valid_loss=-1.81]\n","Epoch 161:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-1.91, valid_loss=-1.81]\n","Epoch 161: 100%|██████████| 4/4 [00:00\u003c00:00, 11.86it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-1.90, valid_loss=-1.81]\n","Epoch 162:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-1.90, valid_loss=-1.81]\n","Epoch 163:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.88, valid_loss=-1.81]\n","Epoch 164:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.89, valid_loss=-1.81]\n","Epoch 165:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.71, valid_loss=-1.81]\n","Epoch 166:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.79, valid_loss=-1.81]\n","Epoch 167:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.84, valid_loss=-1.81]\n","Epoch 168:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.79, valid_loss=-1.81]\n","Epoch 169:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-1.94, valid_loss=-1.81]\n","Epoch 170:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.77, valid_loss=-1.81]\n","Epoch 171:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.87, valid_loss=-1.81]\n","Epoch 172:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-1.90, valid_loss=-1.81]\n","Epoch 173:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.75, valid_loss=-1.81]\n","Epoch 174:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.84, valid_loss=-1.81]\n","Epoch 174: 100%|██████████| 4/4 [00:00\u003c00:00, 11.74it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.84, valid_loss=-1.81]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22951)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.99it/s]\u001b[A\n","Epoch 175:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.85, valid_loss=-1.82]\n","Epoch 176:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.84, valid_loss=-1.82]\n","Epoch 177:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.81, valid_loss=-1.82]\n","Epoch 178:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.81, valid_loss=-1.82]\n","Epoch 179:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.82, valid_loss=-1.82]\n","Epoch 180:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.84, valid_loss=-1.82]\n","Epoch 181:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.91, valid_loss=-1.82]\n","Epoch 182:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.92, valid_loss=-1.82]\n","Epoch 183:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.87, valid_loss=-1.82]\n","Epoch 184:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.78, valid_loss=-1.82]\n","Epoch 185:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.74, valid_loss=-1.82]\n","Epoch 186:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-1.93, valid_loss=-1.82]\n","Epoch 187:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.73, valid_loss=-1.82]\n","Epoch 188:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.80, valid_loss=-1.82]\n","Epoch 189:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.66, valid_loss=-1.82]\n","Epoch 190:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.80, valid_loss=-1.82]\n","Epoch 191:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.03, train_loss_epoch=-1.61, valid_loss=-1.82]\n","Epoch 192:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.83, valid_loss=-1.82]\n","Epoch 193:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.78, valid_loss=-1.82]\n","Epoch 194:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.84, valid_loss=-1.82]\n","Epoch 195:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.85, valid_loss=-1.82]\n","Epoch 196:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.42, train_loss_epoch=-1.97, valid_loss=-1.82]\n","Epoch 197:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.81, valid_loss=-1.82]\n","Epoch 198:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.82, valid_loss=-1.82]\n","Epoch 199:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.75, valid_loss=-1.82]\n","Epoch 199: 100%|██████████| 4/4 [00:00\u003c00:00, 11.75it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.75, valid_loss=-1.82]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22951)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22951)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.98it/s]\u001b[A\n","Epoch 200:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.82, valid_loss=-1.83]\n","Epoch 201:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.84, valid_loss=-1.83]\n","Epoch 202:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.82, valid_loss=-1.83]\n","Epoch 203:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.81, valid_loss=-1.83]\n","Epoch 204:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.76, valid_loss=-1.83]\n","Epoch 205:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-1.94, valid_loss=-1.83]\n","Epoch 206:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-1.91, valid_loss=-1.83]\n","Epoch 207:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-1.94, valid_loss=-1.83]\n","Epoch 208:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.76, valid_loss=-1.83]\n","Epoch 209:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.996, train_loss_epoch=-1.64, valid_loss=-1.83]\n","Epoch 210:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.82, valid_loss=-1.83]\n","Epoch 211:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.87, valid_loss=-1.83]\n","Epoch 212:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.77, valid_loss=-1.83]\n","Epoch 212: 100%|██████████| 4/4 [00:00\u003c00:00, 11.94it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.77, valid_loss=-1.83]\n","Epoch 213:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.75, valid_loss=-1.83]\n","Epoch 214:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.88, valid_loss=-1.83]\n","Epoch 215:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.87, valid_loss=-1.83]\n","Epoch 216:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.82, valid_loss=-1.83]\n","Epoch 217:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-1.94, valid_loss=-1.83]\n","Epoch 218:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.72, valid_loss=-1.83]\n","Epoch 219:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.77, valid_loss=-1.83]\n","Epoch 220:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.68, valid_loss=-1.83]\n","Epoch 221:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.86, valid_loss=-1.83]\n","Epoch 222:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.79, valid_loss=-1.83]\n","Epoch 223:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-1.94, valid_loss=-1.83]\n","Epoch 224:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.84, valid_loss=-1.83]\n","Epoch 224: 100%|██████████| 4/4 [00:00\u003c00:00, 11.73it/s, v_num=0, train_loss_step=-0.956, train_loss_epoch=-1.84, valid_loss=-1.83]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22951)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 19:23:25,087\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('MA(2,9)', 'MA(3,9)', 'MA(1,12)', 'MA(2,12)', 'MA(3,12)', 'MOM(9)', 'MOM(12)', 'RSI(7)', 'RSI(14)', 'EMA(3,9)', 'EMA(5,9)', 'EMA(5,12)', 'DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (168, 24, 1), 'n_pool_kernel_size': (8, 4, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=22951)\u001b[0m `Trainer.fit` stopped: `max_steps=900.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=22951)\u001b[0m \n","\u001b[36m(_train_tune pid=22951)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 4/4 [00:02\u003c00:00,  1.98it/s]\u001b[A\n","\u001b[36m(_train_tune pid=22951)\u001b[0m \r                                                                      \u001b[A\rEpoch 224: 100%|██████████| 4/4 [00:02\u003c00:00,  1.64it/s, v_num=0, train_loss_step=-0.956, train_loss_epoch=-1.84, valid_loss=-1.84]\rEpoch 224: 100%|██████████| 4/4 [00:02\u003c00:00,  1.64it/s, v_num=0, train_loss_step=-0.956, train_loss_epoch=-1.60, valid_loss=-1.84]\rEpoch 224: 100%|██████████| 4/4 [00:02\u003c00:00,  1.64it/s, v_num=0, train_loss_step=-0.956, train_loss_epoch=-1.60, valid_loss=-1.84]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=23466)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=23466)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=23466)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=23466)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=23466)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=23466)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=23466)\u001b[0m 2024-11-12 19:23:33.745100: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=23466)\u001b[0m 2024-11-12 19:23:33.765614: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=23466)\u001b[0m 2024-11-12 19:23:33.790787: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=23466)\u001b[0m 2024-11-12 19:23:33.798588: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=23466)\u001b[0m 2024-11-12 19:23:33.816632: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=23466)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=23466)\u001b[0m 2024-11-12 19:23:34.990642: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=23466)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=23466)\u001b[0m \n","\u001b[36m(_train_tune pid=23466)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=23466)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=23466)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=23466)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=23466)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=23466)\u001b[0m 3 | blocks       | ModuleList       | 4.5 M  | train\n","\u001b[36m(_train_tune pid=23466)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=23466)\u001b[0m 4.5 M     Trainable params\n","\u001b[36m(_train_tune pid=23466)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=23466)\u001b[0m 4.5 M     Total params\n","\u001b[36m(_train_tune pid=23466)\u001b[0m 17.995    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=23466)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=23466)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 19:23:37,994\tERROR tune_controller.py:1331 -- Trial task failed for trial _train_tune_64a1c5ae\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n","    result = ray.get(future)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n","    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(OutOfMemoryError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=23466, ip=172.28.0.12, actor_id=1fbecee091ea842182f85ce301000000, repr=_train_tune)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n","    raise skipped from exception_cause(skipped)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n","    self._ret = self._target(*self._args, **self._kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in \u003clambda\u003e\n","    training_func=lambda: self._trainable_func(self.config),\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n","    output = fn()\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/util.py\", line 130, in inner\n","    return trainable(config, **fn_kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_auto.py\", line 214, in _train_tune\n","    _ = self._fit_model(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_auto.py\", line 362, in _fit_model\n","    model = model.fit(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_windows.py\", line 661, in fit\n","    return self._fit(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_model.py\", line 356, in _fit\n","    trainer.fit(model, datamodule=datamodule)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 538, in fit\n","    call._call_and_handle_interrupt(\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 47, in _call_and_handle_interrupt\n","    return trainer_fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 574, in _fit_impl\n","    self._run(model, ckpt_path=ckpt_path)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 981, in _run\n","    results = self._run_stage()\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1023, in _run_stage\n","    self._run_sanity_check()\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1052, in _run_sanity_check\n","    val_loop.run()\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py\", line 178, in _decorator\n","    return loop_run(self, *args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 135, in run\n","    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 396, in _evaluation_step\n","    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 319, in _call_strategy_hook\n","    output = fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py\", line 411, in validation_step\n","    return self.lightning_module.validation_step(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_windows.py\", line 534, in validation_step\n","    valid_loss_batch = self._compute_valid_loss(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_windows.py\", line 461, in _compute_valid_loss\n","    _, sample_mean, quants = self.loss.sample(distr_args=distr_args)\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/losses/pytorch.py\", line 2002, in sample\n","    samples = distr.sample(sample_shape=(num_samples,))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributions/distribution.py\", line 167, in sample\n","    return self.rsample(sample_shape)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributions/studentT.py\", line 90, in rsample\n","    Y = X * torch.rsqrt(Z / self.df)\n","torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.60 GiB. GPU 0 has a total capacity of 39.56 GiB of which 5.92 GiB is free. Process 27231 has 512.00 MiB memory in use. Process 440557 has 33.14 GiB memory in use. Of the allocated memory 31.03 GiB is allocated by PyTorch, and 1.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial _train_tune_64a1c5ae errored after 0 iterations at 2024-11-12 19:23:37. Total running time: 28min 58s\n","Error file: /tmp/ray/session_2024-11-12_18-17-38_352630_2326/artifacts/2024-11-12_18-54-39/_train_tune_2024-11-12_18-54-39/driver_artifacts/_train_tune_64a1c5ae_25_batch_size=256,h=20,hist_exog_list=MA_2_9_MA_3_9_MA_1_12_MA_2_12_MA_3_12_MOM_9_MOM_12_RSI_7_RSI_14_EMA_3_9_2024-11-12_19-21-43/error.txt\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=23574)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=23574)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=23574)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=23574)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=23574)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=23574)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=23574)\u001b[0m 2024-11-12 19:23:45.640258: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=23574)\u001b[0m 2024-11-12 19:23:45.659891: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=23574)\u001b[0m 2024-11-12 19:23:45.685449: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=23574)\u001b[0m 2024-11-12 19:23:45.692986: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=23574)\u001b[0m 2024-11-12 19:23:45.710168: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=23574)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=23574)\u001b[0m 2024-11-12 19:23:46.877979: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=23574)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=23574)\u001b[0m \n","\u001b[36m(_train_tune pid=23574)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=23574)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=23574)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=23574)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=23574)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=23574)\u001b[0m 3 | blocks       | ModuleList       | 3.7 M  | train\n","\u001b[36m(_train_tune pid=23574)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=23574)\u001b[0m 3.7 M     Trainable params\n","\u001b[36m(_train_tune pid=23574)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=23574)\u001b[0m 3.7 M     Total params\n","\u001b[36m(_train_tune pid=23574)\u001b[0m 14.882    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=23574)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=23574)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/7 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.47]\n","Epoch 2:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-1.71]\n","Epoch 3:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.75]\n","Epoch 4:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.75]\n","Epoch 5:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.82]\n","Epoch 6:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.78]\n","Epoch 7:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.86]\n","Epoch 8:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.82]\n","Epoch 9:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-1.92]\n","Epoch 10:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.83]\n","Epoch 11:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-1.96]\n","Epoch 12:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.87]\n","Epoch 13:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.89]\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.89]\n","\u001b[36m(_train_tune pid=23574)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=23574)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.54it/s]\u001b[A\n","Epoch 14:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.89, valid_loss=-1.91]\n","Epoch 15:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.90, valid_loss=-1.91]\n","Epoch 16:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-2.01, valid_loss=-1.91]\n","Epoch 17:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.91, valid_loss=-1.91]\n","Epoch 18:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.96, valid_loss=-1.91]\n","Epoch 19:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.48, train_loss_epoch=-2.03, valid_loss=-1.91]\n","Epoch 20:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.93, valid_loss=-1.91]\n","Epoch 21:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.97, valid_loss=-1.91]\n","Epoch 22:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.96, valid_loss=-1.91]\n","Epoch 23:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.95, valid_loss=-1.91]\n","Epoch 24:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.97, valid_loss=-1.91]\n","Epoch 25:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.02, valid_loss=-1.91]\n","Epoch 26:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.01, valid_loss=-1.91]\n","Epoch 27:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.00, valid_loss=-1.91]\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.98, valid_loss=-1.91]\n","\u001b[36m(_train_tune pid=23574)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=23574)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.52it/s]\u001b[A\n","Epoch 28:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.98, valid_loss=-1.98]\n","Epoch 29:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.89, valid_loss=-1.98]\n","Epoch 30:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-2.03, valid_loss=-1.98]\n","Epoch 31:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.96, valid_loss=-1.98]\n","Epoch 32:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.92, valid_loss=-1.98]\n","Epoch 33:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.00, valid_loss=-1.98]\n","Epoch 34:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.97, valid_loss=-1.98]\n","Epoch 35:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.02, valid_loss=-1.98]\n","Epoch 36:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.00, valid_loss=-1.98]\n","Epoch 37:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.59, train_loss_epoch=-2.09, valid_loss=-1.98]\n","Epoch 38:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.00, valid_loss=-1.98]\n","Epoch 39:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-2.01, valid_loss=-1.98]\n","Epoch 40:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.02, valid_loss=-1.98]\n","Epoch 41:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.54, train_loss_epoch=-2.09, valid_loss=-1.98]\n","Epoch 42:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.04, valid_loss=-1.98]\n","\u001b[36m(_train_tune pid=23574)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=23574)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.51it/s]\u001b[A\n","Epoch 43:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.99, valid_loss=-1.99]\n","Epoch 44:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 45:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.06, valid_loss=-1.99]\n","Epoch 45: 100%|██████████| 7/7 [00:00\u003c00:00, 23.47it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.06, valid_loss=-1.99]\n","Epoch 46:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 47:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-2.10, valid_loss=-1.99]\n","Epoch 48:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.99, valid_loss=-1.99]\n","Epoch 49:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.05, valid_loss=-1.99]\n","Epoch 50:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.99, valid_loss=-1.99]\n","Epoch 51:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.04, valid_loss=-1.99]\n","Epoch 52:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 53:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.99, valid_loss=-1.99]\n","Epoch 54:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 55:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-2.07, valid_loss=-1.99]\n","Epoch 56:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.05, valid_loss=-1.99]\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.95, valid_loss=-1.99]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=23574)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.56it/s]\u001b[A\n","Epoch 57:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.95, valid_loss=-2.02]\n","Epoch 58:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.08, valid_loss=-2.02]\n","Epoch 59:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.03, valid_loss=-2.02]\n","Epoch 60:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.08, valid_loss=-2.02]\n","Epoch 61:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.01, valid_loss=-2.02]\n","Epoch 62:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.09, valid_loss=-2.02]\n","Epoch 63:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.06, valid_loss=-2.02]\n","Epoch 64:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-2.03, valid_loss=-2.02]\n","Epoch 65:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.05, valid_loss=-2.02]\n","Epoch 66:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.05, valid_loss=-2.02]\n","Epoch 67:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.09, valid_loss=-2.02]\n","Epoch 68:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.05, valid_loss=-2.02]\n","Epoch 69:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-2.01, valid_loss=-2.02]\n","Epoch 70:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.04, valid_loss=-2.02]\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.94, valid_loss=-2.02]\n","\u001b[36m(_train_tune pid=23574)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=23574)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.55it/s]\u001b[A\n","Epoch 71:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.94, valid_loss=-2.02]\n","Epoch 72:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.06, valid_loss=-2.02]\n","Epoch 73:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.09, valid_loss=-2.02]\n","Epoch 74:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.44, train_loss_epoch=-2.13, valid_loss=-2.02]\n","Epoch 75:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.10, valid_loss=-2.02]\n","Epoch 76:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.08, valid_loss=-2.02]\n","Epoch 77:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.04, valid_loss=-2.02]\n","Epoch 78:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.41, train_loss_epoch=-2.12, valid_loss=-2.02]\n","Epoch 79:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.08, valid_loss=-2.02]\n","Epoch 80:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.09, valid_loss=-2.02]\n","Epoch 81:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-2.04, valid_loss=-2.02]\n","Epoch 82:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.08, valid_loss=-2.02]\n","Epoch 83:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.96, valid_loss=-2.02]\n","Epoch 84:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.04, valid_loss=-2.02]\n","Epoch 85:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.07, valid_loss=-2.02]\n","\u001b[36m(_train_tune pid=23574)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=23574)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.54it/s]\u001b[A\n","Epoch 86:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.08, valid_loss=-2.04]\n","Epoch 87:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.60, train_loss_epoch=-2.16, valid_loss=-2.04]\n","Epoch 88:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.13, valid_loss=-2.04]\n","Epoch 89:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.09, valid_loss=-2.04]\n","Epoch 90:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.10, valid_loss=-2.04]\n","Epoch 91:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-2.12, valid_loss=-2.04]\n","Epoch 92:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.06, valid_loss=-2.04]\n","Epoch 93:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.12, valid_loss=-2.04]\n","Epoch 94:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.10, valid_loss=-2.04]\n","Epoch 95:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-2.07, valid_loss=-2.04]\n","Epoch 96:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.46, train_loss_epoch=-2.14, valid_loss=-2.04]\n","Epoch 97:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.11, valid_loss=-2.04]\n","Epoch 98:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-2.13, valid_loss=-2.04]\n","Epoch 99:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.10, valid_loss=-2.04]\n","Epoch 99: 100%|██████████| 7/7 [00:00\u003c00:00, 22.48it/s, v_num=0, train_loss_step=-2.46, train_loss_epoch=-2.10, valid_loss=-2.04]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=23574)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.54it/s]\u001b[A\n","\u001b[36m(_train_tune pid=23574)\u001b[0m \n","Epoch 99:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.46, train_loss_epoch=-2.15, valid_loss=-2.04]        \n","Epoch 100:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.46, train_loss_epoch=-2.15, valid_loss=-2.04]\n","Epoch 100: 100%|██████████| 7/7 [00:00\u003c00:00, 22.05it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.12, valid_loss=-2.04]\n","Epoch 101:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.12, valid_loss=-2.04]\n","Epoch 102:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-2.04, valid_loss=-2.04]\n","Epoch 103:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.10, valid_loss=-2.04]\n","Epoch 104:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.48, train_loss_epoch=-2.17, valid_loss=-2.04]\n","Epoch 105:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.08, valid_loss=-2.04]\n","Epoch 106:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-2.03, valid_loss=-2.04]\n","Epoch 107:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.12, valid_loss=-2.04]\n","Epoch 108:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-2.15, valid_loss=-2.04]\n","Epoch 109:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.09, valid_loss=-2.04]\n","Epoch 110:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-2.03, valid_loss=-2.04]\n","Epoch 111:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-2.15, valid_loss=-2.04]\n","Epoch 112:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.11, valid_loss=-2.04]\n","Epoch 113:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.12, valid_loss=-2.04]\n","Epoch 114:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.63, train_loss_epoch=-2.21, valid_loss=-2.04]\n","\u001b[36m(_train_tune pid=23574)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=23574)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.60it/s]\u001b[A\n","Epoch 114:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.21, valid_loss=-2.04]\n","Epoch 115:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.07, valid_loss=-2.04]\n","Epoch 116:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-2.02, valid_loss=-2.04]\n","Epoch 117:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.47, train_loss_epoch=-2.17, valid_loss=-2.04]\n","Epoch 118:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-2.07, valid_loss=-2.04]\n","Epoch 119:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.13, valid_loss=-2.04]\n","Epoch 120:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.10, valid_loss=-2.04]\n","Epoch 121:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.09, valid_loss=-2.04]\n","Epoch 122:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.12, valid_loss=-2.04]\n","Epoch 123:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.13, valid_loss=-2.04]\n","Epoch 124:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.15, valid_loss=-2.04]\n","Epoch 125:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-2.15, valid_loss=-2.04]\n","Epoch 126:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.16, valid_loss=-2.04]\n","Epoch 127:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-2.04, valid_loss=-2.04]\n","Epoch 128:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.11, valid_loss=-2.04]\n","\u001b[36m(_train_tune pid=23574)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=23574)\u001b[0m `Trainer.fit` stopped: `max_steps=900.0` reached.\n","2024-11-12 19:24:47,616\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('MA(2,9)', 'MA(3,9)', 'MA(1,12)', 'MA(2,12)', 'MA(3,12)', 'MOM(9)', 'MOM(12)', 'RSI(7)', 'RSI(14)', 'EMA(3,9)', 'EMA(5,9)', 'EMA(5,12)', 'DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (168, 24, 1), 'n_pool_kernel_size': (16, 8, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=23574)\u001b[0m \n","\u001b[36m(_train_tune pid=23574)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.54it/s]\u001b[A\n","\u001b[36m(_train_tune pid=23574)\u001b[0m \r                                                                      \u001b[A\rEpoch 128:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.11, valid_loss=-2.05]\rEpoch 128:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.14, valid_loss=-2.05]\rEpoch 128:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.14, valid_loss=-2.05]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=23926)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=23926)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=23926)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=23926)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=23926)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=23926)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=23926)\u001b[0m 2024-11-12 19:24:55.659728: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=23926)\u001b[0m 2024-11-12 19:24:55.679754: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=23926)\u001b[0m 2024-11-12 19:24:55.704848: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=23926)\u001b[0m 2024-11-12 19:24:55.712690: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=23926)\u001b[0m 2024-11-12 19:24:55.730404: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=23926)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=23926)\u001b[0m 2024-11-12 19:24:56.912599: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=23926)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=23926)\u001b[0m \n","\u001b[36m(_train_tune pid=23926)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=23926)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=23926)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=23926)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=23926)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=23926)\u001b[0m 3 | blocks       | ModuleList       | 4.5 M  | train\n","\u001b[36m(_train_tune pid=23926)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=23926)\u001b[0m 4.5 M     Trainable params\n","\u001b[36m(_train_tune pid=23926)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=23926)\u001b[0m 4.5 M     Total params\n","\u001b[36m(_train_tune pid=23926)\u001b[0m 17.995    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=23926)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=23926)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 19:24:59,907\tERROR tune_controller.py:1331 -- Trial task failed for trial _train_tune_d1d0fb02\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n","    result = ray.get(future)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n","    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(OutOfMemoryError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=23926, ip=172.28.0.12, actor_id=fc02096b07f5d7f5890d91b801000000, repr=_train_tune)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n","    raise skipped from exception_cause(skipped)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n","    self._ret = self._target(*self._args, **self._kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in \u003clambda\u003e\n","    training_func=lambda: self._trainable_func(self.config),\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n","    output = fn()\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/util.py\", line 130, in inner\n","    return trainable(config, **fn_kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_auto.py\", line 214, in _train_tune\n","    _ = self._fit_model(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_auto.py\", line 362, in _fit_model\n","    model = model.fit(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_windows.py\", line 661, in fit\n","    return self._fit(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_model.py\", line 356, in _fit\n","    trainer.fit(model, datamodule=datamodule)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 538, in fit\n","    call._call_and_handle_interrupt(\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 47, in _call_and_handle_interrupt\n","    return trainer_fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 574, in _fit_impl\n","    self._run(model, ckpt_path=ckpt_path)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 981, in _run\n","    results = self._run_stage()\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1023, in _run_stage\n","    self._run_sanity_check()\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1052, in _run_sanity_check\n","    val_loop.run()\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py\", line 178, in _decorator\n","    return loop_run(self, *args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 135, in run\n","    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 396, in _evaluation_step\n","    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 319, in _call_strategy_hook\n","    output = fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py\", line 411, in validation_step\n","    return self.lightning_module.validation_step(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_windows.py\", line 534, in validation_step\n","    valid_loss_batch = self._compute_valid_loss(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_windows.py\", line 461, in _compute_valid_loss\n","    _, sample_mean, quants = self.loss.sample(distr_args=distr_args)\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/losses/pytorch.py\", line 2002, in sample\n","    samples = distr.sample(sample_shape=(num_samples,))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributions/distribution.py\", line 167, in sample\n","    return self.rsample(sample_shape)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributions/studentT.py\", line 90, in rsample\n","    Y = X * torch.rsqrt(Z / self.df)\n","torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.60 GiB. GPU 0 has a total capacity of 39.56 GiB of which 5.92 GiB is free. Process 27231 has 512.00 MiB memory in use. Process 449839 has 33.14 GiB memory in use. Of the allocated memory 31.03 GiB is allocated by PyTorch, and 1.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial _train_tune_d1d0fb02 errored after 0 iterations at 2024-11-12 19:24:59. Total running time: 30min 20s\n","Error file: /tmp/ray/session_2024-11-12_18-17-38_352630_2326/artifacts/2024-11-12_18-54-39/_train_tune_2024-11-12_18-54-39/driver_artifacts/_train_tune_d1d0fb02_27_batch_size=256,h=20,hist_exog_list=MA_2_9_MA_3_9_MA_1_12_MA_2_12_MA_3_12_MOM_9_MOM_12_RSI_7_RSI_14_EMA_3_9_2024-11-12_19-23-45/error.txt\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=24034)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=24034)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=24034)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=24034)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=24034)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=24034)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=24034)\u001b[0m 2024-11-12 19:25:07.632481: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=24034)\u001b[0m 2024-11-12 19:25:07.651634: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=24034)\u001b[0m 2024-11-12 19:25:07.675914: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=24034)\u001b[0m 2024-11-12 19:25:07.683398: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=24034)\u001b[0m 2024-11-12 19:25:07.700246: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=24034)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=24034)\u001b[0m 2024-11-12 19:25:08.870096: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=24034)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=24034)\u001b[0m \n","\u001b[36m(_train_tune pid=24034)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=24034)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=24034)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=24034)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=24034)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=24034)\u001b[0m 3 | blocks       | ModuleList       | 3.7 M  | train\n","\u001b[36m(_train_tune pid=24034)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=24034)\u001b[0m 3.7 M     Trainable params\n","\u001b[36m(_train_tune pid=24034)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=24034)\u001b[0m 3.7 M     Total params\n","\u001b[36m(_train_tune pid=24034)\u001b[0m 14.882    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=24034)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=24034)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/7 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.47]\n","Epoch 2:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-1.71]\n","Epoch 3:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.75]\n","Epoch 3: 100%|██████████| 7/7 [00:00\u003c00:00, 20.52it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.75]\n","Epoch 4:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.75]\n","Epoch 5:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.82]\n","Epoch 6:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.78]\n","Epoch 7:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.86]\n","Epoch 8:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.82]\n","Epoch 9:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-1.92]\n","Epoch 10:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.83]\n","Epoch 11:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-1.96]\n","Epoch 12:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.87]\n","Epoch 13:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.89]\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.89]\n","\u001b[36m(_train_tune pid=24034)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24034)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24034)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.50it/s]\u001b[A\n","Epoch 14:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.89, valid_loss=-1.91]\n","Epoch 15:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.90, valid_loss=-1.91]\n","Epoch 16:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-2.01, valid_loss=-1.91]\n","Epoch 17:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.91, valid_loss=-1.91]\n","Epoch 18:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.96, valid_loss=-1.91]\n","Epoch 19:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.48, train_loss_epoch=-2.03, valid_loss=-1.91]\n","Epoch 20:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.93, valid_loss=-1.91]\n","Epoch 21:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.97, valid_loss=-1.91]\n","Epoch 22:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.96, valid_loss=-1.91]\n","Epoch 23:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.95, valid_loss=-1.91]\n","Epoch 24:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.97, valid_loss=-1.91]\n","Epoch 25:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.02, valid_loss=-1.91]\n","Epoch 26:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.01, valid_loss=-1.91]\n","Epoch 26: 100%|██████████| 7/7 [00:00\u003c00:00, 21.05it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.00, valid_loss=-1.91]\n","Epoch 27:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.00, valid_loss=-1.91]\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.98, valid_loss=-1.91]\n","\u001b[36m(_train_tune pid=24034)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24034)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.54it/s]\u001b[A\n","Epoch 28:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.98, valid_loss=-1.98]\n","Epoch 29:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.89, valid_loss=-1.98]\n","Epoch 30:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-2.03, valid_loss=-1.98]\n","Epoch 31:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.96, valid_loss=-1.98]\n","Epoch 32:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.92, valid_loss=-1.98]\n","Epoch 33:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.00, valid_loss=-1.98]\n","Epoch 34:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.97, valid_loss=-1.98]\n","Epoch 35:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.02, valid_loss=-1.98]\n","Epoch 36:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.00, valid_loss=-1.98]\n","Epoch 37:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.59, train_loss_epoch=-2.09, valid_loss=-1.98]\n","Epoch 38:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.00, valid_loss=-1.98]\n","Epoch 39:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-2.01, valid_loss=-1.98]\n","Epoch 40:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.02, valid_loss=-1.98]\n","Epoch 41:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.54, train_loss_epoch=-2.09, valid_loss=-1.98]\n","Epoch 42:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.04, valid_loss=-1.98]\n","\u001b[36m(_train_tune pid=24034)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24034)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.54it/s]\u001b[A\n","Epoch 43:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.99, valid_loss=-1.99]\n","Epoch 44:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 45:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.06, valid_loss=-1.99]\n","Epoch 46:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 47:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-2.10, valid_loss=-1.99]\n","Epoch 48:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.99, valid_loss=-1.99]\n","Epoch 49:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.05, valid_loss=-1.99]\n","Epoch 50:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.99, valid_loss=-1.99]\n","Epoch 51:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.04, valid_loss=-1.99]\n","Epoch 52:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 53:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.99, valid_loss=-1.99]\n","Epoch 54:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 55:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-2.07, valid_loss=-1.99]\n","Epoch 56:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.05, valid_loss=-1.99]\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.95, valid_loss=-1.99]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24034)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24034)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.54it/s]\u001b[A\n","Epoch 57:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.95, valid_loss=-2.02]\n","Epoch 58:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.08, valid_loss=-2.02]\n","Epoch 59:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.03, valid_loss=-2.02]\n","Epoch 60:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.08, valid_loss=-2.02]\n","Epoch 61:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.01, valid_loss=-2.02]\n","Epoch 62:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.09, valid_loss=-2.02]\n","Epoch 63:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.06, valid_loss=-2.02]\n","Epoch 64:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-2.03, valid_loss=-2.02]\n","Epoch 65:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.05, valid_loss=-2.02]\n","Epoch 66:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.05, valid_loss=-2.02]\n","Epoch 67:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.09, valid_loss=-2.02]\n","Epoch 68:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.05, valid_loss=-2.02]\n","Epoch 69:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-2.01, valid_loss=-2.02]\n","Epoch 70:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.04, valid_loss=-2.02]\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.94, valid_loss=-2.02]\n","\u001b[36m(_train_tune pid=24034)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24034)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.53it/s]\u001b[A\n","Epoch 71:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.94, valid_loss=-2.02]\n","Epoch 72:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.06, valid_loss=-2.02]\n","Epoch 73:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.09, valid_loss=-2.02]\n","Epoch 74:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.44, train_loss_epoch=-2.13, valid_loss=-2.02]\n","Epoch 75:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.10, valid_loss=-2.02]\n","Epoch 76:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.08, valid_loss=-2.02]\n","Epoch 77:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.04, valid_loss=-2.02]\n","Epoch 78:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.41, train_loss_epoch=-2.12, valid_loss=-2.02]\n","Epoch 79:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.08, valid_loss=-2.02]\n","Epoch 80:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.09, valid_loss=-2.02]\n","Epoch 81:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-2.04, valid_loss=-2.02]\n","Epoch 82:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.08, valid_loss=-2.02]\n","Epoch 83:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.96, valid_loss=-2.02]\n","Epoch 84:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.04, valid_loss=-2.02]\n","Epoch 85:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.07, valid_loss=-2.02]\n","\u001b[36m(_train_tune pid=24034)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24034)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.58it/s]\u001b[A\n","Epoch 85:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.07, valid_loss=-2.04]\n","Epoch 86:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.08, valid_loss=-2.04]\n","Epoch 87:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.60, train_loss_epoch=-2.16, valid_loss=-2.04]\n","Epoch 88:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.13, valid_loss=-2.04]\n","Epoch 89:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.09, valid_loss=-2.04]\n","Epoch 90:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.10, valid_loss=-2.04]\n","Epoch 91:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-2.12, valid_loss=-2.04]\n","Epoch 92:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.06, valid_loss=-2.04]\n","Epoch 93:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.12, valid_loss=-2.04]\n","Epoch 94:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.10, valid_loss=-2.04]\n","Epoch 95:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-2.07, valid_loss=-2.04]\n","Epoch 96:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.46, train_loss_epoch=-2.14, valid_loss=-2.04]\n","Epoch 97:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.11, valid_loss=-2.04]\n","Epoch 98:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-2.13, valid_loss=-2.04]\n","Epoch 99:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.10, valid_loss=-2.04]\n","Epoch 99: 100%|██████████| 7/7 [00:00\u003c00:00, 20.59it/s, v_num=0, train_loss_step=-2.46, train_loss_epoch=-2.10, valid_loss=-2.04]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24034)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.53it/s]\u001b[A\n","Epoch 100:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.46, train_loss_epoch=-2.15, valid_loss=-2.04]\n","Epoch 101:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.12, valid_loss=-2.04]\n","Epoch 102:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-2.04, valid_loss=-2.04]\n","Epoch 103:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.10, valid_loss=-2.04]\n","Epoch 104:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.48, train_loss_epoch=-2.17, valid_loss=-2.04]\n","Epoch 105:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.08, valid_loss=-2.04]\n","Epoch 106:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-2.03, valid_loss=-2.04]\n","Epoch 107:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.12, valid_loss=-2.04]\n","Epoch 108:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-2.15, valid_loss=-2.04]\n","Epoch 109:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.09, valid_loss=-2.04]\n","Epoch 110:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-2.03, valid_loss=-2.04]\n","Epoch 111:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-2.15, valid_loss=-2.04]\n","Epoch 112:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.11, valid_loss=-2.04]\n","Epoch 113:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.12, valid_loss=-2.04]\n","Epoch 114:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.63, train_loss_epoch=-2.21, valid_loss=-2.04]\n","\u001b[36m(_train_tune pid=24034)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24034)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.57it/s]\u001b[A\n","Epoch 114:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.21, valid_loss=-2.04]\n","Epoch 115:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.07, valid_loss=-2.04]\n","Epoch 116:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-2.02, valid_loss=-2.04]\n","Epoch 117:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.47, train_loss_epoch=-2.17, valid_loss=-2.04]\n","Epoch 118:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-2.07, valid_loss=-2.04]\n","Epoch 119:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.13, valid_loss=-2.04]\n","Epoch 120:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.10, valid_loss=-2.04]\n","Epoch 121:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.09, valid_loss=-2.04]\n","Epoch 122:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.12, valid_loss=-2.04]\n","Epoch 123:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.13, valid_loss=-2.04]\n","Epoch 124:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.15, valid_loss=-2.04]\n","Epoch 125:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-2.15, valid_loss=-2.04]\n","Epoch 126:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.16, valid_loss=-2.04]\n","Epoch 127:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-2.04, valid_loss=-2.04]\n","Epoch 128:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.11, valid_loss=-2.04]\n","\u001b[36m(_train_tune pid=24034)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24034)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 19:26:11,970\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('MA(2,9)', 'MA(3,9)', 'MA(1,12)', 'MA(2,12)', 'MA(3,12)', 'MOM(9)', 'MOM(12)', 'RSI(7)', 'RSI(14)', 'EMA(3,9)', 'EMA(5,9)', 'EMA(5,12)', 'DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (168, 24, 1), 'n_pool_kernel_size': (16, 8, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=24034)\u001b[0m `Trainer.fit` stopped: `max_steps=900.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=24034)\u001b[0m \n","\u001b[36m(_train_tune pid=24034)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.52it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24034)\u001b[0m \r                                                                      \u001b[A\rEpoch 128:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.11, valid_loss=-2.05]\rEpoch 128:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.14, valid_loss=-2.05]\rEpoch 128:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.14, valid_loss=-2.05]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=24395)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=24395)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=24395)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=24395)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=24395)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=24395)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=24395)\u001b[0m 2024-11-12 19:26:19.572183: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=24395)\u001b[0m 2024-11-12 19:26:19.592918: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=24395)\u001b[0m 2024-11-12 19:26:19.618643: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=24395)\u001b[0m 2024-11-12 19:26:19.626531: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=24395)\u001b[0m 2024-11-12 19:26:19.644451: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=24395)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=24395)\u001b[0m 2024-11-12 19:26:20.839274: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=24395)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=24395)\u001b[0m \n","\u001b[36m(_train_tune pid=24395)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=24395)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=24395)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=24395)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=24395)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=24395)\u001b[0m 3 | blocks       | ModuleList       | 3.7 M  | train\n","\u001b[36m(_train_tune pid=24395)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=24395)\u001b[0m 3.7 M     Trainable params\n","\u001b[36m(_train_tune pid=24395)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=24395)\u001b[0m 3.7 M     Total params\n","\u001b[36m(_train_tune pid=24395)\u001b[0m 14.882    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=24395)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=24395)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","                                                                           \n","Epoch 0:   0%|          | 0/7 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.47]\n","Epoch 2:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-1.71]\n","Epoch 3:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.75]\n","Epoch 4:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.75]\n","Epoch 5:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.82]\n","Epoch 6:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.78]\n","Epoch 7:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.86]\n","Epoch 8:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.82]\n","Epoch 9:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-1.92]\n","Epoch 10:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.83]\n","Epoch 11:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-1.96]\n","Epoch 12:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.87]\n","Epoch 13:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.89]\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.89]\n","\u001b[36m(_train_tune pid=24395)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24395)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24395)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.51it/s]\u001b[A\n","Epoch 14:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.89, valid_loss=-1.91]\n","Epoch 15:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.90, valid_loss=-1.91]\n","Epoch 16:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-2.01, valid_loss=-1.91]\n","Epoch 17:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.91, valid_loss=-1.91]\n","Epoch 18:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.96, valid_loss=-1.91]\n","Epoch 19:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.48, train_loss_epoch=-2.03, valid_loss=-1.91]\n","Epoch 20:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.93, valid_loss=-1.91]\n","Epoch 21:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.97, valid_loss=-1.91]\n","Epoch 22:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.96, valid_loss=-1.91]\n","Epoch 23:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.95, valid_loss=-1.91]\n","Epoch 24:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.97, valid_loss=-1.91]\n","Epoch 25:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.02, valid_loss=-1.91]\n","Epoch 26:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.01, valid_loss=-1.91]\n","Epoch 27:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.00, valid_loss=-1.91]\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.98, valid_loss=-1.91]\n","\u001b[36m(_train_tune pid=24395)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24395)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.51it/s]\u001b[A\n","Epoch 28:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.98, valid_loss=-1.98]\n","Epoch 29:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.89, valid_loss=-1.98]\n","Epoch 30:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-2.03, valid_loss=-1.98]\n","Epoch 31:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.96, valid_loss=-1.98]\n","Epoch 32:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.92, valid_loss=-1.98]\n","Epoch 33:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.00, valid_loss=-1.98]\n","Epoch 34:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.97, valid_loss=-1.98]\n","Epoch 35:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.02, valid_loss=-1.98]\n","Epoch 36:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.00, valid_loss=-1.98]\n","Epoch 37:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.59, train_loss_epoch=-2.09, valid_loss=-1.98]\n","Epoch 38:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.00, valid_loss=-1.98]\n","Epoch 39:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-2.01, valid_loss=-1.98]\n","Epoch 40:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.02, valid_loss=-1.98]\n","Epoch 41:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.54, train_loss_epoch=-2.09, valid_loss=-1.98]\n","Epoch 42:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.04, valid_loss=-1.98]\n","\u001b[36m(_train_tune pid=24395)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24395)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24395)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.51it/s]\u001b[A\n","Epoch 43:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.99, valid_loss=-1.99]\n","Epoch 44:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 45:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.06, valid_loss=-1.99]\n","Epoch 46:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 47:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-2.10, valid_loss=-1.99]\n","Epoch 48:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.99, valid_loss=-1.99]\n","Epoch 49:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.05, valid_loss=-1.99]\n","Epoch 50:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.99, valid_loss=-1.99]\n","Epoch 51:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.04, valid_loss=-1.99]\n","Epoch 52:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 53:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.99, valid_loss=-1.99]\n","Epoch 54:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 55:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-2.07, valid_loss=-1.99]\n","Epoch 56:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.05, valid_loss=-1.99]\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.95, valid_loss=-1.99]\n","\u001b[36m(_train_tune pid=24395)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24395)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.53it/s]\u001b[A\n","Epoch 57:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.95, valid_loss=-2.02]\n","Epoch 58:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.08, valid_loss=-2.02]\n","Epoch 59:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.03, valid_loss=-2.02]\n","Epoch 60:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.08, valid_loss=-2.02]\n","Epoch 61:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.01, valid_loss=-2.02]\n","Epoch 62:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.09, valid_loss=-2.02]\n","Epoch 63:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.06, valid_loss=-2.02]\n","Epoch 64:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-2.03, valid_loss=-2.02]\n","Epoch 65:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.05, valid_loss=-2.02]\n","Epoch 66:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.05, valid_loss=-2.02]\n","Epoch 67:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.09, valid_loss=-2.02]\n","Epoch 68:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.05, valid_loss=-2.02]\n","Epoch 69:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-2.01, valid_loss=-2.02]\n","Epoch 70:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.04, valid_loss=-2.02]\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.94, valid_loss=-2.02]\n","\u001b[36m(_train_tune pid=24395)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24395)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.48it/s]\u001b[A\n","Epoch 71:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.94, valid_loss=-2.02]\n","Epoch 72:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.06, valid_loss=-2.02]\n","Epoch 73:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.09, valid_loss=-2.02]\n","Epoch 74:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.44, train_loss_epoch=-2.13, valid_loss=-2.02]\n","Epoch 75:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.10, valid_loss=-2.02]\n","Epoch 76:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.08, valid_loss=-2.02]\n","Epoch 77:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.04, valid_loss=-2.02]\n","Epoch 78:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.41, train_loss_epoch=-2.12, valid_loss=-2.02]\n","Epoch 79:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.08, valid_loss=-2.02]\n","Epoch 80:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.09, valid_loss=-2.02]\n","Epoch 81:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-2.04, valid_loss=-2.02]\n","Epoch 82:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.08, valid_loss=-2.02]\n","Epoch 83:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.96, valid_loss=-2.02]\n","Epoch 84:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.04, valid_loss=-2.02]\n","Epoch 85:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.07, valid_loss=-2.02]\n","\u001b[36m(_train_tune pid=24395)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24395)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.56it/s]\u001b[A\n","Epoch 85:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.07, valid_loss=-2.04]\n","Epoch 86:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.08, valid_loss=-2.04]\n","Epoch 87:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.60, train_loss_epoch=-2.16, valid_loss=-2.04]\n","Epoch 87: 100%|██████████| 7/7 [00:00\u003c00:00, 21.96it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.13, valid_loss=-2.04]\n","Epoch 88:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.13, valid_loss=-2.04]\n","Epoch 89:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.09, valid_loss=-2.04]\n","Epoch 90:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.10, valid_loss=-2.04]\n","Epoch 91:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-2.12, valid_loss=-2.04]\n","Epoch 92:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.06, valid_loss=-2.04]\n","Epoch 93:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.12, valid_loss=-2.04]\n","Epoch 94:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.10, valid_loss=-2.04]\n","Epoch 95:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-2.07, valid_loss=-2.04]\n","Epoch 96:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.46, train_loss_epoch=-2.14, valid_loss=-2.04]\n","Epoch 97:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.11, valid_loss=-2.04]\n","Epoch 98:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-2.13, valid_loss=-2.04]\n","Epoch 99:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.10, valid_loss=-2.04]\n","Epoch 99: 100%|██████████| 7/7 [00:00\u003c00:00, 20.68it/s, v_num=0, train_loss_step=-2.46, train_loss_epoch=-2.10, valid_loss=-2.04]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24395)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24395)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.53it/s]\u001b[A\n","Epoch 100:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.46, train_loss_epoch=-2.15, valid_loss=-2.04]\n","Epoch 101:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.12, valid_loss=-2.04]\n","Epoch 102:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-2.04, valid_loss=-2.04]\n","Epoch 103:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.10, valid_loss=-2.04]\n","Epoch 104:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.48, train_loss_epoch=-2.17, valid_loss=-2.04]\n","Epoch 104: 100%|██████████| 7/7 [00:00\u003c00:00, 21.27it/s, v_num=0, train_loss_step=-2.48, train_loss_epoch=-2.17, valid_loss=-2.04]\n","Epoch 105:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.08, valid_loss=-2.04]\n","Epoch 106:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-2.03, valid_loss=-2.04]\n","Epoch 107:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.12, valid_loss=-2.04]\n","Epoch 108:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-2.15, valid_loss=-2.04]\n","Epoch 109:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.09, valid_loss=-2.04]\n","Epoch 110:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-2.03, valid_loss=-2.04]\n","Epoch 111:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-2.15, valid_loss=-2.04]\n","Epoch 112:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.11, valid_loss=-2.04]\n","Epoch 113:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.12, valid_loss=-2.04]\n","Epoch 114:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.63, train_loss_epoch=-2.21, valid_loss=-2.04]\n","\u001b[36m(_train_tune pid=24395)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24395)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.52it/s]\u001b[A\n","Epoch 114:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.21, valid_loss=-2.04]\n","Epoch 115:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.07, valid_loss=-2.04]\n","Epoch 116:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-2.02, valid_loss=-2.04]\n","Epoch 117:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.47, train_loss_epoch=-2.17, valid_loss=-2.04]\n","Epoch 118:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-2.07, valid_loss=-2.04]\n","Epoch 119:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.13, valid_loss=-2.04]\n","Epoch 120:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.10, valid_loss=-2.04]\n","Epoch 121:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.09, valid_loss=-2.04]\n","Epoch 122:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.12, valid_loss=-2.04]\n","Epoch 123:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.13, valid_loss=-2.04]\n","Epoch 124:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.15, valid_loss=-2.04]\n","Epoch 125:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-2.15, valid_loss=-2.04]\n","Epoch 126:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.16, valid_loss=-2.04]\n","Epoch 127:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-2.04, valid_loss=-2.04]\n","Epoch 128:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.11, valid_loss=-2.04]\n","\u001b[36m(_train_tune pid=24395)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24395)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-12 19:27:25,452\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('MA(2,9)', 'MA(3,9)', 'MA(1,12)', 'MA(2,12)', 'MA(3,12)', 'MOM(9)', 'MOM(12)', 'RSI(7)', 'RSI(14)', 'EMA(3,9)', 'EMA(5,9)', 'EMA(5,12)', 'DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (168, 24, 1), 'n_pool_kernel_size': (16, 8, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=24395)\u001b[0m `Trainer.fit` stopped: `max_steps=900.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=24395)\u001b[0m \n","\u001b[36m(_train_tune pid=24395)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.51it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24395)\u001b[0m \r                                                                      \u001b[A\rEpoch 128:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.11, valid_loss=-2.05]\rEpoch 128:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.14, valid_loss=-2.05]\rEpoch 128:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.14, valid_loss=-2.05]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=24764)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=24764)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=24764)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=24764)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=24764)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=24764)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=24764)\u001b[0m 2024-11-12 19:27:33.670144: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=24764)\u001b[0m 2024-11-12 19:27:33.690490: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=24764)\u001b[0m 2024-11-12 19:27:33.715228: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=24764)\u001b[0m 2024-11-12 19:27:33.722842: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=24764)\u001b[0m 2024-11-12 19:27:33.740723: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=24764)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=24764)\u001b[0m 2024-11-12 19:27:34.928738: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=24764)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=24764)\u001b[0m \n","\u001b[36m(_train_tune pid=24764)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=24764)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=24764)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=24764)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=24764)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=24764)\u001b[0m 3 | blocks       | ModuleList       | 3.7 M  | train\n","\u001b[36m(_train_tune pid=24764)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=24764)\u001b[0m 3.7 M     Trainable params\n","\u001b[36m(_train_tune pid=24764)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=24764)\u001b[0m 3.7 M     Total params\n","\u001b[36m(_train_tune pid=24764)\u001b[0m 14.882    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=24764)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=24764)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/7 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.47]\n","Epoch 2:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-1.71]\n","Epoch 3:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.75]\n","Epoch 4:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.75]\n","Epoch 5:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.82]\n","Epoch 6:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.78]\n","Epoch 7:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.86]\n","Epoch 8:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.82]\n","Epoch 9:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-1.92]\n","Epoch 10:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.83]\n","Epoch 11:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-1.96]\n","Epoch 12:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.87]\n","Epoch 12: 100%|██████████| 7/7 [00:00\u003c00:00, 20.64it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.87]\n","Epoch 13:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.89]\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.89]\n","\u001b[36m(_train_tune pid=24764)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24764)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.51it/s]\u001b[A\n","Epoch 14:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.89, valid_loss=-1.91]\n","Epoch 15:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.90, valid_loss=-1.91]\n","Epoch 16:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-2.01, valid_loss=-1.91]\n","Epoch 17:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.91, valid_loss=-1.91]\n","Epoch 18:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.96, valid_loss=-1.91]\n","Epoch 19:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.48, train_loss_epoch=-2.03, valid_loss=-1.91]\n","Epoch 20:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.93, valid_loss=-1.91]\n","Epoch 21:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.97, valid_loss=-1.91]\n","Epoch 22:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.96, valid_loss=-1.91]\n","Epoch 23:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.95, valid_loss=-1.91]\n","Epoch 23: 100%|██████████| 7/7 [00:00\u003c00:00, 20.54it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.95, valid_loss=-1.91]\n","Epoch 24:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.97, valid_loss=-1.91]\n","Epoch 25:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.02, valid_loss=-1.91]\n","Epoch 26:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.01, valid_loss=-1.91]\n","Epoch 27:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.00, valid_loss=-1.91]\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.98, valid_loss=-1.91]\n","\u001b[36m(_train_tune pid=24764)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24764)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.50it/s]\u001b[A\n","Epoch 28:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.98, valid_loss=-1.98]\n","Epoch 29:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.89, valid_loss=-1.98]\n","Epoch 30:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-2.03, valid_loss=-1.98]\n","Epoch 31:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.96, valid_loss=-1.98]\n","Epoch 32:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.92, valid_loss=-1.98]\n","Epoch 33:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.00, valid_loss=-1.98]\n","Epoch 34:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.97, valid_loss=-1.98]\n","Epoch 35:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.02, valid_loss=-1.98]\n","Epoch 36:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.00, valid_loss=-1.98]\n","Epoch 37:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.59, train_loss_epoch=-2.09, valid_loss=-1.98]\n","Epoch 38:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.00, valid_loss=-1.98]\n","Epoch 39:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-2.01, valid_loss=-1.98]\n","Epoch 40:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.04, valid_loss=-1.98]\n","Epoch 41:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.58, train_loss_epoch=-2.11, valid_loss=-1.98]\n","Epoch 42:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.06, valid_loss=-1.98]\n","\u001b[36m(_train_tune pid=24764)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24764)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24764)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.51it/s]\u001b[A\n","Epoch 42:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.06, valid_loss=-1.99]\n","Epoch 43:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-2.01, valid_loss=-1.99]\n","Epoch 44:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.04, valid_loss=-1.99]\n","Epoch 45:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.07, valid_loss=-1.99]\n","Epoch 46:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 46: 100%|██████████| 7/7 [00:00\u003c00:00, 20.64it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-2.10, valid_loss=-1.99]\n","Epoch 47:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-2.10, valid_loss=-1.99]\n","Epoch 48:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.99, valid_loss=-1.99]\n","Epoch 49:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.04, valid_loss=-1.99]\n","Epoch 50:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.99, valid_loss=-1.99]\n","Epoch 51:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.04, valid_loss=-1.99]\n","Epoch 52:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 53:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.98, valid_loss=-1.99]\n","Epoch 54:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.03, valid_loss=-1.99]\n","Epoch 55:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-2.07, valid_loss=-1.99]\n","Epoch 56:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.05, valid_loss=-1.99]\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.95, valid_loss=-1.99]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24764)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24764)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.48it/s]\u001b[A\n","Epoch 57:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.95, valid_loss=-2.02]\n","Epoch 58:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.36, train_loss_epoch=-2.08, valid_loss=-2.02]\n","Epoch 59:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.03, valid_loss=-2.02]\n","Epoch 60:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.08, valid_loss=-2.02]\n","Epoch 61:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.01, valid_loss=-2.02]\n","Epoch 62:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.09, valid_loss=-2.02]\n","Epoch 63:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.06, valid_loss=-2.02]\n","Epoch 64:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-2.03, valid_loss=-2.02]\n","Epoch 65:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.05, valid_loss=-2.02]\n","Epoch 66:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.05, valid_loss=-2.02]\n","Epoch 67:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.09, valid_loss=-2.02]\n","Epoch 68:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.05, valid_loss=-2.02]\n","Epoch 69:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-2.01, valid_loss=-2.02]\n","Epoch 70:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.04, valid_loss=-2.02]\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.93, valid_loss=-2.02]\n","\u001b[36m(_train_tune pid=24764)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24764)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24764)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.54it/s]\u001b[A\n","Epoch 71:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.93, valid_loss=-2.01]\n","Epoch 72:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.05, valid_loss=-2.01]\n","Epoch 73:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.09, valid_loss=-2.01]\n","Epoch 74:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.44, train_loss_epoch=-2.13, valid_loss=-2.01]\n","Epoch 75:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.10, valid_loss=-2.01]\n","Epoch 76:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.08, valid_loss=-2.01]\n","Epoch 77:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-2.04, valid_loss=-2.01]\n","Epoch 78:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-2.14, valid_loss=-2.01]\n","Epoch 79:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.09, valid_loss=-2.01]\n","Epoch 80:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.10, valid_loss=-2.01]\n","Epoch 81:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-2.05, valid_loss=-2.01]\n","Epoch 82:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.09, valid_loss=-2.01]\n","Epoch 83:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.98, valid_loss=-2.01]\n","Epoch 83: 100%|██████████| 7/7 [00:00\u003c00:00, 19.88it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.06, valid_loss=-2.01]\n","Epoch 84:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.06, valid_loss=-2.01]\n","Epoch 85:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.09, valid_loss=-2.01]\n","\u001b[36m(_train_tune pid=24764)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24764)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24764)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.52it/s]\u001b[A\n","Epoch 85:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.09, valid_loss=-2.03]\n","Epoch 86:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.10, valid_loss=-2.03]\n","Epoch 87:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.59, train_loss_epoch=-2.16, valid_loss=-2.03]\n","Epoch 88:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-2.13, valid_loss=-2.03]\n","Epoch 89:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.08, valid_loss=-2.03]\n","Epoch 90:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.10, valid_loss=-2.03]\n","Epoch 91:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-2.12, valid_loss=-2.03]\n","Epoch 92:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.06, valid_loss=-2.03]\n","Epoch 93:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.12, valid_loss=-2.03]\n","Epoch 94:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.09, valid_loss=-2.03]\n","Epoch 95:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-2.07, valid_loss=-2.03]\n","Epoch 96:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.45, train_loss_epoch=-2.13, valid_loss=-2.03]\n","Epoch 97:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.10, valid_loss=-2.03]\n","Epoch 98:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-2.13, valid_loss=-2.03]\n","Epoch 99:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.10, valid_loss=-2.03]\n","Epoch 99: 100%|██████████| 7/7 [00:00\u003c00:00, 19.77it/s, v_num=0, train_loss_step=-2.46, train_loss_epoch=-2.10, valid_loss=-2.03]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24764)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.50it/s]\u001b[A\n","Epoch 100:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.46, train_loss_epoch=-2.15, valid_loss=-2.04]\n","Epoch 101:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.11, valid_loss=-2.04]\n","Epoch 102:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-2.04, valid_loss=-2.04]\n","Epoch 103:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.10, valid_loss=-2.04]\n","Epoch 104:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.48, train_loss_epoch=-2.16, valid_loss=-2.04]\n","Epoch 105:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-2.08, valid_loss=-2.04]\n","Epoch 106:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-2.03, valid_loss=-2.04]\n","Epoch 107:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.12, valid_loss=-2.04]\n","Epoch 108:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-2.15, valid_loss=-2.04]\n","Epoch 109:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.09, valid_loss=-2.04]\n","Epoch 110:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-2.02, valid_loss=-2.04]\n","Epoch 111:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.40, train_loss_epoch=-2.15, valid_loss=-2.04]\n","Epoch 112:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.11, valid_loss=-2.04]\n","Epoch 113:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.12, valid_loss=-2.04]\n","Epoch 114:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.63, train_loss_epoch=-2.21, valid_loss=-2.04]\n","\u001b[36m(_train_tune pid=24764)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=24764)\u001b[0m `Trainer.fit` stopped: `max_steps=800.0` reached.\n","2024-11-12 19:28:33,676\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('MA(2,9)', 'MA(3,9)', 'MA(1,12)', 'MA(2,12)', 'MA(3,12)', 'MOM(9)', 'MOM(12)', 'RSI(7)', 'RSI(14)', 'EMA(3,9)', 'EMA(5,9)', 'EMA(5,12)', 'DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (168, 24, 1), 'n_pool_kernel_size': (16, 8, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","2024-11-12 19:28:33,700\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/root/ray_results/_train_tune_2024-11-12_18-54-39' in 0.0206s.\n","2024-11-12 19:28:33,702\tERROR tune.py:1037 -- Trials did not complete: [_train_tune_be5546d3, _train_tune_e2f48f06, _train_tune_65a60f7c, _train_tune_64a1c5ae, _train_tune_d1d0fb02]\n","INFO:lightning_fabric.utilities.seed:Seed set to 42\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=24764)\u001b[0m \n","\u001b[36m(_train_tune pid=24764)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.55it/s]\u001b[A\n","\u001b[36m(_train_tune pid=24764)\u001b[0m \r                                                                      \u001b[A\rEpoch 114:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.21, valid_loss=-2.04]\rEpoch 114:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.11, valid_loss=-2.04]\rEpoch 114:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.11, valid_loss=-2.04]\n","\n"]},{"name":"stderr","output_type":"stream","text":["INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","INFO:pytorch_lightning.callbacks.model_summary:\n","  | Name         | Type             | Params | Mode \n","----------------------------------------------------------\n","0 | loss         | DistributionLoss | 3      | eval \n","1 | padder_train | ConstantPad1d    | 0      | train\n","2 | scaler       | TemporalNorm     | 0      | train\n","3 | blocks       | ModuleList       | 3.7 M  | train\n","----------------------------------------------------------\n","3.7 M     Trainable params\n","3         Non-trainable params\n","3.7 M     Total params\n","14.882    Total estimated model params size (MB)\n","33        Modules in train mode\n","1         Modules in eval mode\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d3edbd0882da4d2fb3ba369f7447eed7","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f5091c545fc64a67964f997acd726ca1","version_major":2,"version_minor":0},"text/plain":["Training: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"028f9096e2cc4e6b87e658ceece4bb34","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2ac5ce75a0234782b46b693abea77d78","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"43b74682b7ab4e628e136ab5458a548c","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"82abce83d9ea4002ae64532dfef77d70","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"32366ba8c592495b8308ec0e916b61eb","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"48470a40aaa14a4b8bb99be9c055c51b","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"798f49a48d304c15a2b0cc6783ce4ea3","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0adbf461ad834b22ae3e74ce1e8436c1","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d46ec56a23f041ca9c8e70c322d25493","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_steps=900.0` reached.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n","INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ba7973460e91450b9b642e5c8e263eae","version_major":2,"version_minor":0},"text/plain":["Predicting: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Horizon 20 CV Minutes: 73.07346337238947\n"]}],"source":["trainer = AutoNHITSTrainer(horizons, levels, exog_list, df, val_size, test_size)\n","trainer.run_training()"]},{"cell_type":"markdown","metadata":{"id":"FussJJd_GMW5"},"source":["# Modelling half (12 years total, 6 train, 2 val, 4 test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"kd8klPo4Gu8A"},"outputs":[],"source":["from neuralforecast.core import NeuralForecast\n","from ray import tune\n","from ray.tune.search.hyperopt import HyperOptSearch\n","from neuralforecast.losses.pytorch import DistributionLoss\n","from neuralforecast.auto import  AutoNHITS"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"j2JXf3hYGo65","outputId":"827db69e-fd37-4fc7-9008-833f38ebe0eb"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u003cclass 'pandas.core.frame.DataFrame'\u003e\n","RangeIndex: 2008545 entries, 0 to 2008544\n","Data columns (total 21 columns):\n"," #   Column     Dtype         \n","---  ------     -----         \n"," 0   unique_id  object        \n"," 1   ds         datetime64[ns]\n"," 2   y          float32       \n"," 3   MA(2,9)    int32         \n"," 4   MA(3,9)    int32         \n"," 5   MA(1,12)   int32         \n"," 6   MA(2,12)   int32         \n"," 7   MA(3,12)   int32         \n"," 8   MOM(9)     int32         \n"," 9   MOM(12)    int32         \n"," 10  RSI(7)     int32         \n"," 11  RSI(14)    int32         \n"," 12  EMA(3,9)   int32         \n"," 13  EMA(5,9)   int32         \n"," 14  EMA(5,12)  int32         \n"," 15  DY         float32       \n"," 16  PTBV       float32       \n"," 17  P          float32       \n"," 18  PO         float32       \n"," 19  VO         float32       \n"," 20  PE         float32       \n","dtypes: datetime64[ns](1), float32(7), int32(12), object(1)\n","memory usage: 176.2+ MB\n"]}],"source":["df = pd.read_csv('Data/S\u0026P500/3ProSP500.csv')\n","df['ds'] = pd.to_datetime(df['ds'])\n","df = df.astype({col: 'int32' if dtype == 'int64' else 'float32' if dtype == 'float64' else dtype\n","                for col, dtype in df.dtypes.items()})\n","df = df.rename(columns={'840E': 'y'})\n","df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hwJzeCE0Gqu6","outputId":"7eab95f6-f3df-473b-e662-649e93f62b19"},"outputs":[{"name":"stdout","output_type":"stream","text":["2004 2023\n","20\n"]}],"source":["print(df['ds'].min().year, df['ds'].max().year)\n","print(df['ds'].max().year - df['ds'].min().year + 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"qfvdp3RBGsvt","outputId":"f1e1277a-c1fb-479f-da2c-98811b3a4c94"},"outputs":[{"name":"stdout","output_type":"stream","text":["2012 2023\n","12\n"]}],"source":["# Calculate the starting year and add 8 years\n","starting_year = df['ds'].min().year + 8\n","# Create a new date range\n","new_start_date = pd.to_datetime(str(starting_year) + '-' + str(df['ds'].min().month) + '-' + str(df['ds'].min().day))\n","#Filter out the data based on the new starting year\n","df = df[df['ds'] \u003e= new_start_date]\n","print(df['ds'].min().year, df['ds'].max().year)\n","print(df['ds'].max().year - df['ds'].min().year + 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"i900myd3GbtZ","outputId":"f0d4cdd5-5781-420c-e29b-ec3075f02559"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"summary_counts\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"Period\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"2018-2019\",\n          \"2020-2023\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unique Days Covered\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 368,\n        \"min\": 522,\n        \"max\": 1043,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          522,\n          1043\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"summary_counts"},"text/html":["\n","  \u003cdiv id=\"df-0d006100-ee90-412a-815a-a1cfeedf3c3b\" class=\"colab-df-container\"\u003e\n","    \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003ePeriod\u003c/th\u003e\n","      \u003cth\u003eUnique Days Covered\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e2020-2023\u003c/td\u003e\n","      \u003ctd\u003e1043\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e2018-2019\u003c/td\u003e\n","      \u003ctd\u003e522\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e\n","    \u003cdiv class=\"colab-df-buttons\"\u003e\n","\n","  \u003cdiv class=\"colab-df-container\"\u003e\n","    \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0d006100-ee90-412a-815a-a1cfeedf3c3b')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\"\u003e\n","\n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\"\u003e\n","    \u003cpath d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/\u003e\n","  \u003c/svg\u003e\n","    \u003c/button\u003e\n","\n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","    \u003cscript\u003e\n","      const buttonEl =\n","        document.querySelector('#df-0d006100-ee90-412a-815a-a1cfeedf3c3b button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-0d006100-ee90-412a-815a-a1cfeedf3c3b');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    \u003c/script\u003e\n","  \u003c/div\u003e\n","\n","\n","\u003cdiv id=\"df-a9b8a947-5ffa-45cb-bfd3-befe8a9a2663\"\u003e\n","  \u003cbutton class=\"colab-df-quickchart\" onclick=\"quickchart('df-a9b8a947-5ffa-45cb-bfd3-befe8a9a2663')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\"\u003e\n","\n","\u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\"\u003e\n","    \u003cg\u003e\n","        \u003cpath d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/\u003e\n","    \u003c/g\u003e\n","\u003c/svg\u003e\n","  \u003c/button\u003e\n","\n","\u003cstyle\u003e\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","\u003c/style\u003e\n","\n","  \u003cscript\u003e\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() =\u003e {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-a9b8a947-5ffa-45cb-bfd3-befe8a9a2663 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  \u003c/script\u003e\n","\u003c/div\u003e\n","\n","  \u003cdiv id=\"id_327644d8-874f-42f3-b602-e826d3653cef\"\u003e\n","    \u003cstyle\u003e\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    \u003c/style\u003e\n","    \u003cbutton class=\"colab-df-generate\" onclick=\"generateWithVariable('summary_counts')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\"\u003e\n","\n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/\u003e\n","  \u003c/svg\u003e\n","    \u003c/button\u003e\n","    \u003cscript\u003e\n","      (() =\u003e {\n","      const buttonEl =\n","        document.querySelector('#id_327644d8-874f-42f3-b602-e826d3653cef button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () =\u003e {\n","        google.colab.notebook.generateWithVariable('summary_counts');\n","      }\n","      })();\n","    \u003c/script\u003e\n","  \u003c/div\u003e\n","\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n"],"text/plain":["      Period  Unique Days Covered\n","0  2020-2023                 1043\n","1  2018-2019                  522"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["start_2020_2023 = '2020-01-01'\n","end_2020_2023 = '2023-12-31'\n","\n","start_2018_2019 = '2018-01-01'\n","end_2018_2019 = '2019-12-31'\n","\n","# Filter the DataFrame for each time range and extract unique days\n","unique_days_2020_2023 = df[(df['ds'] \u003e= start_2020_2023) \u0026 (df['ds'] \u003c= end_2020_2023)]['ds'].dt.date.unique()\n","unique_days_2018_2019 = df[(df['ds'] \u003e= start_2018_2019) \u0026 (df['ds'] \u003c= end_2018_2019)]['ds'].dt.date.unique()\n","\n","# Count the number of unique days in each range\n","count_unique_days_2020_2023 = len(unique_days_2020_2023)\n","count_unique_days_2018_2019 = len(unique_days_2018_2019)\n","\n","# Create a summary DataFrame\n","summary_counts = pd.DataFrame({\n","    'Period': ['2020-2023', '2018-2019'],\n","    'Unique Days Covered': [count_unique_days_2020_2023, count_unique_days_2018_2019]\n","})\n","summary_counts"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"axPEBmcaGZRE"},"outputs":[],"source":["exog_list = list(df.columns)\n","exog_list.remove('ds')\n","exog_list.remove('y')\n","exog_list.remove('unique_id')\n","\n","levels = [90]\n","val_size = count_unique_days_2018_2019\n","test_size = count_unique_days_2020_2023\n","horizons = [1, 5, 10, 20]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Ld6QQWq9GMoP"},"outputs":[],"source":["import os\n","from time import time\n","\n","class AutoNHITSTrainer:\n","    def __init__(self, horizons, levels, exog_list, df, val_size, test_size):\n","        self.horizons = horizons\n","        self.levels = levels\n","        self.exog_list = exog_list\n","        self.df = df\n","        self.val_size = val_size\n","        self.test_size = test_size\n","\n","    def check_existing_files(self, horizon):\n","        \"\"\"Checks if model and CSV already exist for a given horizon.\"\"\"\n","        model_path = f'Trained Models/AutoNHITS/8TYhorizon_{horizon}/'\n","        output_csv = f'Data/Test/8TYnhits_model0_1_horizon_{horizon}.csv'\n","        return os.path.exists(model_path) and os.path.exists(output_csv)\n","\n","    def save_results(self, nf, horizon, Y_hat_df):\n","        \"\"\"Saves the trained model and prediction results.\"\"\"\n","        model_path = f'Trained Models/AutoNHITS/8TYhorizon_{horizon}/'\n","        output_csv = f'Data/Test/horizon_{horizon}/8TYnhits_model0_1_horizon_{horizon}.csv'\n","\n","        # Create model directory if it doesn't exist\n","        os.makedirs(model_path, exist_ok=True)\n","\n","        # Save the model, predictions and hyperparameter search\n","        for idx, model in enumerate(nf.models):\n","          hpo = f'HPO/horizon_{horizon}/8TYnhits_model{idx}_horizon_{horizon}_hpo.csv'\n","          results = model.results.get_dataframe()\n","          results.to_csv(hpo, index=False)\n","\n","        nf.save(path=model_path, model_index=None, overwrite=True, save_dataset=False)\n","\n","        for col in Y_hat_df.select_dtypes(include='float32').columns:\n","          Y_hat_df[col] = Y_hat_df[col].astype('float16')\n","\n","        Y_hat_df.to_csv(output_csv, index=False)\n","\n","\n","    def configure_models(self, horizon):\n","        \"\"\"Configures two AutoNHITS models for the given horizon.\"\"\"\n","        # Model 0 Configuration\n","        nhits_config0 = AutoNHITS.get_default_config(h=horizon, backend=\"ray\")\n","        nhits_config0['random_seed'] = 42\n","        nhits_config0['learning_rate'] = tune.choice([0.01, 0.005, 0.001, 0.0005, 0.0001, 0.0005, 0.0001, 0.00005, 0.00001])\n","\n","        # Model 1 Configuration\n","        nhits_config1 = AutoNHITS.get_default_config(h=horizon, backend=\"ray\")\n","        nhits_config1['hist_exog_list'] = self.exog_list\n","        nhits_config1['random_seed'] = 42\n","        nhits_config1['learning_rate'] = tune.choice([0.01, 0.005, 0.001, 0.0005, 0.0001, 0.0005, 0.0001, 0.00005, 0.00001])\n","\n","        # Instantiate the models\n","        nhits_model0 = AutoNHITS(h=horizon,\n","                                 config=nhits_config0,\n","                                 search_alg=HyperOptSearch(), num_samples=30,\n","                                 backend='ray',\n","                                 loss=DistributionLoss(distribution='StudentT', level=self.levels),\n","                                 cpus = 12, gpus = 1)\n","\n","        nhits_model1 = AutoNHITS(h=horizon,\n","                                 config=nhits_config1,\n","                                 search_alg=HyperOptSearch(), num_samples=30,\n","                                 backend='ray',\n","                                 loss=DistributionLoss(distribution='StudentT', level=self.levels),\n","                                 cpus = 12, gpus = 1)\n","\n","        return nhits_model0, nhits_model1\n","\n","    def run_training(self):\n","        \"\"\"Runs the training loop over all horizons.\"\"\"\n","        for horizon in self.horizons:\n","            if self.check_existing_files(horizon):\n","                print(f\"Horizon {horizon}: Model and CSV already exist. Skipping this run.\")\n","                continue\n","\n","            # Configure the models\n","            nhits_model0, nhits_model1 = self.configure_models(horizon)\n","\n","            # Start training and cross-validation\n","            init = time()\n","            nf = NeuralForecast(models=[nhits_model0, nhits_model1], freq='B')\n","            Y_hat_df = nf.cross_validation(df=self.df,\n","                                           val_size=self.val_size,\n","                                           test_size=self.test_size,\n","                                           n_windows=None)\n","\n","            # Save results\n","            self.save_results(nf, horizon, Y_hat_df)\n","\n","            # Log the time taken\n","            end = time()\n","            print(f'Horizon {horizon} CV Minutes: {(end - init) / 60}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"u--_fd5pGW3F","outputId":"1c6f3ac2-2a2f-4114-c458-30625fdab384"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------------------------------------------------------+\n","| Configuration for experiment     _train_tune_2024-11-13_20-14-28   |\n","+--------------------------------------------------------------------+\n","| Search algorithm                 SearchGenerator                   |\n","| Scheduler                        FIFOScheduler                     |\n","| Number of trials                 30                                |\n","+--------------------------------------------------------------------+\n","\n","View detailed results here: /root/ray_results/_train_tune_2024-11-13_20-14-28\n","To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-11-13_11-38-37_188179_3696/artifacts/2024-11-13_20-14-28/_train_tune_2024-11-13_20-14-28/driver_artifacts`\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=159411)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=159411)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=159411)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=159411)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=159411)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=159411)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=159411)\u001b[0m 2024-11-13 20:14:35.819724: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=159411)\u001b[0m 2024-11-13 20:14:35.839290: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=159411)\u001b[0m 2024-11-13 20:14:35.864020: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=159411)\u001b[0m 2024-11-13 20:14:35.871532: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=159411)\u001b[0m 2024-11-13 20:14:35.888976: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=159411)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=159411)\u001b[0m 2024-11-13 20:14:37.057588: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=159411)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=159411)\u001b[0m \n","\u001b[36m(_train_tune pid=159411)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=159411)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=159411)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=159411)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=159411)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=159411)\u001b[0m 3 | blocks       | ModuleList       | 2.4 M  | train\n","\u001b[36m(_train_tune pid=159411)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=159411)\u001b[0m 2.4 M     Trainable params\n","\u001b[36m(_train_tune pid=159411)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=159411)\u001b[0m 2.4 M     Total params\n","\u001b[36m(_train_tune pid=159411)\u001b[0m 9.493     Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=159411)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=159411)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/2 [00:00\u003c?, ?it/s] \n","Epoch 0: 100%|██████████| 2/2 [00:00\u003c00:00,  6.62it/s, v_num=0, train_loss_step=0.874]\n","Epoch 1:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.874, train_loss_epoch=0.919]\n","Epoch 2:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.680, train_loss_epoch=0.732]\n","Epoch 3:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.497, train_loss_epoch=0.555]\n","Epoch 4:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.299, train_loss_epoch=0.352]\n","Epoch 5:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.0705, train_loss_epoch=0.133]\n","Epoch 6:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.195, train_loss_epoch=-0.128]\n","Epoch 7:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.494, train_loss_epoch=-0.417]\n","Epoch 8:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.754, train_loss_epoch=-0.697]\n","Epoch 9:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.987, train_loss_epoch=-0.946]\n","Epoch 10:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.05, train_loss_epoch=-1.03]\n","Epoch 11:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.15]\n","Epoch 12:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.20]\n","Epoch 13:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.20]\n","Epoch 14:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.23]\n","Epoch 15:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.24]\n","Epoch 16:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.27]\n","Epoch 17:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.27]\n","Epoch 18:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.27]\n","Epoch 19:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.27]\n","Epoch 20:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.29]\n","Epoch 21:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.29]\n","Epoch 22:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.27]\n","Epoch 23:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.30]\n","Epoch 24:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.29]\n","Epoch 25:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.27]\n","Epoch 26:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.29]\n","Epoch 27:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.31]\n","Epoch 28:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.30]\n","Epoch 29:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.31]\n","Epoch 30:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.31]\n","Epoch 31:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.31]\n","Epoch 32:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.31]\n","Epoch 33:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.31]\n","Epoch 34:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.31]\n","Epoch 35:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.32]\n","Epoch 36:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.32]\n","Epoch 37:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.32]\n","Epoch 38:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.33]\n","Epoch 39:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.33]\n","Epoch 40:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.32]\n","Epoch 41:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.32]\n","Epoch 42:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.33]\n","Epoch 43:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.32]\n","Epoch 44:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.33]\n","Epoch 45:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.32]\n","Epoch 46:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.32]\n","Epoch 47:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.33]\n","Epoch 48:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.33]\n","Epoch 49:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.33]\n","Epoch 49: 100%|██████████| 2/2 [00:00\u003c00:00, 12.60it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.33]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=159411)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00,  5.02it/s]\u001b[A\n","Epoch 50:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.34, valid_loss=-1.33]\n","Epoch 51:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.34, valid_loss=-1.33]\n","Epoch 52:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.31, valid_loss=-1.33]\n","Epoch 53:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.26, valid_loss=-1.33]\n","Epoch 54:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.32, valid_loss=-1.33]\n","Epoch 55:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.32, valid_loss=-1.33]\n","Epoch 56:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.34, valid_loss=-1.33]\n","Epoch 57:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.33, valid_loss=-1.33]\n","Epoch 58:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.34, valid_loss=-1.33]\n","Epoch 59:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.31, valid_loss=-1.33]\n","Epoch 60:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.34, valid_loss=-1.33]\n","Epoch 61:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.32, valid_loss=-1.33]\n","Epoch 62:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.34, valid_loss=-1.33]\n","Epoch 63:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.33, valid_loss=-1.33]\n","Epoch 64:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.33, valid_loss=-1.33]\n","Epoch 65:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.33, valid_loss=-1.33]\n","Epoch 66:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.34, valid_loss=-1.33]\n","Epoch 67:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.32, valid_loss=-1.33]\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-13 20:14:51,595\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n","2024-11-13 20:14:51,607\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/root/ray_results/_train_tune_2024-11-13_20-14-28' in 0.0098s.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 68:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.34, valid_loss=-1.33]\n","Epoch 69:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.33, valid_loss=-1.33]\n","Epoch 70:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.32, valid_loss=-1.33]\n","Epoch 71:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.34, valid_loss=-1.33]\n","Epoch 72:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.34, valid_loss=-1.33]\n","Epoch 73:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.34, valid_loss=-1.33]\n","Epoch 74:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.34, valid_loss=-1.33]\n","Epoch 75:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.32, valid_loss=-1.33]\n","Epoch 76:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.34, valid_loss=-1.33]\n","Epoch 77:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.35, valid_loss=-1.33]\n","Epoch 78:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.34, valid_loss=-1.33]\n","Epoch 79:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.35, valid_loss=-1.33]\n","Epoch 80:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.35, valid_loss=-1.33]\n","Epoch 81:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.34, valid_loss=-1.33]\n","Epoch 82:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.33, valid_loss=-1.33]\n","Epoch 83:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.32, valid_loss=-1.33]\n","Epoch 84:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.32, valid_loss=-1.33]\n","Epoch 85:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.35, valid_loss=-1.33]\n","Epoch 86:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.32, valid_loss=-1.33]\n","Epoch 87:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.31, valid_loss=-1.33]\n","Epoch 88:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.35, valid_loss=-1.33]\n","Epoch 89:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.32, valid_loss=-1.33]\n","Epoch 90:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.30, valid_loss=-1.33]\n","Epoch 91:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.35, valid_loss=-1.33]\n","Epoch 91: 100%|██████████| 2/2 [00:00\u003c00:00, 13.28it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.35, valid_loss=-1.33]\n","Epoch 92:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.35, valid_loss=-1.33]\n","Epoch 93:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.35, valid_loss=-1.33]\n","Epoch 94:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.32, valid_loss=-1.33]\n","Epoch 95:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.33, valid_loss=-1.33]\n","Epoch 96:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.35, valid_loss=-1.33]\n","Epoch 96: 100%|██████████| 2/2 [00:00\u003c00:00, 13.39it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.33, valid_loss=-1.33]\n","Epoch 97:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.33, valid_loss=-1.33]\n","Epoch 98:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.32, valid_loss=-1.33]\n","Epoch 99:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.35, valid_loss=-1.33]\n","Epoch 99: 100%|██████████| 2/2 [00:00\u003c00:00, 13.34it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.35, valid_loss=-1.33]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-13 20:14:57,057\tWARNING tune.py:1056 -- Experiment has been interrupted, but the most recent state was saved.\n","Resume experiment with: Tuner.restore(path=\"/root/ray_results/_train_tune_2024-11-13_20-14-28\", trainable=...)\n","2024-11-13 20:14:57,063\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 1 trial(s):\n","- _train_tune_d6792489: FileNotFoundError('Could not fetch metrics for _train_tune_d6792489: both result.json and progress.csv were not found at /root/ray_results/_train_tune_2024-11-13_20-14-28/_train_tune_d6792489_2_batch_size=128,h=1,input_size=2,learning_rate=0.0005,loss=ref_ph_de895953,max_steps=600.0000,n_freq_downsam_2024-11-13_20-14-35')\n","INFO:lightning_fabric.utilities.seed:Seed set to 42\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"name":"stdout","output_type":"stream","text":["\n","\u001b[36m(_train_tune pid=159411)\u001b[0m \n","\u001b[36m(_train_tune pid=159411)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00,  5.21it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","INFO:pytorch_lightning.callbacks.model_summary:\n","  | Name         | Type             | Params | Mode \n","----------------------------------------------------------\n","0 | loss         | DistributionLoss | 3      | eval \n","1 | padder_train | ConstantPad1d    | 0      | train\n","2 | scaler       | TemporalNorm     | 0      | train\n","3 | blocks       | ModuleList       | 2.4 M  | train\n","----------------------------------------------------------\n","2.4 M     Trainable params\n","3         Non-trainable params\n","2.4 M     Total params\n","9.493     Total estimated model params size (MB)\n","33        Modules in train mode\n","1         Modules in eval mode\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"de4fd974748941e6bc3271a35b815e0e","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"da5b9fc7ba2e4bc2becea6551b9fb056","version_major":2,"version_minor":0},"text/plain":["Training: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["trainer = AutoNHITSTrainer(horizons, levels, exog_list, df, val_size, test_size)\n","trainer.run_training()"]},{"cell_type":"markdown","metadata":{"id":"STHIBJF8f97w"},"source":["# Modelling Technical Indicators (20 years total, 14 train, 2 val, 4 test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"0TtN-TfOg3GR"},"outputs":[],"source":["from neuralforecast.core import NeuralForecast\n","from ray import tune\n","from ray.tune.search.hyperopt import HyperOptSearch\n","from neuralforecast.losses.pytorch import DistributionLoss\n","from neuralforecast.auto import AutoNHITS"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"mTp2Wr4LgDEl","outputId":"03f204ab-31c8-417e-c2b5-4dffa6832fb2"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u003cclass 'pandas.core.frame.DataFrame'\u003e\n","RangeIndex: 2008545 entries, 0 to 2008544\n","Data columns (total 15 columns):\n"," #   Column     Dtype         \n","---  ------     -----         \n"," 0   unique_id  object        \n"," 1   ds         datetime64[ns]\n"," 2   y          float32       \n"," 3   MA(2,9)    int32         \n"," 4   MA(3,9)    int32         \n"," 5   MA(1,12)   int32         \n"," 6   MA(2,12)   int32         \n"," 7   MA(3,12)   int32         \n"," 8   MOM(9)     int32         \n"," 9   MOM(12)    int32         \n"," 10  RSI(7)     int32         \n"," 11  RSI(14)    int32         \n"," 12  EMA(3,9)   int32         \n"," 13  EMA(5,9)   int32         \n"," 14  EMA(5,12)  int32         \n","dtypes: datetime64[ns](1), float32(1), int32(12), object(1)\n","memory usage: 130.3+ MB\n"]}],"source":["df = pd.read_csv('Data/S\u0026P500/3ProSP500.csv')\n","df['ds'] = pd.to_datetime(df['ds'])\n","df = df.astype({col: 'int32' if dtype == 'int64' else 'float32' if dtype == 'float64' else dtype\n","                for col, dtype in df.dtypes.items()})\n","df = df.rename(columns={'840E': 'y'})\n","\n","selected_columns = [\n","    'unique_id', 'ds', 'y', 'MA(2,9)', 'MA(3,9)', 'MA(1,12)',\n","    'MA(2,12)', 'MA(3,12)', 'MOM(9)', 'MOM(12)', 'RSI(7)',\n","    'RSI(14)', 'EMA(3,9)', 'EMA(5,9)', 'EMA(5,12)'\n","]\n","df = df[selected_columns]\n","\n","df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"YkkdZIwSgE6u","outputId":"299de3fb-5ad3-4fec-82c7-4e5bdc114ec3"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"summary_counts\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"Period\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"2018-2019\",\n          \"2020-2023\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unique Days Covered\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 368,\n        \"min\": 522,\n        \"max\": 1043,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          522,\n          1043\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"summary_counts"},"text/html":["\n","  \u003cdiv id=\"df-e196c177-2e12-4ce1-9535-43852a8f7232\" class=\"colab-df-container\"\u003e\n","    \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003ePeriod\u003c/th\u003e\n","      \u003cth\u003eUnique Days Covered\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e2020-2023\u003c/td\u003e\n","      \u003ctd\u003e1043\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e2018-2019\u003c/td\u003e\n","      \u003ctd\u003e522\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e\n","    \u003cdiv class=\"colab-df-buttons\"\u003e\n","\n","  \u003cdiv class=\"colab-df-container\"\u003e\n","    \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e196c177-2e12-4ce1-9535-43852a8f7232')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\"\u003e\n","\n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\"\u003e\n","    \u003cpath d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/\u003e\n","  \u003c/svg\u003e\n","    \u003c/button\u003e\n","\n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","    \u003cscript\u003e\n","      const buttonEl =\n","        document.querySelector('#df-e196c177-2e12-4ce1-9535-43852a8f7232 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-e196c177-2e12-4ce1-9535-43852a8f7232');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    \u003c/script\u003e\n","  \u003c/div\u003e\n","\n","\n","\u003cdiv id=\"df-a0968323-a47a-4e06-8f62-2f6fc2b76bee\"\u003e\n","  \u003cbutton class=\"colab-df-quickchart\" onclick=\"quickchart('df-a0968323-a47a-4e06-8f62-2f6fc2b76bee')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\"\u003e\n","\n","\u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\"\u003e\n","    \u003cg\u003e\n","        \u003cpath d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/\u003e\n","    \u003c/g\u003e\n","\u003c/svg\u003e\n","  \u003c/button\u003e\n","\n","\u003cstyle\u003e\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","\u003c/style\u003e\n","\n","  \u003cscript\u003e\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() =\u003e {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-a0968323-a47a-4e06-8f62-2f6fc2b76bee button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  \u003c/script\u003e\n","\u003c/div\u003e\n","\n","  \u003cdiv id=\"id_561ccab8-a730-469e-8bf7-e73f8bc6b652\"\u003e\n","    \u003cstyle\u003e\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    \u003c/style\u003e\n","    \u003cbutton class=\"colab-df-generate\" onclick=\"generateWithVariable('summary_counts')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\"\u003e\n","\n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/\u003e\n","  \u003c/svg\u003e\n","    \u003c/button\u003e\n","    \u003cscript\u003e\n","      (() =\u003e {\n","      const buttonEl =\n","        document.querySelector('#id_561ccab8-a730-469e-8bf7-e73f8bc6b652 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () =\u003e {\n","        google.colab.notebook.generateWithVariable('summary_counts');\n","      }\n","      })();\n","    \u003c/script\u003e\n","  \u003c/div\u003e\n","\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n"],"text/plain":["      Period  Unique Days Covered\n","0  2020-2023                 1043\n","1  2018-2019                  522"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["start_2020_2023 = '2020-01-01'\n","end_2020_2023 = '2023-12-31'\n","\n","start_2018_2019 = '2018-01-01'\n","end_2018_2019 = '2019-12-31'\n","\n","# Filter the DataFrame for each time range and extract unique days\n","unique_days_2020_2023 = df[(df['ds'] \u003e= start_2020_2023) \u0026 (df['ds'] \u003c= end_2020_2023)]['ds'].dt.date.unique()\n","unique_days_2018_2019 = df[(df['ds'] \u003e= start_2018_2019) \u0026 (df['ds'] \u003c= end_2018_2019)]['ds'].dt.date.unique()\n","\n","# Count the number of unique days in each range\n","count_unique_days_2020_2023 = len(unique_days_2020_2023)\n","count_unique_days_2018_2019 = len(unique_days_2018_2019)\n","\n","# Create a summary DataFrame\n","summary_counts = pd.DataFrame({\n","    'Period': ['2020-2023', '2018-2019'],\n","    'Unique Days Covered': [count_unique_days_2020_2023, count_unique_days_2018_2019]\n","})\n","summary_counts"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"11veOImWgGxc","outputId":"17ebc583-473d-4fe6-d384-b3259eac7ff7"},"outputs":[{"data":{"text/plain":["['MA(2,9)',\n"," 'MA(3,9)',\n"," 'MA(1,12)',\n"," 'MA(2,12)',\n"," 'MA(3,12)',\n"," 'MOM(9)',\n"," 'MOM(12)',\n"," 'RSI(7)',\n"," 'RSI(14)',\n"," 'EMA(3,9)',\n"," 'EMA(5,9)',\n"," 'EMA(5,12)']"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["exog_list = list(df.columns)\n","exog_list.remove('ds')\n","exog_list.remove('y')\n","exog_list.remove('unique_id')\n","exog_list"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Q9dPHhgygIYk"},"outputs":[],"source":["levels = [90]\n","val_size = count_unique_days_2018_2019\n","test_size = count_unique_days_2020_2023\n","horizons = [1, 5, 10, 20]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"L7iRNaR1gKNK"},"outputs":[],"source":["import os\n","from time import time\n","\n","class AutoNHITSTrainer:\n","    def __init__(self, horizons, levels, exog_list, df, val_size, test_size):\n","        self.horizons = horizons\n","        self.levels = levels\n","        self.exog_list = exog_list\n","        self.df = df\n","        self.val_size = val_size\n","        self.test_size = test_size\n","\n","    def check_existing_files(self, horizon):\n","        \"\"\"Checks if model and CSV already exist for a given horizon.\"\"\"\n","        model_path = f'Trained Models/AutoNHITS/TECHhorizon_{horizon}/'\n","        output_csv = f'Data/Test/horizon_{horizon}/TECHnhits_model1_horizon_{horizon}.csv'\n","        return os.path.exists(model_path) and os.path.exists(output_csv)\n","\n","    def save_results(self, nf, horizon, Y_hat_df):\n","        \"\"\"Saves the trained model and prediction results.\"\"\"\n","        model_path = f'Trained Models/AutoNHITS/TECHhorizon_{horizon}/'\n","        output_csv = f'Data/Test/horizon_{horizon}/TECHnhits_model1_horizon_{horizon}.csv'\n","\n","        # Create model directory if it doesn't exist\n","        os.makedirs(model_path, exist_ok=True)\n","\n","        # Save the model, predictions and hyperparameter search\n","        hpo = f'HPO/horizon_{horizon}/TECHnhits_model1_horizon_{horizon}_hpo.csv'\n","        results = nf.models[0].results.get_dataframe()\n","        results.to_csv(hpo, index=False)\n","\n","        nf.save(path=model_path, model_index=None, overwrite=True, save_dataset=False)\n","\n","        for col in Y_hat_df.select_dtypes(include='float32').columns:\n","            Y_hat_df[col] = Y_hat_df[col].astype('float16')\n","\n","        Y_hat_df.to_csv(output_csv, index=False)\n","\n","    def configure_model(self, horizon):\n","        \"\"\"Configures the AutoNHITS model for the given horizon.\"\"\"\n","        nhits_config = AutoNHITS.get_default_config(h=horizon, backend=\"ray\")\n","        nhits_config['hist_exog_list'] = self.exog_list\n","        nhits_config['random_seed'] = 42\n","        nhits_config['learning_rate'] = tune.choice([0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005, 0.00001])\n","\n","        # Instantiate the model\n","        nhits_model = AutoNHITS(h=horizon,\n","                            config=nhits_config,\n","                            search_alg=HyperOptSearch(), num_samples=30,\n","                            backend='ray',\n","                            loss=DistributionLoss(distribution='StudentT', level=self.levels),\n","                            cpus=12, gpus=1)\n","\n","        return nhits_model\n","\n","    def run_training(self):\n","        \"\"\"Runs the training loop over all horizons.\"\"\"\n","        for horizon in self.horizons:\n","            if self.check_existing_files(horizon):\n","                print(f\"Horizon {horizon}: Model and CSV already exist. Skipping this run.\")\n","                continue\n","\n","            # Configure the model\n","            nhits_model = self.configure_model(horizon)\n","\n","            # Start training and cross-validation\n","            init = time()\n","            nf = NeuralForecast(models=[nhits_model], freq='B')\n","            Y_hat_df = nf.cross_validation(df=self.df,\n","                                           val_size=self.val_size,\n","                                           test_size=self.test_size,\n","                                           n_windows=None)\n","\n","            # Save results\n","            self.save_results(nf, horizon, Y_hat_df)\n","\n","            # Log the time taken\n","            end = time()\n","            print(f'Horizon {horizon} CV Minutes: {(end - init) / 60}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"output_embedded_package_id":"1N8OmPdBDGTnK9ZjHihTbniWvdRzTvFrd"},"id":"HQSA-H5rgNe7","outputId":"20b784a7-8679-42c6-8cf1-d1ebb19d774f"},"outputs":[{"data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{},"output_type":"display_data"}],"source":["# Initialize and run the trainer\n","trainer = AutoNHITSTrainer(horizons, levels, exog_list, df, val_size, test_size)\n","trainer.run_training()"]},{"cell_type":"markdown","metadata":{"id":"YNS_yMEVgQGo"},"source":["# Modelling Firm Indicators (20 years total, 14 train, 2 val, 4 test)"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":16882,"status":"ok","timestamp":1731935003440,"user":{"displayName":"John Alejandro Mendoza Ortiz","userId":"11091031515932154769"},"user_tz":-60},"id":"zJhcOzoCgXFA"},"outputs":[],"source":["from neuralforecast.core import NeuralForecast\n","from ray import tune\n","from ray.tune.search.hyperopt import HyperOptSearch\n","from neuralforecast.losses.pytorch import DistributionLoss\n","from neuralforecast.auto import AutoNHITS"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5734,"status":"ok","timestamp":1731935009170,"user":{"displayName":"John Alejandro Mendoza Ortiz","userId":"11091031515932154769"},"user_tz":-60},"id":"XY-mjITPgP_m","outputId":"14a07c4c-28a9-49ed-e793-4e99c22062c0"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u003cclass 'pandas.core.frame.DataFrame'\u003e\n","RangeIndex: 2008545 entries, 0 to 2008544\n","Data columns (total 9 columns):\n"," #   Column     Dtype         \n","---  ------     -----         \n"," 0   unique_id  object        \n"," 1   ds         datetime64[ns]\n"," 2   y          float32       \n"," 3   DY         float32       \n"," 4   PTBV       float32       \n"," 5   P          float32       \n"," 6   PO         float32       \n"," 7   VO         float32       \n"," 8   PE         float32       \n","dtypes: datetime64[ns](1), float32(7), object(1)\n","memory usage: 84.3+ MB\n"]}],"source":["df = pd.read_csv('Data/S\u0026P500/3ProSP500.csv')\n","df['ds'] = pd.to_datetime(df['ds'])\n","df = df.astype({col: 'int32' if dtype == 'int64' else 'float32' if dtype == 'float64' else dtype\n","                for col, dtype in df.dtypes.items()})\n","df = df.rename(columns={'840E': 'y'})\n","\n","selected_columns = ['unique_id', 'ds', 'y', 'DY',  'PTBV',  'P',  'PO',  'VO',  'PE']\n","df = df[selected_columns]\n","\n","df.info()"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1731935009170,"user":{"displayName":"John Alejandro Mendoza Ortiz","userId":"11091031515932154769"},"user_tz":-60},"id":"Tn3sakvugZFi","outputId":"e0cd29c2-c841-4ed4-c344-629974afe86f"},"outputs":[{"name":"stdout","output_type":"stream","text":["2004 2023\n","20\n"]}],"source":["print(df['ds'].min().year, df['ds'].max().year)\n","print(df['ds'].max().year - df['ds'].min().year + 1)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":125},"executionInfo":{"elapsed":358,"status":"ok","timestamp":1731935009526,"user":{"displayName":"John Alejandro Mendoza Ortiz","userId":"11091031515932154769"},"user_tz":-60},"id":"0-B1zll1ga3W","outputId":"7083eba4-2010-4850-fe9f-60c9d0bf2042"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"summary_counts\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"Period\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"2018-2019\",\n          \"2020-2023\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unique Days Covered\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 368,\n        \"min\": 522,\n        \"max\": 1043,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          522,\n          1043\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"summary_counts"},"text/html":["\n","  \u003cdiv id=\"df-71d2c6e8-89c4-436b-8389-ece146d3a136\" class=\"colab-df-container\"\u003e\n","    \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003ePeriod\u003c/th\u003e\n","      \u003cth\u003eUnique Days Covered\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e2020-2023\u003c/td\u003e\n","      \u003ctd\u003e1043\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e2018-2019\u003c/td\u003e\n","      \u003ctd\u003e522\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e\n","    \u003cdiv class=\"colab-df-buttons\"\u003e\n","\n","  \u003cdiv class=\"colab-df-container\"\u003e\n","    \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-71d2c6e8-89c4-436b-8389-ece146d3a136')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\"\u003e\n","\n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\"\u003e\n","    \u003cpath d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/\u003e\n","  \u003c/svg\u003e\n","    \u003c/button\u003e\n","\n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","    \u003cscript\u003e\n","      const buttonEl =\n","        document.querySelector('#df-71d2c6e8-89c4-436b-8389-ece146d3a136 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-71d2c6e8-89c4-436b-8389-ece146d3a136');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    \u003c/script\u003e\n","  \u003c/div\u003e\n","\n","\n","\u003cdiv id=\"df-66bf24e3-65b0-411a-a961-5c4163b602ed\"\u003e\n","  \u003cbutton class=\"colab-df-quickchart\" onclick=\"quickchart('df-66bf24e3-65b0-411a-a961-5c4163b602ed')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\"\u003e\n","\n","\u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\"\u003e\n","    \u003cg\u003e\n","        \u003cpath d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/\u003e\n","    \u003c/g\u003e\n","\u003c/svg\u003e\n","  \u003c/button\u003e\n","\n","\u003cstyle\u003e\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","\u003c/style\u003e\n","\n","  \u003cscript\u003e\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() =\u003e {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-66bf24e3-65b0-411a-a961-5c4163b602ed button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  \u003c/script\u003e\n","\u003c/div\u003e\n","\n","  \u003cdiv id=\"id_4f66d8c4-6540-4538-9991-300ec13afe46\"\u003e\n","    \u003cstyle\u003e\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    \u003c/style\u003e\n","    \u003cbutton class=\"colab-df-generate\" onclick=\"generateWithVariable('summary_counts')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\"\u003e\n","\n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/\u003e\n","  \u003c/svg\u003e\n","    \u003c/button\u003e\n","    \u003cscript\u003e\n","      (() =\u003e {\n","      const buttonEl =\n","        document.querySelector('#id_4f66d8c4-6540-4538-9991-300ec13afe46 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () =\u003e {\n","        google.colab.notebook.generateWithVariable('summary_counts');\n","      }\n","      })();\n","    \u003c/script\u003e\n","  \u003c/div\u003e\n","\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n"],"text/plain":["      Period  Unique Days Covered\n","0  2020-2023                 1043\n","1  2018-2019                  522"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["start_2020_2023 = '2020-01-01'\n","end_2020_2023 = '2023-12-31'\n","\n","start_2018_2019 = '2018-01-01'\n","end_2018_2019 = '2019-12-31'\n","\n","# Filter the DataFrame for each time range and extract unique days\n","unique_days_2020_2023 = df[(df['ds'] \u003e= start_2020_2023) \u0026 (df['ds'] \u003c= end_2020_2023)]['ds'].dt.date.unique()\n","unique_days_2018_2019 = df[(df['ds'] \u003e= start_2018_2019) \u0026 (df['ds'] \u003c= end_2018_2019)]['ds'].dt.date.unique()\n","\n","# Count the number of unique days in each range\n","count_unique_days_2020_2023 = len(unique_days_2020_2023)\n","count_unique_days_2018_2019 = len(unique_days_2018_2019)\n","\n","# Create a summary DataFrame\n","summary_counts = pd.DataFrame({\n","    'Period': ['2020-2023', '2018-2019'],\n","    'Unique Days Covered': [count_unique_days_2020_2023, count_unique_days_2018_2019]\n","})\n","summary_counts"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1731935009527,"user":{"displayName":"John Alejandro Mendoza Ortiz","userId":"11091031515932154769"},"user_tz":-60},"id":"JxHg0s6hggrM","outputId":"2073c50e-3b9e-4794-95f6-17e8bae9f70e"},"outputs":[{"data":{"text/plain":["['DY', 'PTBV', 'P', 'PO', 'VO', 'PE']"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["exog_list = list(df.columns)\n","exog_list.remove('ds')\n","exog_list.remove('y')\n","exog_list.remove('unique_id')\n","exog_list"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1731935009527,"user":{"displayName":"John Alejandro Mendoza Ortiz","userId":"11091031515932154769"},"user_tz":-60},"id":"n8qmo5h5gi0w"},"outputs":[],"source":["levels = [90]\n","val_size = count_unique_days_2018_2019\n","test_size = count_unique_days_2020_2023\n","horizons = [1, 5, 10, 20]"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1731935009527,"user":{"displayName":"John Alejandro Mendoza Ortiz","userId":"11091031515932154769"},"user_tz":-60},"id":"h-WCD2Pugker"},"outputs":[],"source":["import os\n","from time import time\n","\n","class AutoNHITSTrainer:\n","    def __init__(self, horizons, levels, exog_list, df, val_size, test_size):\n","        self.horizons = horizons\n","        self.levels = levels\n","        self.exog_list = exog_list\n","        self.df = df\n","        self.val_size = val_size\n","        self.test_size = test_size\n","\n","    def check_existing_files(self, horizon):\n","        \"\"\"Checks if model and CSV already exist for a given horizon.\"\"\"\n","        model_path = f'Trained Models/AutoNHITS/FIRMhorizon_{horizon}/'\n","        output_csv = f'Data/Test/horizon_{horizon}/FIRMnhits_model1_horizon_{horizon}.csv'\n","        return os.path.exists(model_path) and os.path.exists(output_csv)\n","\n","    def save_results(self, nf, horizon, Y_hat_df):\n","        \"\"\"Saves the trained model and prediction results.\"\"\"\n","        model_path = f'Trained Models/AutoNHITS/FIRMhorizon_{horizon}/'\n","        output_csv = f'Data/Test/horizon_{horizon}/FIRMnhits_model1_horizon_{horizon}.csv'\n","\n","        # Create model directory if it doesn't exist\n","        os.makedirs(model_path, exist_ok=True)\n","\n","        # Save the model, predictions and hyperparameter search\n","        hpo = f'HPO/horizon_{horizon}/FIRMnhits_model1_horizon_{horizon}_hpo.csv'\n","        results = nf.models[0].results.get_dataframe()\n","        results.to_csv(hpo, index=False)\n","\n","        nf.save(path=model_path, model_index=None, overwrite=True, save_dataset=False)\n","\n","        for col in Y_hat_df.select_dtypes(include='float32').columns:\n","            Y_hat_df[col] = Y_hat_df[col].astype('float16')\n","\n","        Y_hat_df.to_csv(output_csv, index=False)\n","\n","    def configure_model(self, horizon):\n","        \"\"\"Configures the AutoNHITS model for the given horizon.\"\"\"\n","        nhits_config = AutoNHITS.get_default_config(h=horizon, backend=\"ray\")\n","        nhits_config['hist_exog_list'] = self.exog_list\n","        nhits_config['random_seed'] = 42\n","        nhits_config['learning_rate'] = tune.choice([0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005, 0.00001])\n","\n","        # Instantiate the model\n","        nhits_model = AutoNHITS(h=horizon,\n","                            config=nhits_config,\n","                            search_alg=HyperOptSearch(), num_samples=30,\n","                            backend='ray',\n","                            loss=DistributionLoss(distribution='StudentT', level=self.levels),\n","                            cpus=12, gpus=1)\n","\n","        return nhits_model\n","\n","    def run_training(self):\n","        \"\"\"Runs the training loop over all horizons.\"\"\"\n","        for horizon in self.horizons:\n","            if self.check_existing_files(horizon):\n","                print(f\"Horizon {horizon}: Model and CSV already exist. Skipping this run.\")\n","                continue\n","\n","            # Configure the model\n","            nhits_model = self.configure_model(horizon)\n","\n","            # Start training and cross-validation\n","            init = time()\n","            nf = NeuralForecast(models=[nhits_model], freq='B')\n","            Y_hat_df = nf.cross_validation(df=self.df,\n","                                           val_size=self.val_size,\n","                                           test_size=self.test_size,\n","                                           n_windows=None)\n","\n","            # Save results\n","            self.save_results(nf, horizon, Y_hat_df)\n","\n","            # Log the time taken\n","            end = time()\n","            print(f'Horizon {horizon} CV Minutes: {(end - init) / 60}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"fFFprw94gm5K"},"outputs":[{"name":"stdout","output_type":"stream","text":["Horizon 1: Model and CSV already exist. Skipping this run.\n","Horizon 5: Model and CSV already exist. Skipping this run.\n","Horizon 10: Model and CSV already exist. Skipping this run.\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-18 13:03:36,165\tINFO worker.py:1819 -- Started a local Ray instance.\n","2024-11-18 13:03:37,350\tINFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n"]},{"name":"stdout","output_type":"stream","text":["+--------------------------------------------------------------------+\n","| Configuration for experiment     _train_tune_2024-11-18_13-03-34   |\n","+--------------------------------------------------------------------+\n","| Search algorithm                 SearchGenerator                   |\n","| Scheduler                        FIFOScheduler                     |\n","| Number of trials                 30                                |\n","+--------------------------------------------------------------------+\n","\n","View detailed results here: /root/ray_results/_train_tune_2024-11-18_13-03-34\n","To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-11-18_13-03-34_336407_4396/artifacts/2024-11-18_13-03-37/_train_tune_2024-11-18_13-03-34/driver_artifacts`\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=5856)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=5856)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=5856)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=5856)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=5856)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=5856)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=5856)\u001b[0m 2024-11-18 13:03:45.302446: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=5856)\u001b[0m 2024-11-18 13:03:45.322334: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=5856)\u001b[0m 2024-11-18 13:03:45.347761: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=5856)\u001b[0m 2024-11-18 13:03:45.355461: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=5856)\u001b[0m 2024-11-18 13:03:45.375100: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=5856)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=5856)\u001b[0m 2024-11-18 13:03:46.544959: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=5856)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=5856)\u001b[0m \n","\u001b[36m(_train_tune pid=5856)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=5856)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=5856)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=5856)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=5856)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=5856)\u001b[0m 3 | blocks       | ModuleList       | 2.8 M  | train\n","\u001b[36m(_train_tune pid=5856)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=5856)\u001b[0m 2.8 M     Trainable params\n","\u001b[36m(_train_tune pid=5856)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=5856)\u001b[0m 2.8 M     Total params\n","\u001b[36m(_train_tune pid=5856)\u001b[0m 11.257    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=5856)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=5856)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/13 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-0.888]\n","Epoch 2:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.16]\n","Epoch 3:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.13]\n","Epoch 4:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.984, train_loss_epoch=-1.17]\n","Epoch 5:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.21]\n","Epoch 6:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.06, train_loss_epoch=-1.17]\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.842, train_loss_epoch=-1.15]\n","\u001b[36m(_train_tune pid=5856)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5856)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 60.14it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5856)\u001b[0m \n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.15, valid_loss=-1.14]\n","Epoch 8:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.06, train_loss_epoch=-1.23, valid_loss=-1.14]\n","Epoch 9:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.16, valid_loss=-1.14]\n","Epoch 10:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.20, valid_loss=-1.14]\n","Epoch 11:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.21, valid_loss=-1.14]\n","Epoch 12:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.23, valid_loss=-1.14]\n","Epoch 13:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.24, valid_loss=-1.14]\n","Epoch 14:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.856, train_loss_epoch=-1.18, valid_loss=-1.14]\n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.963, train_loss_epoch=-1.22, valid_loss=-1.14]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5856)\u001b[0m \n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5856)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 62.48it/s]\u001b[A\n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.22, valid_loss=-1.17] \n","Epoch 16:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.25, valid_loss=-1.17]\n","Epoch 17:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.25, valid_loss=-1.17]\n","Epoch 18:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.27, valid_loss=-1.17]\n","Epoch 19:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.09, train_loss_epoch=-1.23, valid_loss=-1.17]\n","Epoch 20:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.32, valid_loss=-1.17]\n","Epoch 21:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.24, valid_loss=-1.17]\n","Epoch 22:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.26, valid_loss=-1.17]\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.885, train_loss_epoch=-1.18, valid_loss=-1.17]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5856)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 64.02it/s]\u001b[A\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.18, valid_loss=-1.19] \n","Epoch 24:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.32, valid_loss=-1.19]\n","Epoch 25:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.32, valid_loss=-1.19]\n","Epoch 26:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.26, valid_loss=-1.19]\n","Epoch 27:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.26, valid_loss=-1.19]\n","Epoch 28:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.511, train_loss_epoch=-1.13, valid_loss=-1.19]\n","Epoch 29:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.26, valid_loss=-1.19]\n","Epoch 30:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.23, valid_loss=-1.19]\n","\u001b[36m(_train_tune pid=5856)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5856)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 64.06it/s]\u001b[A\n","Epoch 31:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.00, train_loss_epoch=-1.26, valid_loss=-1.20]\n","Epoch 32:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.24, valid_loss=-1.20]\n","Epoch 33:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.26, valid_loss=-1.20]\n","Epoch 34:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.24, valid_loss=-1.20]\n","Epoch 35:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.856, train_loss_epoch=-1.21, valid_loss=-1.20]\n","Epoch 36:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.29, valid_loss=-1.20]\n","Epoch 37:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.36, train_loss_epoch=-1.30, valid_loss=-1.20]\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.26, valid_loss=-1.20]\n","\u001b[36m(_train_tune pid=5856)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5856)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 66.44it/s]\u001b[A\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.05, train_loss_epoch=-1.26, valid_loss=-1.20]\n","Epoch 39:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.27, valid_loss=-1.20]\n","Epoch 40:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.811, train_loss_epoch=-1.28, valid_loss=-1.20]\n","Epoch 41:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.27, valid_loss=-1.20]\n","Epoch 42:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.26, valid_loss=-1.20]\n","Epoch 43:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.26, valid_loss=-1.20]\n","Epoch 44:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.965, train_loss_epoch=-1.28, valid_loss=-1.20]\n","Epoch 45:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.26, valid_loss=-1.20]\n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.28, valid_loss=-1.20]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5856)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 60.19it/s]\u001b[A\n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.28, valid_loss=-1.21]\n","Epoch 47:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.23, valid_loss=-1.21]\n","Epoch 48:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.30, valid_loss=-1.21]\n","Epoch 49:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.32, valid_loss=-1.21]\n","Epoch 50:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.29, valid_loss=-1.21]\n","Epoch 51:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.26, valid_loss=-1.21]\n","Epoch 52:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.29, valid_loss=-1.21]\n","Epoch 53:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.27, valid_loss=-1.21]\n","\u001b[36m(_train_tune pid=5856)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5856)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 59.89it/s]\u001b[A\n","Epoch 54:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.26, valid_loss=-1.22]\n","Epoch 55:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.706, train_loss_epoch=-1.23, valid_loss=-1.22]\n","Epoch 56:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.30, valid_loss=-1.22]\n","Epoch 57:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.31, valid_loss=-1.22]\n","Epoch 58:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.32, valid_loss=-1.22]\n","Epoch 59:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.01, train_loss_epoch=-1.22, valid_loss=-1.22]\n","Epoch 60:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.27, valid_loss=-1.22]\n","Epoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.26, valid_loss=-1.22]\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-18 13:04:07,151\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (40, 20, 1), 'n_pool_kernel_size': (2, 2, 2), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=5856)\u001b[0m \n","\u001b[36m(_train_tune pid=5856)\u001b[0m \rValidation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5856)\u001b[0m \rValidation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5856)\u001b[0m \rValidation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=5856)\u001b[0m `Trainer.fit` stopped: `max_steps=800.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=5856)\u001b[0m \n","\u001b[36m(_train_tune pid=5856)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 62.68it/s]\u001b[A\n","\u001b[36m(_train_tune pid=5856)\u001b[0m \r                                                                        \u001b[A\rEpoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.26, valid_loss=-1.22]\rEpoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.28, valid_loss=-1.22]\rEpoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.28, valid_loss=-1.22]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=6074)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=6074)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=6074)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=6074)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=6074)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=6074)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=6074)\u001b[0m 2024-11-18 13:04:14.733451: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=6074)\u001b[0m 2024-11-18 13:04:14.753986: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=6074)\u001b[0m 2024-11-18 13:04:14.779803: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=6074)\u001b[0m 2024-11-18 13:04:14.787597: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=6074)\u001b[0m 2024-11-18 13:04:14.805618: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=6074)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=6074)\u001b[0m 2024-11-18 13:04:15.978095: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=6074)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=6074)\u001b[0m \n","\u001b[36m(_train_tune pid=6074)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=6074)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=6074)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=6074)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=6074)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=6074)\u001b[0m 3 | blocks       | ModuleList       | 3.3 M  | train\n","\u001b[36m(_train_tune pid=6074)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=6074)\u001b[0m 3.3 M     Trainable params\n","\u001b[36m(_train_tune pid=6074)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=6074)\u001b[0m 3.3 M     Total params\n","\u001b[36m(_train_tune pid=6074)\u001b[0m 13.080    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=6074)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=6074)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/7 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.290, train_loss_epoch=11.70]\n","Epoch 2:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.986, train_loss_epoch=1.150]\n","Epoch 3:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.867, train_loss_epoch=-0.156]\n","Epoch 4:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.779, train_loss_epoch=-0.675]\n","Epoch 5:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.40]\n","Epoch 6:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.608, train_loss_epoch=-0.966]\n","Epoch 7:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.376, train_loss_epoch=0.266]\n","Epoch 8:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.127, train_loss_epoch=0.676]\n","Epoch 9:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-0.831]\n","Epoch 10:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.295, train_loss_epoch=-0.712]\n","Epoch 11:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-0.208]\n","Epoch 12:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.799, train_loss_epoch=-0.852]\n","Epoch 13:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.206, train_loss_epoch=-0.406]\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.423, train_loss_epoch=-0.51]\n","\u001b[36m(_train_tune pid=6074)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6074)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.76it/s]\u001b[A\n","Epoch 14:   0%|          | 0/7 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-0.763, train_loss_epoch=-0.51, valid_loss=-0.948]\n","Epoch 15:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.545, train_loss_epoch=-0.761, valid_loss=-0.948]\n","Epoch 16:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-0.942, valid_loss=-0.948]\n","Epoch 17:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.35, valid_loss=-0.948]\n","Epoch 18:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.49, valid_loss=-0.948]\n","Epoch 19:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-1.67, valid_loss=-0.948]\n","Epoch 20:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.56, valid_loss=-0.948]\n","Epoch 21:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.64, valid_loss=-0.948]\n","Epoch 21:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.64, valid_loss=-0.948]        \n","Epoch 22:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.64, valid_loss=-0.948]\n","Epoch 23:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.62, valid_loss=-0.948]\n","Epoch 24:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.68, valid_loss=-0.948]\n","Epoch 25:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.69, valid_loss=-0.948]\n","Epoch 26:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.65, valid_loss=-0.948]\n","Epoch 27:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.67, valid_loss=-0.948]\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.63, valid_loss=-0.948]\n","\u001b[36m(_train_tune pid=6074)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6074)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.75it/s]\u001b[A\n","Epoch 28:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.63, valid_loss=-1.63] \n","Epoch 29:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.997, train_loss_epoch=-1.54, valid_loss=-1.63]\n","Epoch 30:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.70, valid_loss=-1.63]\n","Epoch 31:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.62, valid_loss=-1.63]\n","Epoch 32:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.58, valid_loss=-1.63]\n","Epoch 33:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.65, valid_loss=-1.63]\n","Epoch 34:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.65, valid_loss=-1.63]\n","Epoch 35:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.66, valid_loss=-1.63]\n","Epoch 36:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.63, valid_loss=-1.63]\n","Epoch 37:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-1.75, valid_loss=-1.63]\n","Epoch 38:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.66, valid_loss=-1.63]\n","Epoch 39:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.69, valid_loss=-1.63]\n","Epoch 40:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.62, valid_loss=-1.63]\n","Epoch 41:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-1.78, valid_loss=-1.63]\n","Epoch 42:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.71, valid_loss=-1.63]\n","\u001b[36m(_train_tune pid=6074)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6074)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.75it/s]\u001b[A\n","Epoch 42:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.71, valid_loss=-1.63]\n","Epoch 43:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.58, valid_loss=-1.63]\n","Epoch 43: 100%|██████████| 7/7 [00:00\u003c00:00, 35.51it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.66, valid_loss=-1.63]\n","Epoch 44:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.66, valid_loss=-1.63]\n","Epoch 45:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.67, valid_loss=-1.63]\n","Epoch 46:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.62, valid_loss=-1.63]\n","Epoch 47:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.73, valid_loss=-1.63]\n","Epoch 48:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.59, valid_loss=-1.63]\n","Epoch 49:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.68, valid_loss=-1.63]\n","Epoch 50:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.07, train_loss_epoch=-1.57, valid_loss=-1.63]\n","Epoch 51:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.62, valid_loss=-1.63]\n","Epoch 52:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.61, valid_loss=-1.63]\n","Epoch 53:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.56, valid_loss=-1.63]\n","Epoch 54:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.61, valid_loss=-1.63]\n","Epoch 55:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.70, valid_loss=-1.63]\n","Epoch 56:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.59, valid_loss=-1.63]\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.968, train_loss_epoch=-1.56, valid_loss=-1.63]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6074)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.75it/s]\u001b[A\n","Epoch 57:   0%|          | 0/7 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.56, valid_loss=-1.64] \n","Epoch 58:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.71, valid_loss=-1.64]\n","Epoch 59:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.62, valid_loss=-1.64]\n","Epoch 60:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.68, valid_loss=-1.64]\n","Epoch 61:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.59, valid_loss=-1.64]\n","Epoch 62:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.70, valid_loss=-1.64]\n","Epoch 63:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.65, valid_loss=-1.64]\n","Epoch 64:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.62, valid_loss=-1.64]\n","Epoch 65:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.64, valid_loss=-1.64]\n","Epoch 66:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.63, valid_loss=-1.64]\n","Epoch 67:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.66, valid_loss=-1.64]\n","Epoch 68:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.62, valid_loss=-1.64]\n","Epoch 69:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.59, valid_loss=-1.64]\n","Epoch 70:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.60, valid_loss=-1.64]\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.676, train_loss_epoch=-1.51, valid_loss=-1.64]\n","\u001b[36m(_train_tune pid=6074)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6074)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.74it/s]\u001b[A\n","Epoch 71:   0%|          | 0/7 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.51, valid_loss=-1.63] \n","Epoch 72:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.64, valid_loss=-1.63]\n","Epoch 73:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.67, valid_loss=-1.63]\n","Epoch 74:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.69, valid_loss=-1.63]\n","Epoch 75:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.66, valid_loss=-1.63]\n","Epoch 76:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.73, valid_loss=-1.63]\n","Epoch 77:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.63, valid_loss=-1.63]\n","Epoch 78:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.71, valid_loss=-1.63]\n","Epoch 79:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.63, valid_loss=-1.63]\n","Epoch 80:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.66, valid_loss=-1.63]\n","Epoch 81:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.36, train_loss_epoch=-1.58, valid_loss=-1.63]\n","Epoch 82:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.65, valid_loss=-1.63]\n","Epoch 83:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.713, train_loss_epoch=-1.52, valid_loss=-1.63]\n","Epoch 84:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.63, valid_loss=-1.63]\n","Epoch 85:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.65, valid_loss=-1.63]\n","\u001b[36m(_train_tune pid=6074)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6074)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.76it/s]\u001b[A\n","Epoch 86:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.67, valid_loss=-1.63]\n","Epoch 87:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-1.71, valid_loss=-1.63]\n","Epoch 88:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.74, valid_loss=-1.63]\n","Epoch 89:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.59, valid_loss=-1.63]\n","Epoch 90:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.66, valid_loss=-1.63]\n","Epoch 91:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.67, valid_loss=-1.63]\n","Epoch 92:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.61, valid_loss=-1.63]\n","Epoch 93:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.65, valid_loss=-1.63]\n","Epoch 94:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.59, valid_loss=-1.63]\n","Epoch 95:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.59, valid_loss=-1.63]\n","Epoch 96:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.67, valid_loss=-1.63]\n","Epoch 96: 100%|██████████| 7/7 [00:00\u003c00:00, 37.44it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.67, valid_loss=-1.63]\n","Epoch 96: 100%|██████████| 7/7 [00:00\u003c00:00, 37.32it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.65, valid_loss=-1.63]\n","Epoch 97:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.65, valid_loss=-1.63]\n","Epoch 98:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.70, valid_loss=-1.63]\n","Epoch 99:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.64, valid_loss=-1.63]\n","Epoch 99: 100%|██████████| 7/7 [00:00\u003c00:00, 36.15it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.64, valid_loss=-1.63]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6074)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.72it/s]\u001b[A\n","Epoch 100:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.72, valid_loss=-1.63]\n","Epoch 101:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.66, valid_loss=-1.63]\n","Epoch 102:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.64, valid_loss=-1.63]\n","Epoch 103:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.64, valid_loss=-1.63]\n","Epoch 104:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.69, valid_loss=-1.63]\n","Epoch 105:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.36, train_loss_epoch=-1.60, valid_loss=-1.63]\n","Epoch 106:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.07, train_loss_epoch=-1.57, valid_loss=-1.63]\n","Epoch 107:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.67, valid_loss=-1.63]\n","Epoch 108:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.64, valid_loss=-1.63]\n","Epoch 109:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.67, valid_loss=-1.63]\n","Epoch 110:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.853, train_loss_epoch=-1.56, valid_loss=-1.63]\n","Epoch 111:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.69, valid_loss=-1.63]\n","Epoch 112:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.63, valid_loss=-1.63]\n","Epoch 113:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.68, valid_loss=-1.63]\n","Epoch 114:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-1.75, valid_loss=-1.63]\n","\u001b[36m(_train_tune pid=6074)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6074)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.75it/s]\u001b[A\n","Epoch 114:   0%|          | 0/7 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.75, valid_loss=-1.64]\n","Epoch 115:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.60, valid_loss=-1.64]\n","Epoch 116:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.02, train_loss_epoch=-1.57, valid_loss=-1.64]\n","Epoch 117:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.74, valid_loss=-1.64]\n","Epoch 118:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.63, valid_loss=-1.64]\n","Epoch 119:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.63, valid_loss=-1.64]\n","Epoch 120:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.65, valid_loss=-1.64]\n","Epoch 121:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.62, valid_loss=-1.64]\n","Epoch 122:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.66, valid_loss=-1.64]\n","Epoch 123:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.67, valid_loss=-1.64]\n","Epoch 124:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.65, valid_loss=-1.64]\n","Epoch 125:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.68, valid_loss=-1.64]\n","Epoch 126:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.69, valid_loss=-1.64]\n","Epoch 127:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.05, train_loss_epoch=-1.59, valid_loss=-1.64]\n","Epoch 128:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.63, valid_loss=-1.64]\n","\u001b[36m(_train_tune pid=6074)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6074)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.60it/s]\u001b[A\n","Epoch 129:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.66, valid_loss=-1.64]\n","Epoch 130:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.64, valid_loss=-1.64]\n","Epoch 131:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.67, valid_loss=-1.64]\n","Epoch 132:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.64, valid_loss=-1.64]\n","Epoch 133:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.63, valid_loss=-1.64]\n","Epoch 134:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.74, valid_loss=-1.64]\n","Epoch 135:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.65, valid_loss=-1.64]\n","Epoch 136:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.73, valid_loss=-1.64]\n","Epoch 137:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.65, valid_loss=-1.64]\n","Epoch 138:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.64, valid_loss=-1.64]\n","Epoch 139:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.66, valid_loss=-1.64]\n","Epoch 140:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.58, valid_loss=-1.64]\n","Epoch 141:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.68, valid_loss=-1.64]\n","Epoch 142:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.69, valid_loss=-1.64]\n","\u001b[36m(_train_tune pid=6074)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6074)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.74it/s]\u001b[A\n","Epoch 142:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.69, valid_loss=-1.64]\n","Epoch 143:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.61, valid_loss=-1.64]\n","Epoch 144:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.64, valid_loss=-1.64]\n","Epoch 145:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.67, valid_loss=-1.64]\n","Epoch 146:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.63, valid_loss=-1.64]\n","Epoch 147:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.72, valid_loss=-1.64]\n","Epoch 148:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.72, valid_loss=-1.64]\n","Epoch 149:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.65, valid_loss=-1.64]\n","Epoch 150:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.66, valid_loss=-1.64]\n","Epoch 150: 100%|██████████| 7/7 [00:00\u003c00:00, 36.09it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.58, valid_loss=-1.64]\n","Epoch 151:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.58, valid_loss=-1.64]\n","Epoch 152:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-1.71, valid_loss=-1.64]\n","Epoch 153:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.01, train_loss_epoch=-1.59, valid_loss=-1.64]\n","Epoch 154:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.64, valid_loss=-1.64]\n","Epoch 155:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.63, valid_loss=-1.64]\n","Epoch 156:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.57, valid_loss=-1.64]\n","Epoch 157:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.66, valid_loss=-1.64]\n","\u001b[36m(_train_tune pid=6074)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=6074)\u001b[0m `Trainer.fit` stopped: `max_steps=1100.0` reached.\n","2024-11-18 13:05:10,031\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (180, 60, 1), 'n_pool_kernel_size': (2, 2, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=6074)\u001b[0m \n","\u001b[36m(_train_tune pid=6074)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.73it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6074)\u001b[0m \r                                                                      \u001b[A\rEpoch 157:   0%|          | 0/7 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.66, valid_loss=-1.64]\rEpoch 157:   0%|          | 0/7 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.66, valid_loss=-1.64]\rEpoch 157:   0%|          | 0/7 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.66, valid_loss=-1.64]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=6417)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=6417)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=6417)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=6417)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=6417)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=6417)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=6417)\u001b[0m 2024-11-18 13:05:17.701488: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=6417)\u001b[0m 2024-11-18 13:05:17.722886: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=6417)\u001b[0m 2024-11-18 13:05:17.748100: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=6417)\u001b[0m 2024-11-18 13:05:17.756014: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=6417)\u001b[0m 2024-11-18 13:05:17.774337: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=6417)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=6417)\u001b[0m 2024-11-18 13:05:18.924875: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=6417)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=6417)\u001b[0m \n","\u001b[36m(_train_tune pid=6417)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=6417)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=6417)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=6417)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=6417)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=6417)\u001b[0m 3 | blocks       | ModuleList       | 2.6 M  | train\n","\u001b[36m(_train_tune pid=6417)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=6417)\u001b[0m 2.6 M     Trainable params\n","\u001b[36m(_train_tune pid=6417)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=6417)\u001b[0m 2.6 M     Total params\n","\u001b[36m(_train_tune pid=6417)\u001b[0m 10.274    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=6417)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=6417)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/7 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.86, train_loss_epoch=-0.785]\n","Epoch 2:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-0.922]\n","Epoch 3:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-0.877]\n","Epoch 4:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.712, train_loss_epoch=-0.878]\n","Epoch 5:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.01]\n","Epoch 6:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.767, train_loss_epoch=-0.942]\n","Epoch 7:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.466, train_loss_epoch=-0.902]\n","Epoch 8:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.621, train_loss_epoch=-0.945]\n","Epoch 9:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.09]\n","Epoch 10:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.505, train_loss_epoch=-0.962]\n","Epoch 11:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.15]\n","Epoch 12:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.819, train_loss_epoch=-1.05]\n","Epoch 13:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.839, train_loss_epoch=-1.04]\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.62, train_loss_epoch=-1.03]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6417)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.79it/s]\u001b[A\n","Epoch 14:   0%|          | 0/7 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.08, train_loss_epoch=-1.03, valid_loss=-1.07]\n","Epoch 15:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.65, train_loss_epoch=-1.07, valid_loss=-1.07]\n","Epoch 16:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.17, valid_loss=-1.07]\n","Epoch 17:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.13, valid_loss=-1.07]\n","Epoch 18:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.14, valid_loss=-1.07]\n","Epoch 19:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.22, valid_loss=-1.07]\n","Epoch 20:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.21, valid_loss=-1.07]\n","Epoch 21:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.20, valid_loss=-1.07]\n","Epoch 22:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.21, valid_loss=-1.07]\n","Epoch 23:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.21, valid_loss=-1.07]\n","Epoch 24:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.09, train_loss_epoch=-1.22, valid_loss=-1.07]\n","Epoch 25:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.31, valid_loss=-1.07]\n","Epoch 26:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.42, valid_loss=-1.07]\n","Epoch 27:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.37, valid_loss=-1.07]\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.34, valid_loss=-1.07]\n","\u001b[36m(_train_tune pid=6417)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6417)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.76it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6417)\u001b[0m \n","Epoch 29:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.396, train_loss_epoch=-1.24, valid_loss=-1.36]\n","Epoch 30:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.52, valid_loss=-1.36]\n","Epoch 31:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.39, valid_loss=-1.36]\n","Epoch 32:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.42, valid_loss=-1.36]\n","Epoch 33:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.47, valid_loss=-1.36]\n","Epoch 34:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.45, valid_loss=-1.36]\n","Epoch 35:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.50, valid_loss=-1.36]\n","Epoch 36:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.50, valid_loss=-1.36]\n","Epoch 37:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.57, valid_loss=-1.36]\n","Epoch 38:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.46, valid_loss=-1.36]\n","Epoch 39:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.48, valid_loss=-1.36]\n","Epoch 40:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.56, valid_loss=-1.36]\n","Epoch 41:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.60, valid_loss=-1.36]\n","Epoch 42:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.57, valid_loss=-1.36]\n","\u001b[36m(_train_tune pid=6417)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6417)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6417)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.77it/s]\u001b[A\n","Epoch 43:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.54, valid_loss=-1.50]\n","Epoch 44:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.57, valid_loss=-1.50]\n","Epoch 45:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.50, valid_loss=-1.50]\n","Epoch 46:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.49, valid_loss=-1.50]\n","Epoch 47:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.58, valid_loss=-1.50]\n","Epoch 48:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.45, valid_loss=-1.50]\n","Epoch 49:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.56, valid_loss=-1.50]\n","Epoch 50:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.49, valid_loss=-1.50]\n","Epoch 51:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.57, valid_loss=-1.50]\n","Epoch 52:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.60, valid_loss=-1.50]\n","Epoch 53:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.01, train_loss_epoch=-1.47, valid_loss=-1.50]\n","Epoch 54:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.54, valid_loss=-1.50]\n","Epoch 55:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.63, valid_loss=-1.50]\n","Epoch 56:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.61, valid_loss=-1.50]\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.898, train_loss_epoch=-1.49, valid_loss=-1.50]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6417)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.77it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6417)\u001b[0m \n","Epoch 57:   0%|          | 0/7 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.49, valid_loss=-1.55] \n","Epoch 58:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.66, valid_loss=-1.55]\n","Epoch 59:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.57, valid_loss=-1.55]\n","Epoch 60:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.62, valid_loss=-1.55]\n","Epoch 61:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.60, valid_loss=-1.55]\n","Epoch 62:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.63, valid_loss=-1.55]\n","Epoch 63:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.57, valid_loss=-1.55]\n","Epoch 64:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.55, valid_loss=-1.55]\n","Epoch 65:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.60, valid_loss=-1.55]\n","Epoch 66:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.60, valid_loss=-1.55]\n","Epoch 67:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.63, valid_loss=-1.55]\n","Epoch 68:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.63, valid_loss=-1.55]\n","Epoch 69:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.56, valid_loss=-1.55]\n","Epoch 70:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.59, valid_loss=-1.55]\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.613, train_loss_epoch=-1.49, valid_loss=-1.55]\n","\u001b[36m(_train_tune pid=6417)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6417)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.80it/s]\u001b[A\n","Epoch 71:   0%|          | 0/7 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.49, valid_loss=-1.58] \n","Epoch 72:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.57, valid_loss=-1.58]\n","Epoch 73:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.60, valid_loss=-1.58]\n","Epoch 74:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.63, valid_loss=-1.58]\n","Epoch 75:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.60, valid_loss=-1.58]\n","Epoch 76:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.62, valid_loss=-1.58]\n","Epoch 77:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.59, valid_loss=-1.58]\n","Epoch 78:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.68, valid_loss=-1.58]\n","Epoch 79:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.63, valid_loss=-1.58]\n","Epoch 80:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.66, valid_loss=-1.58]\n","Epoch 81:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.55, valid_loss=-1.58]\n","Epoch 82:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.61, valid_loss=-1.58]\n","Epoch 83:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.726, train_loss_epoch=-1.53, valid_loss=-1.58]\n","Epoch 84:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.62, valid_loss=-1.58]\n","Epoch 85:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.64, valid_loss=-1.58]\n","\u001b[36m(_train_tune pid=6417)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=6417)\u001b[0m `Trainer.fit` stopped: `max_steps=600.0` reached.\n","2024-11-18 13:05:49,685\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (168, 24, 1), 'n_pool_kernel_size': (4, 4, 4), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=6417)\u001b[0m \n","\u001b[36m(_train_tune pid=6417)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.81it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6417)\u001b[0m \r                                                                      \u001b[A\rEpoch 85:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.64, valid_loss=-1.60]\rEpoch 85:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.66, valid_loss=-1.60]\rEpoch 85:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.66, valid_loss=-1.60]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=6651)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=6651)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=6651)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=6651)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=6651)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=6651)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=6651)\u001b[0m 2024-11-18 13:05:57.762211: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=6651)\u001b[0m 2024-11-18 13:05:57.781442: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=6651)\u001b[0m 2024-11-18 13:05:57.805949: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=6651)\u001b[0m 2024-11-18 13:05:57.813342: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=6651)\u001b[0m 2024-11-18 13:05:57.830582: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=6651)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=6651)\u001b[0m 2024-11-18 13:05:58.986675: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=6651)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=6651)\u001b[0m \n","\u001b[36m(_train_tune pid=6651)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=6651)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=6651)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=6651)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=6651)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=6651)\u001b[0m 3 | blocks       | ModuleList       | 2.6 M  | train\n","\u001b[36m(_train_tune pid=6651)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=6651)\u001b[0m 2.6 M     Trainable params\n","\u001b[36m(_train_tune pid=6651)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=6651)\u001b[0m 2.6 M     Total params\n","\u001b[36m(_train_tune pid=6651)\u001b[0m 10.294    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=6651)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=6651)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/2 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.25]\n","Epoch 2:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.36, train_loss_epoch=-1.37]\n","Epoch 3:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.33]\n","Epoch 4:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.55]\n","Epoch 5:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.40]\n","Epoch 6:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.42]\n","Epoch 7:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.41]\n","Epoch 8:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.54]\n","Epoch 9:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.51]\n","Epoch 10:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.57]\n","Epoch 11:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.60]\n","Epoch 12:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.57]\n","Epoch 13:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.40]\n","Epoch 14:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.67]\n","Epoch 15:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.59]\n","Epoch 16:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.49]\n","Epoch 17:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.53]\n","Epoch 18:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.65]\n","Epoch 19:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.49]\n","Epoch 20:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.62]\n","Epoch 21:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.49]\n","Epoch 22:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.62]\n","Epoch 23:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.67]\n","Epoch 24:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.61]\n","Epoch 25:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.68]\n","Epoch 26:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.64]\n","Epoch 27:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.51]\n","Epoch 28:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.67]\n","Epoch 29:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.60]\n","Epoch 30:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.73]\n","Epoch 31:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.68]\n","Epoch 32:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.68]\n","Epoch 33:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.65]\n","Epoch 34:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.71]\n","Epoch 35:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.77]\n","Epoch 36:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.72]\n","Epoch 37:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.68]\n","Epoch 38:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.53]\n","Epoch 39:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.63]\n","Epoch 40:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.57]\n","Epoch 41:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.78]\n","Epoch 42:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.60]\n","Epoch 43:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.58]\n","Epoch 44:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.72]\n","Epoch 45:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.60]\n","Epoch 46:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.77]\n","Epoch 47:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.64]\n","Epoch 48:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.84]\n","Epoch 49:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.82]\n","Epoch 49: 100%|██████████| 2/2 [00:00\u003c00:00, 19.09it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.82]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6651)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 16.17it/s]\u001b[A\n","Epoch 50:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.74, valid_loss=-1.60]\n","Epoch 51:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.67, valid_loss=-1.60]\n","Epoch 52:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.73, valid_loss=-1.60]\n","Epoch 53:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.73, valid_loss=-1.60]\n","Epoch 54:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.60, valid_loss=-1.60]\n","Epoch 55:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.67, valid_loss=-1.60]\n","Epoch 56:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.88, valid_loss=-1.60]\n","Epoch 57:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.59, valid_loss=-1.60]\n","Epoch 58:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.64, valid_loss=-1.60]\n","Epoch 59:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.63, valid_loss=-1.60]\n","Epoch 60:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.83, valid_loss=-1.60]\n","Epoch 61:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.67, valid_loss=-1.60]\n","Epoch 62:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.75, valid_loss=-1.60]\n","Epoch 63:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.71, valid_loss=-1.60]\n","Epoch 64:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.62, valid_loss=-1.60]\n","Epoch 65:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.64, valid_loss=-1.60]\n","Epoch 65: 100%|██████████| 2/2 [00:00\u003c00:00, 18.79it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.72, valid_loss=-1.60]\n","Epoch 66:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.72, valid_loss=-1.60]\n","Epoch 66: 100%|██████████| 2/2 [00:00\u003c00:00, 19.30it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.55, valid_loss=-1.60]\n","Epoch 67:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.55, valid_loss=-1.60]\n","Epoch 68:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.70, valid_loss=-1.60]\n","Epoch 69:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.71, valid_loss=-1.60]\n","Epoch 70:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.63, valid_loss=-1.60]\n","Epoch 71:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.70, valid_loss=-1.60]\n","Epoch 72:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.54, valid_loss=-1.60]\n","Epoch 73:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.78, valid_loss=-1.60]\n","Epoch 74:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.82, valid_loss=-1.60]\n","Epoch 75:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.68, valid_loss=-1.60]\n","Epoch 76:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.63, valid_loss=-1.60]\n","Epoch 77:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.73, valid_loss=-1.60]\n","Epoch 78:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.61, valid_loss=-1.60]\n","Epoch 79:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.77, valid_loss=-1.60]\n","Epoch 80:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.65, valid_loss=-1.60]\n","Epoch 81:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.63, valid_loss=-1.60]\n","Epoch 82:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.58, valid_loss=-1.60]\n","Epoch 83:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.73, valid_loss=-1.60]\n","Epoch 84:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.53, valid_loss=-1.60]\n","Epoch 85:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.66, valid_loss=-1.60]\n","Epoch 86:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.56, valid_loss=-1.60]\n","Epoch 87:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.63, valid_loss=-1.60]\n","Epoch 88:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.73, valid_loss=-1.60]\n","Epoch 89:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.68, valid_loss=-1.60]\n","Epoch 90:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.71, valid_loss=-1.60]\n","Epoch 91:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.59, valid_loss=-1.60]\n","Epoch 92:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.57, valid_loss=-1.60]\n","Epoch 93:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.66, valid_loss=-1.60]\n","Epoch 94:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.60, valid_loss=-1.60]\n","Epoch 95:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.70, valid_loss=-1.60]\n","Epoch 96:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.71, valid_loss=-1.60]\n","Epoch 97:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.60, valid_loss=-1.60]\n","Epoch 98:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.72, valid_loss=-1.60]\n","Epoch 99:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.84, valid_loss=-1.60]\n","Epoch 99: 100%|██████████| 2/2 [00:00\u003c00:00, 19.43it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.84, valid_loss=-1.60]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6651)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6651)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 16.17it/s]\u001b[A\n","Epoch 100:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.75, valid_loss=-1.63]\n","Epoch 101:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.66, valid_loss=-1.63]\n","Epoch 102:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.72, valid_loss=-1.63]\n","Epoch 103:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.62, valid_loss=-1.63]\n","Epoch 104:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.69, valid_loss=-1.63]\n","Epoch 105:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.65, valid_loss=-1.63]\n","Epoch 106:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.63, valid_loss=-1.63]\n","Epoch 107:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.65, valid_loss=-1.63]\n","Epoch 108:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.75, valid_loss=-1.63]\n","Epoch 109:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.75, valid_loss=-1.63]\n","Epoch 110:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.83, valid_loss=-1.63]\n","Epoch 111:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.73, valid_loss=-1.63]\n","Epoch 112:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.74, valid_loss=-1.63]\n","Epoch 113:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.72, valid_loss=-1.63]\n","Epoch 114:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.62, valid_loss=-1.63]\n","Epoch 115:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.65, valid_loss=-1.63]\n","Epoch 116:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.68, valid_loss=-1.63]\n","Epoch 117:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.73, valid_loss=-1.63]\n","Epoch 118:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.76, valid_loss=-1.63]\n","Epoch 119:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.62, valid_loss=-1.63]\n","Epoch 120:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.76, valid_loss=-1.63]\n","Epoch 121:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.73, valid_loss=-1.63]\n","Epoch 122:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.70, valid_loss=-1.63]\n","Epoch 123:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.66, valid_loss=-1.63]\n","Epoch 124:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.65, valid_loss=-1.63]\n","Epoch 125:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.82, valid_loss=-1.63]\n","Epoch 126:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.62, valid_loss=-1.63]\n","Epoch 127:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.79, valid_loss=-1.63]\n","Epoch 128:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.79, valid_loss=-1.63]\n","Epoch 129:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.70, valid_loss=-1.63]\n","Epoch 130:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.73, valid_loss=-1.63]\n","Epoch 131:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.73, valid_loss=-1.63]\n","Epoch 132:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.73, valid_loss=-1.63]\n","Epoch 133:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.70, valid_loss=-1.63]\n","Epoch 134:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.76, valid_loss=-1.63]\n","Epoch 135:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.66, valid_loss=-1.63]\n","Epoch 136:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.75, valid_loss=-1.63]\n","Epoch 137:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.70, valid_loss=-1.63]\n","Epoch 138:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.67, valid_loss=-1.63]\n","Epoch 139:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.63, valid_loss=-1.63]\n","Epoch 140:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.75, valid_loss=-1.63]\n","Epoch 141:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.67, valid_loss=-1.63]\n","Epoch 142:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.67, valid_loss=-1.63]\n","Epoch 143:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.57, valid_loss=-1.63]\n","Epoch 144:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.77, valid_loss=-1.63]\n","Epoch 145:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.78, valid_loss=-1.63]\n","Epoch 146:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.75, valid_loss=-1.63]\n","Epoch 147:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.77, valid_loss=-1.63]\n","Epoch 148:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.72, valid_loss=-1.63]\n","Epoch 149:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.71, valid_loss=-1.63]\n","Epoch 149: 100%|██████████| 2/2 [00:00\u003c00:00, 18.74it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.71, valid_loss=-1.63]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6651)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 16.24it/s]\u001b[A\n","Epoch 150:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.77, valid_loss=-1.66]\n","Epoch 151:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.78, valid_loss=-1.66]\n","Epoch 152:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.75, valid_loss=-1.66]\n","Epoch 153:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.70, valid_loss=-1.66]\n","Epoch 154:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.74, valid_loss=-1.66]\n","Epoch 155:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.72, valid_loss=-1.66]\n","Epoch 156:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.68, valid_loss=-1.66]\n","Epoch 157:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.68, valid_loss=-1.66]\n","Epoch 158:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.83, valid_loss=-1.66]\n","Epoch 158: 100%|██████████| 2/2 [00:00\u003c00:00, 18.63it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.83, valid_loss=-1.66]\n","Epoch 158: 100%|██████████| 2/2 [00:00\u003c00:00, 18.52it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.82, valid_loss=-1.66]\n","Epoch 159:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.82, valid_loss=-1.66]\n","Epoch 160:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.72, valid_loss=-1.66]\n","Epoch 161:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.71, valid_loss=-1.66]\n","Epoch 162:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.66, valid_loss=-1.66]\n","Epoch 163:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.79, valid_loss=-1.66]\n","Epoch 164:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.75, valid_loss=-1.66]\n","Epoch 165:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.73, valid_loss=-1.66]\n","Epoch 166:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.83, valid_loss=-1.66]\n","Epoch 167:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.59, valid_loss=-1.66]\n","Epoch 168:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.78, valid_loss=-1.66]\n","Epoch 169:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.71, valid_loss=-1.66]\n","Epoch 170:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.69, valid_loss=-1.66]\n","Epoch 171:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.72, valid_loss=-1.66]\n","Epoch 172:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.69, valid_loss=-1.66]\n","Epoch 173:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.81, valid_loss=-1.66]\n","Epoch 174:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.80, valid_loss=-1.66]\n","Epoch 175:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.66, valid_loss=-1.66]\n","Epoch 176:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.87, valid_loss=-1.66]\n","Epoch 177:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.71, valid_loss=-1.66]\n","Epoch 178:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.64, valid_loss=-1.66]\n","Epoch 179:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.89, valid_loss=-1.66]\n","Epoch 180:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.72, valid_loss=-1.66]\n","Epoch 181:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.81, valid_loss=-1.66]\n","Epoch 182:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.59, valid_loss=-1.66]\n","Epoch 183:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.73, valid_loss=-1.66]\n","Epoch 184:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.76, valid_loss=-1.66]\n","Epoch 185:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.58, valid_loss=-1.66]\n","Epoch 186:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.78, valid_loss=-1.66]\n","Epoch 187:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.78, valid_loss=-1.66]\n","Epoch 188:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.77, valid_loss=-1.66]\n","Epoch 189:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.68, valid_loss=-1.66]\n","Epoch 190:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.66, valid_loss=-1.66]\n","Epoch 191:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.66, valid_loss=-1.66]\n","Epoch 192:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.74, valid_loss=-1.66]\n","Epoch 193:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.74, valid_loss=-1.66]\n","Epoch 194:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.75, valid_loss=-1.66]\n","Epoch 195:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.71, valid_loss=-1.66]\n","Epoch 196:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.85, valid_loss=-1.66]\n","Epoch 197:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.75, valid_loss=-1.66]\n","Epoch 198:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.69, valid_loss=-1.66]\n","Epoch 199:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.76, valid_loss=-1.66]\n","Epoch 199: 100%|██████████| 2/2 [00:00\u003c00:00, 19.38it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.76, valid_loss=-1.66]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6651)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6651)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 16.33it/s]\u001b[A\n","Epoch 200:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.78, valid_loss=-1.63]\n","Epoch 201:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.78, valid_loss=-1.63]\n","Epoch 202:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.73, valid_loss=-1.63]\n","Epoch 203:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.75, valid_loss=-1.63]\n","Epoch 204:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.82, valid_loss=-1.63]\n","Epoch 205:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.76, valid_loss=-1.63]\n","Epoch 206:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.66, valid_loss=-1.63]\n","Epoch 207:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.71, valid_loss=-1.63]\n","Epoch 208:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.75, valid_loss=-1.63]\n","Epoch 209:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.84, valid_loss=-1.63]\n","Epoch 210:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.66, valid_loss=-1.63]\n","Epoch 210: 100%|██████████| 2/2 [00:00\u003c00:00, 19.22it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.66, valid_loss=-1.63]\n","Epoch 210:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.77, valid_loss=-1.63]        \n","Epoch 211:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.77, valid_loss=-1.63]\n","Epoch 212:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.84, valid_loss=-1.63]\n","Epoch 213:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.79, valid_loss=-1.63]\n","Epoch 214:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.72, valid_loss=-1.63]\n","Epoch 215:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.76, valid_loss=-1.63]\n","Epoch 216:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.69, valid_loss=-1.63]\n","Epoch 217:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.80, valid_loss=-1.63]\n","Epoch 218:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.60, valid_loss=-1.63]\n","Epoch 219:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.70, valid_loss=-1.63]\n","Epoch 220:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.80, valid_loss=-1.63]\n","Epoch 221:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.59, valid_loss=-1.63]\n","Epoch 222:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.74, valid_loss=-1.63]\n","Epoch 223:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.74, valid_loss=-1.63]\n","Epoch 224:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.68, valid_loss=-1.63]\n","Epoch 225:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.71, valid_loss=-1.63]\n","Epoch 226:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.73, valid_loss=-1.63]\n","Epoch 227:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.84, valid_loss=-1.63]\n","Epoch 228:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.78, valid_loss=-1.63]\n","Epoch 229:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.75, valid_loss=-1.63]\n","Epoch 230:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.66, valid_loss=-1.63]\n","Epoch 230: 100%|██████████| 2/2 [00:00\u003c00:00, 18.80it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.70, valid_loss=-1.63]\n","Epoch 231:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.70, valid_loss=-1.63]\n","Epoch 232:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.75, valid_loss=-1.63]\n","Epoch 233:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.75, valid_loss=-1.63]\n","Epoch 234:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.76, valid_loss=-1.63]\n","Epoch 235:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.78, valid_loss=-1.63]\n","Epoch 236:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.72, valid_loss=-1.63]\n","Epoch 237:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.81, valid_loss=-1.63]\n","Epoch 238:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.81, valid_loss=-1.63]\n","Epoch 239:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.67, valid_loss=-1.63]\n","Epoch 240:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.80, valid_loss=-1.63]\n","Epoch 241:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.83, valid_loss=-1.63]\n","Epoch 242:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.81, valid_loss=-1.63]\n","Epoch 243:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.87, valid_loss=-1.63]\n","Epoch 244:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.67, valid_loss=-1.63]\n","Epoch 245:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.73, valid_loss=-1.63]\n","Epoch 246:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.82, valid_loss=-1.63]\n","Epoch 247:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.73, valid_loss=-1.63]\n","Epoch 248:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.74, valid_loss=-1.63]\n","Epoch 249:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.71, valid_loss=-1.63]\n","Epoch 249: 100%|██████████| 2/2 [00:00\u003c00:00, 18.74it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.71, valid_loss=-1.63]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6651)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 16.25it/s]\u001b[A\n","Epoch 250:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.74, valid_loss=-1.66]\n","Epoch 251:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.86, valid_loss=-1.66]\n","Epoch 252:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.97, valid_loss=-1.66]\n","Epoch 253:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.78, valid_loss=-1.66]\n","Epoch 254:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.81, valid_loss=-1.66]\n","Epoch 255:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.82, valid_loss=-1.66]\n","Epoch 256:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.83, valid_loss=-1.66]\n","Epoch 257:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.82, valid_loss=-1.66]\n","Epoch 258:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.87, valid_loss=-1.66]\n","Epoch 259:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.75, valid_loss=-1.66]\n","Epoch 260:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.81, valid_loss=-1.66]\n","Epoch 261:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.75, valid_loss=-1.66]\n","Epoch 262:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.76, valid_loss=-1.66]\n","Epoch 263:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.65, valid_loss=-1.66]\n","Epoch 264:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.86, valid_loss=-1.66]\n","Epoch 265:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.86, valid_loss=-1.66]\n","Epoch 266:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.73, valid_loss=-1.66]\n","Epoch 267:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.76, valid_loss=-1.66]\n","Epoch 268:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.67, valid_loss=-1.66]\n","Epoch 269:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.74, valid_loss=-1.66]\n","Epoch 270:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.71, valid_loss=-1.66]\n","Epoch 271:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.77, valid_loss=-1.66]\n","Epoch 272:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.74, valid_loss=-1.66]\n","Epoch 273:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.90, valid_loss=-1.66]\n","Epoch 274:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.75, valid_loss=-1.66]\n","Epoch 275:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.80, valid_loss=-1.66]\n","Epoch 276:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.81, valid_loss=-1.66]\n","Epoch 277:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.80, valid_loss=-1.66]\n","Epoch 278:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.83, valid_loss=-1.66]\n","Epoch 279:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.79, valid_loss=-1.66]\n","Epoch 280:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.58, valid_loss=-1.66]\n","Epoch 281:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.76, valid_loss=-1.66]\n","Epoch 282:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.77, valid_loss=-1.66]\n","Epoch 283:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.85, valid_loss=-1.66]\n","Epoch 284:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.71, valid_loss=-1.66]\n","Epoch 285:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.83, valid_loss=-1.66]\n","Epoch 286:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.91, valid_loss=-1.66]\n","Epoch 287:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.76, valid_loss=-1.66]\n","Epoch 288:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.81, valid_loss=-1.66]\n","Epoch 289:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.76, valid_loss=-1.66]\n","Epoch 290:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.68, valid_loss=-1.66]\n","Epoch 291:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.85, valid_loss=-1.66]\n","Epoch 292:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.84, valid_loss=-1.66]\n","Epoch 293:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.82, valid_loss=-1.66]\n","Epoch 294:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.78, valid_loss=-1.66]\n","Epoch 295:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.76, valid_loss=-1.66]\n","Epoch 296:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.70, valid_loss=-1.66]\n","Epoch 297:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.78, valid_loss=-1.66]\n","Epoch 298:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.78, valid_loss=-1.66]\n","Epoch 299:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.82, valid_loss=-1.66]\n","Epoch 299: 100%|██████████| 2/2 [00:00\u003c00:00, 19.38it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.82, valid_loss=-1.66]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6651)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 16.08it/s]\u001b[A\n","Epoch 300:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.78, valid_loss=-1.65]\n","Epoch 301:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.81, valid_loss=-1.65]\n","Epoch 302:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.75, valid_loss=-1.65]\n","Epoch 303:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.77, valid_loss=-1.65]\n","Epoch 304:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.77, valid_loss=-1.65]\n","Epoch 305:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.74, valid_loss=-1.65]\n","Epoch 306:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.72, valid_loss=-1.65]\n","Epoch 307:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.64, valid_loss=-1.65]\n","Epoch 308:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.76, valid_loss=-1.65]\n","Epoch 309:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.74, valid_loss=-1.65]\n","Epoch 310:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.76, valid_loss=-1.65]\n","Epoch 311:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.76, valid_loss=-1.65]\n","Epoch 312:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.75, valid_loss=-1.65]\n","Epoch 313:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.89, valid_loss=-1.65]\n","Epoch 314:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.77, valid_loss=-1.65]\n","Epoch 315:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.75, valid_loss=-1.65]\n","Epoch 316:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.77, valid_loss=-1.65]\n","Epoch 317:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.70, valid_loss=-1.65]\n","Epoch 318:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.72, valid_loss=-1.65]\n","Epoch 319:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.76, valid_loss=-1.65]\n","Epoch 320:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.88, valid_loss=-1.65]\n","Epoch 321:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.86, valid_loss=-1.65]\n","Epoch 322:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.83, valid_loss=-1.65]\n","Epoch 323:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.75, valid_loss=-1.65]\n","Epoch 324:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.79, valid_loss=-1.65]\n","Epoch 325:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.86, valid_loss=-1.65]\n","Epoch 326:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.82, valid_loss=-1.65]\n","Epoch 327:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.79, valid_loss=-1.65]\n","Epoch 328:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.81, valid_loss=-1.65]\n","Epoch 329:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.82, valid_loss=-1.65]\n","Epoch 330:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.73, valid_loss=-1.65]\n","Epoch 331:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.82, valid_loss=-1.65]\n","Epoch 332:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.81, valid_loss=-1.65]\n","Epoch 333:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.84, valid_loss=-1.65]\n","Epoch 334:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.82, valid_loss=-1.65]\n","Epoch 335:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.73, valid_loss=-1.65]\n","Epoch 336:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.78, valid_loss=-1.65]\n","Epoch 337:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.96, valid_loss=-1.65]\n","Epoch 338:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.80, valid_loss=-1.65]\n","Epoch 339:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.77, valid_loss=-1.65]\n","Epoch 340:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.82, valid_loss=-1.65]\n","Epoch 341:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.87, valid_loss=-1.65]\n","Epoch 342:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.72, valid_loss=-1.65]\n","Epoch 343:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.78, valid_loss=-1.65]\n","Epoch 343: 100%|██████████| 2/2 [00:00\u003c00:00, 19.01it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.92, valid_loss=-1.65]\n","Epoch 344:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.92, valid_loss=-1.65]\n","Epoch 345:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.90, valid_loss=-1.65]\n","Epoch 346:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.69, valid_loss=-1.65]\n","Epoch 347:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.64, valid_loss=-1.65]\n","Epoch 348:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.81, valid_loss=-1.65]\n","Epoch 349:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.59, valid_loss=-1.65]\n","Epoch 349: 100%|██████████| 2/2 [00:00\u003c00:00, 18.97it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.59, valid_loss=-1.65]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-18 13:06:40,647\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (24, 12, 1), 'n_pool_kernel_size': (2, 2, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=6651)\u001b[0m \n","\u001b[36m(_train_tune pid=6651)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 15.87it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6651)\u001b[0m \n","\u001b[36m(_train_tune pid=6651)\u001b[0m \r                                                                      \u001b[A\rEpoch 349: 100%|██████████| 2/2 [00:00\u003c00:00,  6.77it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.59, valid_loss=-1.66]\rEpoch 349: 100%|██████████| 2/2 [00:00\u003c00:00,  6.67it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.63, valid_loss=-1.66]\rEpoch 349: 100%|██████████| 2/2 [00:00\u003c00:00,  6.65it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.63, valid_loss=-1.66]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=6651)\u001b[0m `Trainer.fit` stopped: `max_steps=700.0` reached.\n","\u001b[36m(_train_tune pid=6937)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=6937)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=6937)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=6937)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=6937)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=6937)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=6937)\u001b[0m 2024-11-18 13:06:48.733502: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=6937)\u001b[0m 2024-11-18 13:06:48.753923: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=6937)\u001b[0m 2024-11-18 13:06:48.778768: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=6937)\u001b[0m 2024-11-18 13:06:48.786340: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=6937)\u001b[0m 2024-11-18 13:06:48.804392: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=6937)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=6937)\u001b[0m 2024-11-18 13:06:49.949756: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=6937)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=6937)\u001b[0m \n","\u001b[36m(_train_tune pid=6937)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=6937)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=6937)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=6937)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=6937)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=6937)\u001b[0m 3 | blocks       | ModuleList       | 3.1 M  | train\n","\u001b[36m(_train_tune pid=6937)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=6937)\u001b[0m 3.1 M     Trainable params\n","\u001b[36m(_train_tune pid=6937)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=6937)\u001b[0m 3.1 M     Total params\n","\u001b[36m(_train_tune pid=6937)\u001b[0m 12.363    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=6937)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=6937)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/4 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.930, train_loss_epoch=3.040]\n","Epoch 2:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-0.41]\n","Epoch 3:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.11]\n","Epoch 4:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.21]\n","Epoch 4: 100%|██████████| 4/4 [00:00\u003c00:00, 26.58it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.21]\n","Epoch 4: 100%|██████████| 4/4 [00:00\u003c00:00, 26.39it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.33]\n","Epoch 5:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.33]\n","Epoch 6:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.08, train_loss_epoch=-1.18]\n","Epoch 7:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.33]\n","Epoch 8:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.35]\n","Epoch 9:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.36]\n","Epoch 10:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.972, train_loss_epoch=-1.10]\n","Epoch 11:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.43]\n","Epoch 12:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.01, train_loss_epoch=-1.31]\n","Epoch 13:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.37]\n","Epoch 14:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.35]\n","Epoch 15:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.40]\n","Epoch 16:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.05, train_loss_epoch=-1.28]\n","Epoch 17:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.51]\n","Epoch 18:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.54]\n","Epoch 19:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.45]\n","Epoch 20:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.37]\n","Epoch 21:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.53]\n","Epoch 22:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.42]\n","Epoch 22: 100%|██████████| 4/4 [00:00\u003c00:00, 24.79it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.37]\n","Epoch 23:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.37]\n","Epoch 24:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.36, train_loss_epoch=-1.45]\n","Epoch 24: 100%|██████████| 4/4 [00:00\u003c00:00, 24.26it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.45]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6937)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.06it/s]\u001b[A\n","Epoch 25:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.53, valid_loss=-1.41]\n","Epoch 26:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.47, valid_loss=-1.41]\n","Epoch 27:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.42, valid_loss=-1.41]\n","Epoch 28:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.46, valid_loss=-1.41]\n","Epoch 29:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.77, train_loss_epoch=-1.28, valid_loss=-1.41]\n","Epoch 30:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.41, valid_loss=-1.41]\n","Epoch 31:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.38, valid_loss=-1.41]\n","Epoch 32:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.913, train_loss_epoch=-1.30, valid_loss=-1.41]\n","Epoch 33:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.35, valid_loss=-1.41]\n","Epoch 34:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.32, valid_loss=-1.41]\n","Epoch 35:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.37, valid_loss=-1.41]\n","Epoch 36:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.36, valid_loss=-1.41]\n","Epoch 37:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.39, valid_loss=-1.41]\n","Epoch 38:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.55, valid_loss=-1.41]\n","Epoch 39:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.44, valid_loss=-1.41]\n","Epoch 40:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.39, valid_loss=-1.41]\n","Epoch 41:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.864, train_loss_epoch=-1.14, valid_loss=-1.41]\n","Epoch 42:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.03, train_loss_epoch=-1.29, valid_loss=-1.41]\n","Epoch 43:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.44, valid_loss=-1.41]\n","Epoch 44:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.46, valid_loss=-1.41]\n","Epoch 45:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.45, valid_loss=-1.41]\n","Epoch 46:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.39, valid_loss=-1.41]\n","Epoch 47:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.49, valid_loss=-1.41]\n","Epoch 48:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.50, valid_loss=-1.41]\n","Epoch 49:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.983, train_loss_epoch=-1.27, valid_loss=-1.41]\n","Epoch 49: 100%|██████████| 4/4 [00:00\u003c00:00, 25.11it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.27, valid_loss=-1.41] \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6937)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.07it/s]\u001b[A\n","Epoch 50:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.40, valid_loss=-1.41]\n","Epoch 51:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.36, train_loss_epoch=-1.36, valid_loss=-1.41]\n","Epoch 52:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.49, valid_loss=-1.41]\n","Epoch 53:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.49, valid_loss=-1.41]\n","Epoch 54:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.61, valid_loss=-1.41]\n","Epoch 55:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.37, valid_loss=-1.41]\n","Epoch 56:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.48, valid_loss=-1.41]\n","Epoch 57:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.53, valid_loss=-1.41]\n","Epoch 58:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.05, train_loss_epoch=-1.32, valid_loss=-1.41]\n","Epoch 59:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.46, valid_loss=-1.41]\n","Epoch 60:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.33, valid_loss=-1.41]\n","Epoch 61:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.48, valid_loss=-1.41]\n","Epoch 62:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.41, valid_loss=-1.41]\n","Epoch 63:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.44, valid_loss=-1.41]\n","Epoch 64:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.45, valid_loss=-1.41]\n","Epoch 65:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.08, train_loss_epoch=-1.42, valid_loss=-1.41]\n","Epoch 66:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.38, valid_loss=-1.41]\n","Epoch 67:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.727, train_loss_epoch=-1.25, valid_loss=-1.41]\n","Epoch 68:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.48, valid_loss=-1.41]\n","Epoch 69:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.46, valid_loss=-1.41]\n","Epoch 70:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.29, valid_loss=-1.41]\n","Epoch 71:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.879, train_loss_epoch=-1.29, valid_loss=-1.41]\n","Epoch 72:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.469, train_loss_epoch=-1.21, valid_loss=-1.41]\n","Epoch 73:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.42, valid_loss=-1.41]\n","Epoch 74:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.53, valid_loss=-1.41]\n","Epoch 74: 100%|██████████| 4/4 [00:00\u003c00:00, 24.73it/s, v_num=0, train_loss_step=-0.951, train_loss_epoch=-1.53, valid_loss=-1.41]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6937)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.05it/s]\u001b[A\n","Epoch 75:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.951, train_loss_epoch=-1.33, valid_loss=-1.43]\n","Epoch 76:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.45, valid_loss=-1.43]\n","Epoch 77:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.45, valid_loss=-1.43]\n","Epoch 78:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.99, train_loss_epoch=-1.33, valid_loss=-1.43]\n","Epoch 79:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.53, valid_loss=-1.43]\n","Epoch 80:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.45, valid_loss=-1.43]\n","Epoch 81:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.46, valid_loss=-1.43]\n","Epoch 82:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.33, valid_loss=-1.43]\n","Epoch 83:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.39, valid_loss=-1.43]\n","Epoch 84:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.632, train_loss_epoch=-1.21, valid_loss=-1.43]\n","Epoch 85:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.43, valid_loss=-1.43]\n","Epoch 86:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.43, valid_loss=-1.43]\n","Epoch 87:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.38, valid_loss=-1.43]\n","Epoch 88:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.48, valid_loss=-1.43]\n","Epoch 89:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.06, train_loss_epoch=-1.40, valid_loss=-1.43]\n","Epoch 90:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.941, train_loss_epoch=-1.24, valid_loss=-1.43]\n","Epoch 91:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.977, train_loss_epoch=-1.27, valid_loss=-1.43]\n","Epoch 92:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.48, valid_loss=-1.43]\n","Epoch 93:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.41, valid_loss=-1.43]\n","Epoch 94:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.56, valid_loss=-1.43]\n","Epoch 95:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.53, valid_loss=-1.43]\n","Epoch 96:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.40, valid_loss=-1.43]\n","Epoch 97:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.39, valid_loss=-1.43]\n","Epoch 97: 100%|██████████| 4/4 [00:00\u003c00:00, 25.09it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.46, valid_loss=-1.43]\n","Epoch 98:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.46, valid_loss=-1.43]\n","Epoch 99:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.37, valid_loss=-1.43]\n","Epoch 99: 100%|██████████| 4/4 [00:00\u003c00:00, 25.51it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.37, valid_loss=-1.43]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6937)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.05it/s]\u001b[A\n","Epoch 100:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.50, valid_loss=-1.42]\n","Epoch 101:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.50, valid_loss=-1.42]\n","Epoch 102:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.43, valid_loss=-1.42]\n","Epoch 103:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.46, valid_loss=-1.42]\n","Epoch 104:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.44, valid_loss=-1.42]\n","Epoch 105:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.50, valid_loss=-1.42]\n","Epoch 106:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.48, valid_loss=-1.42]\n","Epoch 107:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.43, valid_loss=-1.42]\n","Epoch 108:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.49, valid_loss=-1.42]\n","Epoch 109:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.40, valid_loss=-1.42]\n","Epoch 110:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.46, valid_loss=-1.42]\n","Epoch 111:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.38, valid_loss=-1.42]\n","Epoch 112:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.863, train_loss_epoch=-1.24, valid_loss=-1.42]\n","Epoch 113:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.57, valid_loss=-1.42]\n","Epoch 113: 100%|██████████| 4/4 [00:00\u003c00:00, 25.63it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.57, valid_loss=-1.42]\n","Epoch 114:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.54, valid_loss=-1.42]\n","Epoch 115:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.45, valid_loss=-1.42]\n","Epoch 116:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.43, valid_loss=-1.42]\n","Epoch 117:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.45, valid_loss=-1.42]\n","Epoch 118:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.955, train_loss_epoch=-1.30, valid_loss=-1.42]\n","Epoch 119:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.55, valid_loss=-1.42]\n","Epoch 120:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.05, train_loss_epoch=-1.33, valid_loss=-1.42]\n","Epoch 121:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.45, valid_loss=-1.42]\n","Epoch 122:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.41, valid_loss=-1.42]\n","Epoch 123:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.31, valid_loss=-1.42]\n","Epoch 124:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.55, valid_loss=-1.42]\n","Epoch 124: 100%|██████████| 4/4 [00:00\u003c00:00, 24.89it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.55, valid_loss=-1.42]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6937)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.06it/s]\u001b[A\n","Epoch 125:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.43, valid_loss=-1.43]\n","Epoch 126:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.53, valid_loss=-1.43]\n","Epoch 127:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.04, train_loss_epoch=-1.40, valid_loss=-1.43]\n","Epoch 128:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.39, valid_loss=-1.43]\n","Epoch 129:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.37, valid_loss=-1.43]\n","Epoch 130:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.32, valid_loss=-1.43]\n","Epoch 131:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.45, valid_loss=-1.43]\n","Epoch 132:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.35, valid_loss=-1.43]\n","Epoch 133:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.34, valid_loss=-1.43]\n","Epoch 134:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.40, valid_loss=-1.43]\n","Epoch 135:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.49, valid_loss=-1.43]\n","Epoch 136:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.56, valid_loss=-1.43]\n","Epoch 137:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.744, train_loss_epoch=-1.29, valid_loss=-1.43]\n","Epoch 138:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.44, valid_loss=-1.43]\n","Epoch 139:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.34, valid_loss=-1.43]\n","Epoch 140:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.36, valid_loss=-1.43]\n","Epoch 141:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.46, valid_loss=-1.43]\n","Epoch 142:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.05, train_loss_epoch=-1.35, valid_loss=-1.43]\n","Epoch 143:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.48, valid_loss=-1.43]\n","Epoch 144:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.48, valid_loss=-1.43]\n","Epoch 145:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.40, valid_loss=-1.43]\n","Epoch 146:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.42, valid_loss=-1.43]\n","Epoch 147:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.34, valid_loss=-1.43]\n","Epoch 148:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.53, valid_loss=-1.43]\n","Epoch 149:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.995, train_loss_epoch=-1.31, valid_loss=-1.43]\n","Epoch 149: 100%|██████████| 4/4 [00:00\u003c00:00, 25.14it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.31, valid_loss=-1.43] \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6937)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.05it/s]\u001b[A\n","Epoch 150:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.45, valid_loss=-1.41]\n","Epoch 151:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.43, valid_loss=-1.41]\n","Epoch 152:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.38, valid_loss=-1.41]\n","Epoch 153:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.46, valid_loss=-1.41]\n","Epoch 154:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.51, valid_loss=-1.41]\n","Epoch 155:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.98, train_loss_epoch=-1.32, valid_loss=-1.41]\n","Epoch 156:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.39, valid_loss=-1.41]\n","Epoch 156: 100%|██████████| 4/4 [00:00\u003c00:00, 25.56it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.46, valid_loss=-1.41]\n","Epoch 157:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.46, valid_loss=-1.41]\n","Epoch 158:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.39, valid_loss=-1.41]\n","Epoch 159:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-0.781, valid_loss=-1.41]\n","Epoch 160:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.587, train_loss_epoch=-1.24, valid_loss=-1.41]\n","Epoch 161:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.32, valid_loss=-1.41]\n","Epoch 162:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.30, valid_loss=-1.41]\n","Epoch 163:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.36, valid_loss=-1.41]\n","Epoch 164:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.27, valid_loss=-1.41]\n","Epoch 165:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.01, train_loss_epoch=-1.25, valid_loss=-1.41]\n","Epoch 166:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.28, valid_loss=-1.41]\n","Epoch 167:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.02, train_loss_epoch=-1.28, valid_loss=-1.41]\n","Epoch 168:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.09, valid_loss=-1.41]\n","Epoch 169:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.53, valid_loss=-1.41]\n","Epoch 170:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.34, valid_loss=-1.41]\n","Epoch 171:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.53, valid_loss=-1.41]\n","Epoch 172:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.53, valid_loss=-1.41]\n","Epoch 173:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.38, valid_loss=-1.41]\n","Epoch 174:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.41, valid_loss=-1.41]\n","Epoch 174: 100%|██████████| 4/4 [00:00\u003c00:00, 25.20it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.41, valid_loss=-1.41]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6937)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.07it/s]\u001b[A\n","Epoch 175:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.43, valid_loss=-1.23]\n","Epoch 176:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.43, valid_loss=-1.23]\n","Epoch 177:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.41, valid_loss=-1.23]\n","Epoch 178:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.37, valid_loss=-1.23]\n","Epoch 179:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.42, valid_loss=-1.23]\n","Epoch 180:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.37, valid_loss=-1.23]\n","Epoch 181:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.52, valid_loss=-1.23]\n","Epoch 182:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.41, valid_loss=-1.23]\n","Epoch 183:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.48, valid_loss=-1.23]\n","Epoch 184:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.38, valid_loss=-1.23]\n","Epoch 185:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.07, train_loss_epoch=-1.30, valid_loss=-1.23]\n","Epoch 186:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.61, valid_loss=-1.23]\n","Epoch 187:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.42, valid_loss=-1.23]\n","Epoch 188:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.29, valid_loss=-1.23]\n","Epoch 189:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.725, train_loss_epoch=-1.22, valid_loss=-1.23]\n","Epoch 190:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.43, valid_loss=-1.23]\n","Epoch 191:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.688, train_loss_epoch=-1.27, valid_loss=-1.23]\n","Epoch 192:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.31, valid_loss=-1.23]\n","Epoch 193:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.41, valid_loss=-1.23]\n","Epoch 194:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.48, valid_loss=-1.23]\n","Epoch 195:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.49, valid_loss=-1.23]\n","Epoch 196:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.59, valid_loss=-1.23]\n","Epoch 197:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.42, valid_loss=-1.23]\n","Epoch 198:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.46, valid_loss=-1.23]\n","Epoch 199:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.01, train_loss_epoch=-1.40, valid_loss=-1.23]\n","Epoch 199: 100%|██████████| 4/4 [00:00\u003c00:00, 24.24it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.40, valid_loss=-1.23]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6937)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6937)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.07it/s]\u001b[A\n","Epoch 200:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.49, valid_loss=-1.42]\n","Epoch 201:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.35, valid_loss=-1.42]\n","Epoch 202:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.40, valid_loss=-1.42]\n","Epoch 203:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.07, train_loss_epoch=-1.32, valid_loss=-1.42]\n","Epoch 204:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.35, valid_loss=-1.42]\n","Epoch 205:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.50, valid_loss=-1.42]\n","Epoch 206:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.59, valid_loss=-1.42]\n","Epoch 207:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.54, valid_loss=-1.42]\n","Epoch 208:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.44, valid_loss=-1.42]\n","Epoch 209:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.742, train_loss_epoch=-1.24, valid_loss=-1.42]\n","Epoch 210:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.42, valid_loss=-1.42]\n","Epoch 210: 100%|██████████| 4/4 [00:00\u003c00:00, 26.15it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.42, valid_loss=-1.42]\n","Epoch 210: 100%|██████████| 4/4 [00:00\u003c00:00, 25.99it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.42, valid_loss=-1.42]\n","Epoch 211:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.42, valid_loss=-1.42]\n","Epoch 212:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.36, valid_loss=-1.42]\n","Epoch 213:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.39, valid_loss=-1.42]\n","Epoch 214:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.49, valid_loss=-1.42]\n","Epoch 215:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.56, valid_loss=-1.42]\n","Epoch 216:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.40, valid_loss=-1.42]\n","Epoch 217:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.41, valid_loss=-1.42]\n","Epoch 218:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.691, train_loss_epoch=-1.27, valid_loss=-1.42]\n","Epoch 219:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.40, valid_loss=-1.42]\n","Epoch 220:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.919, train_loss_epoch=-1.35, valid_loss=-1.42]\n","Epoch 221:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.52, valid_loss=-1.42]\n","Epoch 222:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.44, valid_loss=-1.42]\n","Epoch 223:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.51, valid_loss=-1.42]\n","Epoch 224:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.44, valid_loss=-1.42]\n","Epoch 224: 100%|██████████| 4/4 [00:00\u003c00:00, 25.45it/s, v_num=0, train_loss_step=-0.558, train_loss_epoch=-1.44, valid_loss=-1.42]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6937)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.05it/s]\u001b[A\n","Epoch 225:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.558, train_loss_epoch=-1.24, valid_loss=-1.42]\n","Epoch 226:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.43, valid_loss=-1.42]\n","Epoch 227:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.44, valid_loss=-1.42]\n","Epoch 228:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.49, valid_loss=-1.42]\n","Epoch 229:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.988, train_loss_epoch=-1.25, valid_loss=-1.42]\n","Epoch 230:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.37, valid_loss=-1.42]\n","Epoch 231:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.94, train_loss_epoch=-1.30, valid_loss=-1.42]\n","Epoch 232:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.834, train_loss_epoch=-1.30, valid_loss=-1.42]\n","Epoch 233:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.48, valid_loss=-1.42]\n","Epoch 234:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.36, train_loss_epoch=-1.41, valid_loss=-1.42]\n","Epoch 235:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.54, valid_loss=-1.42]\n","Epoch 236:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.40, valid_loss=-1.42]\n","Epoch 237:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.709, train_loss_epoch=-1.22, valid_loss=-1.42]\n","Epoch 238:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.46, valid_loss=-1.42]\n","Epoch 239:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.814, train_loss_epoch=-1.29, valid_loss=-1.42]\n","Epoch 240:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.45, valid_loss=-1.42]\n","Epoch 241:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.51, valid_loss=-1.42]\n","Epoch 242:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.41, valid_loss=-1.42]\n","Epoch 243:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.08, train_loss_epoch=-1.36, valid_loss=-1.42]\n","Epoch 244:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.45, valid_loss=-1.42]\n","Epoch 245:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.31, valid_loss=-1.42]\n","Epoch 246:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.39, valid_loss=-1.42]\n","Epoch 247:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.31, valid_loss=-1.42]\n","Epoch 248:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.38, valid_loss=-1.42]\n","Epoch 249:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.56, valid_loss=-1.42]\n","Epoch 249: 100%|██████████| 4/4 [00:00\u003c00:00, 25.36it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.56, valid_loss=-1.42]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6937)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6937)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.07it/s]\u001b[A\n","Epoch 250:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.45, valid_loss=-1.44]\n","Epoch 251:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.812, train_loss_epoch=-1.21, valid_loss=-1.44]\n","Epoch 252:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.04, train_loss_epoch=-1.32, valid_loss=-1.44]\n","Epoch 253:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.45, valid_loss=-1.44]\n","Epoch 254:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.01, train_loss_epoch=-1.40, valid_loss=-1.44]\n","Epoch 255:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.43, valid_loss=-1.44]\n","Epoch 256:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.49, valid_loss=-1.44]\n","Epoch 257:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.51, valid_loss=-1.44]\n","Epoch 258:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.58, valid_loss=-1.44]\n","Epoch 259:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.685, train_loss_epoch=-1.28, valid_loss=-1.44]\n","Epoch 260:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.48, valid_loss=-1.44]\n","Epoch 261:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.41, valid_loss=-1.44]\n","Epoch 262:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.55, valid_loss=-1.44]\n","Epoch 263:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.704, train_loss_epoch=-1.30, valid_loss=-1.44]\n","Epoch 264:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.34, valid_loss=-1.44]\n","Epoch 265:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.65, valid_loss=-1.44]\n","Epoch 266:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.48, valid_loss=-1.44]\n","Epoch 267:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.36, valid_loss=-1.44]\n","Epoch 268:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.949, train_loss_epoch=-1.33, valid_loss=-1.44]\n","Epoch 269:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.58, valid_loss=-1.44]\n","Epoch 270:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.38, valid_loss=-1.44]\n","Epoch 271:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.32, valid_loss=-1.44]\n","Epoch 272:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.39, valid_loss=-1.44]\n","Epoch 273:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.08, train_loss_epoch=-1.33, valid_loss=-1.44]\n","Epoch 274:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.918, train_loss_epoch=-1.34, valid_loss=-1.44]\n","Epoch 274: 100%|██████████| 4/4 [00:00\u003c00:00, 25.55it/s, v_num=0, train_loss_step=-0.845, train_loss_epoch=-1.34, valid_loss=-1.44]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=6937)\u001b[0m `Trainer.fit` stopped: `max_steps=1100.0` reached.\n","2024-11-18 13:07:59,100\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (40, 20, 1), 'n_pool_kernel_size': (2, 2, 2), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=6937)\u001b[0m \n","\u001b[36m(_train_tune pid=6937)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.06it/s]\u001b[A\n","\u001b[36m(_train_tune pid=6937)\u001b[0m \r                                                                      \u001b[A\rEpoch 274: 100%|██████████| 4/4 [00:02\u003c00:00,  1.88it/s, v_num=0, train_loss_step=-0.845, train_loss_epoch=-1.34, valid_loss=-1.44]\rEpoch 274: 100%|██████████| 4/4 [00:02\u003c00:00,  1.88it/s, v_num=0, train_loss_step=-0.845, train_loss_epoch=-1.31, valid_loss=-1.44]\rEpoch 274: 100%|██████████| 4/4 [00:02\u003c00:00,  1.88it/s, v_num=0, train_loss_step=-0.845, train_loss_epoch=-1.31, valid_loss=-1.44]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=7339)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=7339)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=7339)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=7339)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=7339)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=7339)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=7339)\u001b[0m 2024-11-18 13:08:06.863852: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=7339)\u001b[0m 2024-11-18 13:08:06.884281: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=7339)\u001b[0m 2024-11-18 13:08:06.908726: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=7339)\u001b[0m 2024-11-18 13:08:06.916195: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=7339)\u001b[0m 2024-11-18 13:08:06.935308: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=7339)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=7339)\u001b[0m 2024-11-18 13:08:08.084549: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=7339)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=7339)\u001b[0m \n","\u001b[36m(_train_tune pid=7339)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=7339)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=7339)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=7339)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=7339)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=7339)\u001b[0m 3 | blocks       | ModuleList       | 2.8 M  | train\n","\u001b[36m(_train_tune pid=7339)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=7339)\u001b[0m 2.8 M     Trainable params\n","\u001b[36m(_train_tune pid=7339)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=7339)\u001b[0m 2.8 M     Total params\n","\u001b[36m(_train_tune pid=7339)\u001b[0m 11.163    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=7339)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=7339)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/13 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=1.060]\n","Epoch 2:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.56]\n","Epoch 3:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.65]\n","Epoch 4:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.65]\n","Epoch 5:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.69]\n","Epoch 6:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.73]\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.76]\n","\u001b[36m(_train_tune pid=7339)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7339)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 64.83it/s]\u001b[A\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.76, valid_loss=-1.74]\n","Epoch 8:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.76, valid_loss=-1.74]\n","Epoch 9:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.75, valid_loss=-1.74]\n","Epoch 10:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.78, valid_loss=-1.74]\n","Epoch 11:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-1.83, valid_loss=-1.74]\n","Epoch 12:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.78, valid_loss=-1.74]\n","Epoch 13:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.82, valid_loss=-1.74]\n","Epoch 14:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.77, valid_loss=-1.74]\n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.78, valid_loss=-1.74]\n","\u001b[36m(_train_tune pid=7339)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7339)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 64.23it/s]\u001b[A\n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.78, valid_loss=-1.79]\n","Epoch 16:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-1.84, valid_loss=-1.79]\n","Epoch 17:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-1.86, valid_loss=-1.79]\n","Epoch 17: 100%|██████████| 13/13 [00:00\u003c00:00, 62.83it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.79, valid_loss=-1.79]\n","Epoch 18:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.79, valid_loss=-1.79]\n","Epoch 19:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.83, valid_loss=-1.79]\n","Epoch 20:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.87, valid_loss=-1.79]\n","Epoch 21:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.84, valid_loss=-1.79]\n","Epoch 22:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.88, valid_loss=-1.79]\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.86, valid_loss=-1.79]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7339)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 66.44it/s]\u001b[A\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.86, valid_loss=-1.85]\n","Epoch 24:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.42, train_loss_epoch=-1.90, valid_loss=-1.85]\n","Epoch 25:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-1.88, valid_loss=-1.85]\n","Epoch 26:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.88, valid_loss=-1.85]\n","Epoch 27:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.85, valid_loss=-1.85]\n","Epoch 28:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.84, valid_loss=-1.85]\n","Epoch 29:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.89, valid_loss=-1.85]\n","Epoch 30:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.87, valid_loss=-1.85]\n","\u001b[36m(_train_tune pid=7339)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7339)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 67.03it/s]\u001b[A\n","Epoch 30:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.87, valid_loss=-1.86]\n","Epoch 31:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.87, valid_loss=-1.86]\n","Epoch 32:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.88, valid_loss=-1.86]\n","Epoch 33:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.86, valid_loss=-1.86]\n","Epoch 34:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.89, valid_loss=-1.86]\n","Epoch 35:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.88, valid_loss=-1.86]\n","Epoch 36:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.42, train_loss_epoch=-1.92, valid_loss=-1.86]\n","Epoch 37:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.89, valid_loss=-1.86]\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.86, valid_loss=-1.86]\n","\u001b[36m(_train_tune pid=7339)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7339)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 61.72it/s]\u001b[A\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.86, valid_loss=-1.86]\n","Epoch 39:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.91, valid_loss=-1.86]\n","Epoch 40:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.85, valid_loss=-1.86]\n","Epoch 41:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.91, valid_loss=-1.86]\n","Epoch 42:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.89, valid_loss=-1.86]\n","Epoch 43:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.91, valid_loss=-1.86]\n","Epoch 44:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.92, valid_loss=-1.86]\n","Epoch 45:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.93, valid_loss=-1.86]\n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-1.93, valid_loss=-1.86]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7339)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 66.99it/s]\u001b[A\n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.93, valid_loss=-1.89]\n","Epoch 47:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.90, valid_loss=-1.89]\n","Epoch 48:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-1.95, valid_loss=-1.89]\n","Epoch 49:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.90, valid_loss=-1.89]\n","Epoch 50:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.93, valid_loss=-1.89]\n","Epoch 51:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.91, valid_loss=-1.89]\n","Epoch 52:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.48, train_loss_epoch=-1.96, valid_loss=-1.89]\n","Epoch 53:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.89, valid_loss=-1.89]\n","\u001b[36m(_train_tune pid=7339)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7339)\u001b[0m \n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7339)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 63.54it/s]\u001b[A\n","Epoch 54:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.90, valid_loss=-1.89]\n","Epoch 55:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.86, valid_loss=-1.89]\n","Epoch 56:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-1.96, valid_loss=-1.89]\n","Epoch 57:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.92, valid_loss=-1.89]\n","Epoch 58:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-1.95, valid_loss=-1.89]\n","Epoch 58: 100%|██████████| 13/13 [00:00\u003c00:00, 60.82it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.91, valid_loss=-1.89]\n","Epoch 59:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.91, valid_loss=-1.89]\n","Epoch 60:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.90, valid_loss=-1.89]\n","Epoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.91, valid_loss=-1.89]\n","\u001b[36m(_train_tune pid=7339)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-18 13:08:25,351\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (60, 8, 1), 'n_pool_kernel_size': (8, 4, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=7339)\u001b[0m `Trainer.fit` stopped: `max_steps=800.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=7339)\u001b[0m \n","\u001b[36m(_train_tune pid=7339)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 63.68it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7339)\u001b[0m \r                                                                        \u001b[A\rEpoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.91, valid_loss=-1.89]\rEpoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.93, valid_loss=-1.89]\rEpoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.93, valid_loss=-1.89]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=7513)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=7513)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=7513)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=7513)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=7513)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=7513)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=7513)\u001b[0m 2024-11-18 13:08:32.764791: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=7513)\u001b[0m 2024-11-18 13:08:32.784769: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=7513)\u001b[0m 2024-11-18 13:08:32.809381: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=7513)\u001b[0m 2024-11-18 13:08:32.816992: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=7513)\u001b[0m 2024-11-18 13:08:32.834284: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=7513)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=7513)\u001b[0m 2024-11-18 13:08:33.988413: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=7513)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=7513)\u001b[0m \n","\u001b[36m(_train_tune pid=7513)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=7513)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=7513)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=7513)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=7513)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=7513)\u001b[0m 3 | blocks       | ModuleList       | 2.9 M  | train\n","\u001b[36m(_train_tune pid=7513)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=7513)\u001b[0m 2.9 M     Trainable params\n","\u001b[36m(_train_tune pid=7513)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=7513)\u001b[0m 2.9 M     Total params\n","\u001b[36m(_train_tune pid=7513)\u001b[0m 11.452    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=7513)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=7513)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/2 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-0.865]\n","Epoch 2:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.921, train_loss_epoch=-0.919]\n","Epoch 3:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.14]\n","Epoch 4:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.00, train_loss_epoch=-1.06]\n","Epoch 5:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.01, train_loss_epoch=-1.01]\n","Epoch 6:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.06]\n","Epoch 7:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.03, train_loss_epoch=-1.05]\n","Epoch 8:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.06]\n","Epoch 9:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.14]\n","Epoch 10:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.16]\n","Epoch 11:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.964, train_loss_epoch=-0.968]\n","Epoch 12:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.22]\n","Epoch 13:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-0.978]\n","Epoch 14:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.23]\n","Epoch 15:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.09]\n","Epoch 16:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.28]\n","Epoch 17:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.05, train_loss_epoch=-1.14]\n","Epoch 18:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.14]\n","Epoch 19:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.27]\n","Epoch 20:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.22]\n","Epoch 21:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.20]\n","Epoch 22:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.23]\n","Epoch 23:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.22]\n","Epoch 24:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.23]\n","Epoch 25:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.19]\n","Epoch 26:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.19]\n","Epoch 27:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.37]\n","Epoch 28:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.02, train_loss_epoch=-1.20]\n","Epoch 29:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.998, train_loss_epoch=-1.05]\n","Epoch 30:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.24]\n","Epoch 31:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.25]\n","Epoch 32:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.36, train_loss_epoch=-1.38]\n","Epoch 33:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.36]\n","Epoch 34:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.08, train_loss_epoch=-1.21]\n","Epoch 35:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.24]\n","Epoch 36:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.35]\n","Epoch 37:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.09]\n","Epoch 38:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.32]\n","Epoch 39:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.24]\n","Epoch 40:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.30]\n","Epoch 41:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.24]\n","Epoch 42:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.20]\n","Epoch 43:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.30]\n","Epoch 44:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.26]\n","Epoch 45:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.20]\n","Epoch 46:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.20]\n","Epoch 47:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.13]\n","Epoch 48:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.17]\n","Epoch 49:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.19]\n","Epoch 49: 100%|██████████| 2/2 [00:00\u003c00:00, 18.76it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.19]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7513)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 15.53it/s]\u001b[A\n","Epoch 50:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.34, valid_loss=-1.16]\n","Epoch 51:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.06, train_loss_epoch=-1.24, valid_loss=-1.16]\n","Epoch 52:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.32, valid_loss=-1.16]\n","Epoch 53:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.18, valid_loss=-1.16]\n","Epoch 54:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.29, valid_loss=-1.16]\n","Epoch 55:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.30, valid_loss=-1.16]\n","Epoch 56:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.27, valid_loss=-1.16]\n","Epoch 57:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.28, valid_loss=-1.16]\n","Epoch 58:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.37, valid_loss=-1.16]\n","Epoch 59:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.27, valid_loss=-1.16]\n","Epoch 60:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.26, valid_loss=-1.16]\n","Epoch 61:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.37, valid_loss=-1.16]\n","Epoch 62:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.09, train_loss_epoch=-1.17, valid_loss=-1.16]\n","Epoch 63:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.24, valid_loss=-1.16]\n","Epoch 64:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.08, train_loss_epoch=-1.21, valid_loss=-1.16]\n","Epoch 65:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.39, valid_loss=-1.16]\n","Epoch 66:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.38, valid_loss=-1.16]\n","Epoch 67:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.14, valid_loss=-1.16]\n","Epoch 68:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.39, valid_loss=-1.16]\n","Epoch 69:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.47, valid_loss=-1.16]\n","Epoch 70:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.24, valid_loss=-1.16]\n","Epoch 71:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.34, valid_loss=-1.16]\n","Epoch 72:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.33, valid_loss=-1.16]\n","Epoch 73:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.27, valid_loss=-1.16]\n","Epoch 74:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.44, valid_loss=-1.16]\n","Epoch 75:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.39, valid_loss=-1.16]\n","Epoch 76:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.26, valid_loss=-1.16]\n","Epoch 77:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.24, valid_loss=-1.16]\n","Epoch 78:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.36, valid_loss=-1.16]\n","Epoch 79:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.30, valid_loss=-1.16]\n","Epoch 80:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.29, valid_loss=-1.16]\n","Epoch 81:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.36, valid_loss=-1.16]\n","Epoch 82:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.34, valid_loss=-1.16]\n","Epoch 83:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.36, train_loss_epoch=-1.38, valid_loss=-1.16]\n","Epoch 84:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.32, valid_loss=-1.16]\n","Epoch 85:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.35, valid_loss=-1.16]\n","Epoch 86:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.39, valid_loss=-1.16]\n","Epoch 87:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.45, valid_loss=-1.16]\n","Epoch 88:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.36, train_loss_epoch=-1.38, valid_loss=-1.16]\n","Epoch 89:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.27, valid_loss=-1.16]\n","Epoch 90:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.29, valid_loss=-1.16]\n","Epoch 91:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.36, valid_loss=-1.16]\n","Epoch 92:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.31, valid_loss=-1.16]\n","Epoch 93:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.29, valid_loss=-1.16]\n","Epoch 94:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.19, valid_loss=-1.16]\n","Epoch 95:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.37, valid_loss=-1.16]\n","Epoch 96:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.40, valid_loss=-1.16]\n","Epoch 97:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.35, valid_loss=-1.16]\n","Epoch 98:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.42, valid_loss=-1.16]\n","Epoch 99:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.29, valid_loss=-1.16]\n","Epoch 99: 100%|██████████| 2/2 [00:00\u003c00:00, 18.69it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.29, valid_loss=-1.16]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7513)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 15.55it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7513)\u001b[0m \n","Epoch 99: 100%|██████████| 2/2 [00:00\u003c00:00,  6.65it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.29, valid_loss=-1.23]\n","Epoch 100:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.49, valid_loss=-1.23]\n","Epoch 101:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.29, valid_loss=-1.23]\n","Epoch 102:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.34, valid_loss=-1.23]\n","Epoch 103:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.31, valid_loss=-1.23]\n","Epoch 104:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.31, valid_loss=-1.23]\n","Epoch 105:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.45, valid_loss=-1.23]\n","Epoch 106:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.45, valid_loss=-1.23]\n","Epoch 107:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.30, valid_loss=-1.23]\n","Epoch 108:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.36, valid_loss=-1.23]\n","Epoch 109:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.36, train_loss_epoch=-1.38, valid_loss=-1.23]\n","Epoch 110:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.26, valid_loss=-1.23]\n","Epoch 111:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.38, valid_loss=-1.23]\n","Epoch 112:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.35, valid_loss=-1.23]\n","Epoch 113:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.36, train_loss_epoch=-1.40, valid_loss=-1.23]\n","Epoch 114:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.38, valid_loss=-1.23]\n","Epoch 115:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.47, valid_loss=-1.23]\n","Epoch 116:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.35, valid_loss=-1.23]\n","Epoch 117:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.50, valid_loss=-1.23]\n","Epoch 118:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.41, valid_loss=-1.23]\n","Epoch 119:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.42, valid_loss=-1.23]\n","Epoch 120:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.47, valid_loss=-1.23]\n","Epoch 121:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.31, valid_loss=-1.23]\n","Epoch 122:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.39, valid_loss=-1.23]\n","Epoch 123:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.43, valid_loss=-1.23]\n","Epoch 124:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.47, valid_loss=-1.23]\n","Epoch 125:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.46, valid_loss=-1.23]\n","Epoch 126:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.26, valid_loss=-1.23]\n","Epoch 127:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.44, valid_loss=-1.23]\n","Epoch 128:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.50, valid_loss=-1.23]\n","Epoch 129:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.46, valid_loss=-1.23]\n","Epoch 130:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.51, valid_loss=-1.23]\n","Epoch 131:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.47, valid_loss=-1.23]\n","Epoch 132:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.31, valid_loss=-1.23]\n","Epoch 133:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.51, valid_loss=-1.23]\n","Epoch 134:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.38, valid_loss=-1.23]\n","Epoch 135:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.52, valid_loss=-1.23]\n","Epoch 136:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.40, valid_loss=-1.23]\n","Epoch 137:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.42, valid_loss=-1.23]\n","Epoch 138:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.61, valid_loss=-1.23]\n","Epoch 139:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.45, valid_loss=-1.23]\n","Epoch 140:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.54, valid_loss=-1.23]\n","Epoch 141:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.52, valid_loss=-1.23]\n","Epoch 142:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.53, valid_loss=-1.23]\n","Epoch 143:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.44, valid_loss=-1.23]\n","Epoch 144:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.50, valid_loss=-1.23]\n","Epoch 145:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.54, valid_loss=-1.23]\n","Epoch 146:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.49, valid_loss=-1.23]\n","Epoch 147:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.47, valid_loss=-1.23]\n","Epoch 148:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.65, valid_loss=-1.23]\n","Epoch 149:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.48, valid_loss=-1.23]\n","Epoch 149: 100%|██████████| 2/2 [00:00\u003c00:00, 18.10it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.48, valid_loss=-1.23]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7513)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7513)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 14.86it/s]\u001b[A\n","Epoch 150:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.52, valid_loss=-1.42]\n","Epoch 151:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.47, valid_loss=-1.42]\n","Epoch 152:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.56, valid_loss=-1.42]\n","Epoch 153:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.45, valid_loss=-1.42]\n","Epoch 154:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.47, valid_loss=-1.42]\n","Epoch 155:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.38, valid_loss=-1.42]\n","Epoch 156:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.67, valid_loss=-1.42]\n","Epoch 157:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.52, valid_loss=-1.42]\n","Epoch 158:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.59, valid_loss=-1.42]\n","Epoch 158: 100%|██████████| 2/2 [00:00\u003c00:00, 17.06it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.49, valid_loss=-1.42]\n","Epoch 158:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.49, valid_loss=-1.42]        \n","Epoch 159:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.49, valid_loss=-1.42]\n","Epoch 160:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.52, valid_loss=-1.42]\n","Epoch 161:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.48, valid_loss=-1.42]\n","Epoch 162:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.51, valid_loss=-1.42]\n","Epoch 163:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.54, valid_loss=-1.42]\n","Epoch 164:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.62, valid_loss=-1.42]\n","Epoch 165:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.64, valid_loss=-1.42]\n","Epoch 166:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.67, valid_loss=-1.42]\n","Epoch 167:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.54, valid_loss=-1.42]\n","Epoch 168:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.53, valid_loss=-1.42]\n","Epoch 169:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.59, valid_loss=-1.42]\n","Epoch 170:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.65, valid_loss=-1.42]\n","Epoch 171:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.74, valid_loss=-1.42]\n","Epoch 172:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.65, valid_loss=-1.42]\n","Epoch 173:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.62, valid_loss=-1.42]\n","Epoch 174:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.59, valid_loss=-1.42]\n","Epoch 175:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.49, valid_loss=-1.42]\n","Epoch 176:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.55, valid_loss=-1.42]\n","Epoch 177:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.68, valid_loss=-1.42]\n","Epoch 178:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.63, valid_loss=-1.42]\n","Epoch 179:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.59, valid_loss=-1.42]\n","Epoch 180:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.60, valid_loss=-1.42]\n","Epoch 181:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.63, valid_loss=-1.42]\n","Epoch 182:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.55, valid_loss=-1.42]\n","Epoch 183:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.65, valid_loss=-1.42]\n","Epoch 184:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.63, valid_loss=-1.42]\n","Epoch 185:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.51, valid_loss=-1.42]\n","Epoch 186:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.65, valid_loss=-1.42]\n","Epoch 187:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.73, valid_loss=-1.42]\n","Epoch 188:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.65, valid_loss=-1.42]\n","Epoch 189:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.53, valid_loss=-1.42]\n","Epoch 190:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.68, valid_loss=-1.42]\n","Epoch 191:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.47, valid_loss=-1.42]\n","Epoch 192:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.65, valid_loss=-1.42]\n","Epoch 193:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.60, valid_loss=-1.42]\n","Epoch 194:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.66, valid_loss=-1.42]\n","Epoch 195:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.67, valid_loss=-1.42]\n","Epoch 196:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.68, valid_loss=-1.42]\n","Epoch 197:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.60, valid_loss=-1.42]\n","Epoch 198:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.71, valid_loss=-1.42]\n","Epoch 199:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.56, valid_loss=-1.42]\n","Epoch 199: 100%|██████████| 2/2 [00:00\u003c00:00, 17.30it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.56, valid_loss=-1.42]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7513)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7513)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 15.39it/s]\u001b[A\n","Epoch 200:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.63, valid_loss=-1.51]\n","Epoch 201:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.55, valid_loss=-1.51]\n","Epoch 202:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.59, valid_loss=-1.51]\n","Epoch 203:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.55, valid_loss=-1.51]\n","Epoch 204:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.64, valid_loss=-1.51]\n","Epoch 205:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.69, valid_loss=-1.51]\n","Epoch 206:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.72, valid_loss=-1.51]\n","Epoch 207:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.59, valid_loss=-1.51]\n","Epoch 208:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.83, valid_loss=-1.51]\n","Epoch 209:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.69, valid_loss=-1.51]\n","Epoch 209: 100%|██████████| 2/2 [00:00\u003c00:00, 18.65it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.69, valid_loss=-1.51]\n","Epoch 210:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.62, valid_loss=-1.51]\n","Epoch 211:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.74, valid_loss=-1.51]\n","Epoch 212:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.64, valid_loss=-1.51]\n","Epoch 213:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.80, valid_loss=-1.51]\n","Epoch 214:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.64, valid_loss=-1.51]\n","Epoch 215:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.76, valid_loss=-1.51]\n","Epoch 216:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.65, valid_loss=-1.51]\n","Epoch 217:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.72, valid_loss=-1.51]\n","Epoch 218:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.74, valid_loss=-1.51]\n","Epoch 219:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.69, valid_loss=-1.51]\n","Epoch 220:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.73, valid_loss=-1.51]\n","Epoch 221:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.67, valid_loss=-1.51]\n","Epoch 222:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.71, valid_loss=-1.51]\n","Epoch 223:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.73, valid_loss=-1.51]\n","Epoch 224:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.58, valid_loss=-1.51]\n","Epoch 225:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.81, valid_loss=-1.51]\n","Epoch 226:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.63, valid_loss=-1.51]\n","Epoch 227:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.83, valid_loss=-1.51]\n","Epoch 228:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.66, valid_loss=-1.51]\n","Epoch 229:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.62, valid_loss=-1.51]\n","Epoch 230:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.68, valid_loss=-1.51]\n","Epoch 231:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.79, valid_loss=-1.51]\n","Epoch 232:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.74, valid_loss=-1.51]\n","Epoch 233:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.72, valid_loss=-1.51]\n","Epoch 234:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.73, valid_loss=-1.51]\n","Epoch 235:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.69, valid_loss=-1.51]\n","Epoch 236:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.64, valid_loss=-1.51]\n","Epoch 237:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.70, valid_loss=-1.51]\n","Epoch 238:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.73, valid_loss=-1.51]\n","Epoch 239:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.59, valid_loss=-1.51]\n","Epoch 240:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.75, valid_loss=-1.51]\n","Epoch 241:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.59, valid_loss=-1.51]\n","Epoch 242:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.77, valid_loss=-1.51]\n","Epoch 243:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.73, valid_loss=-1.51]\n","Epoch 244:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.72, valid_loss=-1.51]\n","Epoch 245:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.69, valid_loss=-1.51]\n","Epoch 246:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.71, valid_loss=-1.51]\n","Epoch 247:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.73, valid_loss=-1.51]\n","Epoch 248:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.55, valid_loss=-1.51]\n","Epoch 249:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.68, valid_loss=-1.51]\n","Epoch 249: 100%|██████████| 2/2 [00:00\u003c00:00, 18.83it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.68, valid_loss=-1.51]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7513)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 15.53it/s]\u001b[A\n","Epoch 250:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.82, valid_loss=-1.57]\n","Epoch 251:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.78, valid_loss=-1.57]\n","Epoch 252:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.65, valid_loss=-1.57]\n","Epoch 253:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.63, valid_loss=-1.57]\n","Epoch 254:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.73, valid_loss=-1.57]\n","Epoch 255:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.84, valid_loss=-1.57]\n","Epoch 256:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.76, valid_loss=-1.57]\n","Epoch 257:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.65, valid_loss=-1.57]\n","Epoch 258:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.70, valid_loss=-1.57]\n","Epoch 259:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.53, valid_loss=-1.57]\n","Epoch 260:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.78, valid_loss=-1.57]\n","Epoch 261:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.89, valid_loss=-1.57]\n","Epoch 262:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.76, valid_loss=-1.57]\n","Epoch 263:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.84, valid_loss=-1.57]\n","Epoch 264:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.72, valid_loss=-1.57]\n","Epoch 265:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.70, valid_loss=-1.57]\n","Epoch 266:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.64, valid_loss=-1.57]\n","Epoch 267:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.72, valid_loss=-1.57]\n","Epoch 268:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.72, valid_loss=-1.57]\n","Epoch 269:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.68, valid_loss=-1.57]\n","Epoch 270:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.73, valid_loss=-1.57]\n","Epoch 271:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.80, valid_loss=-1.57]\n","Epoch 272:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.61, valid_loss=-1.57]\n","Epoch 273:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.71, valid_loss=-1.57]\n","Epoch 274:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.77, valid_loss=-1.57]\n","Epoch 275:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.63, valid_loss=-1.57]\n","Epoch 276:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.67, valid_loss=-1.57]\n","Epoch 277:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.66, valid_loss=-1.57]\n","Epoch 278:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.74, valid_loss=-1.57]\n","Epoch 279:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.77, valid_loss=-1.57]\n","Epoch 280:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.76, valid_loss=-1.57]\n","Epoch 281:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.75, valid_loss=-1.57]\n","Epoch 282:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.79, valid_loss=-1.57]\n","Epoch 283:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.68, valid_loss=-1.57]\n","Epoch 284:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.70, valid_loss=-1.57]\n","Epoch 285:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.68, valid_loss=-1.57]\n","Epoch 286:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.69, valid_loss=-1.57]\n","Epoch 287:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.74, valid_loss=-1.57]\n","Epoch 288:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.70, valid_loss=-1.57]\n","Epoch 289:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.59, valid_loss=-1.57]\n","Epoch 290:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.75, valid_loss=-1.57]\n","Epoch 291:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.84, valid_loss=-1.57]\n","Epoch 292:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.58, valid_loss=-1.57]\n","Epoch 292: 100%|██████████| 2/2 [00:00\u003c00:00, 17.21it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.58, valid_loss=-1.57]\n","Epoch 293:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.77, valid_loss=-1.57]\n","Epoch 294:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.69, valid_loss=-1.57]\n","Epoch 295:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.72, valid_loss=-1.57]\n","Epoch 296:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.73, valid_loss=-1.57]\n","Epoch 297:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.76, valid_loss=-1.57]\n","Epoch 298:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.75, valid_loss=-1.57]\n","Epoch 299:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.79, valid_loss=-1.57]\n","Epoch 299: 100%|██████████| 2/2 [00:00\u003c00:00, 18.56it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.79, valid_loss=-1.57]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7513)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7513)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 15.51it/s]\u001b[A\n","Epoch 300:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.73, valid_loss=-1.61]\n","Epoch 301:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.73, valid_loss=-1.61]\n","Epoch 302:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.79, valid_loss=-1.61]\n","Epoch 302: 100%|██████████| 2/2 [00:00\u003c00:00, 18.78it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.79, valid_loss=-1.61]\n","Epoch 302: 100%|██████████| 2/2 [00:00\u003c00:00, 18.65it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.73, valid_loss=-1.61]\n","Epoch 303:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.73, valid_loss=-1.61]\n","Epoch 304:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.67, valid_loss=-1.61]\n","Epoch 305:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.78, valid_loss=-1.61]\n","Epoch 306:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.77, valid_loss=-1.61]\n","Epoch 307:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.69, valid_loss=-1.61]\n","Epoch 308:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.87, valid_loss=-1.61]\n","Epoch 309:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.72, valid_loss=-1.61]\n","Epoch 310:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.85, valid_loss=-1.61]\n","Epoch 311:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.92, valid_loss=-1.61]\n","Epoch 312:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.68, valid_loss=-1.61]\n","Epoch 313:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.81, valid_loss=-1.61]\n","Epoch 314:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.70, valid_loss=-1.61]\n","Epoch 315:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.74, valid_loss=-1.61]\n","Epoch 316:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.76, valid_loss=-1.61]\n","Epoch 317:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.79, valid_loss=-1.61]\n","Epoch 318:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.77, valid_loss=-1.61]\n","Epoch 319:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.69, valid_loss=-1.61]\n","Epoch 320:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.80, valid_loss=-1.61]\n","Epoch 321:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.76, valid_loss=-1.61]\n","Epoch 322:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.75, valid_loss=-1.61]\n","Epoch 323:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.69, valid_loss=-1.61]\n","Epoch 324:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.82, valid_loss=-1.61]\n","Epoch 325:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.92, valid_loss=-1.61]\n","Epoch 326:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.77, valid_loss=-1.61]\n","Epoch 327:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.83, valid_loss=-1.61]\n","Epoch 328:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.77, valid_loss=-1.61]\n","Epoch 329:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.67, valid_loss=-1.61]\n","Epoch 330:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.63, valid_loss=-1.61]\n","Epoch 331:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.76, valid_loss=-1.61]\n","Epoch 332:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.83, valid_loss=-1.61]\n","Epoch 333:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.85, valid_loss=-1.61]\n","Epoch 334:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.68, valid_loss=-1.61]\n","Epoch 335:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.72, valid_loss=-1.61]\n","Epoch 336:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.75, valid_loss=-1.61]\n","Epoch 337:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.72, valid_loss=-1.61]\n","Epoch 338:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.76, valid_loss=-1.61]\n","Epoch 339:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.84, valid_loss=-1.61]\n","Epoch 340:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.72, valid_loss=-1.61]\n","Epoch 341:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.75, valid_loss=-1.61]\n","Epoch 342:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.85, valid_loss=-1.61]\n","Epoch 343:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.74, valid_loss=-1.61]\n","Epoch 344:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.81, valid_loss=-1.61]\n","Epoch 345:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.83, valid_loss=-1.61]\n","Epoch 346:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.78, valid_loss=-1.61]\n","Epoch 347:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.75, valid_loss=-1.61]\n","Epoch 348:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.77, valid_loss=-1.61]\n","Epoch 349:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.79, valid_loss=-1.61]\n","Epoch 349: 100%|██████████| 2/2 [00:00\u003c00:00, 18.46it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.79, valid_loss=-1.61]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7513)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 15.40it/s]\u001b[A\n","Epoch 350:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.74, valid_loss=-1.63]\n","Epoch 351:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.77, valid_loss=-1.63]\n","Epoch 352:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.69, valid_loss=-1.63]\n","Epoch 352: 100%|██████████| 2/2 [00:00\u003c00:00, 19.19it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.69, valid_loss=-1.63]\n","Epoch 353:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.83, valid_loss=-1.63]\n","Epoch 354:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.81, valid_loss=-1.63]\n","Epoch 355:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.83, valid_loss=-1.63]\n","Epoch 356:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.78, valid_loss=-1.63]\n","Epoch 357:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.76, valid_loss=-1.63]\n","Epoch 358:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.75, valid_loss=-1.63]\n","Epoch 359:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.79, valid_loss=-1.63]\n","Epoch 360:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.76, valid_loss=-1.63]\n","Epoch 361:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.75, valid_loss=-1.63]\n","Epoch 362:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.68, valid_loss=-1.63]\n","Epoch 363:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.75, valid_loss=-1.63]\n","Epoch 364:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.77, valid_loss=-1.63]\n","Epoch 365:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.82, valid_loss=-1.63]\n","Epoch 366:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.76, valid_loss=-1.63]\n","Epoch 367:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.87, valid_loss=-1.63]\n","Epoch 368:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.68, valid_loss=-1.63]\n","Epoch 369:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.71, valid_loss=-1.63]\n","Epoch 370:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.77, valid_loss=-1.63]\n","Epoch 371:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.69, valid_loss=-1.63]\n","Epoch 372:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.70, valid_loss=-1.63]\n","Epoch 373:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.75, valid_loss=-1.63]\n","Epoch 374:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.86, valid_loss=-1.63]\n","Epoch 375:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.79, valid_loss=-1.63]\n","Epoch 376:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.70, valid_loss=-1.63]\n","Epoch 377:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.77, valid_loss=-1.63]\n","Epoch 378:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.85, valid_loss=-1.63]\n","Epoch 379:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.82, valid_loss=-1.63]\n","Epoch 380:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.77, valid_loss=-1.63]\n","Epoch 381:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.79, valid_loss=-1.63]\n","Epoch 382:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.67, valid_loss=-1.63]\n","Epoch 383:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.79, valid_loss=-1.63]\n","Epoch 384:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.86, valid_loss=-1.63]\n","Epoch 385:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.78, valid_loss=-1.63]\n","Epoch 386:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.76, valid_loss=-1.63]\n","Epoch 387:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.74, valid_loss=-1.63]\n","Epoch 388:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.81, valid_loss=-1.63]\n","Epoch 389:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.82, valid_loss=-1.63]\n","Epoch 390:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.77, valid_loss=-1.63]\n","Epoch 391:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.81, valid_loss=-1.63]\n","Epoch 392:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.75, valid_loss=-1.63]\n","Epoch 393:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.89, valid_loss=-1.63]\n","Epoch 394:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.79, valid_loss=-1.63]\n","Epoch 395:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.84, valid_loss=-1.63]\n","Epoch 396:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.77, valid_loss=-1.63]\n","Epoch 397:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.74, valid_loss=-1.63]\n","Epoch 398:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.80, valid_loss=-1.63]\n","Epoch 399:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.78, valid_loss=-1.63]\n","Epoch 399: 100%|██████████| 2/2 [00:00\u003c00:00, 18.44it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.78, valid_loss=-1.63]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7513)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 15.56it/s]\u001b[A\n","Epoch 400:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.54, valid_loss=-1.63]\n","Epoch 401:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.76, valid_loss=-1.63]\n","Epoch 402:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.76, valid_loss=-1.63]\n","Epoch 403:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.61, valid_loss=-1.63]\n","Epoch 404:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.80, valid_loss=-1.63]\n","Epoch 405:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.91, valid_loss=-1.63]\n","Epoch 406:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.78, valid_loss=-1.63]\n","Epoch 407:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.75, valid_loss=-1.63]\n","Epoch 408:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.72, valid_loss=-1.63]\n","Epoch 409:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.68, valid_loss=-1.63]\n","Epoch 410:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.73, valid_loss=-1.63]\n","Epoch 410: 100%|██████████| 2/2 [00:00\u003c00:00, 16.69it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.73, valid_loss=-1.63]\n","Epoch 411:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.68, valid_loss=-1.63]\n","Epoch 412:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.79, valid_loss=-1.63]\n","Epoch 413:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.67, valid_loss=-1.63]\n","Epoch 414:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.78, valid_loss=-1.63]\n","Epoch 415:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.77, valid_loss=-1.63]\n","Epoch 416:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.85, valid_loss=-1.63]\n","Epoch 417:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.79, valid_loss=-1.63]\n","Epoch 418:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.81, valid_loss=-1.63]\n","Epoch 419:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.79, valid_loss=-1.63]\n","Epoch 420:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.82, valid_loss=-1.63]\n","Epoch 421:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.80, valid_loss=-1.63]\n","Epoch 422:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.83, valid_loss=-1.63]\n","Epoch 423:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.73, valid_loss=-1.63]\n","Epoch 424:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.89, valid_loss=-1.63]\n","Epoch 425:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.79, valid_loss=-1.63]\n","Epoch 426:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.89, valid_loss=-1.63]\n","Epoch 427:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.78, valid_loss=-1.63]\n","Epoch 428:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.82, valid_loss=-1.63]\n","Epoch 429:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.87, valid_loss=-1.63]\n","Epoch 430:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.73, valid_loss=-1.63]\n","Epoch 431:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.85, valid_loss=-1.63]\n","Epoch 432:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.93, valid_loss=-1.63]\n","Epoch 432: 100%|██████████| 2/2 [00:00\u003c00:00, 18.99it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.74, valid_loss=-1.63]\n","Epoch 433:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.74, valid_loss=-1.63]\n","Epoch 434:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.86, valid_loss=-1.63]\n","Epoch 435:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.80, valid_loss=-1.63]\n","Epoch 436:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.76, valid_loss=-1.63]\n","Epoch 437:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.86, valid_loss=-1.63]\n","Epoch 438:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.77, valid_loss=-1.63]\n","Epoch 439:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.73, valid_loss=-1.63]\n","Epoch 440:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.76, valid_loss=-1.63]\n","Epoch 441:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.81, valid_loss=-1.63]\n","Epoch 442:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.83, valid_loss=-1.63]\n","Epoch 443:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.74, valid_loss=-1.63]\n","Epoch 444:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.95, valid_loss=-1.63]\n","Epoch 445:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.84, valid_loss=-1.63]\n","Epoch 446:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.83, valid_loss=-1.63]\n","Epoch 447:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.90, valid_loss=-1.63]\n","Epoch 448:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.72, valid_loss=-1.63]\n","Epoch 449:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.84, valid_loss=-1.63]\n","Epoch 449: 100%|██████████| 2/2 [00:00\u003c00:00, 18.67it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.84, valid_loss=-1.63]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7513)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7513)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 15.54it/s]\u001b[A\n","Epoch 450:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.84, valid_loss=-1.63]\n","Epoch 451:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.85, valid_loss=-1.63]\n","Epoch 452:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.82, valid_loss=-1.63]\n","Epoch 453:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.79, valid_loss=-1.63]\n","Epoch 454:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.81, valid_loss=-1.63]\n","Epoch 455:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.86, valid_loss=-1.63]\n","Epoch 456:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.82, valid_loss=-1.63]\n","Epoch 457:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.88, valid_loss=-1.63]\n","Epoch 458:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.71, valid_loss=-1.63]\n","Epoch 459:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.84, valid_loss=-1.63]\n","Epoch 460:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.76, valid_loss=-1.63]\n","Epoch 461:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.81, valid_loss=-1.63]\n","Epoch 462:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.84, valid_loss=-1.63]\n","Epoch 463:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.82, valid_loss=-1.63]\n","Epoch 464:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.83, valid_loss=-1.63]\n","Epoch 465:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.80, valid_loss=-1.63]\n","Epoch 466:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.73, valid_loss=-1.63]\n","Epoch 467:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.91, valid_loss=-1.63]\n","Epoch 468:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.69, valid_loss=-1.63]\n","Epoch 469:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.77, valid_loss=-1.63]\n","Epoch 470:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.91, valid_loss=-1.63]\n","Epoch 471:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.78, valid_loss=-1.63]\n","Epoch 472:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.76, valid_loss=-1.63]\n","Epoch 473:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.82, valid_loss=-1.63]\n","Epoch 474:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.82, valid_loss=-1.63]\n","Epoch 475:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.84, valid_loss=-1.63]\n","Epoch 476:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.83, valid_loss=-1.63]\n","Epoch 477:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.71, valid_loss=-1.63]\n","Epoch 478:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.82, valid_loss=-1.63]\n","Epoch 479:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.85, valid_loss=-1.63]\n","Epoch 480:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.83, valid_loss=-1.63]\n","Epoch 481:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.86, valid_loss=-1.63]\n","Epoch 482:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.85, valid_loss=-1.63]\n","Epoch 483:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.81, valid_loss=-1.63]\n","Epoch 484:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.77, valid_loss=-1.63]\n","Epoch 485:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.92, valid_loss=-1.63]\n","Epoch 486:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.83, valid_loss=-1.63]\n","Epoch 487:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.82, valid_loss=-1.63]\n","Epoch 488:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.85, valid_loss=-1.63]\n","Epoch 489:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.77, valid_loss=-1.63]\n","Epoch 490:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.83, valid_loss=-1.63]\n","Epoch 491:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.91, valid_loss=-1.63]\n","Epoch 492:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.89, valid_loss=-1.63]\n","Epoch 493:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.86, valid_loss=-1.63]\n","Epoch 494:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.82, valid_loss=-1.63]\n","Epoch 495:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.79, valid_loss=-1.63]\n","Epoch 496:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.88, valid_loss=-1.63]\n","Epoch 496: 100%|██████████| 2/2 [00:00\u003c00:00, 18.95it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.88, valid_loss=-1.63]\n","Epoch 497:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.76, valid_loss=-1.63]\n","Epoch 498:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.74, valid_loss=-1.63]\n","Epoch 499:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.75, valid_loss=-1.63]\n","Epoch 499: 100%|██████████| 2/2 [00:00\u003c00:00, 18.69it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.75, valid_loss=-1.63]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7513)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 15.57it/s]\u001b[A\n","Epoch 499: 100%|██████████| 2/2 [00:00\u003c00:00,  6.67it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.75, valid_loss=-1.67]\n","Epoch 500:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.78, valid_loss=-1.67]\n","Epoch 501:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.84, valid_loss=-1.67]\n","Epoch 502:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.95, valid_loss=-1.67]\n","Epoch 503:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.75, valid_loss=-1.67]\n","Epoch 504:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.71, valid_loss=-1.67]\n","Epoch 505:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.88, valid_loss=-1.67]\n","Epoch 506:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.86, valid_loss=-1.67]\n","Epoch 507:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.75, valid_loss=-1.67]\n","Epoch 508:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.67, valid_loss=-1.67]\n","Epoch 509:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.86, valid_loss=-1.67]\n","Epoch 510:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.72, valid_loss=-1.67]\n","Epoch 511:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.75, valid_loss=-1.67]\n","Epoch 512:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.84, valid_loss=-1.67]\n","Epoch 513:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.82, valid_loss=-1.67]\n","Epoch 514:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.78, valid_loss=-1.67]\n","Epoch 515:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.69, valid_loss=-1.67]\n","Epoch 516:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.83, valid_loss=-1.67]\n","Epoch 517:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.81, valid_loss=-1.67]\n","Epoch 518:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.91, valid_loss=-1.67]\n","Epoch 519:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.90, valid_loss=-1.67]\n","Epoch 520:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.87, valid_loss=-1.67]\n","Epoch 521:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.91, valid_loss=-1.67]\n","Epoch 522:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.97, valid_loss=-1.67]\n","Epoch 523:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.73, valid_loss=-1.67]\n","Epoch 524:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.81, valid_loss=-1.67]\n","Epoch 525:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.76, valid_loss=-1.67]\n","Epoch 526:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.86, valid_loss=-1.67]\n","Epoch 527:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.80, valid_loss=-1.67]\n","Epoch 528:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.73, valid_loss=-1.67]\n","Epoch 529:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.95, valid_loss=-1.67]\n","Epoch 530:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.75, valid_loss=-1.67]\n","Epoch 531:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.84, valid_loss=-1.67]\n","Epoch 532:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.80, valid_loss=-1.67]\n","Epoch 533:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.82, valid_loss=-1.67]\n","Epoch 534:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.84, valid_loss=-1.67]\n","Epoch 535:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.83, valid_loss=-1.67]\n","Epoch 536:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.86, valid_loss=-1.67]\n","Epoch 536: 100%|██████████| 2/2 [00:00\u003c00:00, 18.47it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.86, valid_loss=-1.67]\n","Epoch 537:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.73, valid_loss=-1.67]\n","Epoch 538:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.86, valid_loss=-1.67]\n","Epoch 539:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.97, valid_loss=-1.67]\n","Epoch 540:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.83, valid_loss=-1.67]\n","Epoch 541:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.81, valid_loss=-1.67]\n","Epoch 542:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.87, valid_loss=-1.67]\n","Epoch 543:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.66, valid_loss=-1.67]\n","Epoch 544:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.84, valid_loss=-1.67]\n","Epoch 545:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.83, valid_loss=-1.67]\n","Epoch 546:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.83, valid_loss=-1.67]\n","Epoch 547:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.85, valid_loss=-1.67]\n","Epoch 548:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.80, valid_loss=-1.67]\n","Epoch 549:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.81, valid_loss=-1.67]\n","Epoch 549: 100%|██████████| 2/2 [00:00\u003c00:00, 18.90it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.81, valid_loss=-1.67]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7513)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7513)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 15.49it/s]\u001b[A\n","Epoch 550:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.00, valid_loss=-1.68]\n","Epoch 551:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.78, valid_loss=-1.68]\n","Epoch 552:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.68, valid_loss=-1.68]\n","Epoch 553:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.81, valid_loss=-1.68]\n","Epoch 554:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.79, valid_loss=-1.68]\n","Epoch 555:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.86, valid_loss=-1.68]\n","Epoch 556:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.75, valid_loss=-1.68]\n","Epoch 557:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.91, valid_loss=-1.68]\n","Epoch 558:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.83, valid_loss=-1.68]\n","Epoch 559:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.87, valid_loss=-1.68]\n","Epoch 560:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.83, valid_loss=-1.68]\n","Epoch 561:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.90, valid_loss=-1.68]\n","Epoch 562:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.86, valid_loss=-1.68]\n","Epoch 563:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.83, valid_loss=-1.68]\n","Epoch 564:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.77, valid_loss=-1.68]\n","Epoch 565:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.92, valid_loss=-1.68]\n","Epoch 566:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.76, valid_loss=-1.68]\n","Epoch 567:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.88, valid_loss=-1.68]\n","Epoch 568:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.79, valid_loss=-1.68]\n","Epoch 569:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.77, valid_loss=-1.68]\n","Epoch 570:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.83, valid_loss=-1.68]\n","Epoch 571:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.87, valid_loss=-1.68]\n","Epoch 572:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.72, valid_loss=-1.68]\n","Epoch 573:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.79, valid_loss=-1.68]\n","Epoch 574:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.79, valid_loss=-1.68]\n","Epoch 575:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.81, valid_loss=-1.68]\n","Epoch 576:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.88, valid_loss=-1.68]\n","Epoch 577:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.80, valid_loss=-1.68]\n","Epoch 578:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.92, valid_loss=-1.68]\n","Epoch 579:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.75, valid_loss=-1.68]\n","Epoch 580:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.72, valid_loss=-1.68]\n","Epoch 581:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.78, valid_loss=-1.68]\n","Epoch 582:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.83, valid_loss=-1.68]\n","Epoch 583:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.77, valid_loss=-1.68]\n","Epoch 584:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.86, valid_loss=-1.68]\n","Epoch 585:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.85, valid_loss=-1.68]\n","Epoch 586:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.80, valid_loss=-1.68]\n","Epoch 587:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.88, valid_loss=-1.68]\n","Epoch 588:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.84, valid_loss=-1.68]\n","Epoch 589:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.84, valid_loss=-1.68]\n","Epoch 590:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.83, valid_loss=-1.68]\n","Epoch 591:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.87, valid_loss=-1.68]\n","Epoch 592:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.83, valid_loss=-1.68]\n","Epoch 593:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.82, valid_loss=-1.68]\n","Epoch 594:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.91, valid_loss=-1.68]\n","Epoch 595:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.86, valid_loss=-1.68]\n","Epoch 596:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.84, valid_loss=-1.68]\n","Epoch 597:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.76, valid_loss=-1.68]\n","Epoch 598:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.89, valid_loss=-1.68]\n","Epoch 599:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.90, valid_loss=-1.68]\n","Epoch 599: 100%|██████████| 2/2 [00:00\u003c00:00, 18.21it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.90, valid_loss=-1.68]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7513)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7513)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 15.38it/s]\u001b[A\n","Epoch 600:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.93, valid_loss=-1.67]\n","Epoch 601:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.75, valid_loss=-1.67]\n","Epoch 602:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.81, valid_loss=-1.67]\n","Epoch 603:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.71, valid_loss=-1.67]\n","Epoch 604:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.89, valid_loss=-1.67]\n","Epoch 605:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.85, valid_loss=-1.67]\n","Epoch 606:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.82, valid_loss=-1.67]\n","Epoch 607:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.89, valid_loss=-1.67]\n","Epoch 608:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.70, valid_loss=-1.67]\n","Epoch 609:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.74, valid_loss=-1.67]\n","Epoch 610:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.80, valid_loss=-1.67]\n","Epoch 611:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.80, valid_loss=-1.67]\n","Epoch 612:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.80, valid_loss=-1.67]\n","Epoch 613:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.85, valid_loss=-1.67]\n","Epoch 614:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.91, valid_loss=-1.67]\n","Epoch 615:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.83, valid_loss=-1.67]\n","Epoch 616:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.77, valid_loss=-1.67]\n","Epoch 617:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.73, valid_loss=-1.67]\n","Epoch 618:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.89, valid_loss=-1.67]\n","Epoch 619:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.83, valid_loss=-1.67]\n","Epoch 620:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.82, valid_loss=-1.67]\n","Epoch 621:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.80, valid_loss=-1.67]\n","Epoch 622:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.93, valid_loss=-1.67]\n","Epoch 623:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.74, valid_loss=-1.67]\n","Epoch 624:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.86, valid_loss=-1.67]\n","Epoch 625:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.81, valid_loss=-1.67]\n","Epoch 626:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.86, valid_loss=-1.67]\n","Epoch 627:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.88, valid_loss=-1.67]\n","Epoch 627: 100%|██████████| 2/2 [00:00\u003c00:00, 18.47it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.82, valid_loss=-1.67]\n","Epoch 628:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.82, valid_loss=-1.67]\n","Epoch 629:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.72, valid_loss=-1.67]\n","Epoch 630:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.77, valid_loss=-1.67]\n","Epoch 631:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.79, valid_loss=-1.67]\n","Epoch 632:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.71, valid_loss=-1.67]\n","Epoch 633:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.81, valid_loss=-1.67]\n","Epoch 634:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.77, valid_loss=-1.67]\n","Epoch 635:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.71, valid_loss=-1.67]\n","Epoch 636:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.86, valid_loss=-1.67]\n","Epoch 637:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.84, valid_loss=-1.67]\n","Epoch 638:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.82, valid_loss=-1.67]\n","Epoch 639:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.82, valid_loss=-1.67]\n","Epoch 640:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.85, valid_loss=-1.67]\n","Epoch 641:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.78, valid_loss=-1.67]\n","Epoch 642:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.89, valid_loss=-1.67]\n","Epoch 643:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.94, valid_loss=-1.67]\n","Epoch 644:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.84, valid_loss=-1.67]\n","Epoch 645:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.83, valid_loss=-1.67]\n","Epoch 646:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.87, valid_loss=-1.67]\n","Epoch 647:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.90, valid_loss=-1.67]\n","Epoch 648:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.73, valid_loss=-1.67]\n","Epoch 649:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.84, valid_loss=-1.67]\n","Epoch 649: 100%|██████████| 2/2 [00:00\u003c00:00, 18.20it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.84, valid_loss=-1.67]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-18 13:09:51,493\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (168, 24, 1), 'n_pool_kernel_size': (16, 8, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=7513)\u001b[0m `Trainer.fit` stopped: `max_steps=1300.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=7513)\u001b[0m \n","\u001b[36m(_train_tune pid=7513)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 15.13it/s]\u001b[A\n","\u001b[36m(_train_tune pid=7513)\u001b[0m \r                                                                      \u001b[A\rEpoch 649: 100%|██████████| 2/2 [00:00\u003c00:00,  6.46it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.84, valid_loss=-1.69]\rEpoch 649: 100%|██████████| 2/2 [00:00\u003c00:00,  6.44it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.72, valid_loss=-1.69]\rEpoch 649: 100%|██████████| 2/2 [00:00\u003c00:00,  6.42it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.72, valid_loss=-1.69]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=7948)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=7948)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=7948)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=7948)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=7948)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=7948)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=7948)\u001b[0m 2024-11-18 13:09:59.664865: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=7948)\u001b[0m 2024-11-18 13:09:59.686412: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=7948)\u001b[0m 2024-11-18 13:09:59.711721: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=7948)\u001b[0m 2024-11-18 13:09:59.719445: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=7948)\u001b[0m 2024-11-18 13:09:59.737611: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=7948)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=7948)\u001b[0m 2024-11-18 13:10:00.883058: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=7948)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=7948)\u001b[0m \n","\u001b[36m(_train_tune pid=7948)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=7948)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=7948)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=7948)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=7948)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=7948)\u001b[0m 3 | blocks       | ModuleList       | 2.7 M  | train\n","\u001b[36m(_train_tune pid=7948)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=7948)\u001b[0m 2.7 M     Trainable params\n","\u001b[36m(_train_tune pid=7948)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=7948)\u001b[0m 2.7 M     Total params\n","\u001b[36m(_train_tune pid=7948)\u001b[0m 10.632    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=7948)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=7948)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=7948)\u001b[0m \rSanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","\u001b[36m(_train_tune pid=7948)\u001b[0m \rSanity Checking:   0%|          | 0/2 [00:00\u003c?, ?it/s]\rSanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-18 13:10:03,687\tERROR tune_controller.py:1331 -- Trial task failed for trial _train_tune_a76ac303\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n","    result = ray.get(future)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2753, in get\n","    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 904, in get_objects\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(OutOfMemoryError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=7948, ip=172.28.0.12, actor_id=0a675b4516a79a108550182601000000, repr=_train_tune)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n","    raise skipped from exception_cause(skipped)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n","    self._ret = self._target(*self._args, **self._kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in \u003clambda\u003e\n","    training_func=lambda: self._trainable_func(self.config),\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n","    output = fn()\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/util.py\", line 130, in inner\n","    return trainable(config, **fn_kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_auto.py\", line 214, in _train_tune\n","    _ = self._fit_model(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_auto.py\", line 362, in _fit_model\n","    model = model.fit(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_windows.py\", line 661, in fit\n","    return self._fit(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_model.py\", line 356, in _fit\n","    trainer.fit(model, datamodule=datamodule)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 538, in fit\n","    call._call_and_handle_interrupt(\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 47, in _call_and_handle_interrupt\n","    return trainer_fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 574, in _fit_impl\n","    self._run(model, ckpt_path=ckpt_path)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 981, in _run\n","    results = self._run_stage()\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1023, in _run_stage\n","    self._run_sanity_check()\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1052, in _run_sanity_check\n","    val_loop.run()\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py\", line 178, in _decorator\n","    return loop_run(self, *args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 135, in run\n","    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 396, in _evaluation_step\n","    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 319, in _call_strategy_hook\n","    output = fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py\", line 411, in validation_step\n","    return self.lightning_module.validation_step(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_windows.py\", line 534, in validation_step\n","    valid_loss_batch = self._compute_valid_loss(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_windows.py\", line 461, in _compute_valid_loss\n","    _, sample_mean, quants = self.loss.sample(distr_args=distr_args)\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/losses/pytorch.py\", line 2002, in sample\n","    samples = distr.sample(sample_shape=(num_samples,))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributions/distribution.py\", line 167, in sample\n","    return self.rsample(sample_shape)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributions/studentT.py\", line 90, in rsample\n","    Y = X * torch.rsqrt(Z / self.df)\n","torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.60 GiB. GPU 0 has a total capacity of 39.56 GiB of which 9.15 GiB is free. Process 52335 has 416.00 MiB memory in use. Process 99867 has 29.99 GiB memory in use. Of the allocated memory 29.28 GiB is allocated by PyTorch, and 229.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial _train_tune_a76ac303 errored after 0 iterations at 2024-11-18 13:10:03. Total running time: 6min 26s\n","Error file: /tmp/ray/session_2024-11-18_13-03-34_336407_4396/artifacts/2024-11-18_13-03-37/_train_tune_2024-11-18_13-03-34/driver_artifacts/_train_tune_a76ac303_8_batch_size=256,h=20,hist_exog_list=DY_PTBV_P_PO_VO_PE,input_size=40,learning_rate=0.0050,loss=ref_ph_de8959_2024-11-18_13-08-32/error.txt\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=8058)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=8058)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=8058)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=8058)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=8058)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=8058)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=8058)\u001b[0m 2024-11-18 13:10:11.811976: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=8058)\u001b[0m 2024-11-18 13:10:11.833078: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=8058)\u001b[0m 2024-11-18 13:10:11.858863: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=8058)\u001b[0m 2024-11-18 13:10:11.866664: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=8058)\u001b[0m 2024-11-18 13:10:11.886278: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=8058)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=8058)\u001b[0m 2024-11-18 13:10:13.049009: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=8058)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=8058)\u001b[0m \n","\u001b[36m(_train_tune pid=8058)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=8058)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=8058)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=8058)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=8058)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=8058)\u001b[0m 3 | blocks       | ModuleList       | 3.3 M  | train\n","\u001b[36m(_train_tune pid=8058)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=8058)\u001b[0m 3.3 M     Trainable params\n","\u001b[36m(_train_tune pid=8058)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=8058)\u001b[0m 3.3 M     Total params\n","\u001b[36m(_train_tune pid=8058)\u001b[0m 13.080    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=8058)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=8058)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/7 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.600, train_loss_epoch=5.900]\n","Epoch 2:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-0.261]\n","Epoch 3:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.811, train_loss_epoch=-1.24]\n","Epoch 4:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.43]\n","Epoch 4: 100%|██████████| 7/7 [00:00\u003c00:00, 40.06it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.43]\n","Epoch 5:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.51]\n","Epoch 6:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.48]\n","Epoch 7:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.63]\n","Epoch 8:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.66]\n","Epoch 9:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.73]\n","Epoch 10:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.63]\n","Epoch 10: 100%|██████████| 7/7 [00:00\u003c00:00, 41.58it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.63]\n","Epoch 10: 100%|██████████| 7/7 [00:00\u003c00:00, 41.35it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-1.77]\n","Epoch 11:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-1.77]\n","Epoch 12:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.69]\n","Epoch 13:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.72]\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.67]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8058)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 40.48it/s]\u001b[A\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.67, valid_loss=-1.76]\n","Epoch 15:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.72, valid_loss=-1.76]\n","Epoch 16:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-1.79, valid_loss=-1.76]\n","Epoch 17:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.71, valid_loss=-1.76]\n","Epoch 18:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.75, valid_loss=-1.76]\n","Epoch 19:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-1.83, valid_loss=-1.76]\n","Epoch 20:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.73, valid_loss=-1.76]\n","Epoch 21:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.76, valid_loss=-1.76]\n","Epoch 22:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.77, valid_loss=-1.76]\n","Epoch 23:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.75, valid_loss=-1.76]\n","Epoch 24:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.75, valid_loss=-1.76]\n","Epoch 25:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.80, valid_loss=-1.76]\n","Epoch 26:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-1.84, valid_loss=-1.76]\n","Epoch 27:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.80, valid_loss=-1.76]\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.80, valid_loss=-1.76]\n","\u001b[36m(_train_tune pid=8058)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8058)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 39.59it/s]\u001b[A\n","Epoch 29:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.07, train_loss_epoch=-1.71, valid_loss=-1.80]\n","Epoch 30:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-1.88, valid_loss=-1.80]\n","Epoch 31:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.75, valid_loss=-1.80]\n","Epoch 32:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.77, valid_loss=-1.80]\n","Epoch 33:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.80, valid_loss=-1.80]\n","Epoch 34:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.72, valid_loss=-1.80]\n","Epoch 35:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.85, valid_loss=-1.80]\n","Epoch 36:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.66, valid_loss=-1.80]\n","Epoch 37:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-1.71, valid_loss=-1.80]\n","Epoch 38:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.55, valid_loss=-1.80]\n","Epoch 39:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.69, valid_loss=-1.80]\n","Epoch 40:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.75, valid_loss=-1.80]\n","Epoch 41:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-1.84, valid_loss=-1.80]\n","Epoch 42:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.78, valid_loss=-1.80]\n","\u001b[36m(_train_tune pid=8058)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8058)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8058)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 38.85it/s]\u001b[A\n","Epoch 42:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.78, valid_loss=-1.71]\n","Epoch 42: 100%|██████████| 7/7 [00:00\u003c00:00, 20.29it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.73, valid_loss=-1.71]\n","Epoch 43:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.73, valid_loss=-1.71]\n","Epoch 44:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.78, valid_loss=-1.71]\n","Epoch 45:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.75, valid_loss=-1.71]\n","Epoch 46:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.76, valid_loss=-1.71]\n","Epoch 47:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.81, valid_loss=-1.71]\n","Epoch 48:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.79, valid_loss=-1.71]\n","Epoch 49:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.83, valid_loss=-1.71]\n","Epoch 50:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.75, valid_loss=-1.71]\n","Epoch 51:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.82, valid_loss=-1.71]\n","Epoch 52:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.82, valid_loss=-1.71]\n","Epoch 53:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.74, valid_loss=-1.71]\n","Epoch 54:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.84, valid_loss=-1.71]\n","Epoch 55:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-1.90, valid_loss=-1.71]\n","Epoch 56:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.84, valid_loss=-1.71]\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.72, valid_loss=-1.71]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8058)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 38.20it/s]\u001b[A\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.72, valid_loss=-1.79]\n","Epoch 58:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-1.86, valid_loss=-1.79]\n","Epoch 59:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.79, valid_loss=-1.79]\n","Epoch 60:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.83, valid_loss=-1.79]\n","Epoch 61:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.80, valid_loss=-1.79]\n","Epoch 62:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.87, valid_loss=-1.79]\n","Epoch 63:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.81, valid_loss=-1.79]\n","Epoch 64:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.82, valid_loss=-1.79]\n","Epoch 65:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.83, valid_loss=-1.79]\n","Epoch 66:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.87, valid_loss=-1.79]\n","Epoch 67:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.88, valid_loss=-1.79]\n","Epoch 68:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.88, valid_loss=-1.79]\n","Epoch 69:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.82, valid_loss=-1.79]\n","Epoch 70:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.81, valid_loss=-1.79]\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.03, train_loss_epoch=-1.76, valid_loss=-1.79]\n","\u001b[36m(_train_tune pid=8058)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8058)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 38.20it/s]\u001b[A\n","Epoch 72:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.89, valid_loss=-1.83]\n","Epoch 73:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.91, valid_loss=-1.83]\n","Epoch 74:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-1.90, valid_loss=-1.83]\n","Epoch 75:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.90, valid_loss=-1.83]\n","Epoch 76:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.86, valid_loss=-1.83]\n","Epoch 77:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.84, valid_loss=-1.83]\n","Epoch 78:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-1.89, valid_loss=-1.83]\n","Epoch 79:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.87, valid_loss=-1.83]\n","Epoch 80:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.86, valid_loss=-1.83]\n","Epoch 81:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.84, valid_loss=-1.83]\n","Epoch 82:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.85, valid_loss=-1.83]\n","Epoch 83:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.927, train_loss_epoch=-1.77, valid_loss=-1.83]\n","Epoch 84:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.88, valid_loss=-1.83]\n","Epoch 85:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.87, valid_loss=-1.83]\n","\u001b[36m(_train_tune pid=8058)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8058)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 38.84it/s]\u001b[A\n","Epoch 86:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.83, valid_loss=-1.82]\n","Epoch 87:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.46, train_loss_epoch=-1.93, valid_loss=-1.82]\n","Epoch 88:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.93, valid_loss=-1.82]\n","Epoch 89:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.90, valid_loss=-1.82]\n","Epoch 90:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.86, valid_loss=-1.82]\n","Epoch 91:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.93, valid_loss=-1.82]\n","Epoch 92:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.87, valid_loss=-1.82]\n","Epoch 93:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.89, valid_loss=-1.82]\n","Epoch 93: 100%|██████████| 7/7 [00:00\u003c00:00, 46.08it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.89, valid_loss=-1.82]\n","Epoch 94:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.90, valid_loss=-1.82]\n","Epoch 95:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.84, valid_loss=-1.82]\n","Epoch 96:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-1.92, valid_loss=-1.82]\n","Epoch 97:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-1.96, valid_loss=-1.82]\n","Epoch 98:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-1.94, valid_loss=-1.82]\n","Epoch 99:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.87, valid_loss=-1.82]\n","Epoch 99: 100%|██████████| 7/7 [00:00\u003c00:00, 46.93it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-1.87, valid_loss=-1.82]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8058)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 39.61it/s]\u001b[A\n","Epoch 100:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-1.93, valid_loss=-1.85]\n","Epoch 101:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.90, valid_loss=-1.85]\n","Epoch 102:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.89, valid_loss=-1.85]\n","Epoch 103:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.99, valid_loss=-1.85]\n","Epoch 104:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-1.96, valid_loss=-1.85]\n","Epoch 105:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.81, valid_loss=-1.85]\n","Epoch 106:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.83, valid_loss=-1.85]\n","Epoch 107:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.93, valid_loss=-1.85]\n","Epoch 108:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.95, valid_loss=-1.85]\n","Epoch 109:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.90, valid_loss=-1.85]\n","Epoch 110:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.80, valid_loss=-1.85]\n","Epoch 110: 100%|██████████| 7/7 [00:00\u003c00:00, 50.27it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.80, valid_loss=-1.85]\n","Epoch 111:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.91, valid_loss=-1.85]\n","Epoch 112:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.95, valid_loss=-1.85]\n","Epoch 113:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.90, valid_loss=-1.85]\n","Epoch 114:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-1.93, valid_loss=-1.85]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8058)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 39.76it/s]\u001b[A\n","Epoch 114:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.93, valid_loss=-1.83]\n","Epoch 115:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.86, valid_loss=-1.83]\n","Epoch 116:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.79, valid_loss=-1.83]\n","Epoch 117:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-1.95, valid_loss=-1.83]\n","Epoch 118:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.88, valid_loss=-1.83]\n","Epoch 119:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.88, valid_loss=-1.83]\n","Epoch 120:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.91, valid_loss=-1.83]\n","Epoch 121:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.89, valid_loss=-1.83]\n","Epoch 122:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.87, valid_loss=-1.83]\n","Epoch 123:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.88, valid_loss=-1.83]\n","Epoch 124:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.92, valid_loss=-1.83]\n","Epoch 124: 100%|██████████| 7/7 [00:00\u003c00:00, 51.21it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.92, valid_loss=-1.83]\n","Epoch 125:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.94, valid_loss=-1.83]\n","Epoch 126:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.93, valid_loss=-1.83]\n","Epoch 127:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.81, valid_loss=-1.83]\n","Epoch 128:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.90, valid_loss=-1.83]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=8058)\u001b[0m `Trainer.fit` stopped: `max_steps=900.0` reached.\n","2024-11-18 13:10:36,410\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (180, 60, 1), 'n_pool_kernel_size': (2, 2, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=8058)\u001b[0m \n","\u001b[36m(_train_tune pid=8058)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 38.63it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8058)\u001b[0m \r                                                                      \u001b[A\rEpoch 128:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.90, valid_loss=-1.84]\rEpoch 128:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.88, valid_loss=-1.84]\rEpoch 128:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.88, valid_loss=-1.84]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=8248)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=8248)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=8248)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=8248)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=8248)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=8248)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=8248)\u001b[0m 2024-11-18 13:10:43.787535: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=8248)\u001b[0m 2024-11-18 13:10:43.808361: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=8248)\u001b[0m 2024-11-18 13:10:43.834592: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=8248)\u001b[0m 2024-11-18 13:10:43.842594: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=8248)\u001b[0m 2024-11-18 13:10:43.861016: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=8248)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=8248)\u001b[0m 2024-11-18 13:10:45.012566: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=8248)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=8248)\u001b[0m \n","\u001b[36m(_train_tune pid=8248)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=8248)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=8248)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=8248)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=8248)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=8248)\u001b[0m 3 | blocks       | ModuleList       | 2.9 M  | train\n","\u001b[36m(_train_tune pid=8248)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=8248)\u001b[0m 2.9 M     Trainable params\n","\u001b[36m(_train_tune pid=8248)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=8248)\u001b[0m 2.9 M     Total params\n","\u001b[36m(_train_tune pid=8248)\u001b[0m 11.687    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=8248)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=8248)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/13 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.810, train_loss_epoch=12.60]\n","Epoch 2:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.240, train_loss_epoch=7.900]\n","Epoch 3:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.790, train_loss_epoch=7.930]\n","Epoch 4:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.110, train_loss_epoch=7.770]\n","Epoch 5:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.140, train_loss_epoch=7.470]\n","Epoch 6:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.440, train_loss_epoch=6.760]\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.270, train_loss_epoch=6.070]\n","\u001b[36m(_train_tune pid=8248)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8248)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 74.21it/s]\u001b[A\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.530, train_loss_epoch=6.070, valid_loss=5.560]\n","Epoch 8:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.450, train_loss_epoch=5.680, valid_loss=5.560]\n","Epoch 9:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.170, train_loss_epoch=5.370, valid_loss=5.560]\n","Epoch 10:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.730, train_loss_epoch=5.250, valid_loss=5.560]\n","Epoch 11:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.530, train_loss_epoch=5.160, valid_loss=5.560]\n","Epoch 12:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.380, train_loss_epoch=5.100, valid_loss=5.560]\n","Epoch 13:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.470, train_loss_epoch=5.760, valid_loss=5.560]\n","Epoch 14:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.480, train_loss_epoch=5.340, valid_loss=5.560]\n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.280, train_loss_epoch=5.110, valid_loss=5.560]\n","\u001b[36m(_train_tune pid=8248)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8248)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 72.92it/s]\u001b[A\n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.220, train_loss_epoch=5.110, valid_loss=4.960]\n","Epoch 16:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.740, train_loss_epoch=5.010, valid_loss=4.960]\n","Epoch 17:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.910, train_loss_epoch=4.690, valid_loss=4.960]\n","Epoch 18:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.800, train_loss_epoch=4.590, valid_loss=4.960]\n","Epoch 19:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.570, train_loss_epoch=4.880, valid_loss=4.960]\n","Epoch 20:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.960, train_loss_epoch=4.470, valid_loss=4.960]\n","Epoch 20: 100%|██████████| 13/13 [00:00\u003c00:00, 60.30it/s, v_num=0, train_loss_step=3.940, train_loss_epoch=4.470, valid_loss=4.960]\n","Epoch 21:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.940, train_loss_epoch=4.390, valid_loss=4.960]\n","Epoch 22:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.280, train_loss_epoch=4.450, valid_loss=4.960]\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.900, train_loss_epoch=4.530, valid_loss=4.960]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8248)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 70.88it/s]\u001b[A\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.400, train_loss_epoch=4.530, valid_loss=4.550]\n","Epoch 24:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.830, train_loss_epoch=4.410, valid_loss=4.550]\n","Epoch 25:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.530, train_loss_epoch=4.580, valid_loss=4.550]\n","Epoch 26:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.690, train_loss_epoch=4.500, valid_loss=4.550]\n","Epoch 27:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.290, train_loss_epoch=4.540, valid_loss=4.550]\n","Epoch 28:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.100, train_loss_epoch=5.340, valid_loss=4.550]\n","Epoch 29:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.170, train_loss_epoch=5.890, valid_loss=4.550]\n","Epoch 30:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.440, train_loss_epoch=5.160, valid_loss=4.550]\n","\u001b[36m(_train_tune pid=8248)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8248)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 72.86it/s]\u001b[A\n","Epoch 30:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.850, train_loss_epoch=5.160, valid_loss=4.530]\n","Epoch 31:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.880, train_loss_epoch=4.780, valid_loss=4.530]\n","Epoch 32:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.940, train_loss_epoch=4.400, valid_loss=4.530]\n","Epoch 33:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.350, train_loss_epoch=4.300, valid_loss=4.530]\n","Epoch 34:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.340, train_loss_epoch=4.130, valid_loss=4.530]\n","Epoch 35:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.940, train_loss_epoch=4.130, valid_loss=4.530]\n","Epoch 35: 100%|██████████| 13/13 [00:00\u003c00:00, 58.87it/s, v_num=0, train_loss_step=3.940, train_loss_epoch=4.130, valid_loss=4.530]\n","Epoch 36:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.280, train_loss_epoch=4.120, valid_loss=4.530]\n","Epoch 37:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.960, train_loss_epoch=4.070, valid_loss=4.530]\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.510, train_loss_epoch=4.110, valid_loss=4.530]\n","\u001b[36m(_train_tune pid=8248)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8248)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 73.56it/s]\u001b[A\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.500, train_loss_epoch=4.110, valid_loss=4.220]\n","Epoch 39:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.820, train_loss_epoch=4.120, valid_loss=4.220]\n","Epoch 40:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.020, train_loss_epoch=4.060, valid_loss=4.220]\n","Epoch 41:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.400, train_loss_epoch=4.030, valid_loss=4.220]\n","Epoch 42:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.940, train_loss_epoch=4.100, valid_loss=4.220]\n","Epoch 43:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.900, train_loss_epoch=4.050, valid_loss=4.220]\n","Epoch 44:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.560, train_loss_epoch=3.980, valid_loss=4.220]\n","Epoch 45:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.490, train_loss_epoch=4.090, valid_loss=4.220]\n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.860, train_loss_epoch=4.070, valid_loss=4.220]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8248)\u001b[0m \n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=8248)\u001b[0m `Trainer.fit` stopped: `max_steps=600.0` reached.\n","2024-11-18 13:10:58,591\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (168, 24, 1), 'n_pool_kernel_size': (2, 2, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=8248)\u001b[0m \n","\u001b[36m(_train_tune pid=8248)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 71.15it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8248)\u001b[0m \r                                                                        \u001b[A\rEpoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.110, train_loss_epoch=4.070, valid_loss=4.170]\rEpoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.110, train_loss_epoch=4.090, valid_loss=4.170]\rEpoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.110, train_loss_epoch=4.090, valid_loss=4.170]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=8405)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=8405)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=8405)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=8405)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=8405)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=8405)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=8405)\u001b[0m 2024-11-18 13:11:06.712635: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=8405)\u001b[0m 2024-11-18 13:11:06.733260: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=8405)\u001b[0m 2024-11-18 13:11:06.759708: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=8405)\u001b[0m 2024-11-18 13:11:06.767865: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=8405)\u001b[0m 2024-11-18 13:11:06.788165: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=8405)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=8405)\u001b[0m 2024-11-18 13:11:07.939676: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=8405)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=8405)\u001b[0m \n","\u001b[36m(_train_tune pid=8405)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=8405)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=8405)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=8405)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=8405)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=8405)\u001b[0m 3 | blocks       | ModuleList       | 2.6 M  | train\n","\u001b[36m(_train_tune pid=8405)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=8405)\u001b[0m 2.6 M     Trainable params\n","\u001b[36m(_train_tune pid=8405)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=8405)\u001b[0m 2.6 M     Total params\n","\u001b[36m(_train_tune pid=8405)\u001b[0m 10.384    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=8405)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=8405)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/4 [00:00\u003c?, ?it/s] \n","Epoch 0:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-0.807]        \n","Epoch 1:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-0.807]\n","Epoch 2:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.53]\n","Epoch 3:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.38]\n","Epoch 4:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.38]\n","Epoch 5:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.60]\n","Epoch 6:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.43]\n","Epoch 7:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.44]\n","Epoch 8:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.41]\n","Epoch 9:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.62]\n","Epoch 10:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.37]\n","Epoch 11:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.82]\n","Epoch 12:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.51]\n","Epoch 13:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.54]\n","Epoch 14:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.45]\n","Epoch 15:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.09, train_loss_epoch=-1.47]\n","Epoch 16:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.49]\n","Epoch 17:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.77]\n","Epoch 17: 100%|██████████| 4/4 [00:00\u003c00:00, 38.39it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.74]\n","Epoch 18:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.74]\n","Epoch 19:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.71]\n","Epoch 20:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.63]\n","Epoch 21:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.75]\n","Epoch 22:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.62]\n","Epoch 23:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.66]\n","Epoch 24:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.55]\n","Epoch 24: 100%|██████████| 4/4 [00:00\u003c00:00, 38.45it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-1.55]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8405)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 24.59it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8405)\u001b[0m \n","Epoch 25:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-1.78, valid_loss=-1.71]\n","Epoch 26:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.66, valid_loss=-1.71]\n","Epoch 27:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.70, valid_loss=-1.71]\n","Epoch 28:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.66, valid_loss=-1.71]\n","Epoch 29:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.668, train_loss_epoch=-1.45, valid_loss=-1.71]\n","Epoch 30:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.71, valid_loss=-1.71]\n","Epoch 31:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.65, valid_loss=-1.71]\n","Epoch 32:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.69, valid_loss=-1.71]\n","Epoch 33:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.65, valid_loss=-1.71]\n","Epoch 34:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.56, valid_loss=-1.71]\n","Epoch 35:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.67, valid_loss=-1.71]\n","Epoch 36:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.65, valid_loss=-1.71]\n","Epoch 37:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.63, valid_loss=-1.71]\n","Epoch 38:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.68, valid_loss=-1.71]\n","Epoch 39:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.68, valid_loss=-1.71]\n","Epoch 40:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.71, valid_loss=-1.71]\n","Epoch 41:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.65, valid_loss=-1.71]\n","Epoch 42:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.05, train_loss_epoch=-1.63, valid_loss=-1.71]\n","Epoch 43:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.74, valid_loss=-1.71]\n","Epoch 44:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.63, valid_loss=-1.71]\n","Epoch 45:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.69, valid_loss=-1.71]\n","Epoch 45: 100%|██████████| 4/4 [00:00\u003c00:00, 38.18it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.69, valid_loss=-1.71]\n","Epoch 45: 100%|██████████| 4/4 [00:00\u003c00:00, 37.80it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.61, valid_loss=-1.71]\n","Epoch 46:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.61, valid_loss=-1.71]\n","Epoch 47:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.68, valid_loss=-1.71]\n","Epoch 48:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.73, valid_loss=-1.71]\n","Epoch 49:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.63, valid_loss=-1.71]\n","Epoch 49: 100%|██████████| 4/4 [00:00\u003c00:00, 38.45it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.63, valid_loss=-1.71]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8405)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 25.12it/s]\u001b[A\n","Epoch 50:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.74, valid_loss=-1.72]\n","Epoch 51:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.65, valid_loss=-1.72]\n","Epoch 52:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.75, valid_loss=-1.72]\n","Epoch 52: 100%|██████████| 4/4 [00:00\u003c00:00, 37.50it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.75, valid_loss=-1.72]\n","Epoch 52:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.73, valid_loss=-1.72]        \n","Epoch 53:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.73, valid_loss=-1.72]\n","Epoch 54:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-1.75, valid_loss=-1.72]\n","Epoch 55:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.67, valid_loss=-1.72]\n","Epoch 56:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.70, valid_loss=-1.72]\n","Epoch 57:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.82, valid_loss=-1.72]\n","Epoch 58:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.74, valid_loss=-1.72]\n","Epoch 59:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-1.82, valid_loss=-1.72]\n","Epoch 60:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.73, valid_loss=-1.72]\n","Epoch 61:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.70, valid_loss=-1.72]\n","Epoch 62:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.62, valid_loss=-1.72]\n","Epoch 63:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.72, valid_loss=-1.72]\n","Epoch 64:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.64, valid_loss=-1.72]\n","Epoch 65:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.64, valid_loss=-1.72]\n","Epoch 66:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.63, valid_loss=-1.72]\n","Epoch 67:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.64, valid_loss=-1.72]\n","Epoch 68:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.73, valid_loss=-1.72]\n","Epoch 69:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.65, valid_loss=-1.72]\n","Epoch 70:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.71, valid_loss=-1.72]\n","Epoch 71:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.67, valid_loss=-1.72]\n","Epoch 72:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.768, train_loss_epoch=-1.45, valid_loss=-1.72]\n","Epoch 73:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.64, valid_loss=-1.72]\n","Epoch 74:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.73, valid_loss=-1.72]\n","Epoch 74: 100%|██████████| 4/4 [00:00\u003c00:00, 37.58it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.73, valid_loss=-1.72]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8405)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8405)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 26.41it/s]\u001b[A\n","Epoch 75:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.61, valid_loss=-1.73]\n","Epoch 76:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.75, valid_loss=-1.73]\n","Epoch 77:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.68, valid_loss=-1.73]\n","Epoch 78:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.63, valid_loss=-1.73]\n","Epoch 79:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.81, valid_loss=-1.73]\n","Epoch 80:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.74, valid_loss=-1.73]\n","Epoch 81:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.81, valid_loss=-1.73]\n","Epoch 82:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.65, valid_loss=-1.73]\n","Epoch 83:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.74, valid_loss=-1.73]\n","Epoch 84:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.603, train_loss_epoch=-1.39, valid_loss=-1.73]\n","Epoch 84: 100%|██████████| 4/4 [00:00\u003c00:00, 37.64it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.76, valid_loss=-1.73]\n","Epoch 85:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.76, valid_loss=-1.73]\n","Epoch 85: 100%|██████████| 4/4 [00:00\u003c00:00, 38.00it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.73, valid_loss=-1.73]\n","Epoch 86:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.73, valid_loss=-1.73]\n","Epoch 87:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.74, valid_loss=-1.73]\n","Epoch 88:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.85, valid_loss=-1.73]\n","Epoch 89:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.55, valid_loss=-1.73]\n","Epoch 90:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.63, valid_loss=-1.73]\n","Epoch 91:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.61, valid_loss=-1.73]\n","Epoch 92:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-1.82, valid_loss=-1.73]\n","Epoch 93:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.78, valid_loss=-1.73]\n","Epoch 94:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-1.84, valid_loss=-1.73]\n","Epoch 95:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.75, valid_loss=-1.73]\n","Epoch 96:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.62, valid_loss=-1.73]\n","Epoch 97:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.80, valid_loss=-1.73]\n","Epoch 98:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.82, valid_loss=-1.73]\n","Epoch 99:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.56, valid_loss=-1.73]\n","Epoch 99: 100%|██████████| 4/4 [00:00\u003c00:00, 36.21it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.56, valid_loss=-1.73]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8405)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 26.11it/s]\u001b[A\n","Epoch 100:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.59, valid_loss=-1.75]\n","Epoch 101:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.85, valid_loss=-1.75]\n","Epoch 102:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.60, valid_loss=-1.75]\n","Epoch 103:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.66, valid_loss=-1.75]\n","Epoch 104:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.67, valid_loss=-1.75]\n","Epoch 105:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.81, valid_loss=-1.75]\n","Epoch 106:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.72, valid_loss=-1.75]\n","Epoch 107:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.65, valid_loss=-1.75]\n","Epoch 108:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.75, valid_loss=-1.75]\n","Epoch 109:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.74, valid_loss=-1.75]\n","Epoch 110:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.71, valid_loss=-1.75]\n","Epoch 111:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.65, valid_loss=-1.75]\n","Epoch 112:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.71, valid_loss=-1.75]\n","Epoch 113:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-1.85, valid_loss=-1.75]\n","Epoch 114:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.78, valid_loss=-1.75]\n","Epoch 115:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.69, valid_loss=-1.75]\n","Epoch 116:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.64, valid_loss=-1.75]\n","Epoch 117:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.64, valid_loss=-1.75]\n","Epoch 118:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.01, train_loss_epoch=-1.51, valid_loss=-1.75]\n","Epoch 119:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-1.91, valid_loss=-1.75]\n","Epoch 120:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.59, valid_loss=-1.75]\n","Epoch 121:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.75, valid_loss=-1.75]\n","Epoch 122:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.84, valid_loss=-1.75]\n","Epoch 123:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.70, valid_loss=-1.75]\n","Epoch 124:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.70, valid_loss=-1.75]\n","Epoch 124: 100%|██████████| 4/4 [00:00\u003c00:00, 36.35it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.70, valid_loss=-1.75]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8405)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 23.79it/s]\u001b[A\n","Epoch 125:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.66, valid_loss=-1.75]\n","Epoch 126:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.82, valid_loss=-1.75]\n","Epoch 127:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.68, valid_loss=-1.75]\n","Epoch 128:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.84, valid_loss=-1.75]\n","Epoch 129:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.75, valid_loss=-1.75]\n","Epoch 130:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.66, valid_loss=-1.75]\n","Epoch 131:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.84, valid_loss=-1.75]\n","Epoch 132:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.76, valid_loss=-1.75]\n","Epoch 133:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.71, valid_loss=-1.75]\n","Epoch 134:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.72, valid_loss=-1.75]\n","Epoch 135:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.66, valid_loss=-1.75]\n","Epoch 136:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-1.94, valid_loss=-1.75]\n","Epoch 137:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.62, valid_loss=-1.75]\n","Epoch 138:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.81, valid_loss=-1.75]\n","Epoch 139:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.82, valid_loss=-1.75]\n","Epoch 140:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.66, valid_loss=-1.75]\n","Epoch 141:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.86, valid_loss=-1.75]\n","Epoch 142:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.58, valid_loss=-1.75]\n","Epoch 143:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.86, valid_loss=-1.75]\n","Epoch 144:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.83, valid_loss=-1.75]\n","Epoch 145:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.71, valid_loss=-1.75]\n","Epoch 146:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.83, valid_loss=-1.75]\n","Epoch 147:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.76, valid_loss=-1.75]\n","Epoch 148:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.82, valid_loss=-1.75]\n","Epoch 149:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.04, train_loss_epoch=-1.54, valid_loss=-1.75]\n","Epoch 149: 100%|██████████| 4/4 [00:00\u003c00:00, 38.10it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.54, valid_loss=-1.75]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8405)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 26.41it/s]\u001b[A\n","Epoch 150:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.70, valid_loss=-1.77]\n","Epoch 151:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.74, valid_loss=-1.77]\n","Epoch 152:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.72, valid_loss=-1.77]\n","Epoch 153:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.66, valid_loss=-1.77]\n","Epoch 154:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-1.86, valid_loss=-1.77]\n","Epoch 155:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.914, train_loss_epoch=-1.54, valid_loss=-1.77]\n","Epoch 156:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.66, valid_loss=-1.77]\n","Epoch 157:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.75, valid_loss=-1.77]\n","Epoch 158:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.70, valid_loss=-1.77]\n","Epoch 159:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.90, valid_loss=-1.77]\n","Epoch 159: 100%|██████████| 4/4 [00:00\u003c00:00, 37.01it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.69, valid_loss=-1.77]\n","Epoch 160:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.69, valid_loss=-1.77]\n","Epoch 160: 100%|██████████| 4/4 [00:00\u003c00:00, 38.48it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-1.69, valid_loss=-1.77]\n","Epoch 161:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-1.88, valid_loss=-1.77]\n","Epoch 162:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-1.90, valid_loss=-1.77]\n","Epoch 163:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.75, valid_loss=-1.77]\n","Epoch 164:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.90, valid_loss=-1.77]\n","Epoch 165:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.05, train_loss_epoch=-1.67, valid_loss=-1.77]\n","Epoch 166:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.75, valid_loss=-1.77]\n","Epoch 167:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.77, valid_loss=-1.77]\n","Epoch 168:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.74, valid_loss=-1.77]\n","Epoch 169:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-1.92, valid_loss=-1.77]\n","Epoch 170:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.73, valid_loss=-1.77]\n","Epoch 171:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.89, valid_loss=-1.77]\n","Epoch 172:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-1.88, valid_loss=-1.77]\n","Epoch 173:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.68, valid_loss=-1.77]\n","Epoch 174:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.78, valid_loss=-1.77]\n","Epoch 174: 100%|██████████| 4/4 [00:00\u003c00:00, 37.71it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.78, valid_loss=-1.77]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8405)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 24.50it/s]\u001b[A\n","Epoch 175:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.74, valid_loss=-1.75]\n","Epoch 176:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.73, valid_loss=-1.75]\n","Epoch 177:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.85, valid_loss=-1.75]\n","Epoch 178:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.72, valid_loss=-1.75]\n","Epoch 179:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.62, valid_loss=-1.75]\n","Epoch 180:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.77, valid_loss=-1.75]\n","Epoch 181:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.92, valid_loss=-1.75]\n","Epoch 182:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.84, valid_loss=-1.75]\n","Epoch 183:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.73, valid_loss=-1.75]\n","Epoch 184:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.65, valid_loss=-1.75]\n","Epoch 185:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.59, valid_loss=-1.75]\n","Epoch 186:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.95, valid_loss=-1.75]\n","Epoch 187:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.73, valid_loss=-1.75]\n","Epoch 188:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.79, valid_loss=-1.75]\n","Epoch 189:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.08, train_loss_epoch=-1.55, valid_loss=-1.75]\n","Epoch 190:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.70, valid_loss=-1.75]\n","Epoch 191:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.838, train_loss_epoch=-1.56, valid_loss=-1.75]\n","Epoch 192:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.80, valid_loss=-1.75]\n","Epoch 193:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.67, valid_loss=-1.75]\n","Epoch 194:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.83, valid_loss=-1.75]\n","Epoch 195:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.77, valid_loss=-1.75]\n","Epoch 196:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-1.89, valid_loss=-1.75]\n","Epoch 197:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.80, valid_loss=-1.75]\n","Epoch 197: 100%|██████████| 4/4 [00:00\u003c00:00, 37.16it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.83, valid_loss=-1.75]\n","Epoch 197:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.83, valid_loss=-1.75]        \n","Epoch 198:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.83, valid_loss=-1.75]\n","Epoch 199:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.69, valid_loss=-1.75]\n","Epoch 199: 100%|██████████| 4/4 [00:00\u003c00:00, 37.37it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.69, valid_loss=-1.75]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8405)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 24.61it/s]\u001b[A\n","Epoch 200:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.82, valid_loss=-1.76]\n","Epoch 201:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.72, valid_loss=-1.76]\n","Epoch 202:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.75, valid_loss=-1.76]\n","Epoch 202: 100%|██████████| 4/4 [00:00\u003c00:00, 37.79it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.75, valid_loss=-1.76]\n","Epoch 203:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.67, valid_loss=-1.76]\n","Epoch 204:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.66, valid_loss=-1.76]\n","Epoch 205:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-1.93, valid_loss=-1.76]\n","Epoch 206:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-1.85, valid_loss=-1.76]\n","Epoch 207:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-1.85, valid_loss=-1.76]\n","Epoch 208:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.74, valid_loss=-1.76]\n","Epoch 209:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.973, train_loss_epoch=-1.63, valid_loss=-1.76]\n","Epoch 210:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.81, valid_loss=-1.76]\n","Epoch 211:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.74, valid_loss=-1.76]\n","Epoch 212:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.66, valid_loss=-1.76]\n","Epoch 213:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.70, valid_loss=-1.76]\n","Epoch 214:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.83, valid_loss=-1.76]\n","Epoch 215:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-1.88, valid_loss=-1.76]\n","Epoch 216:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.70, valid_loss=-1.76]\n","Epoch 217:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-1.93, valid_loss=-1.76]\n","Epoch 218:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.70, valid_loss=-1.76]\n","Epoch 219:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.70, valid_loss=-1.76]\n","Epoch 220:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.04, train_loss_epoch=-1.58, valid_loss=-1.76]\n","Epoch 221:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.76, valid_loss=-1.76]\n","Epoch 222:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.74, valid_loss=-1.76]\n","Epoch 223:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-1.93, valid_loss=-1.76]\n","Epoch 224:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.78, valid_loss=-1.76]\n","Epoch 224: 100%|██████████| 4/4 [00:00\u003c00:00, 37.14it/s, v_num=0, train_loss_step=-0.864, train_loss_epoch=-1.78, valid_loss=-1.76]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8405)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 26.10it/s]\u001b[A\n","Epoch 225:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.864, train_loss_epoch=-1.59, valid_loss=-1.76]\n","Epoch 226:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.81, valid_loss=-1.76]\n","Epoch 227:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.78, valid_loss=-1.76]\n","Epoch 228:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.76, valid_loss=-1.76]\n","Epoch 229:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.61, valid_loss=-1.76]\n","Epoch 230:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.64, valid_loss=-1.76]\n","Epoch 231:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.05, train_loss_epoch=-1.61, valid_loss=-1.76]\n","Epoch 232:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.60, valid_loss=-1.76]\n","Epoch 233:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.75, valid_loss=-1.76]\n","Epoch 234:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.67, valid_loss=-1.76]\n","Epoch 235:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.85, valid_loss=-1.76]\n","Epoch 236:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.70, valid_loss=-1.76]\n","Epoch 237:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.785, train_loss_epoch=-1.55, valid_loss=-1.76]\n","Epoch 238:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.84, valid_loss=-1.76]\n","Epoch 239:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.60, valid_loss=-1.76]\n","Epoch 240:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.78, valid_loss=-1.76]\n","Epoch 241:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.84, valid_loss=-1.76]\n","Epoch 242:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.73, valid_loss=-1.76]\n","Epoch 243:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.73, valid_loss=-1.76]\n","Epoch 244:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.90, valid_loss=-1.76]\n","Epoch 245:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.61, valid_loss=-1.76]\n","Epoch 246:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.66, valid_loss=-1.76]\n","Epoch 247:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.69, valid_loss=-1.76]\n","Epoch 248:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.77, valid_loss=-1.76]\n","Epoch 249:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-1.89, valid_loss=-1.76]\n","Epoch 249: 100%|██████████| 4/4 [00:00\u003c00:00, 36.90it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.89, valid_loss=-1.76]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8405)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8405)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 26.41it/s]\u001b[A\n","Epoch 250:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.81, valid_loss=-1.75]\n","Epoch 251:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.66, valid_loss=-1.75]\n","Epoch 252:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.65, valid_loss=-1.75]\n","Epoch 253:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.81, valid_loss=-1.75]\n","Epoch 254:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.66, valid_loss=-1.75]\n","Epoch 255:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.77, valid_loss=-1.75]\n","Epoch 256:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.85, valid_loss=-1.75]\n","Epoch 257:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.86, valid_loss=-1.75]\n","Epoch 258:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-1.89, valid_loss=-1.75]\n","Epoch 259:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.947, train_loss_epoch=-1.60, valid_loss=-1.75]\n","Epoch 260:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.77, valid_loss=-1.75]\n","Epoch 261:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.73, valid_loss=-1.75]\n","Epoch 262:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.83, valid_loss=-1.75]\n","Epoch 263:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.891, train_loss_epoch=-1.65, valid_loss=-1.75]\n","Epoch 264:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.79, valid_loss=-1.75]\n","Epoch 265:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-1.90, valid_loss=-1.75]\n","Epoch 265: 100%|██████████| 4/4 [00:00\u003c00:00, 38.38it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-1.90, valid_loss=-1.75]\n","Epoch 266:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-1.90, valid_loss=-1.75]\n","Epoch 267:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.78, valid_loss=-1.75]\n","Epoch 268:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.65, valid_loss=-1.75]\n","Epoch 269:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-1.89, valid_loss=-1.75]\n","Epoch 270:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.73, valid_loss=-1.75]\n","Epoch 271:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.81, valid_loss=-1.75]\n","Epoch 272:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.76, valid_loss=-1.75]\n","Epoch 273:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.68, valid_loss=-1.75]\n","Epoch 274:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.66, valid_loss=-1.75]\n","Epoch 274: 100%|██████████| 4/4 [00:00\u003c00:00, 38.08it/s, v_num=0, train_loss_step=-1.02, train_loss_epoch=-1.66, valid_loss=-1.75]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8405)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 25.78it/s]\u001b[A\n","Epoch 275:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.02, train_loss_epoch=-1.51, valid_loss=-1.75]\n","Epoch 276:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.82, valid_loss=-1.75]\n","Epoch 277:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.80, valid_loss=-1.75]\n","Epoch 278:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.76, valid_loss=-1.75]\n","Epoch 279:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.74, valid_loss=-1.75]\n","Epoch 280:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.90, valid_loss=-1.75]\n","Epoch 281:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.78, valid_loss=-1.75]\n","Epoch 282:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.93, valid_loss=-1.75]\n","Epoch 283:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.79, valid_loss=-1.75]\n","Epoch 284:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.86, valid_loss=-1.75]\n","Epoch 285:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.77, valid_loss=-1.75]\n","Epoch 286:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.36, train_loss_epoch=-1.64, valid_loss=-1.75]\n","Epoch 287:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.83, valid_loss=-1.75]\n","Epoch 288:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.89, valid_loss=-1.75]\n","Epoch 289:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.88, valid_loss=-1.75]\n","Epoch 290:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.79, valid_loss=-1.75]\n","Epoch 291:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.83, valid_loss=-1.75]\n","Epoch 292:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.68, valid_loss=-1.75]\n","Epoch 293:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.80, valid_loss=-1.75]\n","Epoch 294:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-1.88, valid_loss=-1.75]\n","Epoch 295:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-1.95, valid_loss=-1.75]\n","Epoch 296:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.84, valid_loss=-1.75]\n","Epoch 297:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.80, valid_loss=-1.75]\n","Epoch 298:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.80, valid_loss=-1.75]\n","Epoch 299:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.77, valid_loss=-1.75]\n","Epoch 299: 100%|██████████| 4/4 [00:00\u003c00:00, 37.86it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.77, valid_loss=-1.75]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8405)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 26.36it/s]\u001b[A\n","Epoch 300:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.85, valid_loss=-1.75]\n","Epoch 301:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.85, valid_loss=-1.75]\n","Epoch 302:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.74, valid_loss=-1.75]\n","Epoch 303:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.73, valid_loss=-1.75]\n","Epoch 303: 100%|██████████| 4/4 [00:00\u003c00:00, 38.36it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-1.73, valid_loss=-1.75]\n","Epoch 304:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-1.85, valid_loss=-1.75]\n","Epoch 304: 100%|██████████| 4/4 [00:00\u003c00:00, 37.94it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.85, valid_loss=-1.75]\n","Epoch 305:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.86, valid_loss=-1.75]\n","Epoch 306:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.88, valid_loss=-1.75]\n","Epoch 307:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.73, valid_loss=-1.75]\n","Epoch 308:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.98, train_loss_epoch=-1.59, valid_loss=-1.75]\n","Epoch 309:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.86, valid_loss=-1.75]\n","Epoch 310:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.74, valid_loss=-1.75]\n","Epoch 311:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-1.95, valid_loss=-1.75]\n","Epoch 312:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-1.89, valid_loss=-1.75]\n","Epoch 313:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.89, valid_loss=-1.75]\n","Epoch 314:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.84, valid_loss=-1.75]\n","Epoch 315:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.87, valid_loss=-1.75]\n","Epoch 316:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.84, valid_loss=-1.75]\n","Epoch 317:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-2.03, valid_loss=-1.75]\n","Epoch 318:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.81, valid_loss=-1.75]\n","Epoch 319:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.78, valid_loss=-1.75]\n","Epoch 320:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.77, valid_loss=-1.75]\n","Epoch 321:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.86, valid_loss=-1.75]\n","Epoch 322:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.89, valid_loss=-1.75]\n","Epoch 323:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.86, valid_loss=-1.75]\n","Epoch 324:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.88, valid_loss=-1.75]\n","Epoch 324: 100%|██████████| 4/4 [00:00\u003c00:00, 38.59it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-1.88, valid_loss=-1.75]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8405)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 26.29it/s]\u001b[A\n","Epoch 325:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-1.90, valid_loss=-1.75]\n","Epoch 326:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.82, valid_loss=-1.75]\n","Epoch 327:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.85, valid_loss=-1.75]\n","Epoch 328:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.82, valid_loss=-1.75]\n","Epoch 329:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.92, valid_loss=-1.75]\n","Epoch 330:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.91, valid_loss=-1.75]\n","Epoch 331:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.71, valid_loss=-1.75]\n","Epoch 332:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.72, valid_loss=-1.75]\n","Epoch 332: 100%|██████████| 4/4 [00:00\u003c00:00, 35.81it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.72, valid_loss=-1.75]\n","Epoch 333:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.82, valid_loss=-1.75]\n","Epoch 334:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-1.87, valid_loss=-1.75]\n","Epoch 335:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.90, valid_loss=-1.75]\n","Epoch 336:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-1.89, valid_loss=-1.75]\n","Epoch 337:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.83, valid_loss=-1.75]\n","Epoch 338:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.73, valid_loss=-1.75]\n","Epoch 339:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.81, valid_loss=-1.75]\n","Epoch 340:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.75, valid_loss=-1.75]\n","Epoch 341:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.67, valid_loss=-1.75]\n","Epoch 342:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.85, valid_loss=-1.75]\n","Epoch 343:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.83, valid_loss=-1.75]\n","Epoch 344:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.77, valid_loss=-1.75]\n","Epoch 345:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.76, valid_loss=-1.75]\n","Epoch 346:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.81, valid_loss=-1.75]\n","Epoch 347:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.87, valid_loss=-1.75]\n","Epoch 348:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.83, valid_loss=-1.75]\n","Epoch 349:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.72, valid_loss=-1.75]\n","Epoch 349: 100%|██████████| 4/4 [00:00\u003c00:00, 36.21it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.72, valid_loss=-1.75]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8405)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 24.25it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8405)\u001b[0m \n","Epoch 350:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.85, valid_loss=-1.75]\n","Epoch 351:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.79, valid_loss=-1.75]\n","Epoch 352:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.77, valid_loss=-1.75]\n","Epoch 353:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.64, valid_loss=-1.75]\n","Epoch 354:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-1.89, valid_loss=-1.75]\n","Epoch 355:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.75, valid_loss=-1.75]\n","Epoch 356:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.72, valid_loss=-1.75]\n","Epoch 357:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.46, train_loss_epoch=-1.91, valid_loss=-1.75]\n","Epoch 358:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.81, valid_loss=-1.75]\n","Epoch 359:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.91, valid_loss=-1.75]\n","Epoch 360:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.72, valid_loss=-1.75]\n","Epoch 361:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.76, valid_loss=-1.75]\n","Epoch 362:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.81, valid_loss=-1.75]\n","Epoch 363:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.86, valid_loss=-1.75]\n","Epoch 364:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.77, valid_loss=-1.75]\n","Epoch 365:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.88, valid_loss=-1.75]\n","Epoch 366:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.67, valid_loss=-1.75]\n","Epoch 367:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.60, valid_loss=-1.75]\n","Epoch 368:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.76, valid_loss=-1.75]\n","Epoch 369:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.91, valid_loss=-1.75]\n","Epoch 370:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.88, valid_loss=-1.75]\n","Epoch 371:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.70, valid_loss=-1.75]\n","Epoch 372:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.78, valid_loss=-1.75]\n","Epoch 373:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.75, valid_loss=-1.75]\n","Epoch 374:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.85, valid_loss=-1.75]\n","Epoch 374: 100%|██████████| 4/4 [00:00\u003c00:00, 38.38it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.85, valid_loss=-1.75]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-18 13:11:53,851\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (1, 1, 1), 'n_pool_kernel_size': (2, 2, 2), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=8405)\u001b[0m `Trainer.fit` stopped: `max_steps=1500.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=8405)\u001b[0m \n","\u001b[36m(_train_tune pid=8405)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 23.36it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8405)\u001b[0m \r                                                                      \u001b[A\rEpoch 374: 100%|██████████| 4/4 [00:00\u003c00:00, 13.23it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.85, valid_loss=-1.75]\rEpoch 374: 100%|██████████| 4/4 [00:00\u003c00:00, 13.20it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.86, valid_loss=-1.75]\rEpoch 374: 100%|██████████| 4/4 [00:00\u003c00:00, 13.15it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.86, valid_loss=-1.75]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=8693)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=8693)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=8693)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=8693)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=8693)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=8693)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=8693)\u001b[0m 2024-11-18 13:12:01.803575: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=8693)\u001b[0m 2024-11-18 13:12:01.824331: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=8693)\u001b[0m 2024-11-18 13:12:01.849699: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=8693)\u001b[0m 2024-11-18 13:12:01.857424: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=8693)\u001b[0m 2024-11-18 13:12:01.875516: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=8693)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=8693)\u001b[0m 2024-11-18 13:12:03.075342: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=8693)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=8693)\u001b[0m \n","\u001b[36m(_train_tune pid=8693)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=8693)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=8693)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=8693)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=8693)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=8693)\u001b[0m 3 | blocks       | ModuleList       | 3.1 M  | train\n","\u001b[36m(_train_tune pid=8693)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=8693)\u001b[0m 3.1 M     Trainable params\n","\u001b[36m(_train_tune pid=8693)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=8693)\u001b[0m 3.1 M     Total params\n","\u001b[36m(_train_tune pid=8693)\u001b[0m 12.547    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=8693)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=8693)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00,  2.23it/s]\n","                                                                           \n","Epoch 0:   0%|          | 0/4 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.260, train_loss_epoch=20.70]\n","Epoch 2:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.000, train_loss_epoch=7.420]\n","Epoch 3:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.860, train_loss_epoch=7.810]\n","Epoch 4:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.170, train_loss_epoch=7.710]\n","Epoch 5:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.950, train_loss_epoch=8.040]\n","Epoch 6:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.300, train_loss_epoch=8.140]\n","Epoch 7:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.990, train_loss_epoch=7.770]\n","Epoch 8:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.620, train_loss_epoch=7.960]\n","Epoch 9:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.580, train_loss_epoch=7.960]\n","Epoch 10:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.400, train_loss_epoch=8.120]\n","Epoch 11:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.020, train_loss_epoch=8.010]\n","Epoch 12:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.230, train_loss_epoch=8.000]\n","Epoch 13:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.760, train_loss_epoch=7.870]\n","Epoch 14:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.820, train_loss_epoch=7.510]\n","Epoch 15:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.420, train_loss_epoch=7.610]\n","Epoch 16:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.110, train_loss_epoch=7.380]\n","Epoch 17:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.160, train_loss_epoch=6.910]\n","Epoch 18:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.450, train_loss_epoch=6.890]\n","Epoch 19:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.930, train_loss_epoch=6.600]\n","Epoch 20:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.20, train_loss_epoch=7.900]\n","Epoch 21:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.060, train_loss_epoch=6.420]\n","Epoch 22:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.870, train_loss_epoch=6.250]\n","Epoch 23:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.020, train_loss_epoch=6.180]\n","Epoch 24:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.600, train_loss_epoch=6.280]\n","Epoch 24: 100%|██████████| 4/4 [00:00\u003c00:00, 39.12it/s, v_num=0, train_loss_step=5.640, train_loss_epoch=6.280]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8693)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8693)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 26.04it/s]\u001b[A\n","Epoch 25:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.640, train_loss_epoch=6.010, valid_loss=5.920]\n","Epoch 26:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.970, train_loss_epoch=6.020, valid_loss=5.920]\n","Epoch 27:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.060, train_loss_epoch=5.960, valid_loss=5.920]\n","Epoch 28:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.570, train_loss_epoch=6.110, valid_loss=5.920]\n","Epoch 29:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.910, train_loss_epoch=5.900, valid_loss=5.920]\n","Epoch 30:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.380, train_loss_epoch=5.700, valid_loss=5.920]\n","Epoch 31:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.420, train_loss_epoch=5.630, valid_loss=5.920]\n","Epoch 32:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.510, train_loss_epoch=6.110, valid_loss=5.920]\n","Epoch 33:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.040, train_loss_epoch=5.760, valid_loss=5.920]\n","Epoch 34:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.470, train_loss_epoch=5.550, valid_loss=5.920]\n","Epoch 35:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.150, train_loss_epoch=5.470, valid_loss=5.920]\n","Epoch 36:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.190, train_loss_epoch=5.400, valid_loss=5.920]\n","Epoch 37:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.020, train_loss_epoch=5.410, valid_loss=5.920]\n","Epoch 38:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.930, train_loss_epoch=5.330, valid_loss=5.920]\n","Epoch 39:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.710, train_loss_epoch=5.430, valid_loss=5.920]\n","Epoch 40:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.540, train_loss_epoch=5.450, valid_loss=5.920]\n","Epoch 41:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.810, train_loss_epoch=5.390, valid_loss=5.920]\n","Epoch 42:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.020, train_loss_epoch=5.260, valid_loss=5.920]\n","Epoch 43:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.730, train_loss_epoch=5.200, valid_loss=5.920]\n","Epoch 44:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.080, train_loss_epoch=5.230, valid_loss=5.920]\n","Epoch 45:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.160, train_loss_epoch=5.260, valid_loss=5.920]\n","Epoch 46:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.380, train_loss_epoch=5.040, valid_loss=5.920]\n","Epoch 47:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.690, train_loss_epoch=5.070, valid_loss=5.920]\n","Epoch 48:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.240, train_loss_epoch=5.220, valid_loss=5.920]\n","Epoch 49:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.580, train_loss_epoch=5.300, valid_loss=5.920]\n","Epoch 49: 100%|██████████| 4/4 [00:00\u003c00:00, 37.76it/s, v_num=0, train_loss_step=4.950, train_loss_epoch=5.300, valid_loss=5.920]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8693)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 27.72it/s]\u001b[A\n","Epoch 50:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.950, train_loss_epoch=5.120, valid_loss=5.060]\n","Epoch 51:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.610, train_loss_epoch=5.530, valid_loss=5.060]\n","Epoch 52:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.630, train_loss_epoch=5.560, valid_loss=5.060]\n","Epoch 53:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.990, train_loss_epoch=5.170, valid_loss=5.060]\n","Epoch 54:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.630, train_loss_epoch=5.220, valid_loss=5.060]\n","Epoch 55:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.610, train_loss_epoch=5.660, valid_loss=5.060]\n","Epoch 56:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.290, train_loss_epoch=5.290, valid_loss=5.060]\n","Epoch 57:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.650, train_loss_epoch=5.070, valid_loss=5.060]\n","Epoch 58:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.510, train_loss_epoch=5.420, valid_loss=5.060]\n","Epoch 59:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.150, train_loss_epoch=5.280, valid_loss=5.060]\n","Epoch 60:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.090, train_loss_epoch=5.170, valid_loss=5.060]\n","Epoch 61:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.100, train_loss_epoch=5.300, valid_loss=5.060]\n","Epoch 62:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.910, train_loss_epoch=5.190, valid_loss=5.060]\n","Epoch 63:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.860, train_loss_epoch=5.020, valid_loss=5.060]\n","Epoch 64:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.860, train_loss_epoch=5.050, valid_loss=5.060]\n","Epoch 65:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.660, train_loss_epoch=5.250, valid_loss=5.060]\n","Epoch 66:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.030, train_loss_epoch=5.060, valid_loss=5.060]\n","Epoch 67:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.410, train_loss_epoch=5.480, valid_loss=5.060]\n","Epoch 68:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.310, train_loss_epoch=6.270, valid_loss=5.060]\n","Epoch 69:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.810, train_loss_epoch=6.580, valid_loss=5.060]\n","Epoch 70:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.690, train_loss_epoch=6.630, valid_loss=5.060]\n","Epoch 71:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.910, train_loss_epoch=7.110, valid_loss=5.060]\n","Epoch 72:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.990, train_loss_epoch=6.390, valid_loss=5.060]\n","Epoch 73:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.130, train_loss_epoch=6.830, valid_loss=5.060]\n","Epoch 74:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.820, train_loss_epoch=6.400, valid_loss=5.060]\n","Epoch 74: 100%|██████████| 4/4 [00:00\u003c00:00, 37.53it/s, v_num=0, train_loss_step=5.710, train_loss_epoch=6.400, valid_loss=5.060]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8693)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 27.65it/s]\u001b[A\n","Epoch 75:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.710, train_loss_epoch=6.040, valid_loss=5.830]\n","Epoch 76:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.790, train_loss_epoch=5.960, valid_loss=5.830]\n","Epoch 77:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.930, train_loss_epoch=5.910, valid_loss=5.830]\n","Epoch 78:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.810, train_loss_epoch=5.700, valid_loss=5.830]\n","Epoch 79:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.720, train_loss_epoch=5.300, valid_loss=5.830]\n","Epoch 80:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.750, train_loss_epoch=5.710, valid_loss=5.830]\n","Epoch 81:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.610, train_loss_epoch=5.610, valid_loss=5.830]\n","Epoch 82:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.160, train_loss_epoch=5.310, valid_loss=5.830]\n","Epoch 83:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.830, train_loss_epoch=5.410, valid_loss=5.830]\n","Epoch 84:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.280, train_loss_epoch=5.500, valid_loss=5.830]\n","Epoch 85:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.060, train_loss_epoch=5.150, valid_loss=5.830]\n","Epoch 86:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.480, train_loss_epoch=4.930, valid_loss=5.830]\n","Epoch 87:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.760, train_loss_epoch=4.980, valid_loss=5.830]\n","Epoch 88:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.650, train_loss_epoch=4.900, valid_loss=5.830]\n","Epoch 89:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.710, train_loss_epoch=4.840, valid_loss=5.830]\n","Epoch 90:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.280, train_loss_epoch=4.980, valid_loss=5.830]\n","Epoch 91:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.880, train_loss_epoch=5.090, valid_loss=5.830]\n","Epoch 92:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.290, train_loss_epoch=5.030, valid_loss=5.830]\n","Epoch 93:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.000, train_loss_epoch=4.950, valid_loss=5.830]\n","Epoch 94:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.650, train_loss_epoch=4.820, valid_loss=5.830]\n","Epoch 95:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.740, train_loss_epoch=4.850, valid_loss=5.830]\n","Epoch 96:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.020, train_loss_epoch=4.890, valid_loss=5.830]\n","Epoch 97:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.040, train_loss_epoch=4.760, valid_loss=5.830]\n","Epoch 98:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.140, train_loss_epoch=5.130, valid_loss=5.830]\n","Epoch 99:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.370, train_loss_epoch=5.060, valid_loss=5.830]\n","Epoch 99: 100%|██████████| 4/4 [00:00\u003c00:00, 38.13it/s, v_num=0, train_loss_step=4.650, train_loss_epoch=5.060, valid_loss=5.830]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8693)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 24.14it/s]\u001b[A\n","Epoch 100:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.650, train_loss_epoch=4.820, valid_loss=4.950]\n","Epoch 101:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.380, train_loss_epoch=4.750, valid_loss=4.950]\n","Epoch 102:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.500, train_loss_epoch=4.810, valid_loss=4.950]\n","Epoch 103:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.800, train_loss_epoch=4.820, valid_loss=4.950]\n","Epoch 104:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.370, train_loss_epoch=5.130, valid_loss=4.950]\n","Epoch 105:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.960, train_loss_epoch=4.800, valid_loss=4.950]\n","Epoch 106:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.960, train_loss_epoch=4.940, valid_loss=4.950]\n","Epoch 107:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.320, train_loss_epoch=4.610, valid_loss=4.950]\n","Epoch 108:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.080, train_loss_epoch=4.760, valid_loss=4.950]\n","Epoch 109:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.060, train_loss_epoch=4.440, valid_loss=4.950]\n","Epoch 110:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.920, train_loss_epoch=4.790, valid_loss=4.950]\n","Epoch 111:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.800, train_loss_epoch=4.680, valid_loss=4.950]\n","Epoch 112:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.770, train_loss_epoch=4.710, valid_loss=4.950]\n","Epoch 113:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.510, train_loss_epoch=4.610, valid_loss=4.950]\n","Epoch 114:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.200, train_loss_epoch=4.570, valid_loss=4.950]\n","Epoch 115:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.530, train_loss_epoch=4.380, valid_loss=4.950]\n","Epoch 116:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.540, train_loss_epoch=4.640, valid_loss=4.950]\n","Epoch 117:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.000, train_loss_epoch=4.420, valid_loss=4.950]\n","Epoch 118:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.880, train_loss_epoch=4.920, valid_loss=4.950]\n","Epoch 119:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.140, train_loss_epoch=4.360, valid_loss=4.950]\n","Epoch 120:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.940, train_loss_epoch=4.610, valid_loss=4.950]\n","Epoch 121:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.660, train_loss_epoch=4.390, valid_loss=4.950]\n","Epoch 122:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.860, train_loss_epoch=4.390, valid_loss=4.950]\n","Epoch 123:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.230, train_loss_epoch=4.770, valid_loss=4.950]\n","Epoch 124:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.390, train_loss_epoch=4.760, valid_loss=4.950]\n","Epoch 124: 100%|██████████| 4/4 [00:00\u003c00:00, 38.28it/s, v_num=0, train_loss_step=4.320, train_loss_epoch=4.760, valid_loss=4.950]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8693)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 27.71it/s]\u001b[A\n","Epoch 125:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.320, train_loss_epoch=4.560, valid_loss=4.550]\n","Epoch 126:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.580, train_loss_epoch=4.600, valid_loss=4.550]\n","Epoch 127:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.230, train_loss_epoch=5.130, valid_loss=4.550]\n","Epoch 128:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.360, train_loss_epoch=4.650, valid_loss=4.550]\n","Epoch 129:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.940, train_loss_epoch=4.650, valid_loss=4.550]\n","Epoch 130:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.630, train_loss_epoch=4.650, valid_loss=4.550]\n","Epoch 131:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.170, train_loss_epoch=4.440, valid_loss=4.550]\n","Epoch 132:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.510, train_loss_epoch=4.540, valid_loss=4.550]\n","Epoch 133:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.930, train_loss_epoch=4.610, valid_loss=4.550]\n","Epoch 134:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.200, train_loss_epoch=4.740, valid_loss=4.550]\n","Epoch 135:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.560, train_loss_epoch=4.600, valid_loss=4.550]\n","Epoch 136:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.680, train_loss_epoch=4.280, valid_loss=4.550]\n","Epoch 137:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.640, train_loss_epoch=4.760, valid_loss=4.550]\n","Epoch 138:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.910, train_loss_epoch=4.350, valid_loss=4.550]\n","Epoch 139:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.370, train_loss_epoch=4.630, valid_loss=4.550]\n","Epoch 140:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.370, train_loss_epoch=4.410, valid_loss=4.550]\n","Epoch 141:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.140, train_loss_epoch=4.410, valid_loss=4.550]\n","Epoch 142:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.980, train_loss_epoch=4.290, valid_loss=4.550]\n","Epoch 143:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.210, train_loss_epoch=4.860, valid_loss=4.550]\n","Epoch 144:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.990, train_loss_epoch=5.520, valid_loss=4.550]\n","Epoch 145:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.600, train_loss_epoch=6.550, valid_loss=4.550]\n","Epoch 146:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.860, train_loss_epoch=5.860, valid_loss=4.550]\n","Epoch 147:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.510, train_loss_epoch=5.710, valid_loss=4.550]\n","Epoch 148:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.160, train_loss_epoch=5.610, valid_loss=4.550]\n","Epoch 149:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.920, train_loss_epoch=5.680, valid_loss=4.550]\n","Epoch 149: 100%|██████████| 4/4 [00:00\u003c00:00, 38.96it/s, v_num=0, train_loss_step=5.180, train_loss_epoch=5.680, valid_loss=4.550]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8693)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 27.99it/s]\u001b[A\n","Epoch 150:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.180, train_loss_epoch=5.350, valid_loss=5.100]\n","Epoch 151:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.740, train_loss_epoch=5.340, valid_loss=5.100]\n","Epoch 152:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.770, train_loss_epoch=4.970, valid_loss=5.100]\n","Epoch 153:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.800, train_loss_epoch=4.890, valid_loss=5.100]\n","Epoch 154:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.700, train_loss_epoch=5.110, valid_loss=5.100]\n","Epoch 155:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.880, train_loss_epoch=5.120, valid_loss=5.100]\n","Epoch 156:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.190, train_loss_epoch=4.780, valid_loss=5.100]\n","Epoch 157:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.340, train_loss_epoch=4.680, valid_loss=5.100]\n","Epoch 157: 100%|██████████| 4/4 [00:00\u003c00:00, 38.34it/s, v_num=0, train_loss_step=4.650, train_loss_epoch=4.700, valid_loss=5.100]\n","Epoch 158:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.650, train_loss_epoch=4.700, valid_loss=5.100]\n","Epoch 159:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.900, train_loss_epoch=4.390, valid_loss=5.100]\n","Epoch 160:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.590, train_loss_epoch=4.670, valid_loss=5.100]\n","Epoch 161:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.960, train_loss_epoch=4.410, valid_loss=5.100]\n","Epoch 162:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.870, train_loss_epoch=4.610, valid_loss=5.100]\n","Epoch 163:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.060, train_loss_epoch=4.500, valid_loss=5.100]\n","Epoch 164:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.190, train_loss_epoch=4.770, valid_loss=5.100]\n","Epoch 165:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.260, train_loss_epoch=4.570, valid_loss=5.100]\n","Epoch 166:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.500, train_loss_epoch=4.540, valid_loss=5.100]\n","Epoch 167:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.070, train_loss_epoch=4.680, valid_loss=5.100]\n","Epoch 168:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.640, train_loss_epoch=4.560, valid_loss=5.100]\n","Epoch 169:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.270, train_loss_epoch=4.440, valid_loss=5.100]\n","Epoch 170:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.100, train_loss_epoch=4.540, valid_loss=5.100]\n","Epoch 171:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.750, train_loss_epoch=4.420, valid_loss=5.100]\n","Epoch 172:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.180, train_loss_epoch=4.460, valid_loss=5.100]\n","Epoch 173:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.360, train_loss_epoch=4.500, valid_loss=5.100]\n","Epoch 174:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.300, train_loss_epoch=4.440, valid_loss=5.100]\n","Epoch 174: 100%|██████████| 4/4 [00:00\u003c00:00, 35.04it/s, v_num=0, train_loss_step=4.170, train_loss_epoch=4.440, valid_loss=5.100]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8693)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 27.27it/s]\u001b[A\n","Epoch 175:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.170, train_loss_epoch=4.320, valid_loss=4.630]\n","Epoch 176:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.320, train_loss_epoch=4.430, valid_loss=4.630]\n","Epoch 177:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.570, train_loss_epoch=4.470, valid_loss=4.630]\n","Epoch 178:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.610, train_loss_epoch=4.520, valid_loss=4.630]\n","Epoch 179:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.720, train_loss_epoch=4.110, valid_loss=4.630]\n","Epoch 180:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.410, train_loss_epoch=4.430, valid_loss=4.630]\n","Epoch 181:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.010, train_loss_epoch=4.190, valid_loss=4.630]\n","Epoch 182:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.300, train_loss_epoch=4.370, valid_loss=4.630]\n","Epoch 183:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.860, train_loss_epoch=4.120, valid_loss=4.630]\n","Epoch 184:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.200, train_loss_epoch=4.210, valid_loss=4.630]\n","Epoch 185:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.560, train_loss_epoch=4.400, valid_loss=4.630]\n","Epoch 186:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.210, train_loss_epoch=4.230, valid_loss=4.630]\n","Epoch 187:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.730, train_loss_epoch=4.330, valid_loss=4.630]\n","Epoch 188:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.090, train_loss_epoch=4.200, valid_loss=4.630]\n","Epoch 189:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.860, train_loss_epoch=4.180, valid_loss=4.630]\n","Epoch 190:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.430, train_loss_epoch=4.690, valid_loss=4.630]\n","Epoch 191:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.420, train_loss_epoch=4.350, valid_loss=4.630]\n","Epoch 192:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.320, train_loss_epoch=4.550, valid_loss=4.630]\n","Epoch 193:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.580, train_loss_epoch=4.500, valid_loss=4.630]\n","Epoch 194:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.190, train_loss_epoch=4.250, valid_loss=4.630]\n","Epoch 195:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.850, train_loss_epoch=4.480, valid_loss=4.630]\n","Epoch 196:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.100, train_loss_epoch=4.510, valid_loss=4.630]\n","Epoch 197:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.710, train_loss_epoch=4.720, valid_loss=4.630]\n","Epoch 198:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.210, train_loss_epoch=4.380, valid_loss=4.630]\n","Epoch 199:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.260, train_loss_epoch=4.390, valid_loss=4.630]\n","Epoch 199: 100%|██████████| 4/4 [00:00\u003c00:00, 38.94it/s, v_num=0, train_loss_step=4.170, train_loss_epoch=4.390, valid_loss=4.630]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8693)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 25.58it/s]\u001b[A\n","Epoch 200:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.170, train_loss_epoch=4.420, valid_loss=4.410]\n","Epoch 201:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.810, train_loss_epoch=4.300, valid_loss=4.410]\n","Epoch 202:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.960, train_loss_epoch=4.270, valid_loss=4.410]\n","Epoch 203:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.950, train_loss_epoch=4.290, valid_loss=4.410]\n","Epoch 204:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.150, train_loss_epoch=4.310, valid_loss=4.410]\n","Epoch 205:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.190, train_loss_epoch=4.170, valid_loss=4.410]\n","Epoch 206:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.800, train_loss_epoch=4.180, valid_loss=4.410]\n","Epoch 207:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.910, train_loss_epoch=4.580, valid_loss=4.410]\n","Epoch 208:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.280, train_loss_epoch=4.790, valid_loss=4.410]\n","Epoch 209:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.670, train_loss_epoch=4.940, valid_loss=4.410]\n","Epoch 210:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.610, train_loss_epoch=4.790, valid_loss=4.410]\n","Epoch 211:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.380, train_loss_epoch=4.560, valid_loss=4.410]\n","Epoch 212:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.940, train_loss_epoch=4.560, valid_loss=4.410]\n","Epoch 213:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.100, train_loss_epoch=4.380, valid_loss=4.410]\n","Epoch 214:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.460, train_loss_epoch=4.180, valid_loss=4.410]\n","Epoch 215:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.870, train_loss_epoch=4.200, valid_loss=4.410]\n","Epoch 216:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.400, train_loss_epoch=4.240, valid_loss=4.410]\n","Epoch 217:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.120, train_loss_epoch=4.180, valid_loss=4.410]\n","Epoch 218:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.770, train_loss_epoch=4.310, valid_loss=4.410]\n","Epoch 219:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.140, train_loss_epoch=4.160, valid_loss=4.410]\n","Epoch 220:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.790, train_loss_epoch=3.970, valid_loss=4.410]\n","Epoch 221:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.270, train_loss_epoch=3.860, valid_loss=4.410]\n","Epoch 222:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.880, train_loss_epoch=4.350, valid_loss=4.410]\n","Epoch 223:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.050, train_loss_epoch=4.460, valid_loss=4.410]\n","Epoch 224:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.200, train_loss_epoch=3.960, valid_loss=4.410]\n","Epoch 224: 100%|██████████| 4/4 [00:00\u003c00:00, 37.98it/s, v_num=0, train_loss_step=4.340, train_loss_epoch=3.960, valid_loss=4.410]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8693)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 27.27it/s]\u001b[A\n","Epoch 225:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.340, train_loss_epoch=4.340, valid_loss=4.280]\n","Epoch 226:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.090, train_loss_epoch=4.090, valid_loss=4.280]\n","Epoch 227:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.390, train_loss_epoch=3.920, valid_loss=4.280]\n","Epoch 228:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.930, train_loss_epoch=4.050, valid_loss=4.280]\n","Epoch 229:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.210, train_loss_epoch=4.070, valid_loss=4.280]\n","Epoch 230:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.330, train_loss_epoch=4.120, valid_loss=4.280]\n","Epoch 231:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.210, train_loss_epoch=4.150, valid_loss=4.280]\n","Epoch 232:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.000, train_loss_epoch=4.810, valid_loss=4.280]\n","Epoch 233:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.900, train_loss_epoch=4.010, valid_loss=4.280]\n","Epoch 234:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.960, train_loss_epoch=3.950, valid_loss=4.280]\n","Epoch 235:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.830, train_loss_epoch=4.080, valid_loss=4.280]\n","Epoch 236:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.470, train_loss_epoch=4.390, valid_loss=4.280]\n","Epoch 237:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.050, train_loss_epoch=4.490, valid_loss=4.280]\n","Epoch 238:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.410, train_loss_epoch=4.270, valid_loss=4.280]\n","Epoch 239:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.240, train_loss_epoch=4.170, valid_loss=4.280]\n","Epoch 240:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.930, train_loss_epoch=4.070, valid_loss=4.280]\n","Epoch 241:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.460, train_loss_epoch=4.170, valid_loss=4.280]\n","Epoch 241: 100%|██████████| 4/4 [00:00\u003c00:00, 38.13it/s, v_num=0, train_loss_step=3.900, train_loss_epoch=3.990, valid_loss=4.280]\n","Epoch 242:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.900, train_loss_epoch=3.990, valid_loss=4.280]\n","Epoch 243:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.140, train_loss_epoch=4.060, valid_loss=4.280]\n","Epoch 244:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.460, train_loss_epoch=3.810, valid_loss=4.280]\n","Epoch 244: 100%|██████████| 4/4 [00:00\u003c00:00, 38.61it/s, v_num=0, train_loss_step=4.140, train_loss_epoch=4.070, valid_loss=4.280]\n","Epoch 245:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.140, train_loss_epoch=4.070, valid_loss=4.280]\n","Epoch 246:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.150, train_loss_epoch=4.020, valid_loss=4.280]\n","Epoch 247:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.610, train_loss_epoch=3.940, valid_loss=4.280]\n","Epoch 248:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.210, train_loss_epoch=3.780, valid_loss=4.280]\n","Epoch 249:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.550, train_loss_epoch=4.140, valid_loss=4.280]\n","Epoch 249: 100%|██████████| 4/4 [00:00\u003c00:00, 38.27it/s, v_num=0, train_loss_step=5.170, train_loss_epoch=4.140, valid_loss=4.280]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8693)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 24.71it/s]\u001b[A\n","Epoch 250:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.170, train_loss_epoch=4.310, valid_loss=4.180]\n","Epoch 251:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.230, train_loss_epoch=4.080, valid_loss=4.180]\n","Epoch 252:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.220, train_loss_epoch=4.130, valid_loss=4.180]\n","Epoch 253:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.310, train_loss_epoch=3.960, valid_loss=4.180]\n","Epoch 254:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.020, train_loss_epoch=4.090, valid_loss=4.180]\n","Epoch 255:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.890, train_loss_epoch=4.030, valid_loss=4.180]\n","Epoch 255: 100%|██████████| 4/4 [00:00\u003c00:00, 38.22it/s, v_num=0, train_loss_step=4.940, train_loss_epoch=4.280, valid_loss=4.180]\n","Epoch 256:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.940, train_loss_epoch=4.280, valid_loss=4.180]\n","Epoch 256: 100%|██████████| 4/4 [00:00\u003c00:00, 38.52it/s, v_num=0, train_loss_step=4.470, train_loss_epoch=4.030, valid_loss=4.180]\n","Epoch 257:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.470, train_loss_epoch=4.030, valid_loss=4.180]\n","Epoch 258:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.070, train_loss_epoch=3.860, valid_loss=4.180]\n","Epoch 258: 100%|██████████| 4/4 [00:00\u003c00:00, 39.36it/s, v_num=0, train_loss_step=3.970, train_loss_epoch=3.950, valid_loss=4.180]\n","Epoch 259:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.970, train_loss_epoch=3.950, valid_loss=4.180]\n","Epoch 260:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.510, train_loss_epoch=3.850, valid_loss=4.180]\n","Epoch 261:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.390, train_loss_epoch=4.160, valid_loss=4.180]\n","Epoch 262:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.420, train_loss_epoch=4.180, valid_loss=4.180]\n","Epoch 262: 100%|██████████| 4/4 [00:00\u003c00:00, 38.14it/s, v_num=0, train_loss_step=5.740, train_loss_epoch=4.380, valid_loss=4.180]\n","Epoch 263:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.740, train_loss_epoch=4.380, valid_loss=4.180]\n","Epoch 263: 100%|██████████| 4/4 [00:00\u003c00:00, 38.13it/s, v_num=0, train_loss_step=5.740, train_loss_epoch=4.380, valid_loss=4.180]\n","Epoch 264:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.640, train_loss_epoch=4.090, valid_loss=4.180]\n","Epoch 265:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.820, train_loss_epoch=3.980, valid_loss=4.180]\n","Epoch 266:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.160, train_loss_epoch=3.730, valid_loss=4.180]\n","Epoch 267:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.860, train_loss_epoch=3.940, valid_loss=4.180]\n","Epoch 268:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.570, train_loss_epoch=3.880, valid_loss=4.180]\n","Epoch 269:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.910, train_loss_epoch=4.070, valid_loss=4.180]\n","Epoch 270:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.500, train_loss_epoch=3.880, valid_loss=4.180]\n","Epoch 271:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.330, train_loss_epoch=4.090, valid_loss=4.180]\n","Epoch 272:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.990, train_loss_epoch=4.010, valid_loss=4.180]\n","Epoch 273:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.430, train_loss_epoch=4.060, valid_loss=4.180]\n","Epoch 274:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.640, train_loss_epoch=4.120, valid_loss=4.180]\n","Epoch 274: 100%|██████████| 4/4 [00:00\u003c00:00, 38.19it/s, v_num=0, train_loss_step=4.750, train_loss_epoch=4.120, valid_loss=4.180]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8693)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 24.63it/s]\u001b[A\n","Epoch 275:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.750, train_loss_epoch=4.180, valid_loss=4.280]\n","Epoch 276:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.270, train_loss_epoch=3.700, valid_loss=4.280]\n","Epoch 277:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.520, train_loss_epoch=3.830, valid_loss=4.280]\n","Epoch 278:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.900, train_loss_epoch=4.180, valid_loss=4.280]\n","Epoch 279:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.760, train_loss_epoch=3.900, valid_loss=4.280]\n","Epoch 280:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.350, train_loss_epoch=4.340, valid_loss=4.280]\n","Epoch 281:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.150, train_loss_epoch=3.830, valid_loss=4.280]\n","Epoch 282:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.240, train_loss_epoch=3.820, valid_loss=4.280]\n","Epoch 283:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.460, train_loss_epoch=4.100, valid_loss=4.280]\n","Epoch 284:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.140, train_loss_epoch=4.000, valid_loss=4.280]\n","Epoch 285:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.670, train_loss_epoch=3.840, valid_loss=4.280]\n","Epoch 286:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.110, train_loss_epoch=3.970, valid_loss=4.280]\n","Epoch 287:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.490, train_loss_epoch=3.850, valid_loss=4.280]\n","Epoch 288:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.320, train_loss_epoch=4.070, valid_loss=4.280]\n","Epoch 289:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.060, train_loss_epoch=4.020, valid_loss=4.280]\n","Epoch 290:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.350, train_loss_epoch=4.180, valid_loss=4.280]\n","Epoch 291:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.180, train_loss_epoch=4.440, valid_loss=4.280]\n","Epoch 292:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.770, train_loss_epoch=4.150, valid_loss=4.280]\n","Epoch 293:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.340, train_loss_epoch=4.200, valid_loss=4.280]\n","Epoch 294:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.690, train_loss_epoch=4.550, valid_loss=4.280]\n","Epoch 295:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.060, train_loss_epoch=4.210, valid_loss=4.280]\n","Epoch 296:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.610, train_loss_epoch=4.190, valid_loss=4.280]\n","Epoch 297:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.330, train_loss_epoch=4.140, valid_loss=4.280]\n","Epoch 298:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.710, train_loss_epoch=3.900, valid_loss=4.280]\n","Epoch 299:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.910, train_loss_epoch=4.220, valid_loss=4.280]\n","Epoch 299: 100%|██████████| 4/4 [00:00\u003c00:00, 37.98it/s, v_num=0, train_loss_step=3.380, train_loss_epoch=4.220, valid_loss=4.280]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-18 13:12:39,643\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (24, 12, 1), 'n_pool_kernel_size': (1, 1, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=8693)\u001b[0m `Trainer.fit` stopped: `max_steps=1200.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=8693)\u001b[0m \n","\u001b[36m(_train_tune pid=8693)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 27.33it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8693)\u001b[0m \r                                                                      \u001b[A\rEpoch 299: 100%|██████████| 4/4 [00:00\u003c00:00, 14.37it/s, v_num=0, train_loss_step=3.380, train_loss_epoch=4.220, valid_loss=4.060]\rEpoch 299: 100%|██████████| 4/4 [00:00\u003c00:00, 14.32it/s, v_num=0, train_loss_step=3.380, train_loss_epoch=3.820, valid_loss=4.060]\rEpoch 299: 100%|██████████| 4/4 [00:00\u003c00:00, 14.27it/s, v_num=0, train_loss_step=3.380, train_loss_epoch=3.820, valid_loss=4.060]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=8945)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=8945)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=8945)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=8945)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=8945)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=8945)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=8945)\u001b[0m 2024-11-18 13:12:47.825916: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=8945)\u001b[0m 2024-11-18 13:12:47.846350: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=8945)\u001b[0m 2024-11-18 13:12:47.871111: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=8945)\u001b[0m 2024-11-18 13:12:47.878671: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=8945)\u001b[0m 2024-11-18 13:12:47.896406: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=8945)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=8945)\u001b[0m 2024-11-18 13:12:49.045242: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=8945)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=8945)\u001b[0m \n","\u001b[36m(_train_tune pid=8945)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=8945)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=8945)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=8945)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=8945)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=8945)\u001b[0m 3 | blocks       | ModuleList       | 2.7 M  | train\n","\u001b[36m(_train_tune pid=8945)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=8945)\u001b[0m 2.7 M     Trainable params\n","\u001b[36m(_train_tune pid=8945)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=8945)\u001b[0m 2.7 M     Total params\n","\u001b[36m(_train_tune pid=8945)\u001b[0m 10.612    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=8945)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=8945)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/7 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.050, train_loss_epoch=73.70]\n","Epoch 2:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.660, train_loss_epoch=7.130]\n","Epoch 3:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.260, train_loss_epoch=7.320]\n","Epoch 4:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.800, train_loss_epoch=7.340]\n","Epoch 5:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.350, train_loss_epoch=7.420]\n","Epoch 6:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.610, train_loss_epoch=7.490]\n","Epoch 7:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.510, train_loss_epoch=7.310]\n","Epoch 8:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.130, train_loss_epoch=7.370]\n","Epoch 9:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.920, train_loss_epoch=7.320]\n","Epoch 9: 100%|██████████| 7/7 [00:00\u003c00:00, 39.28it/s, v_num=0, train_loss_step=7.670, train_loss_epoch=7.400]\n","Epoch 10:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.670, train_loss_epoch=7.400]\n","Epoch 11:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.270, train_loss_epoch=7.280]\n","Epoch 12:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.480, train_loss_epoch=7.290]\n","Epoch 13:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.110, train_loss_epoch=7.180]\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.360, train_loss_epoch=7.020]\n","\u001b[36m(_train_tune pid=8945)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8945)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.83it/s]\u001b[A\n","Epoch 14:   0%|          | 0/7 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=6.970, train_loss_epoch=7.020, valid_loss=6.880]\n","Epoch 15:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.840, train_loss_epoch=7.000, valid_loss=6.880]\n","Epoch 16:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.900, train_loss_epoch=6.930, valid_loss=6.880]\n","Epoch 17:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.940, train_loss_epoch=6.850, valid_loss=6.880]\n","Epoch 18:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.890, train_loss_epoch=6.770, valid_loss=6.880]\n","Epoch 19:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.080, train_loss_epoch=6.670, valid_loss=6.880]\n","Epoch 19: 100%|██████████| 7/7 [00:00\u003c00:00, 36.50it/s, v_num=0, train_loss_step=7.050, train_loss_epoch=6.620, valid_loss=6.880]\n","Epoch 20:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.050, train_loss_epoch=6.620, valid_loss=6.880]\n","Epoch 21:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.660, train_loss_epoch=6.490, valid_loss=6.880]\n","Epoch 22:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.670, train_loss_epoch=6.530, valid_loss=6.880]\n","Epoch 23:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.940, train_loss_epoch=6.270, valid_loss=6.880]\n","Epoch 24:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.780, train_loss_epoch=6.170, valid_loss=6.880]\n","Epoch 25:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.100, train_loss_epoch=6.160, valid_loss=6.880]\n","Epoch 26:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.040, train_loss_epoch=6.090, valid_loss=6.880]\n","Epoch 27:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.260, train_loss_epoch=6.030, valid_loss=6.880]\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.550, train_loss_epoch=6.000, valid_loss=6.880]\n","\u001b[36m(_train_tune pid=8945)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8945)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.86it/s]\u001b[A\n","Epoch 28:   0%|          | 0/7 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=6.140, train_loss_epoch=6.000, valid_loss=5.770]\n","Epoch 28: 100%|██████████| 7/7 [00:02\u003c00:00,  3.46it/s, v_num=0, train_loss_step=6.200, train_loss_epoch=5.910, valid_loss=5.770]\n","Epoch 29:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.200, train_loss_epoch=5.910, valid_loss=5.770]\n","Epoch 30:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.230, train_loss_epoch=5.760, valid_loss=5.770]\n","Epoch 31:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.650, train_loss_epoch=5.770, valid_loss=5.770]\n","Epoch 32:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.070, train_loss_epoch=5.760, valid_loss=5.770]\n","Epoch 33:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.370, train_loss_epoch=5.660, valid_loss=5.770]\n","Epoch 34:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.730, train_loss_epoch=5.650, valid_loss=5.770]\n","Epoch 35:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.850, train_loss_epoch=5.510, valid_loss=5.770]\n","Epoch 36:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.450, train_loss_epoch=5.510, valid_loss=5.770]\n","Epoch 37:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.660, train_loss_epoch=5.550, valid_loss=5.770]\n","Epoch 38:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.450, train_loss_epoch=5.480, valid_loss=5.770]\n","Epoch 39:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.790, train_loss_epoch=5.490, valid_loss=5.770]\n","Epoch 40:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.150, train_loss_epoch=5.400, valid_loss=5.770]\n","Epoch 41:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.760, train_loss_epoch=5.460, valid_loss=5.770]\n","Epoch 42:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.100, train_loss_epoch=5.310, valid_loss=5.770]\n","\u001b[36m(_train_tune pid=8945)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8945)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.85it/s]\u001b[A\n","Epoch 43:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.150, train_loss_epoch=5.350, valid_loss=5.210]\n","Epoch 44:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.170, train_loss_epoch=5.310, valid_loss=5.210]\n","Epoch 45:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.570, train_loss_epoch=5.170, valid_loss=5.210]\n","Epoch 46:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.680, train_loss_epoch=5.190, valid_loss=5.210]\n","Epoch 47:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.200, train_loss_epoch=5.230, valid_loss=5.210]\n","Epoch 48:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.580, train_loss_epoch=5.250, valid_loss=5.210]\n","Epoch 48: 100%|██████████| 7/7 [00:00\u003c00:00, 38.30it/s, v_num=0, train_loss_step=4.990, train_loss_epoch=5.150, valid_loss=5.210]\n","Epoch 49:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.990, train_loss_epoch=5.150, valid_loss=5.210]\n","Epoch 50:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.800, train_loss_epoch=5.280, valid_loss=5.210]\n","Epoch 51:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.480, train_loss_epoch=5.220, valid_loss=5.210]\n","Epoch 52:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.900, train_loss_epoch=5.400, valid_loss=5.210]\n","Epoch 53:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.560, train_loss_epoch=5.160, valid_loss=5.210]\n","Epoch 54:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.490, train_loss_epoch=5.200, valid_loss=5.210]\n","Epoch 55:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.190, train_loss_epoch=5.100, valid_loss=5.210]\n","Epoch 56:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.760, train_loss_epoch=5.040, valid_loss=5.210]\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.090, train_loss_epoch=5.050, valid_loss=5.210]\n","\u001b[36m(_train_tune pid=8945)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8945)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.84it/s]\u001b[A\n","Epoch 57:   0%|          | 0/7 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=5.010, train_loss_epoch=5.050, valid_loss=4.890]\n","Epoch 58:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.130, train_loss_epoch=5.020, valid_loss=4.890]\n","Epoch 59:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.040, train_loss_epoch=5.040, valid_loss=4.890]\n","Epoch 60:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.670, train_loss_epoch=4.960, valid_loss=4.890]\n","Epoch 61:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.010, train_loss_epoch=4.970, valid_loss=4.890]\n","Epoch 62:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.670, train_loss_epoch=4.890, valid_loss=4.890]\n","Epoch 63:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.790, train_loss_epoch=4.910, valid_loss=4.890]\n","Epoch 64:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.510, train_loss_epoch=5.040, valid_loss=4.890]\n","Epoch 65:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.690, train_loss_epoch=4.900, valid_loss=4.890]\n","Epoch 66:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.720, train_loss_epoch=5.010, valid_loss=4.890]\n","Epoch 67:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.490, train_loss_epoch=5.030, valid_loss=4.890]\n","Epoch 68:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.970, train_loss_epoch=4.930, valid_loss=4.890]\n","Epoch 69:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.830, train_loss_epoch=4.850, valid_loss=4.890]\n","Epoch 70:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.430, train_loss_epoch=5.050, valid_loss=4.890]\n","Epoch 70: 100%|██████████| 7/7 [00:00\u003c00:00, 39.64it/s, v_num=0, train_loss_step=5.140, train_loss_epoch=5.050, valid_loss=4.890]\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.140, train_loss_epoch=4.830, valid_loss=4.890]\n","\u001b[36m(_train_tune pid=8945)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8945)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.85it/s]\u001b[A\n","Epoch 71:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=4.770, train_loss_epoch=4.830, valid_loss=4.630]\n","Epoch 72:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.650, train_loss_epoch=4.900, valid_loss=4.630]\n","Epoch 73:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.750, train_loss_epoch=4.760, valid_loss=4.630]\n","Epoch 74:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.430, train_loss_epoch=4.700, valid_loss=4.630]\n","Epoch 75:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.560, train_loss_epoch=4.720, valid_loss=4.630]\n","Epoch 76:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.870, train_loss_epoch=4.770, valid_loss=4.630]\n","Epoch 77:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.090, train_loss_epoch=4.780, valid_loss=4.630]\n","Epoch 78:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.150, train_loss_epoch=4.650, valid_loss=4.630]\n","Epoch 79:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.070, train_loss_epoch=4.900, valid_loss=4.630]\n","Epoch 80:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.000, train_loss_epoch=4.900, valid_loss=4.630]\n","Epoch 81:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.430, train_loss_epoch=4.700, valid_loss=4.630]\n","Epoch 82:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.450, train_loss_epoch=4.830, valid_loss=4.630]\n","Epoch 83:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.150, train_loss_epoch=4.900, valid_loss=4.630]\n","Epoch 84:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.690, train_loss_epoch=4.700, valid_loss=4.630]\n","Epoch 85:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.130, train_loss_epoch=4.640, valid_loss=4.630]\n","\u001b[36m(_train_tune pid=8945)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8945)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.82it/s]\u001b[A\n","Epoch 85:   0%|          | 0/7 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=4.870, train_loss_epoch=4.640, valid_loss=4.670]\n","Epoch 86:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.630, train_loss_epoch=4.720, valid_loss=4.670]\n","Epoch 87:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.400, train_loss_epoch=4.840, valid_loss=4.670]\n","Epoch 88:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.080, train_loss_epoch=4.580, valid_loss=4.670]\n","Epoch 89:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.940, train_loss_epoch=4.570, valid_loss=4.670]\n","Epoch 90:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.540, train_loss_epoch=4.960, valid_loss=4.670]\n","Epoch 91:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.530, train_loss_epoch=4.850, valid_loss=4.670]\n","Epoch 92:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.540, train_loss_epoch=4.750, valid_loss=4.670]\n","Epoch 93:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.610, train_loss_epoch=4.690, valid_loss=4.670]\n","Epoch 94:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.470, train_loss_epoch=4.690, valid_loss=4.670]\n","Epoch 95:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.570, train_loss_epoch=4.640, valid_loss=4.670]\n","Epoch 96:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.870, train_loss_epoch=4.700, valid_loss=4.670]\n","Epoch 97:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.880, train_loss_epoch=4.530, valid_loss=4.670]\n","Epoch 98:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.110, train_loss_epoch=4.540, valid_loss=4.670]\n","Epoch 99:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.330, train_loss_epoch=4.590, valid_loss=4.670]\n","Epoch 99: 100%|██████████| 7/7 [00:00\u003c00:00, 39.97it/s, v_num=0, train_loss_step=4.060, train_loss_epoch=4.590, valid_loss=4.670]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8945)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8945)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.89it/s]\u001b[A\n","Epoch 100:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.060, train_loss_epoch=4.550, valid_loss=4.490]\n","Epoch 101:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.470, train_loss_epoch=4.640, valid_loss=4.490]\n","Epoch 102:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.970, train_loss_epoch=4.970, valid_loss=4.490]\n","Epoch 103:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.380, train_loss_epoch=4.620, valid_loss=4.490]\n","Epoch 104:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.100, train_loss_epoch=4.560, valid_loss=4.490]\n","Epoch 105:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.680, train_loss_epoch=4.640, valid_loss=4.490]\n","Epoch 106:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.110, train_loss_epoch=4.680, valid_loss=4.490]\n","Epoch 107:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.900, train_loss_epoch=4.440, valid_loss=4.490]\n","Epoch 108:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.050, train_loss_epoch=4.480, valid_loss=4.490]\n","Epoch 109:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.790, train_loss_epoch=4.490, valid_loss=4.490]\n","Epoch 110:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.990, train_loss_epoch=4.570, valid_loss=4.490]\n","Epoch 111:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.700, train_loss_epoch=4.490, valid_loss=4.490]\n","Epoch 112:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.040, train_loss_epoch=4.660, valid_loss=4.490]\n","Epoch 112: 100%|██████████| 7/7 [00:00\u003c00:00, 39.31it/s, v_num=0, train_loss_step=4.800, train_loss_epoch=4.660, valid_loss=4.490]\n","Epoch 113:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.800, train_loss_epoch=4.670, valid_loss=4.490]\n","Epoch 114:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.980, train_loss_epoch=4.660, valid_loss=4.490]\n","\u001b[36m(_train_tune pid=8945)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8945)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.88it/s]\u001b[A\n","Epoch 114:   0%|          | 0/7 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=4.620, train_loss_epoch=4.660, valid_loss=4.460]\n","Epoch 115:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.470, train_loss_epoch=4.700, valid_loss=4.460]\n","Epoch 116:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.040, train_loss_epoch=4.810, valid_loss=4.460]\n","Epoch 117:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.170, train_loss_epoch=4.490, valid_loss=4.460]\n","Epoch 118:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.890, train_loss_epoch=4.560, valid_loss=4.460]\n","Epoch 119:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.540, train_loss_epoch=4.350, valid_loss=4.460]\n","Epoch 120:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.000, train_loss_epoch=4.420, valid_loss=4.460]\n","Epoch 121:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.240, train_loss_epoch=4.600, valid_loss=4.460]\n","Epoch 122:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.370, train_loss_epoch=4.660, valid_loss=4.460]\n","Epoch 123:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.180, train_loss_epoch=4.490, valid_loss=4.460]\n","Epoch 124:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.430, train_loss_epoch=4.360, valid_loss=4.460]\n","Epoch 125:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.120, train_loss_epoch=4.510, valid_loss=4.460]\n","Epoch 126:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.660, train_loss_epoch=4.570, valid_loss=4.460]\n","Epoch 127:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.870, train_loss_epoch=4.580, valid_loss=4.460]\n","Epoch 128:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.160, train_loss_epoch=4.750, valid_loss=4.460]\n","\u001b[36m(_train_tune pid=8945)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8945)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.90it/s]\u001b[A\n","Epoch 129:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.050, train_loss_epoch=4.440, valid_loss=4.410]\n","Epoch 130:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.310, train_loss_epoch=4.450, valid_loss=4.410]\n","Epoch 131:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.930, train_loss_epoch=4.490, valid_loss=4.410]\n","Epoch 132:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.230, train_loss_epoch=4.550, valid_loss=4.410]\n","Epoch 133:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.570, train_loss_epoch=4.510, valid_loss=4.410]\n","Epoch 134:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.520, train_loss_epoch=4.340, valid_loss=4.410]\n","Epoch 135:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.600, train_loss_epoch=4.660, valid_loss=4.410]\n","Epoch 136:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.760, train_loss_epoch=4.380, valid_loss=4.410]\n","Epoch 137:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.200, train_loss_epoch=4.400, valid_loss=4.410]\n","Epoch 138:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.390, train_loss_epoch=4.410, valid_loss=4.410]\n","Epoch 139:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.780, train_loss_epoch=4.270, valid_loss=4.410]\n","Epoch 140:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.110, train_loss_epoch=4.330, valid_loss=4.410]\n","Epoch 141:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.830, train_loss_epoch=4.450, valid_loss=4.410]\n","Epoch 142:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.810, train_loss_epoch=4.250, valid_loss=4.410]\n","\u001b[36m(_train_tune pid=8945)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8945)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.88it/s]\u001b[A\n","Epoch 143:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.700, train_loss_epoch=4.700, valid_loss=4.280]\n","Epoch 144:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.030, train_loss_epoch=5.250, valid_loss=4.280]\n","Epoch 145:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.030, train_loss_epoch=5.390, valid_loss=4.280]\n","Epoch 146:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.250, train_loss_epoch=5.400, valid_loss=4.280]\n","Epoch 147:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.920, train_loss_epoch=5.000, valid_loss=4.280]\n","Epoch 148:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.970, train_loss_epoch=4.630, valid_loss=4.280]\n","Epoch 149:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.080, train_loss_epoch=4.690, valid_loss=4.280]\n","Epoch 150:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.270, train_loss_epoch=4.520, valid_loss=4.280]\n","Epoch 151:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.470, train_loss_epoch=4.470, valid_loss=4.280]\n","Epoch 152:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.120, train_loss_epoch=4.490, valid_loss=4.280]\n","Epoch 153:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.850, train_loss_epoch=4.530, valid_loss=4.280]\n","Epoch 154:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.100, train_loss_epoch=4.500, valid_loss=4.280]\n","Epoch 155:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.900, train_loss_epoch=4.320, valid_loss=4.280]\n","Epoch 156:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.750, train_loss_epoch=4.440, valid_loss=4.280]\n","Epoch 157:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.660, train_loss_epoch=4.270, valid_loss=4.280]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8945)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.85it/s]\u001b[A\n","Epoch 157:   0%|          | 0/7 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=4.210, train_loss_epoch=4.270, valid_loss=4.190]\n","Epoch 158:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.370, train_loss_epoch=4.330, valid_loss=4.190]\n","Epoch 159:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.760, train_loss_epoch=4.220, valid_loss=4.190]\n","Epoch 160:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.980, train_loss_epoch=4.270, valid_loss=4.190]\n","Epoch 161:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.810, train_loss_epoch=4.510, valid_loss=4.190]\n","Epoch 161: 100%|██████████| 7/7 [00:00\u003c00:00, 39.55it/s, v_num=0, train_loss_step=4.450, train_loss_epoch=4.480, valid_loss=4.190]\n","Epoch 162:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.450, train_loss_epoch=4.480, valid_loss=4.190]\n","Epoch 163:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.520, train_loss_epoch=4.320, valid_loss=4.190]\n","Epoch 164:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.710, train_loss_epoch=4.420, valid_loss=4.190]\n","Epoch 165:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.290, train_loss_epoch=4.370, valid_loss=4.190]\n","Epoch 166:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.680, train_loss_epoch=4.540, valid_loss=4.190]\n","Epoch 167:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.870, train_loss_epoch=4.280, valid_loss=4.190]\n","Epoch 168:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.860, train_loss_epoch=4.410, valid_loss=4.190]\n","Epoch 169:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.810, train_loss_epoch=4.400, valid_loss=4.190]\n","Epoch 170:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.270, train_loss_epoch=4.320, valid_loss=4.190]\n","Epoch 171:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.120, train_loss_epoch=4.310, valid_loss=4.190]\n","\u001b[36m(_train_tune pid=8945)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8945)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.89it/s]\u001b[A\n","Epoch 171:   0%|          | 0/7 [00:01\u003c?, ?it/s, v_num=0, train_loss_step=4.320, train_loss_epoch=4.310, valid_loss=4.220]\n","Epoch 172:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.410, train_loss_epoch=4.330, valid_loss=4.220]\n","Epoch 173:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.060, train_loss_epoch=4.420, valid_loss=4.220]\n","Epoch 174:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.820, train_loss_epoch=4.380, valid_loss=4.220]\n","Epoch 175:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.320, train_loss_epoch=4.300, valid_loss=4.220]\n","Epoch 176:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.800, train_loss_epoch=4.230, valid_loss=4.220]\n","Epoch 177:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.610, train_loss_epoch=4.200, valid_loss=4.220]\n","Epoch 178:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.680, train_loss_epoch=4.350, valid_loss=4.220]\n","Epoch 179:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.380, train_loss_epoch=4.350, valid_loss=4.220]\n","Epoch 180:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.220, train_loss_epoch=4.190, valid_loss=4.220]\n","Epoch 181:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.710, train_loss_epoch=4.370, valid_loss=4.220]\n","Epoch 182:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.120, train_loss_epoch=4.110, valid_loss=4.220]\n","Epoch 183:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.630, train_loss_epoch=4.370, valid_loss=4.220]\n","Epoch 184:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.750, train_loss_epoch=4.220, valid_loss=4.220]\n","Epoch 185:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.750, train_loss_epoch=4.230, valid_loss=4.220]\n","\u001b[36m(_train_tune pid=8945)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8945)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.85it/s]\u001b[A\n","Epoch 186:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.650, train_loss_epoch=4.340, valid_loss=4.200]\n","Epoch 187:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.030, train_loss_epoch=4.450, valid_loss=4.200]\n","Epoch 188:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.480, train_loss_epoch=4.400, valid_loss=4.200]\n","Epoch 189:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.490, train_loss_epoch=4.360, valid_loss=4.200]\n","Epoch 190:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.560, train_loss_epoch=4.410, valid_loss=4.200]\n","Epoch 191:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.190, train_loss_epoch=4.330, valid_loss=4.200]\n","Epoch 192:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.850, train_loss_epoch=4.420, valid_loss=4.200]\n","Epoch 193:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.340, train_loss_epoch=4.170, valid_loss=4.200]\n","Epoch 194:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.680, train_loss_epoch=4.300, valid_loss=4.200]\n","Epoch 195:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.000, train_loss_epoch=4.270, valid_loss=4.200]\n","Epoch 196:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.240, train_loss_epoch=4.270, valid_loss=4.200]\n","Epoch 197:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.820, train_loss_epoch=4.190, valid_loss=4.200]\n","Epoch 198:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.000, train_loss_epoch=4.230, valid_loss=4.200]\n","Epoch 199:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.100, train_loss_epoch=4.250, valid_loss=4.200]\n","Epoch 199: 100%|██████████| 7/7 [00:00\u003c00:00, 40.29it/s, v_num=0, train_loss_step=4.100, train_loss_epoch=4.250, valid_loss=4.200]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-18 13:13:53,226\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (40, 20, 1), 'n_pool_kernel_size': (4, 4, 4), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=8945)\u001b[0m `Trainer.fit` stopped: `max_steps=1400.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=8945)\u001b[0m \n","\u001b[36m(_train_tune pid=8945)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 7/7 [00:01\u003c00:00,  3.84it/s]\u001b[A\n","\u001b[36m(_train_tune pid=8945)\u001b[0m \r                                                                      \u001b[A\rEpoch 199: 100%|██████████| 7/7 [00:02\u003c00:00,  3.47it/s, v_num=0, train_loss_step=4.100, train_loss_epoch=4.250, valid_loss=4.150]\rEpoch 199: 100%|██████████| 7/7 [00:02\u003c00:00,  3.47it/s, v_num=0, train_loss_step=4.100, train_loss_epoch=4.230, valid_loss=4.150]\rEpoch 199: 100%|██████████| 7/7 [00:02\u003c00:00,  3.46it/s, v_num=0, train_loss_step=4.100, train_loss_epoch=4.230, valid_loss=4.150]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=9309)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=9309)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=9309)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=9309)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=9309)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=9309)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=9309)\u001b[0m 2024-11-18 13:14:00.867880: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=9309)\u001b[0m 2024-11-18 13:14:00.888565: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=9309)\u001b[0m 2024-11-18 13:14:00.913644: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=9309)\u001b[0m 2024-11-18 13:14:00.921231: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=9309)\u001b[0m 2024-11-18 13:14:00.938989: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=9309)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=9309)\u001b[0m 2024-11-18 13:14:02.117807: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=9309)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=9309)\u001b[0m \n","\u001b[36m(_train_tune pid=9309)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=9309)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=9309)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=9309)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=9309)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=9309)\u001b[0m 3 | blocks       | ModuleList       | 2.6 M  | train\n","\u001b[36m(_train_tune pid=9309)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=9309)\u001b[0m 2.6 M     Trainable params\n","\u001b[36m(_train_tune pid=9309)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=9309)\u001b[0m 2.6 M     Total params\n","\u001b[36m(_train_tune pid=9309)\u001b[0m 10.356    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=9309)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=9309)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/7 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.18]\n","Epoch 2:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.54]\n","Epoch 3:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.52]\n","Epoch 4:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.50]\n","Epoch 5:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.60]\n","Epoch 6:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.57]\n","Epoch 7:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.64]\n","Epoch 8:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.59]\n","Epoch 9:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.74]\n","Epoch 9: 100%|██████████| 7/7 [00:00\u003c00:00, 37.57it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.64]\n","Epoch 10:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.64]\n","Epoch 11:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-1.77]\n","Epoch 12:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.65]\n","Epoch 13:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.66]\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.68]\n","\u001b[36m(_train_tune pid=9309)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9309)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.42it/s]\u001b[A\n","Epoch 14:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.68, valid_loss=-1.68]\n","Epoch 14: 100%|██████████| 7/7 [00:02\u003c00:00,  3.12it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.69, valid_loss=-1.68]\n","Epoch 15:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.69, valid_loss=-1.68]\n","Epoch 16:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.74, valid_loss=-1.68]\n","Epoch 17:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.68, valid_loss=-1.68]\n","Epoch 18:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.67, valid_loss=-1.68]\n","Epoch 19:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-1.78, valid_loss=-1.68]\n","Epoch 20:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.72, valid_loss=-1.68]\n","Epoch 20: 100%|██████████| 7/7 [00:00\u003c00:00, 39.81it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.71, valid_loss=-1.68]\n","Epoch 21:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.71, valid_loss=-1.68]\n","Epoch 22:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.69, valid_loss=-1.68]\n","Epoch 23:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.71, valid_loss=-1.68]\n","Epoch 24:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.69, valid_loss=-1.68]\n","Epoch 25:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.71, valid_loss=-1.68]\n","Epoch 26:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.76, valid_loss=-1.68]\n","Epoch 27:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.74, valid_loss=-1.68]\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.75, valid_loss=-1.68]\n","\u001b[36m(_train_tune pid=9309)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9309)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.38it/s]\u001b[A\n","Epoch 28:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.75, valid_loss=-1.71]\n","Epoch 29:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.917, train_loss_epoch=-1.61, valid_loss=-1.71]\n","Epoch 30:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.80, valid_loss=-1.71]\n","Epoch 31:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.69, valid_loss=-1.71]\n","Epoch 32:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.67, valid_loss=-1.71]\n","Epoch 33:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.75, valid_loss=-1.71]\n","Epoch 34:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.74, valid_loss=-1.71]\n","Epoch 35:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.77, valid_loss=-1.71]\n","Epoch 36:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.72, valid_loss=-1.71]\n","Epoch 37:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-1.81, valid_loss=-1.71]\n","Epoch 38:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.75, valid_loss=-1.71]\n","Epoch 39:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.75, valid_loss=-1.71]\n","Epoch 40:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.76, valid_loss=-1.71]\n","Epoch 41:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-1.79, valid_loss=-1.71]\n","Epoch 42:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.75, valid_loss=-1.71]\n","\u001b[36m(_train_tune pid=9309)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9309)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.44it/s]\u001b[A\n","Epoch 43:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.73, valid_loss=-1.72]\n","Epoch 44:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.74, valid_loss=-1.72]\n","Epoch 45:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.73, valid_loss=-1.72]\n","Epoch 46:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.73, valid_loss=-1.72]\n","Epoch 47:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.80, valid_loss=-1.72]\n","Epoch 48:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.73, valid_loss=-1.72]\n","Epoch 49:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.74, valid_loss=-1.72]\n","Epoch 50:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.71, valid_loss=-1.72]\n","Epoch 51:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.76, valid_loss=-1.72]\n","Epoch 52:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.73, valid_loss=-1.72]\n","Epoch 53:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.03, train_loss_epoch=-1.67, valid_loss=-1.72]\n","Epoch 54:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.75, valid_loss=-1.72]\n","Epoch 55:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.82, valid_loss=-1.72]\n","Epoch 56:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.80, valid_loss=-1.72]\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.07, train_loss_epoch=-1.65, valid_loss=-1.72]\n","\u001b[36m(_train_tune pid=9309)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9309)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.46it/s]\u001b[A\n","Epoch 57:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.65, valid_loss=-1.73]\n","Epoch 58:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-1.84, valid_loss=-1.73]\n","Epoch 59:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.73, valid_loss=-1.73]\n","Epoch 60:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.79, valid_loss=-1.73]\n","Epoch 61:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.70, valid_loss=-1.73]\n","Epoch 62:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.78, valid_loss=-1.73]\n","Epoch 63:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.73, valid_loss=-1.73]\n","Epoch 64:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.73, valid_loss=-1.73]\n","Epoch 65:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.69, valid_loss=-1.73]\n","Epoch 66:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.72, valid_loss=-1.73]\n","Epoch 67:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.76, valid_loss=-1.73]\n","Epoch 68:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.78, valid_loss=-1.73]\n","Epoch 68: 100%|██████████| 7/7 [00:00\u003c00:00, 41.32it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.78, valid_loss=-1.73]\n","Epoch 69:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.69, valid_loss=-1.73]\n","Epoch 70:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.73, valid_loss=-1.73]\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.763, train_loss_epoch=-1.62, valid_loss=-1.73]\n","\u001b[36m(_train_tune pid=9309)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-18 13:14:27,911\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (1, 1, 1), 'n_pool_kernel_size': (8, 4, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=9309)\u001b[0m `Trainer.fit` stopped: `max_steps=500.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=9309)\u001b[0m \n","\u001b[36m(_train_tune pid=9309)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.46it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9309)\u001b[0m \r                                                                      \u001b[A\rEpoch 71:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.62, valid_loss=-1.73] \rEpoch 71:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.66, valid_loss=-1.73]\rEpoch 71:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.66, valid_loss=-1.73]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=9517)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=9517)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=9517)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=9517)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=9517)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=9517)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=9517)\u001b[0m 2024-11-18 13:14:35.854054: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=9517)\u001b[0m 2024-11-18 13:14:35.874519: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=9517)\u001b[0m 2024-11-18 13:14:35.900178: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=9517)\u001b[0m 2024-11-18 13:14:35.907808: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=9517)\u001b[0m 2024-11-18 13:14:35.926085: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=9517)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=9517)\u001b[0m 2024-11-18 13:14:37.090711: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=9517)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=9517)\u001b[0m \n","\u001b[36m(_train_tune pid=9517)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=9517)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=9517)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=9517)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=9517)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=9517)\u001b[0m 3 | blocks       | ModuleList       | 2.6 M  | train\n","\u001b[36m(_train_tune pid=9517)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=9517)\u001b[0m 2.6 M     Trainable params\n","\u001b[36m(_train_tune pid=9517)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=9517)\u001b[0m 2.6 M     Total params\n","\u001b[36m(_train_tune pid=9517)\u001b[0m 10.313    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=9517)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=9517)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/7 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.786, train_loss_epoch=-0.273]\n","Epoch 2:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.01]\n","Epoch 3:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.17]\n","Epoch 4:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.869, train_loss_epoch=-1.16]\n","Epoch 5:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.33]\n","Epoch 6:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.34]\n","Epoch 7:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.38]\n","Epoch 8:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.951, train_loss_epoch=-1.26]\n","Epoch 9:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.45]\n","Epoch 10:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.42]\n","Epoch 11:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.55]\n","Epoch 12:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.44]\n","Epoch 13:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.46]\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.47]\n","\u001b[36m(_train_tune pid=9517)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9517)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.38it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9517)\u001b[0m \n","Epoch 14:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.47, valid_loss=-1.47]\n","Epoch 15:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.48, valid_loss=-1.47]\n","Epoch 16:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.54, valid_loss=-1.47]\n","Epoch 17:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.49, valid_loss=-1.47]\n","Epoch 18:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.48, valid_loss=-1.47]\n","Epoch 19:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.60, valid_loss=-1.47]\n","Epoch 19: 100%|██████████| 7/7 [00:00\u003c00:00, 41.36it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.60, valid_loss=-1.47]\n","Epoch 20:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.59, valid_loss=-1.47]\n","Epoch 21:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.60, valid_loss=-1.47]\n","Epoch 22:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.55, valid_loss=-1.47]\n","Epoch 23:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.52, valid_loss=-1.47]\n","Epoch 24:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.52, valid_loss=-1.47]\n","Epoch 25:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.54, valid_loss=-1.47]\n","Epoch 26:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.59, valid_loss=-1.47]\n","Epoch 27:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.58, valid_loss=-1.47]\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.60, valid_loss=-1.47]\n","\u001b[36m(_train_tune pid=9517)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9517)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.40it/s]\u001b[A\n","Epoch 28:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.60, valid_loss=-1.53]\n","Epoch 28: 100%|██████████| 7/7 [00:02\u003c00:00,  3.10it/s, v_num=0, train_loss_step=-0.748, train_loss_epoch=-1.60, valid_loss=-1.53]\n","Epoch 29:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.748, train_loss_epoch=-1.45, valid_loss=-1.53]\n","Epoch 30:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.64, valid_loss=-1.53]\n","Epoch 31:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.54, valid_loss=-1.53]\n","Epoch 32:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.54, valid_loss=-1.53]\n","Epoch 32:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.64, valid_loss=-1.53]        \n","Epoch 33:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.64, valid_loss=-1.53]\n","Epoch 34:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.62, valid_loss=-1.53]\n","Epoch 35:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.65, valid_loss=-1.53]\n","Epoch 36:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.61, valid_loss=-1.53]\n","Epoch 37:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-1.71, valid_loss=-1.53]\n","Epoch 38:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.65, valid_loss=-1.53]\n","Epoch 39:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.67, valid_loss=-1.53]\n","Epoch 40:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.62, valid_loss=-1.53]\n","Epoch 41:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.66, valid_loss=-1.53]\n","Epoch 42:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.63, valid_loss=-1.53]\n","\u001b[36m(_train_tune pid=9517)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9517)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.39it/s]\u001b[A\n","Epoch 43:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.67, valid_loss=-1.60]\n","Epoch 44:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.62, valid_loss=-1.60]\n","Epoch 44: 100%|██████████| 7/7 [00:00\u003c00:00, 38.16it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.62, valid_loss=-1.60]\n","Epoch 45:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.62, valid_loss=-1.60]\n","Epoch 46:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.65, valid_loss=-1.60]\n","Epoch 47:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.70, valid_loss=-1.60]\n","Epoch 48:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.37, train_loss_epoch=-1.60, valid_loss=-1.60]\n","Epoch 49:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.63, valid_loss=-1.60]\n","Epoch 50:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.09, train_loss_epoch=-1.60, valid_loss=-1.60]\n","Epoch 51:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.67, valid_loss=-1.60]\n","Epoch 52:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.65, valid_loss=-1.60]\n","Epoch 53:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.05, train_loss_epoch=-1.57, valid_loss=-1.60]\n","Epoch 54:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.62, valid_loss=-1.60]\n","Epoch 55:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.70, valid_loss=-1.60]\n","Epoch 56:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.65, valid_loss=-1.60]\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.97, train_loss_epoch=-1.55, valid_loss=-1.60]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9517)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.37it/s]\u001b[A\n","Epoch 57:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.55, valid_loss=-1.60]\n","Epoch 58:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.73, valid_loss=-1.60]\n","Epoch 59:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.60, valid_loss=-1.60]\n","Epoch 60:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.68, valid_loss=-1.60]\n","Epoch 61:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.60, valid_loss=-1.60]\n","Epoch 62:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.68, valid_loss=-1.60]\n","Epoch 63:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.63, valid_loss=-1.60]\n","Epoch 64:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.61, valid_loss=-1.60]\n","Epoch 65:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-1.58, valid_loss=-1.60]\n","Epoch 66:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.63, valid_loss=-1.60]\n","Epoch 67:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.65, valid_loss=-1.60]\n","Epoch 68:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.72, valid_loss=-1.60]\n","Epoch 69:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.63, valid_loss=-1.60]\n","Epoch 70:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.62, valid_loss=-1.60]\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.636, train_loss_epoch=-1.53, valid_loss=-1.60]\n","\u001b[36m(_train_tune pid=9517)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9517)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.37it/s]\u001b[A\n","Epoch 71:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.53, valid_loss=-1.64] \n","Epoch 72:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.67, valid_loss=-1.64]\n","Epoch 73:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.69, valid_loss=-1.64]\n","Epoch 74:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.70, valid_loss=-1.64]\n","Epoch 75:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.67, valid_loss=-1.64]\n","Epoch 76:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.68, valid_loss=-1.64]\n","Epoch 77:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.62, valid_loss=-1.64]\n","Epoch 78:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.74, valid_loss=-1.64]\n","Epoch 79:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.67, valid_loss=-1.64]\n","Epoch 80:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.64, valid_loss=-1.64]\n","Epoch 81:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.63, valid_loss=-1.64]\n","Epoch 82:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.69, valid_loss=-1.64]\n","Epoch 83:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.837, train_loss_epoch=-1.57, valid_loss=-1.64]\n","Epoch 84:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.64, valid_loss=-1.64]\n","Epoch 85:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.67, valid_loss=-1.64]\n","\u001b[36m(_train_tune pid=9517)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=9517)\u001b[0m `Trainer.fit` stopped: `max_steps=600.0` reached.\n","2024-11-18 13:15:08,097\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (1, 1, 1), 'n_pool_kernel_size': (16, 8, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=9517)\u001b[0m \n","\u001b[36m(_train_tune pid=9517)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.39it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9517)\u001b[0m \r                                                                      \u001b[A\rEpoch 85:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.67, valid_loss=-1.64]\rEpoch 85:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.70, valid_loss=-1.64]\rEpoch 85:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.70, valid_loss=-1.64]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=9743)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=9743)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=9743)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=9743)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=9743)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=9743)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=9743)\u001b[0m 2024-11-18 13:15:15.871451: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=9743)\u001b[0m 2024-11-18 13:15:15.892393: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=9743)\u001b[0m 2024-11-18 13:15:15.917888: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=9743)\u001b[0m 2024-11-18 13:15:15.925617: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=9743)\u001b[0m 2024-11-18 13:15:15.944269: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=9743)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=9743)\u001b[0m 2024-11-18 13:15:17.097327: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=9743)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=9743)\u001b[0m \n","\u001b[36m(_train_tune pid=9743)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=9743)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=9743)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=9743)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=9743)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=9743)\u001b[0m 3 | blocks       | ModuleList       | 2.8 M  | train\n","\u001b[36m(_train_tune pid=9743)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=9743)\u001b[0m 2.8 M     Trainable params\n","\u001b[36m(_train_tune pid=9743)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=9743)\u001b[0m 2.8 M     Total params\n","\u001b[36m(_train_tune pid=9743)\u001b[0m 11.288    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=9743)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=9743)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/7 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.06, train_loss_epoch=1.490]\n","Epoch 2:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.54]\n","Epoch 3:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.55]\n","Epoch 4:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.59]\n","Epoch 5:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-1.68]\n","Epoch 6:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.67]\n","Epoch 7:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.67]\n","Epoch 8:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.66]\n","Epoch 9:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.74]\n","Epoch 10:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.63]\n","Epoch 11:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-1.76]\n","Epoch 12:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.67]\n","Epoch 13:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.69]\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.65]\n","\u001b[36m(_train_tune pid=9743)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9743)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 37.28it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9743)\u001b[0m \n","Epoch 14: 100%|██████████| 7/7 [00:00\u003c00:00, 19.76it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.70, valid_loss=-1.75]\n","Epoch 15:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.70, valid_loss=-1.75]\n","Epoch 16:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-1.77, valid_loss=-1.75]\n","Epoch 17:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.66, valid_loss=-1.75]\n","Epoch 18:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.74, valid_loss=-1.75]\n","Epoch 19:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-1.82, valid_loss=-1.75]\n","Epoch 20:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.71, valid_loss=-1.75]\n","Epoch 21:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.73, valid_loss=-1.75]\n","Epoch 22:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.74, valid_loss=-1.75]\n","Epoch 23:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.76, valid_loss=-1.75]\n","Epoch 24:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.74, valid_loss=-1.75]\n","Epoch 25:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.78, valid_loss=-1.75]\n","Epoch 26:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-1.83, valid_loss=-1.75]\n","Epoch 27:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.79, valid_loss=-1.75]\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.80, valid_loss=-1.75]\n","\u001b[36m(_train_tune pid=9743)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9743)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 42.69it/s]\u001b[A\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.80, valid_loss=-1.81]\n","Epoch 29:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.09, train_loss_epoch=-1.72, valid_loss=-1.81]\n","Epoch 30:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-1.87, valid_loss=-1.81]\n","Epoch 31:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.77, valid_loss=-1.81]\n","Epoch 32:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.76, valid_loss=-1.81]\n","Epoch 33:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.82, valid_loss=-1.81]\n","Epoch 34:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.75, valid_loss=-1.81]\n","Epoch 35:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.87, valid_loss=-1.81]\n","Epoch 36:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.83, valid_loss=-1.81]\n","Epoch 37:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-1.88, valid_loss=-1.81]\n","Epoch 38:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.80, valid_loss=-1.81]\n","Epoch 39:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.86, valid_loss=-1.81]\n","Epoch 40:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.90, valid_loss=-1.81]\n","Epoch 41:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-1.96, valid_loss=-1.81]\n","Epoch 42:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.89, valid_loss=-1.81]\n","\u001b[36m(_train_tune pid=9743)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9743)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 39.92it/s]\u001b[A\n","Epoch 43:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.85, valid_loss=-1.88]\n","Epoch 44:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.87, valid_loss=-1.88]\n","Epoch 45:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.81, valid_loss=-1.88]\n","Epoch 46:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.82, valid_loss=-1.88]\n","Epoch 47:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-1.86, valid_loss=-1.88]\n","Epoch 48:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.82, valid_loss=-1.88]\n","Epoch 49:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.86, valid_loss=-1.88]\n","Epoch 50:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.74, valid_loss=-1.88]\n","Epoch 51:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.85, valid_loss=-1.88]\n","Epoch 52:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.86, valid_loss=-1.88]\n","Epoch 52: 100%|██████████| 7/7 [00:00\u003c00:00, 50.82it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.77, valid_loss=-1.88]\n","Epoch 53:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.77, valid_loss=-1.88]\n","Epoch 54:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.84, valid_loss=-1.88]\n","Epoch 55:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.90, valid_loss=-1.88]\n","Epoch 56:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-1.88, valid_loss=-1.88]\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.74, valid_loss=-1.88]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9743)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 43.53it/s]\u001b[A\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.74, valid_loss=-1.87]\n","Epoch 58:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-1.90, valid_loss=-1.87]\n","Epoch 59:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.84, valid_loss=-1.87]\n","Epoch 60:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.90, valid_loss=-1.87]\n","Epoch 61:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.86, valid_loss=-1.87]\n","Epoch 62:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.92, valid_loss=-1.87]\n","Epoch 63:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.87, valid_loss=-1.87]\n","Epoch 64:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.87, valid_loss=-1.87]\n","Epoch 65:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.88, valid_loss=-1.87]\n","Epoch 66:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.91, valid_loss=-1.87]\n","Epoch 67:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.94, valid_loss=-1.87]\n","Epoch 68:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.93, valid_loss=-1.87]\n","Epoch 69:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.87, valid_loss=-1.87]\n","Epoch 70:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.86, valid_loss=-1.87]\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.00, train_loss_epoch=-1.81, valid_loss=-1.87]\n","\u001b[36m(_train_tune pid=9743)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9743)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 39.30it/s]\u001b[A\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.81, valid_loss=-1.92]\n","Epoch 72:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.95, valid_loss=-1.92]\n","Epoch 73:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.95, valid_loss=-1.92]\n","Epoch 74:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-1.97, valid_loss=-1.92]\n","Epoch 75:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.97, valid_loss=-1.92]\n","Epoch 76:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.94, valid_loss=-1.92]\n","Epoch 77:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.91, valid_loss=-1.92]\n","Epoch 78:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-1.96, valid_loss=-1.92]\n","Epoch 79:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.94, valid_loss=-1.92]\n","Epoch 80:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-1.92, valid_loss=-1.92]\n","Epoch 81:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.91, valid_loss=-1.92]\n","Epoch 82:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.91, valid_loss=-1.92]\n","Epoch 83:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.06, train_loss_epoch=-1.83, valid_loss=-1.92]\n","Epoch 84:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.92, valid_loss=-1.92]\n","Epoch 85:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.91, valid_loss=-1.92]\n","\u001b[36m(_train_tune pid=9743)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9743)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 41.50it/s]\u001b[A\n","Epoch 86:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.88, valid_loss=-1.86]\n","Epoch 87:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.50, train_loss_epoch=-1.97, valid_loss=-1.86]\n","Epoch 88:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-1.98, valid_loss=-1.86]\n","Epoch 89:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.96, valid_loss=-1.86]\n","Epoch 90:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.90, valid_loss=-1.86]\n","Epoch 91:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-1.98, valid_loss=-1.86]\n","Epoch 92:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.92, valid_loss=-1.86]\n","Epoch 93:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.93, valid_loss=-1.86]\n","Epoch 94:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.94, valid_loss=-1.86]\n","Epoch 95:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.89, valid_loss=-1.86]\n","Epoch 96:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-1.98, valid_loss=-1.86]\n","Epoch 97:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.00, valid_loss=-1.86]\n","Epoch 98:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.00, valid_loss=-1.86]\n","Epoch 99:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.92, valid_loss=-1.86]\n","Epoch 99: 100%|██████████| 7/7 [00:00\u003c00:00, 50.06it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-1.92, valid_loss=-1.86]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9743)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 38.40it/s]\u001b[A\n","Epoch 100:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-1.99, valid_loss=-1.92]\n","Epoch 101:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.94, valid_loss=-1.92]\n","Epoch 102:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.93, valid_loss=-1.92]\n","Epoch 103:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.99, valid_loss=-1.92]\n","Epoch 104:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-1.94, valid_loss=-1.92]\n","Epoch 105:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.85, valid_loss=-1.92]\n","Epoch 106:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.85, valid_loss=-1.92]\n","Epoch 107:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.95, valid_loss=-1.92]\n","Epoch 108:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-1.97, valid_loss=-1.92]\n","Epoch 109:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.94, valid_loss=-1.92]\n","Epoch 110:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.84, valid_loss=-1.92]\n","Epoch 111:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.94, valid_loss=-1.92]\n","Epoch 112:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-1.99, valid_loss=-1.92]\n","Epoch 113:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.97, valid_loss=-1.92]\n","Epoch 114:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.46, train_loss_epoch=-1.99, valid_loss=-1.92]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9743)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 43.04it/s]\u001b[A\n","Epoch 114:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.99, valid_loss=-1.92]\n","Epoch 115:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.90, valid_loss=-1.92]\n","Epoch 116:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.84, valid_loss=-1.92]\n","Epoch 117:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-1.99, valid_loss=-1.92]\n","Epoch 118:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.92, valid_loss=-1.92]\n","Epoch 119:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.93, valid_loss=-1.92]\n","Epoch 120:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.96, valid_loss=-1.92]\n","Epoch 121:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.95, valid_loss=-1.92]\n","Epoch 122:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.92, valid_loss=-1.92]\n","Epoch 123:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.95, valid_loss=-1.92]\n","Epoch 124:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.97, valid_loss=-1.92]\n","Epoch 125:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.02, valid_loss=-1.92]\n","Epoch 126:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.98, valid_loss=-1.92]\n","Epoch 127:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.87, valid_loss=-1.92]\n","Epoch 128:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.95, valid_loss=-1.92]\n","\u001b[36m(_train_tune pid=9743)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9743)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 41.78it/s]\u001b[A\n","Epoch 128:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.95, valid_loss=-1.93]\n","Epoch 129:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.91, valid_loss=-1.93]\n","Epoch 130:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.92, valid_loss=-1.93]\n","Epoch 131:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.00, valid_loss=-1.93]\n","Epoch 132:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.94, valid_loss=-1.93]\n","Epoch 133:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.92, valid_loss=-1.93]\n","Epoch 134:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.57, train_loss_epoch=-2.04, valid_loss=-1.93]\n","Epoch 135:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.91, valid_loss=-1.93]\n","Epoch 136:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.36, train_loss_epoch=-2.02, valid_loss=-1.93]\n","Epoch 137:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.98, valid_loss=-1.93]\n","Epoch 138:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.88, valid_loss=-1.93]\n","Epoch 139:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.96, valid_loss=-1.93]\n","Epoch 140:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.89, valid_loss=-1.93]\n","Epoch 141:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.98, valid_loss=-1.93]\n","Epoch 142:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.96, valid_loss=-1.93]\n","\u001b[36m(_train_tune pid=9743)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9743)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 38.77it/s]\u001b[A\n","Epoch 142:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.96, valid_loss=-1.92]\n","Epoch 143:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.95, valid_loss=-1.92]\n","Epoch 144:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.97, valid_loss=-1.92]\n","Epoch 145:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.00, valid_loss=-1.92]\n","Epoch 146:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.92, valid_loss=-1.92]\n","Epoch 147:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-2.01, valid_loss=-1.92]\n","Epoch 148:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-2.03, valid_loss=-1.92]\n","Epoch 149:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.91, valid_loss=-1.92]\n","Epoch 150:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.91, valid_loss=-1.92]\n","Epoch 151:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.89, valid_loss=-1.92]\n","Epoch 152:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.61, train_loss_epoch=-2.07, valid_loss=-1.92]\n","Epoch 153:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.87, valid_loss=-1.92]\n","Epoch 154:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.94, valid_loss=-1.92]\n","Epoch 155:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.00, valid_loss=-1.92]\n","Epoch 156:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.95, valid_loss=-1.92]\n","Epoch 157:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.49, train_loss_epoch=-2.05, valid_loss=-1.92]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9743)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 38.04it/s]\u001b[A\n","Epoch 157:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-2.05, valid_loss=-1.94]\n","Epoch 158:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.93, valid_loss=-1.94]\n","Epoch 159:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.94, valid_loss=-1.94]\n","Epoch 160:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.97, valid_loss=-1.94]\n","Epoch 161:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.91, valid_loss=-1.94]\n","Epoch 162:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.95, valid_loss=-1.94]\n","Epoch 163:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.62, train_loss_epoch=-2.05, valid_loss=-1.94]\n","Epoch 164:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.87, valid_loss=-1.94]\n","Epoch 165:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.96, valid_loss=-1.94]\n","Epoch 166:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.01, valid_loss=-1.94]\n","Epoch 167:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.91, valid_loss=-1.94]\n","Epoch 168:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.98, valid_loss=-1.94]\n","Epoch 169:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.94, valid_loss=-1.94]\n","Epoch 170:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.56, train_loss_epoch=-2.07, valid_loss=-1.94]\n","Epoch 171:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.97, valid_loss=-1.94]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=9743)\u001b[0m `Trainer.fit` stopped: `max_steps=1200.0` reached.\n","2024-11-18 13:15:46,166\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (180, 60, 1), 'n_pool_kernel_size': (4, 4, 4), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=9743)\u001b[0m \n","\u001b[36m(_train_tune pid=9743)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 38.67it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9743)\u001b[0m \r                                                                      \u001b[A\rEpoch 171:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.97, valid_loss=-1.94]\rEpoch 171:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.95, valid_loss=-1.94]\rEpoch 171:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.95, valid_loss=-1.94]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=9957)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=9957)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=9957)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=9957)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=9957)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=9957)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=9957)\u001b[0m 2024-11-18 13:15:53.820142: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=9957)\u001b[0m 2024-11-18 13:15:53.839982: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=9957)\u001b[0m 2024-11-18 13:15:53.864902: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=9957)\u001b[0m 2024-11-18 13:15:53.872516: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=9957)\u001b[0m 2024-11-18 13:15:53.890094: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=9957)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=9957)\u001b[0m 2024-11-18 13:15:55.040845: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=9957)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=9957)\u001b[0m \n","\u001b[36m(_train_tune pid=9957)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=9957)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=9957)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=9957)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=9957)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=9957)\u001b[0m 3 | blocks       | ModuleList       | 2.8 M  | train\n","\u001b[36m(_train_tune pid=9957)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=9957)\u001b[0m 2.8 M     Trainable params\n","\u001b[36m(_train_tune pid=9957)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=9957)\u001b[0m 2.8 M     Total params\n","\u001b[36m(_train_tune pid=9957)\u001b[0m 11.257    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=9957)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=9957)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/4 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=14.20, train_loss_epoch=40.50]\n","Epoch 2:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=14.80, train_loss_epoch=15.30]\n","Epoch 3:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=16.50, train_loss_epoch=16.10]\n","Epoch 4:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=15.10, train_loss_epoch=15.90]\n","Epoch 5:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=16.30, train_loss_epoch=16.50]\n","Epoch 6:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=15.90, train_loss_epoch=15.80]\n","Epoch 7:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=32.50, train_loss_epoch=19.50]\n","Epoch 8:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=15.30, train_loss_epoch=15.80]\n","Epoch 9:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=16.50, train_loss_epoch=18.00]\n","Epoch 10:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=16.20, train_loss_epoch=17.10]\n","Epoch 11:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=16.00, train_loss_epoch=17.50]\n","Epoch 12:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=15.00, train_loss_epoch=14.90]\n","Epoch 13:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.00, train_loss_epoch=13.50]\n","Epoch 14:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.00, train_loss_epoch=13.40]\n","Epoch 15:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.60, train_loss_epoch=13.80]\n","Epoch 16:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.30, train_loss_epoch=13.60]\n","Epoch 17:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.50, train_loss_epoch=13.20]\n","Epoch 18:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.90, train_loss_epoch=12.70]\n","Epoch 19:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.20, train_loss_epoch=11.80]\n","Epoch 20:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=14.50, train_loss_epoch=12.10]\n","Epoch 21:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.90, train_loss_epoch=11.40]\n","Epoch 22:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.90, train_loss_epoch=12.30]\n","Epoch 23:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.10, train_loss_epoch=12.50]\n","Epoch 24:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.40, train_loss_epoch=12.20]\n","Epoch 24: 100%|██████████| 4/4 [00:00\u003c00:00, 38.26it/s, v_num=0, train_loss_step=11.00, train_loss_epoch=12.20]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9957)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9957)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 25.53it/s]\u001b[A\n","Epoch 25:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.00, train_loss_epoch=11.40, valid_loss=11.00]\n","Epoch 26:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.80, train_loss_epoch=11.00, valid_loss=11.00]\n","Epoch 27:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.20, train_loss_epoch=11.10, valid_loss=11.00]\n","Epoch 28:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.20, train_loss_epoch=10.90, valid_loss=11.00]\n","Epoch 29:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.50, train_loss_epoch=10.40, valid_loss=11.00]\n","Epoch 30:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.90, train_loss_epoch=17.90, valid_loss=11.00]\n","Epoch 31:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=15.70, train_loss_epoch=18.40, valid_loss=11.00]\n","Epoch 32:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=19.90, train_loss_epoch=18.00, valid_loss=11.00]\n","Epoch 33:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=18.90, train_loss_epoch=18.30, valid_loss=11.00]\n","Epoch 34:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=17.60, train_loss_epoch=18.00, valid_loss=11.00]\n","Epoch 35:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=16.60, train_loss_epoch=17.40, valid_loss=11.00]\n","Epoch 36:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=16.10, train_loss_epoch=16.90, valid_loss=11.00]\n","Epoch 37:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=14.90, train_loss_epoch=15.50, valid_loss=11.00]\n","Epoch 38:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=14.40, train_loss_epoch=14.80, valid_loss=11.00]\n","Epoch 39:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=14.70, train_loss_epoch=14.60, valid_loss=11.00]\n","Epoch 40:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=14.00, train_loss_epoch=14.20, valid_loss=11.00]\n","Epoch 41:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.40, train_loss_epoch=13.60, valid_loss=11.00]\n","Epoch 42:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.00, train_loss_epoch=12.50, valid_loss=11.00]\n","Epoch 43:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.60, train_loss_epoch=12.90, valid_loss=11.00]\n","Epoch 44:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.50, train_loss_epoch=12.90, valid_loss=11.00]\n","Epoch 45:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.00, train_loss_epoch=12.40, valid_loss=11.00]\n","Epoch 46:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.40, train_loss_epoch=11.60, valid_loss=11.00]\n","Epoch 47:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.50, train_loss_epoch=12.20, valid_loss=11.00]\n","Epoch 48:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.90, train_loss_epoch=12.70, valid_loss=11.00]\n","Epoch 49:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.70, train_loss_epoch=13.30, valid_loss=11.00]\n","Epoch 49: 100%|██████████| 4/4 [00:00\u003c00:00, 35.18it/s, v_num=0, train_loss_step=12.80, train_loss_epoch=13.30, valid_loss=11.00]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9957)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 25.56it/s]\u001b[A\n","Epoch 50:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.80, train_loss_epoch=13.10, valid_loss=12.90]\n","Epoch 51:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=14.30, train_loss_epoch=13.30, valid_loss=12.90]\n","Epoch 52:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.70, train_loss_epoch=12.90, valid_loss=12.90]\n","Epoch 53:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.90, train_loss_epoch=11.90, valid_loss=12.90]\n","Epoch 54:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.10, train_loss_epoch=12.00, valid_loss=12.90]\n","Epoch 55:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=15.50, train_loss_epoch=12.40, valid_loss=12.90]\n","Epoch 56:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.10, train_loss_epoch=13.60, valid_loss=12.90]\n","Epoch 57:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.80, train_loss_epoch=12.30, valid_loss=12.90]\n","Epoch 58:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.80, train_loss_epoch=12.10, valid_loss=12.90]\n","Epoch 59:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.20, train_loss_epoch=11.40, valid_loss=12.90]\n","Epoch 60:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.00, train_loss_epoch=11.90, valid_loss=12.90]\n","Epoch 61:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.50, train_loss_epoch=12.70, valid_loss=12.90]\n","Epoch 62:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.10, train_loss_epoch=13.10, valid_loss=12.90]\n","Epoch 63:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.80, train_loss_epoch=13.00, valid_loss=12.90]\n","Epoch 64:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.80, train_loss_epoch=12.90, valid_loss=12.90]\n","Epoch 65:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.50, train_loss_epoch=13.00, valid_loss=12.90]\n","Epoch 66:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.10, train_loss_epoch=12.50, valid_loss=12.90]\n","Epoch 67:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.10, train_loss_epoch=12.50, valid_loss=12.90]\n","Epoch 68:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.20, train_loss_epoch=11.90, valid_loss=12.90]\n","Epoch 69:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.60, train_loss_epoch=11.00, valid_loss=12.90]\n","Epoch 70:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.30, train_loss_epoch=11.90, valid_loss=12.90]\n","Epoch 71:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=15.30, train_loss_epoch=13.60, valid_loss=12.90]\n","Epoch 72:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.10, train_loss_epoch=13.30, valid_loss=12.90]\n","Epoch 73:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=14.60, train_loss_epoch=13.70, valid_loss=12.90]\n","Epoch 74:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.40, train_loss_epoch=13.30, valid_loss=12.90]\n","Epoch 74: 100%|██████████| 4/4 [00:00\u003c00:00, 37.68it/s, v_num=0, train_loss_step=12.70, train_loss_epoch=13.30, valid_loss=12.90]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9957)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 27.61it/s]\u001b[A\n","Epoch 75:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.70, train_loss_epoch=12.90, valid_loss=12.60]\n","Epoch 76:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.50, train_loss_epoch=12.70, valid_loss=12.60]\n","Epoch 77:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.60, train_loss_epoch=12.60, valid_loss=12.60]\n","Epoch 78:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.60, train_loss_epoch=12.50, valid_loss=12.60]\n","Epoch 79:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.50, train_loss_epoch=12.00, valid_loss=12.60]\n","Epoch 80:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.20, train_loss_epoch=12.30, valid_loss=12.60]\n","Epoch 81:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.50, train_loss_epoch=12.30, valid_loss=12.60]\n","Epoch 82:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.10, train_loss_epoch=11.20, valid_loss=12.60]\n","Epoch 83:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=19.90, train_loss_epoch=14.70, valid_loss=12.60]\n","Epoch 84:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=18.80, train_loss_epoch=16.80, valid_loss=12.60]\n","Epoch 85:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=17.90, train_loss_epoch=17.80, valid_loss=12.60]\n","Epoch 86:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=15.90, train_loss_epoch=17.20, valid_loss=12.60]\n","Epoch 87:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=14.90, train_loss_epoch=15.10, valid_loss=12.60]\n","Epoch 88:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=14.30, train_loss_epoch=14.80, valid_loss=12.60]\n","Epoch 89:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=14.40, train_loss_epoch=14.70, valid_loss=12.60]\n","Epoch 90:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=15.00, train_loss_epoch=14.70, valid_loss=12.60]\n","Epoch 91:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=14.20, train_loss_epoch=14.40, valid_loss=12.60]\n","Epoch 92:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=14.40, train_loss_epoch=14.20, valid_loss=12.60]\n","Epoch 93:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.80, train_loss_epoch=13.80, valid_loss=12.60]\n","Epoch 94:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.00, train_loss_epoch=13.30, valid_loss=12.60]\n","Epoch 95:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.70, train_loss_epoch=13.20, valid_loss=12.60]\n","Epoch 96:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.20, train_loss_epoch=13.10, valid_loss=12.60]\n","Epoch 97:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.60, train_loss_epoch=13.00, valid_loss=12.60]\n","Epoch 98:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.80, train_loss_epoch=12.10, valid_loss=12.60]\n","Epoch 99:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.10, train_loss_epoch=12.00, valid_loss=12.60]\n","Epoch 99: 100%|██████████| 4/4 [00:00\u003c00:00, 38.07it/s, v_num=0, train_loss_step=11.10, train_loss_epoch=12.00, valid_loss=12.60]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9957)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 27.51it/s]\u001b[A\n","Epoch 100:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.10, train_loss_epoch=11.50, valid_loss=11.10]\n","Epoch 101:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.40, train_loss_epoch=10.80, valid_loss=11.10]\n","Epoch 102:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.30, train_loss_epoch=10.70, valid_loss=11.10]\n","Epoch 103:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=14.90, train_loss_epoch=17.00, valid_loss=11.10]\n","Epoch 104:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=19.00, train_loss_epoch=17.00, valid_loss=11.10]\n","Epoch 105:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=22.20, train_loss_epoch=18.10, valid_loss=11.10]\n","Epoch 106:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=19.20, train_loss_epoch=17.70, valid_loss=11.10]\n","Epoch 107:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=16.50, train_loss_epoch=17.00, valid_loss=11.10]\n","Epoch 108:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=17.20, train_loss_epoch=16.80, valid_loss=11.10]\n","Epoch 109:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=16.10, train_loss_epoch=16.40, valid_loss=11.10]\n","Epoch 110:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=18.00, train_loss_epoch=16.60, valid_loss=11.10]\n","Epoch 111:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=14.40, train_loss_epoch=21.00, valid_loss=11.10]\n","Epoch 112:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=14.40, train_loss_epoch=14.70, valid_loss=11.10]\n","Epoch 113:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=15.50, train_loss_epoch=14.50, valid_loss=11.10]\n","Epoch 114:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=18.10, train_loss_epoch=17.80, valid_loss=11.10]\n","Epoch 115:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=16.10, train_loss_epoch=17.50, valid_loss=11.10]\n","Epoch 116:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=14.30, train_loss_epoch=15.10, valid_loss=11.10]\n","Epoch 117:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.90, train_loss_epoch=14.90, valid_loss=11.10]\n","Epoch 118:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=16.40, train_loss_epoch=15.40, valid_loss=11.10]\n","Epoch 119:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=14.50, train_loss_epoch=14.90, valid_loss=11.10]\n","Epoch 120:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=14.80, train_loss_epoch=14.60, valid_loss=11.10]\n","Epoch 120: 100%|██████████| 4/4 [00:00\u003c00:00, 38.18it/s, v_num=0, train_loss_step=13.20, train_loss_epoch=14.60, valid_loss=11.10]\n","Epoch 121:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.20, train_loss_epoch=13.90, valid_loss=11.10]\n","Epoch 121: 100%|██████████| 4/4 [00:00\u003c00:00, 38.26it/s, v_num=0, train_loss_step=13.20, train_loss_epoch=13.90, valid_loss=11.10]\n","Epoch 122:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.10, train_loss_epoch=13.80, valid_loss=11.10]\n","Epoch 122: 100%|██████████| 4/4 [00:00\u003c00:00, 37.77it/s, v_num=0, train_loss_step=14.30, train_loss_epoch=13.80, valid_loss=11.10]\n","Epoch 123:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=14.30, train_loss_epoch=14.00, valid_loss=11.10]\n","Epoch 124:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=14.70, train_loss_epoch=14.00, valid_loss=11.10]\n","Epoch 124: 100%|██████████| 4/4 [00:00\u003c00:00, 37.61it/s, v_num=0, train_loss_step=13.10, train_loss_epoch=14.00, valid_loss=11.10]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9957)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 25.91it/s]\u001b[A\n","Epoch 125:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.10, train_loss_epoch=13.50, valid_loss=13.40]\n","Epoch 126:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.50, train_loss_epoch=13.60, valid_loss=13.40]\n","Epoch 127:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=15.20, train_loss_epoch=13.80, valid_loss=13.40]\n","Epoch 128:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.70, train_loss_epoch=13.10, valid_loss=13.40]\n","Epoch 129:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.10, train_loss_epoch=12.80, valid_loss=13.40]\n","Epoch 129:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.50, train_loss_epoch=12.70, valid_loss=13.40]        \n","Epoch 130:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.50, train_loss_epoch=12.70, valid_loss=13.40]\n","Epoch 131:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.90, train_loss_epoch=12.30, valid_loss=13.40]\n","Epoch 132:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.30, train_loss_epoch=11.70, valid_loss=13.40]\n","Epoch 133:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.30, train_loss_epoch=12.00, valid_loss=13.40]\n","Epoch 134:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.80, train_loss_epoch=12.30, valid_loss=13.40]\n","Epoch 135:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.80, train_loss_epoch=11.70, valid_loss=13.40]\n","Epoch 136:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.80, train_loss_epoch=12.90, valid_loss=13.40]\n","Epoch 137:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=15.20, train_loss_epoch=14.10, valid_loss=13.40]\n","Epoch 138:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.00, train_loss_epoch=13.60, valid_loss=13.40]\n","Epoch 139:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.10, train_loss_epoch=13.50, valid_loss=13.40]\n","Epoch 140:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.20, train_loss_epoch=13.20, valid_loss=13.40]\n","Epoch 141:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.00, train_loss_epoch=12.70, valid_loss=13.40]\n","Epoch 142:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.00, train_loss_epoch=12.50, valid_loss=13.40]\n","Epoch 143:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=13.00, train_loss_epoch=12.60, valid_loss=13.40]\n","Epoch 144:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.20, train_loss_epoch=11.90, valid_loss=13.40]\n","Epoch 145:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=14.10, train_loss_epoch=12.30, valid_loss=13.40]\n","Epoch 146:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.90, train_loss_epoch=11.10, valid_loss=13.40]\n","Epoch 147:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.760, train_loss_epoch=10.30, valid_loss=13.40]\n","Epoch 148:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.670, train_loss_epoch=9.570, valid_loss=13.40]\n","Epoch 149:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.200, train_loss_epoch=9.130, valid_loss=13.40]\n","Epoch 149: 100%|██████████| 4/4 [00:00\u003c00:00, 35.07it/s, v_num=0, train_loss_step=8.560, train_loss_epoch=9.130, valid_loss=13.40]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9957)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9957)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 25.73it/s]\u001b[A\n","Epoch 150:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.560, train_loss_epoch=8.850, valid_loss=8.380]\n","Epoch 151:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=21.90, train_loss_epoch=11.60, valid_loss=8.380]\n","Epoch 152:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.970, train_loss_epoch=9.680, valid_loss=8.380]\n","Epoch 153:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.50, train_loss_epoch=10.60, valid_loss=8.380]\n","Epoch 154:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.10, train_loss_epoch=11.20, valid_loss=8.380]\n","Epoch 155:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=12.30, train_loss_epoch=11.30, valid_loss=8.380]\n","Epoch 156:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.50, train_loss_epoch=11.10, valid_loss=8.380]\n","Epoch 157:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.60, train_loss_epoch=10.90, valid_loss=8.380]\n","Epoch 158:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.00, train_loss_epoch=11.00, valid_loss=8.380]\n","Epoch 159:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.10, train_loss_epoch=10.70, valid_loss=8.380]\n","Epoch 160:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.80, train_loss_epoch=10.90, valid_loss=8.380]\n","Epoch 161:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.50, train_loss_epoch=10.80, valid_loss=8.380]\n","Epoch 162:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.60, train_loss_epoch=11.00, valid_loss=8.380]\n","Epoch 163:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.10, train_loss_epoch=10.60, valid_loss=8.380]\n","Epoch 164:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.50, train_loss_epoch=10.90, valid_loss=8.380]\n","Epoch 165:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.970, train_loss_epoch=10.50, valid_loss=8.380]\n","Epoch 166:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.00, train_loss_epoch=10.40, valid_loss=8.380]\n","Epoch 167:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.30, train_loss_epoch=10.70, valid_loss=8.380]\n","Epoch 168:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.70, train_loss_epoch=10.50, valid_loss=8.380]\n","Epoch 168: 100%|██████████| 4/4 [00:00\u003c00:00, 39.24it/s, v_num=0, train_loss_step=10.10, train_loss_epoch=10.50, valid_loss=8.380]\n","Epoch 169:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.10, train_loss_epoch=10.30, valid_loss=8.380]\n","Epoch 170:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.060, train_loss_epoch=9.950, valid_loss=8.380]\n","Epoch 171:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.010, train_loss_epoch=9.820, valid_loss=8.380]\n","Epoch 172:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.220, train_loss_epoch=9.700, valid_loss=8.380]\n","Epoch 172: 100%|██████████| 4/4 [00:00\u003c00:00, 38.32it/s, v_num=0, train_loss_step=9.220, train_loss_epoch=9.700, valid_loss=8.380]\n","Epoch 173:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.150, train_loss_epoch=9.400, valid_loss=8.380]\n","Epoch 174:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.140, train_loss_epoch=9.330, valid_loss=8.380]\n","Epoch 174: 100%|██████████| 4/4 [00:00\u003c00:00, 38.36it/s, v_num=0, train_loss_step=8.290, train_loss_epoch=9.330, valid_loss=8.380]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9957)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 26.67it/s]\u001b[A\n","Epoch 175:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.290, train_loss_epoch=8.830, valid_loss=8.700]\n","Epoch 176:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.430, train_loss_epoch=8.680, valid_loss=8.700]\n","Epoch 177:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.450, train_loss_epoch=8.280, valid_loss=8.700]\n","Epoch 178:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.340, train_loss_epoch=8.700, valid_loss=8.700]\n","Epoch 179:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.930, train_loss_epoch=9.350, valid_loss=8.700]\n","Epoch 180:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.10, train_loss_epoch=9.910, valid_loss=8.700]\n","Epoch 181:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.550, train_loss_epoch=9.810, valid_loss=8.700]\n","Epoch 182:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.20, train_loss_epoch=9.950, valid_loss=8.700]\n","Epoch 183:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.320, train_loss_epoch=9.660, valid_loss=8.700]\n","Epoch 184:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.310, train_loss_epoch=9.590, valid_loss=8.700]\n","Epoch 185:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.540, train_loss_epoch=9.560, valid_loss=8.700]\n","Epoch 186:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.260, train_loss_epoch=9.380, valid_loss=8.700]\n","Epoch 187:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.450, train_loss_epoch=9.260, valid_loss=8.700]\n","Epoch 188:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.550, train_loss_epoch=8.720, valid_loss=8.700]\n","Epoch 189:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.510, train_loss_epoch=8.310, valid_loss=8.700]\n","Epoch 190:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.190, train_loss_epoch=8.480, valid_loss=8.700]\n","Epoch 191:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.350, train_loss_epoch=8.040, valid_loss=8.700]\n","Epoch 192:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.760, train_loss_epoch=9.180, valid_loss=8.700]\n","Epoch 193:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.20, train_loss_epoch=10.00, valid_loss=8.700]\n","Epoch 194:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.980, train_loss_epoch=10.00, valid_loss=8.700]\n","Epoch 195:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.80, train_loss_epoch=10.20, valid_loss=8.700]\n","Epoch 196:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.410, train_loss_epoch=9.860, valid_loss=8.700]\n","Epoch 197:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.20, train_loss_epoch=10.10, valid_loss=8.700]\n","Epoch 198:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.290, train_loss_epoch=9.740, valid_loss=8.700]\n","Epoch 199:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.440, train_loss_epoch=9.540, valid_loss=8.700]\n","Epoch 199: 100%|██████████| 4/4 [00:00\u003c00:00, 38.27it/s, v_num=0, train_loss_step=8.360, train_loss_epoch=9.540, valid_loss=8.700]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9957)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 27.68it/s]\u001b[A\n","Epoch 200:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.360, train_loss_epoch=9.020, valid_loss=8.730]\n","Epoch 201:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.070, train_loss_epoch=8.680, valid_loss=8.730]\n","Epoch 202:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.910, train_loss_epoch=8.430, valid_loss=8.730]\n","Epoch 203:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.520, train_loss_epoch=8.230, valid_loss=8.730]\n","Epoch 204:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.720, train_loss_epoch=8.160, valid_loss=8.730]\n","Epoch 205:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.720, train_loss_epoch=7.920, valid_loss=8.730]\n","Epoch 206:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.340, train_loss_epoch=7.840, valid_loss=8.730]\n","Epoch 207:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.830, train_loss_epoch=8.130, valid_loss=8.730]\n","Epoch 208:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.230, train_loss_epoch=8.860, valid_loss=8.730]\n","Epoch 209:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.980, train_loss_epoch=9.110, valid_loss=8.730]\n","Epoch 210:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.020, train_loss_epoch=9.100, valid_loss=8.730]\n","Epoch 211:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.660, train_loss_epoch=8.870, valid_loss=8.730]\n","Epoch 212:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.330, train_loss_epoch=8.820, valid_loss=8.730]\n","Epoch 213:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.910, train_loss_epoch=8.370, valid_loss=8.730]\n","Epoch 214:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.620, train_loss_epoch=7.740, valid_loss=8.730]\n","Epoch 215:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.770, train_loss_epoch=7.340, valid_loss=8.730]\n","Epoch 216:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.370, train_loss_epoch=8.560, valid_loss=8.730]\n","Epoch 217:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.80, train_loss_epoch=10.60, valid_loss=8.730]\n","Epoch 218:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.00, train_loss_epoch=10.70, valid_loss=8.730]\n","Epoch 219:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.90, train_loss_epoch=10.80, valid_loss=8.730]\n","Epoch 220:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.460, train_loss_epoch=10.20, valid_loss=8.730]\n","Epoch 221:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.060, train_loss_epoch=9.850, valid_loss=8.730]\n","Epoch 222:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.90, train_loss_epoch=10.20, valid_loss=8.730]\n","Epoch 223:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=11.00, train_loss_epoch=10.10, valid_loss=8.730]\n","Epoch 224:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.500, train_loss_epoch=9.450, valid_loss=8.730]\n","Epoch 224: 100%|██████████| 4/4 [00:00\u003c00:00, 39.01it/s, v_num=0, train_loss_step=9.380, train_loss_epoch=9.450, valid_loss=8.730]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-18 13:16:23,435\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (180, 60, 1), 'n_pool_kernel_size': (2, 2, 2), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=9957)\u001b[0m `Trainer.fit` stopped: `max_steps=900.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=9957)\u001b[0m \n","\u001b[36m(_train_tune pid=9957)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 27.65it/s]\u001b[A\n","\u001b[36m(_train_tune pid=9957)\u001b[0m \r                                                                      \u001b[A\rEpoch 224: 100%|██████████| 4/4 [00:00\u003c00:00, 14.61it/s, v_num=0, train_loss_step=9.380, train_loss_epoch=9.450, valid_loss=9.300]\rEpoch 224: 100%|██████████| 4/4 [00:00\u003c00:00, 14.56it/s, v_num=0, train_loss_step=9.380, train_loss_epoch=9.600, valid_loss=9.300]\rEpoch 224: 100%|██████████| 4/4 [00:00\u003c00:00, 14.51it/s, v_num=0, train_loss_step=9.380, train_loss_epoch=9.600, valid_loss=9.300]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=10171)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=10171)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=10171)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=10171)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=10171)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=10171)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=10171)\u001b[0m 2024-11-18 13:16:30.876847: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=10171)\u001b[0m 2024-11-18 13:16:30.898020: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=10171)\u001b[0m 2024-11-18 13:16:30.923865: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=10171)\u001b[0m 2024-11-18 13:16:30.931713: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=10171)\u001b[0m 2024-11-18 13:16:30.950279: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=10171)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=10171)\u001b[0m 2024-11-18 13:16:32.113212: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=10171)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=10171)\u001b[0m \n","\u001b[36m(_train_tune pid=10171)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=10171)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=10171)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=10171)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=10171)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=10171)\u001b[0m 3 | blocks       | ModuleList       | 3.7 M  | train\n","\u001b[36m(_train_tune pid=10171)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=10171)\u001b[0m 3.7 M     Trainable params\n","\u001b[36m(_train_tune pid=10171)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=10171)\u001b[0m 3.7 M     Total params\n","\u001b[36m(_train_tune pid=10171)\u001b[0m 14.748    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=10171)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=10171)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00,  2.25it/s]\n","                                                                           \n","Epoch 0:   0%|          | 0/4 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=18.50, train_loss_epoch=103.0]\n","Epoch 2:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.950, train_loss_epoch=11.20]\n","Epoch 3:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.080, train_loss_epoch=8.820]\n","Epoch 4:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.540, train_loss_epoch=8.050]\n","Epoch 5:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.290, train_loss_epoch=7.520]\n","Epoch 6:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.100, train_loss_epoch=7.160]\n","Epoch 7:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.440, train_loss_epoch=6.800]\n","Epoch 8:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.550, train_loss_epoch=6.670]\n","Epoch 9:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.240, train_loss_epoch=6.480]\n","Epoch 10:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.720, train_loss_epoch=6.550]\n","Epoch 11:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.280, train_loss_epoch=6.410]\n","Epoch 12:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.590, train_loss_epoch=6.460]\n","Epoch 13:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.260, train_loss_epoch=6.390]\n","Epoch 14:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.870, train_loss_epoch=6.300]\n","Epoch 15:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.240, train_loss_epoch=6.360]\n","Epoch 16:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.220, train_loss_epoch=6.350]\n","Epoch 17:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.700, train_loss_epoch=6.210]\n","Epoch 18:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.860, train_loss_epoch=6.240]\n","Epoch 19:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.850, train_loss_epoch=6.240]\n","Epoch 20:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.40, train_loss_epoch=7.380]\n","Epoch 21:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.950, train_loss_epoch=6.260]\n","Epoch 22:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.930, train_loss_epoch=6.220]\n","Epoch 23:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.120, train_loss_epoch=6.260]\n","Epoch 24:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.800, train_loss_epoch=6.430]\n","Epoch 24: 100%|██████████| 4/4 [00:00\u003c00:00, 29.01it/s, v_num=0, train_loss_step=5.940, train_loss_epoch=6.430]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10171)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10171)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 24.81it/s]\u001b[A\n","Epoch 25:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.940, train_loss_epoch=6.200, valid_loss=6.220]\n","Epoch 26:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.230, train_loss_epoch=6.280, valid_loss=6.220]\n","Epoch 27:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.460, train_loss_epoch=6.330, valid_loss=6.220]\n","Epoch 28:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.820, train_loss_epoch=6.410, valid_loss=6.220]\n","Epoch 29:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.270, train_loss_epoch=6.250, valid_loss=6.220]\n","Epoch 30:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.970, train_loss_epoch=6.170, valid_loss=6.220]\n","Epoch 31:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.920, train_loss_epoch=6.160, valid_loss=6.220]\n","Epoch 32:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.830, train_loss_epoch=6.620, valid_loss=6.220]\n","Epoch 33:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.590, train_loss_epoch=6.310, valid_loss=6.220]\n","Epoch 33: 100%|██████████| 4/4 [00:00\u003c00:00, 22.88it/s, v_num=0, train_loss_step=5.970, train_loss_epoch=6.170, valid_loss=6.220]\n","Epoch 34:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.970, train_loss_epoch=6.170, valid_loss=6.220]\n","Epoch 35:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.630, train_loss_epoch=6.070, valid_loss=6.220]\n","Epoch 36:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.890, train_loss_epoch=6.110, valid_loss=6.220]\n","Epoch 37:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.960, train_loss_epoch=6.100, valid_loss=6.220]\n","Epoch 38:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.760, train_loss_epoch=6.070, valid_loss=6.220]\n","Epoch 39:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.450, train_loss_epoch=6.220, valid_loss=6.220]\n","Epoch 40:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.230, train_loss_epoch=6.160, valid_loss=6.220]\n","Epoch 40: 100%|██████████| 4/4 [00:00\u003c00:00, 25.12it/s, v_num=0, train_loss_step=6.230, train_loss_epoch=6.160, valid_loss=6.220]\n","Epoch 41:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.530, train_loss_epoch=6.250, valid_loss=6.220]\n","Epoch 42:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.600, train_loss_epoch=5.970, valid_loss=6.220]\n","Epoch 43:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.770, train_loss_epoch=6.030, valid_loss=6.220]\n","Epoch 44:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.900, train_loss_epoch=6.050, valid_loss=6.220]\n","Epoch 45:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.890, train_loss_epoch=6.030, valid_loss=6.220]\n","Epoch 46:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.540, train_loss_epoch=6.120, valid_loss=6.220]\n","Epoch 47:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.630, train_loss_epoch=5.970, valid_loss=6.220]\n","Epoch 48:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.060, train_loss_epoch=6.080, valid_loss=6.220]\n","Epoch 49:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.220, train_loss_epoch=6.100, valid_loss=6.220]\n","Epoch 49: 100%|██████████| 4/4 [00:00\u003c00:00, 22.25it/s, v_num=0, train_loss_step=5.860, train_loss_epoch=6.100, valid_loss=6.220]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10171)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 24.44it/s]\u001b[A\n","Epoch 50:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.860, train_loss_epoch=6.010, valid_loss=6.030]\n","Epoch 51:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.260, train_loss_epoch=6.360, valid_loss=6.030]\n","Epoch 52:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.300, train_loss_epoch=6.350, valid_loss=6.030]\n","Epoch 53:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.950, train_loss_epoch=6.000, valid_loss=6.030]\n","Epoch 54:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.410, train_loss_epoch=6.130, valid_loss=6.030]\n","Epoch 55:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.040, train_loss_epoch=6.530, valid_loss=6.030]\n","Epoch 56:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.850, train_loss_epoch=5.980, valid_loss=6.030]\n","Epoch 57:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.620, train_loss_epoch=5.940, valid_loss=6.030]\n","Epoch 58:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.960, train_loss_epoch=6.020, valid_loss=6.030]\n","Epoch 59:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.930, train_loss_epoch=5.990, valid_loss=6.030]\n","Epoch 60:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.760, train_loss_epoch=5.960, valid_loss=6.030]\n","Epoch 61:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.710, train_loss_epoch=5.940, valid_loss=6.030]\n","Epoch 62:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.820, train_loss_epoch=5.960, valid_loss=6.030]\n","Epoch 63:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.710, train_loss_epoch=5.900, valid_loss=6.030]\n","Epoch 64:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.840, train_loss_epoch=5.940, valid_loss=6.030]\n","Epoch 65:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.410, train_loss_epoch=6.070, valid_loss=6.030]\n","Epoch 65: 100%|██████████| 4/4 [00:00\u003c00:00, 34.26it/s, v_num=0, train_loss_step=5.670, train_loss_epoch=5.870, valid_loss=6.030]\n","Epoch 66:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.670, train_loss_epoch=5.870, valid_loss=6.030]\n","Epoch 67:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.610, train_loss_epoch=6.110, valid_loss=6.030]\n","Epoch 68:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.460, train_loss_epoch=6.070, valid_loss=6.030]\n","Epoch 69:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.030, train_loss_epoch=5.950, valid_loss=6.030]\n","Epoch 70:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.870, train_loss_epoch=5.910, valid_loss=6.030]\n","Epoch 71:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.510, train_loss_epoch=6.330, valid_loss=6.030]\n","Epoch 72:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.030, train_loss_epoch=5.930, valid_loss=6.030]\n","Epoch 73:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.720, train_loss_epoch=6.110, valid_loss=6.030]\n","Epoch 74:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.070, train_loss_epoch=5.940, valid_loss=6.030]\n","Epoch 74: 100%|██████████| 4/4 [00:00\u003c00:00, 33.28it/s, v_num=0, train_loss_step=5.820, train_loss_epoch=5.940, valid_loss=6.030]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10171)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 23.36it/s]\u001b[A\n","Epoch 75:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.820, train_loss_epoch=5.890, valid_loss=5.860]\n","Epoch 76:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.690, train_loss_epoch=5.810, valid_loss=5.860]\n","Epoch 77:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.970, train_loss_epoch=5.910, valid_loss=5.860]\n","Epoch 78:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.990, train_loss_epoch=5.920, valid_loss=5.860]\n","Epoch 79:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.500, train_loss_epoch=5.760, valid_loss=5.860]\n","Epoch 80:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.050, train_loss_epoch=6.150, valid_loss=5.860]\n","Epoch 81:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.100, train_loss_epoch=6.170, valid_loss=5.860]\n","Epoch 82:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.410, train_loss_epoch=5.740, valid_loss=5.860]\n","Epoch 83:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.430, train_loss_epoch=5.980, valid_loss=5.860]\n","Epoch 84:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.000, train_loss_epoch=6.110, valid_loss=5.860]\n","Epoch 85:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.770, train_loss_epoch=5.830, valid_loss=5.860]\n","Epoch 86:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.300, train_loss_epoch=5.720, valid_loss=5.860]\n","Epoch 87:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.640, train_loss_epoch=5.790, valid_loss=5.860]\n","Epoch 88:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.550, train_loss_epoch=5.750, valid_loss=5.860]\n","Epoch 89:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.660, train_loss_epoch=5.790, valid_loss=5.860]\n","Epoch 90:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.110, train_loss_epoch=5.880, valid_loss=5.860]\n","Epoch 91:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.670, train_loss_epoch=5.760, valid_loss=5.860]\n","Epoch 92:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.000, train_loss_epoch=5.840, valid_loss=5.860]\n","Epoch 93:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.870, train_loss_epoch=5.790, valid_loss=5.860]\n","Epoch 94:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.460, train_loss_epoch=5.700, valid_loss=5.860]\n","Epoch 95:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.670, train_loss_epoch=5.750, valid_loss=5.860]\n","Epoch 95: 100%|██████████| 4/4 [00:00\u003c00:00, 22.07it/s, v_num=0, train_loss_step=5.940, train_loss_epoch=5.780, valid_loss=5.860]\n","Epoch 96:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.940, train_loss_epoch=5.780, valid_loss=5.860]\n","Epoch 97:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.770, train_loss_epoch=5.730, valid_loss=5.860]\n","Epoch 98:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.480, train_loss_epoch=5.700, valid_loss=5.860]\n","Epoch 99:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.070, train_loss_epoch=5.810, valid_loss=5.860]\n","Epoch 99: 100%|██████████| 4/4 [00:00\u003c00:00, 24.88it/s, v_num=0, train_loss_step=5.490, train_loss_epoch=5.810, valid_loss=5.860]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10171)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 23.26it/s]\u001b[A\n","Epoch 100:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.490, train_loss_epoch=5.670, valid_loss=5.750]\n","Epoch 101:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.330, train_loss_epoch=5.630, valid_loss=5.750]\n","Epoch 102:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.540, train_loss_epoch=5.670, valid_loss=5.750]\n","Epoch 103:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.840, train_loss_epoch=5.740, valid_loss=5.750]\n","Epoch 104:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.350, train_loss_epoch=6.120, valid_loss=5.750]\n","Epoch 105:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.040, train_loss_epoch=5.800, valid_loss=5.750]\n","Epoch 106:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.120, train_loss_epoch=6.060, valid_loss=5.750]\n","Epoch 107:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.390, train_loss_epoch=5.640, valid_loss=5.750]\n","Epoch 108:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.220, train_loss_epoch=5.850, valid_loss=5.750]\n","Epoch 109:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.530, train_loss_epoch=5.670, valid_loss=5.750]\n","Epoch 110:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.750, train_loss_epoch=5.720, valid_loss=5.750]\n","Epoch 111:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.570, train_loss_epoch=5.650, valid_loss=5.750]\n","Epoch 112:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.580, train_loss_epoch=5.650, valid_loss=5.750]\n","Epoch 113:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.520, train_loss_epoch=5.640, valid_loss=5.750]\n","Epoch 114:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.340, train_loss_epoch=5.580, valid_loss=5.750]\n","Epoch 115:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.920, train_loss_epoch=5.460, valid_loss=5.750]\n","Epoch 116:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.540, train_loss_epoch=5.640, valid_loss=5.750]\n","Epoch 117:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.180, train_loss_epoch=5.530, valid_loss=5.750]\n","Epoch 118:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.780, train_loss_epoch=5.900, valid_loss=5.750]\n","Epoch 119:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.300, train_loss_epoch=5.550, valid_loss=5.750]\n","Epoch 120:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.950, train_loss_epoch=5.700, valid_loss=5.750]\n","Epoch 121:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.140, train_loss_epoch=5.500, valid_loss=5.750]\n","Epoch 122:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.280, train_loss_epoch=5.550, valid_loss=5.750]\n","Epoch 123:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.010, train_loss_epoch=5.750, valid_loss=5.750]\n","Epoch 124:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.190, train_loss_epoch=5.760, valid_loss=5.750]\n","Epoch 124: 100%|██████████| 4/4 [00:00\u003c00:00, 28.16it/s, v_num=0, train_loss_step=5.410, train_loss_epoch=5.760, valid_loss=5.750]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10171)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 22.76it/s]\u001b[A\n","Epoch 125:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.410, train_loss_epoch=5.540, valid_loss=5.630]\n","Epoch 126:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.490, train_loss_epoch=5.580, valid_loss=5.630]\n","Epoch 127:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.910, train_loss_epoch=5.910, valid_loss=5.630]\n","Epoch 128:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.210, train_loss_epoch=5.510, valid_loss=5.630]\n","Epoch 129:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.940, train_loss_epoch=5.410, valid_loss=5.630]\n","Epoch 130:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.430, train_loss_epoch=5.540, valid_loss=5.630]\n","Epoch 131:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.270, train_loss_epoch=5.530, valid_loss=5.630]\n","Epoch 132:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.420, train_loss_epoch=5.540, valid_loss=5.630]\n","Epoch 133:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.790, train_loss_epoch=5.620, valid_loss=5.630]\n","Epoch 134:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.150, train_loss_epoch=5.680, valid_loss=5.630]\n","Epoch 135:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.450, train_loss_epoch=5.520, valid_loss=5.630]\n","Epoch 136:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.900, train_loss_epoch=5.390, valid_loss=5.630]\n","Epoch 137:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.560, train_loss_epoch=5.800, valid_loss=5.630]\n","Epoch 138:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.940, train_loss_epoch=5.410, valid_loss=5.630]\n","Epoch 139:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.340, train_loss_epoch=5.490, valid_loss=5.630]\n","Epoch 140:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.460, train_loss_epoch=5.520, valid_loss=5.630]\n","Epoch 141:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.240, train_loss_epoch=5.460, valid_loss=5.630]\n","Epoch 142:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.020, train_loss_epoch=5.420, valid_loss=5.630]\n","Epoch 143:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.910, train_loss_epoch=5.630, valid_loss=5.630]\n","Epoch 144:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.130, train_loss_epoch=5.450, valid_loss=5.630]\n","Epoch 145:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.600, train_loss_epoch=6.060, valid_loss=5.630]\n","Epoch 146:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.410, train_loss_epoch=5.490, valid_loss=5.630]\n","Epoch 147:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.330, train_loss_epoch=5.490, valid_loss=5.630]\n","Epoch 148:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.250, train_loss_epoch=5.470, valid_loss=5.630]\n","Epoch 149:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.830, train_loss_epoch=5.610, valid_loss=5.630]\n","Epoch 149: 100%|██████████| 4/4 [00:00\u003c00:00, 33.35it/s, v_num=0, train_loss_step=5.360, train_loss_epoch=5.610, valid_loss=5.630]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10171)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10171)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 23.07it/s]\u001b[A\n","Epoch 150:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.360, train_loss_epoch=5.500, valid_loss=5.590]\n","Epoch 151:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.990, train_loss_epoch=5.650, valid_loss=5.590]\n","Epoch 152:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.230, train_loss_epoch=5.440, valid_loss=5.590]\n","Epoch 153:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.410, train_loss_epoch=5.480, valid_loss=5.590]\n","Epoch 154:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.180, train_loss_epoch=5.690, valid_loss=5.590]\n","Epoch 155:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.660, train_loss_epoch=5.800, valid_loss=5.590]\n","Epoch 156:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.910, train_loss_epoch=5.600, valid_loss=5.590]\n","Epoch 157:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.250, train_loss_epoch=5.430, valid_loss=5.590]\n","Epoch 158:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.560, train_loss_epoch=5.510, valid_loss=5.590]\n","Epoch 159:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.850, train_loss_epoch=5.330, valid_loss=5.590]\n","Epoch 160:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.420, train_loss_epoch=5.460, valid_loss=5.590]\n","Epoch 161:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.960, train_loss_epoch=5.330, valid_loss=5.590]\n","Epoch 162:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.820, train_loss_epoch=5.570, valid_loss=5.590]\n","Epoch 163:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.960, train_loss_epoch=5.350, valid_loss=5.590]\n","Epoch 164:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.000, train_loss_epoch=5.580, valid_loss=5.590]\n","Epoch 165:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.880, train_loss_epoch=5.310, valid_loss=5.590]\n","Epoch 166:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.230, train_loss_epoch=5.380, valid_loss=5.590]\n","Epoch 167:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.910, train_loss_epoch=5.560, valid_loss=5.590]\n","Epoch 168:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.660, train_loss_epoch=5.510, valid_loss=5.590]\n","Epoch 169:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.190, train_loss_epoch=5.380, valid_loss=5.590]\n","Epoch 170:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.060, train_loss_epoch=5.340, valid_loss=5.590]\n","Epoch 171:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.920, train_loss_epoch=5.300, valid_loss=5.590]\n","Epoch 172:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.170, train_loss_epoch=5.380, valid_loss=5.590]\n","Epoch 173:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.290, train_loss_epoch=5.370, valid_loss=5.590]\n","Epoch 174:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.380, train_loss_epoch=5.430, valid_loss=5.590]\n","Epoch 174: 100%|██████████| 4/4 [00:00\u003c00:00, 24.77it/s, v_num=0, train_loss_step=5.130, train_loss_epoch=5.430, valid_loss=5.590]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10171)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 23.68it/s]\u001b[A\n","Epoch 175:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.130, train_loss_epoch=5.360, valid_loss=5.550]\n","Epoch 176:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.390, train_loss_epoch=5.420, valid_loss=5.550]\n","Epoch 177:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.540, train_loss_epoch=5.460, valid_loss=5.550]\n","Epoch 178:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.630, train_loss_epoch=5.460, valid_loss=5.550]\n","Epoch 179:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.690, train_loss_epoch=5.260, valid_loss=5.550]\n","Epoch 180:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.520, train_loss_epoch=5.460, valid_loss=5.550]\n","Epoch 181:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.180, train_loss_epoch=5.370, valid_loss=5.550]\n","Epoch 182:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.440, train_loss_epoch=5.440, valid_loss=5.550]\n","Epoch 183:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.020, train_loss_epoch=5.320, valid_loss=5.550]\n","Epoch 184:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.330, train_loss_epoch=5.400, valid_loss=5.550]\n","Epoch 185:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.480, train_loss_epoch=5.430, valid_loss=5.550]\n","Epoch 186:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.260, train_loss_epoch=5.390, valid_loss=5.550]\n","Epoch 187:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.800, train_loss_epoch=5.480, valid_loss=5.550]\n","Epoch 188:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.280, train_loss_epoch=5.400, valid_loss=5.550]\n","Epoch 189:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.080, train_loss_epoch=5.320, valid_loss=5.550]\n","Epoch 190:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.160, train_loss_epoch=5.580, valid_loss=5.550]\n","Epoch 191:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.380, train_loss_epoch=5.410, valid_loss=5.550]\n","Epoch 192:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.400, train_loss_epoch=5.430, valid_loss=5.550]\n","Epoch 193:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.490, train_loss_epoch=5.420, valid_loss=5.550]\n","Epoch 194:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.280, train_loss_epoch=5.380, valid_loss=5.550]\n","Epoch 195:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.830, train_loss_epoch=5.500, valid_loss=5.550]\n","Epoch 195: 100%|██████████| 4/4 [00:00\u003c00:00, 33.09it/s, v_num=0, train_loss_step=4.760, train_loss_epoch=5.240, valid_loss=5.550]\n","Epoch 196:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.760, train_loss_epoch=5.240, valid_loss=5.550]\n","Epoch 197:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.520, train_loss_epoch=5.410, valid_loss=5.550]\n","Epoch 198:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.100, train_loss_epoch=5.310, valid_loss=5.550]\n","Epoch 199:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.250, train_loss_epoch=5.340, valid_loss=5.550]\n","Epoch 199: 100%|██████████| 4/4 [00:00\u003c00:00, 33.86it/s, v_num=0, train_loss_step=5.100, train_loss_epoch=5.340, valid_loss=5.550]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10171)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 24.93it/s]\u001b[A\n","Epoch 200:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.100, train_loss_epoch=5.320, valid_loss=5.500]\n","Epoch 201:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.710, train_loss_epoch=5.200, valid_loss=5.500]\n","Epoch 202:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.860, train_loss_epoch=5.230, valid_loss=5.500]\n","Epoch 203:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.000, train_loss_epoch=5.260, valid_loss=5.500]\n","Epoch 204:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.180, train_loss_epoch=5.350, valid_loss=5.500]\n","Epoch 204: 100%|██████████| 4/4 [00:00\u003c00:00, 34.25it/s, v_num=0, train_loss_step=5.180, train_loss_epoch=5.350, valid_loss=5.500]\n","Epoch 204: 100%|██████████| 4/4 [00:00\u003c00:00, 33.93it/s, v_num=0, train_loss_step=5.260, train_loss_epoch=5.340, valid_loss=5.500]\n","Epoch 205:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.260, train_loss_epoch=5.340, valid_loss=5.500]\n","Epoch 206:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.020, train_loss_epoch=5.280, valid_loss=5.500]\n","Epoch 207:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.270, train_loss_epoch=5.340, valid_loss=5.500]\n","Epoch 208:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.800, train_loss_epoch=5.220, valid_loss=5.500]\n","Epoch 209:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.300, train_loss_epoch=5.340, valid_loss=5.500]\n","Epoch 210:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.130, train_loss_epoch=5.270, valid_loss=5.500]\n","Epoch 211:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.240, train_loss_epoch=5.310, valid_loss=5.500]\n","Epoch 212:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.850, train_loss_epoch=5.470, valid_loss=5.500]\n","Epoch 213:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.150, train_loss_epoch=5.300, valid_loss=5.500]\n","Epoch 214:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.510, train_loss_epoch=5.150, valid_loss=5.500]\n","Epoch 215:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.150, train_loss_epoch=5.310, valid_loss=5.500]\n","Epoch 216:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.480, train_loss_epoch=5.380, valid_loss=5.500]\n","Epoch 217:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.120, train_loss_epoch=5.290, valid_loss=5.500]\n","Epoch 218:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.750, train_loss_epoch=5.440, valid_loss=5.500]\n","Epoch 219:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.260, train_loss_epoch=5.300, valid_loss=5.500]\n","Epoch 220:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.990, train_loss_epoch=5.230, valid_loss=5.500]\n","Epoch 221:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.710, train_loss_epoch=5.140, valid_loss=5.500]\n","Epoch 222:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.920, train_loss_epoch=5.460, valid_loss=5.500]\n","Epoch 223:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.950, train_loss_epoch=5.460, valid_loss=5.500]\n","Epoch 224:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.660, train_loss_epoch=5.160, valid_loss=5.500]\n","Epoch 224: 100%|██████████| 4/4 [00:00\u003c00:00, 35.26it/s, v_num=0, train_loss_step=5.480, train_loss_epoch=5.160, valid_loss=5.500]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10171)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10171)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 27.57it/s]\u001b[A\n","Epoch 225:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.480, train_loss_epoch=5.340, valid_loss=5.450]\n","Epoch 226:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.250, train_loss_epoch=5.300, valid_loss=5.450]\n","Epoch 227:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.740, train_loss_epoch=5.150, valid_loss=5.450]\n","Epoch 228:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.090, train_loss_epoch=5.240, valid_loss=5.450]\n","Epoch 229:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.330, train_loss_epoch=5.300, valid_loss=5.450]\n","Epoch 230:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.440, train_loss_epoch=5.330, valid_loss=5.450]\n","Epoch 231:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.420, train_loss_epoch=5.320, valid_loss=5.450]\n","Epoch 232:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.110, train_loss_epoch=6.000, valid_loss=5.450]\n","Epoch 233:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.170, train_loss_epoch=5.250, valid_loss=5.450]\n","Epoch 234:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.090, train_loss_epoch=5.270, valid_loss=5.450]\n","Epoch 235:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.070, train_loss_epoch=5.230, valid_loss=5.450]\n","Epoch 236:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.540, train_loss_epoch=5.570, valid_loss=5.450]\n","Epoch 237:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.920, train_loss_epoch=5.440, valid_loss=5.450]\n","Epoch 238:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.480, train_loss_epoch=5.320, valid_loss=5.450]\n","Epoch 239:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.300, train_loss_epoch=5.270, valid_loss=5.450]\n","Epoch 240:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.190, train_loss_epoch=5.250, valid_loss=5.450]\n","Epoch 241:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.650, train_loss_epoch=5.340, valid_loss=5.450]\n","Epoch 242:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.000, train_loss_epoch=5.200, valid_loss=5.450]\n","Epoch 243:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.350, train_loss_epoch=5.300, valid_loss=5.450]\n","Epoch 244:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.890, train_loss_epoch=5.150, valid_loss=5.450]\n","Epoch 245:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.250, train_loss_epoch=5.240, valid_loss=5.450]\n","Epoch 246:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.320, train_loss_epoch=5.270, valid_loss=5.450]\n","Epoch 247:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.840, train_loss_epoch=5.150, valid_loss=5.450]\n","Epoch 248:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.700, train_loss_epoch=5.110, valid_loss=5.450]\n","Epoch 249:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.580, train_loss_epoch=5.330, valid_loss=5.450]\n","Epoch 249: 100%|██████████| 4/4 [00:00\u003c00:00, 34.77it/s, v_num=0, train_loss_step=6.260, train_loss_epoch=5.330, valid_loss=5.450]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10171)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 28.01it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10171)\u001b[0m \n","Epoch 250:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.260, train_loss_epoch=5.500, valid_loss=5.400]\n","Epoch 251:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.230, train_loss_epoch=5.250, valid_loss=5.400]\n","Epoch 252:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.270, train_loss_epoch=5.270, valid_loss=5.400]\n","Epoch 253:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.670, train_loss_epoch=5.090, valid_loss=5.400]\n","Epoch 254:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.150, train_loss_epoch=5.210, valid_loss=5.400]\n","Epoch 255:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.090, train_loss_epoch=5.200, valid_loss=5.400]\n","Epoch 256:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.070, train_loss_epoch=5.430, valid_loss=5.400]\n","Epoch 257:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.410, train_loss_epoch=5.310, valid_loss=5.400]\n","Epoch 258:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.420, train_loss_epoch=5.030, valid_loss=5.400]\n","Epoch 259:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.230, train_loss_epoch=5.210, valid_loss=5.400]\n","Epoch 260:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.940, train_loss_epoch=5.170, valid_loss=5.400]\n","Epoch 261:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.400, train_loss_epoch=5.260, valid_loss=5.400]\n","Epoch 262:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.560, train_loss_epoch=5.290, valid_loss=5.400]\n","Epoch 262: 100%|██████████| 4/4 [00:00\u003c00:00, 35.76it/s, v_num=0, train_loss_step=6.490, train_loss_epoch=5.520, valid_loss=5.400]\n","Epoch 263:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.490, train_loss_epoch=5.520, valid_loss=5.400]\n","Epoch 264:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.750, train_loss_epoch=5.090, valid_loss=5.400]\n","Epoch 265:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.080, train_loss_epoch=5.160, valid_loss=5.400]\n","Epoch 266:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.440, train_loss_epoch=5.020, valid_loss=5.400]\n","Epoch 267:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.980, train_loss_epoch=5.160, valid_loss=5.400]\n","Epoch 268:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.670, train_loss_epoch=5.060, valid_loss=5.400]\n","Epoch 269:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.940, train_loss_epoch=5.130, valid_loss=5.400]\n","Epoch 270:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.740, train_loss_epoch=5.110, valid_loss=5.400]\n","Epoch 271:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.580, train_loss_epoch=5.330, valid_loss=5.400]\n","Epoch 272:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.200, train_loss_epoch=5.190, valid_loss=5.400]\n","Epoch 273:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.600, train_loss_epoch=5.290, valid_loss=5.400]\n","Epoch 274:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.720, train_loss_epoch=5.320, valid_loss=5.400]\n","Epoch 274: 100%|██████████| 4/4 [00:00\u003c00:00, 34.78it/s, v_num=0, train_loss_step=5.820, train_loss_epoch=5.320, valid_loss=5.400]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10171)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 28.43it/s]\u001b[A\n","Epoch 275:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.820, train_loss_epoch=5.350, valid_loss=5.370]\n","Epoch 275: 100%|██████████| 4/4 [00:00\u003c00:00, 36.08it/s, v_num=0, train_loss_step=4.700, train_loss_epoch=5.060, valid_loss=5.370]\n","Epoch 276:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.700, train_loss_epoch=5.060, valid_loss=5.370]\n","Epoch 277:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.820, train_loss_epoch=5.070, valid_loss=5.370]\n","Epoch 278:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.110, train_loss_epoch=5.420, valid_loss=5.370]\n","Epoch 279:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.960, train_loss_epoch=5.120, valid_loss=5.370]\n","Epoch 280:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.460, train_loss_epoch=5.520, valid_loss=5.370]\n","Epoch 281:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.520, train_loss_epoch=5.010, valid_loss=5.370]\n","Epoch 282:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.730, train_loss_epoch=5.050, valid_loss=5.370]\n","Epoch 283:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.580, train_loss_epoch=5.300, valid_loss=5.370]\n","Epoch 284:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.310, train_loss_epoch=5.210, valid_loss=5.370]\n","Epoch 285:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.880, train_loss_epoch=5.130, valid_loss=5.370]\n","Epoch 286:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.280, train_loss_epoch=5.200, valid_loss=5.370]\n","Epoch 287:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.780, train_loss_epoch=5.070, valid_loss=5.370]\n","Epoch 288:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.400, train_loss_epoch=5.240, valid_loss=5.370]\n","Epoch 289:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.120, train_loss_epoch=5.140, valid_loss=5.370]\n","Epoch 289: 100%|██████████| 4/4 [00:00\u003c00:00, 36.39it/s, v_num=0, train_loss_step=5.120, train_loss_epoch=5.140, valid_loss=5.370]\n","Epoch 290:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.660, train_loss_epoch=5.320, valid_loss=5.370]\n","Epoch 291:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.780, train_loss_epoch=5.310, valid_loss=5.370]\n","Epoch 292:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.980, train_loss_epoch=5.140, valid_loss=5.370]\n","Epoch 293:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.100, train_loss_epoch=5.160, valid_loss=5.370]\n","Epoch 294:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.580, train_loss_epoch=5.500, valid_loss=5.370]\n","Epoch 295:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.840, train_loss_epoch=5.080, valid_loss=5.370]\n","Epoch 296:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.750, train_loss_epoch=5.060, valid_loss=5.370]\n","Epoch 297:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.390, train_loss_epoch=5.220, valid_loss=5.370]\n","Epoch 298:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.860, train_loss_epoch=5.070, valid_loss=5.370]\n","Epoch 299:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.960, train_loss_epoch=5.380, valid_loss=5.370]\n","Epoch 299: 100%|██████████| 4/4 [00:00\u003c00:00, 35.34it/s, v_num=0, train_loss_step=4.670, train_loss_epoch=5.380, valid_loss=5.370]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10171)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 28.45it/s]\u001b[A\n","Epoch 300:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.670, train_loss_epoch=5.050, valid_loss=5.350]\n","Epoch 301:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.800, train_loss_epoch=5.060, valid_loss=5.350]\n","Epoch 302:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.960, train_loss_epoch=5.110, valid_loss=5.350]\n","Epoch 303:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.080, train_loss_epoch=5.110, valid_loss=5.350]\n","Epoch 304:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.380, train_loss_epoch=4.960, valid_loss=5.350]\n","Epoch 305:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.230, train_loss_epoch=5.670, valid_loss=5.350]\n","Epoch 306:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.720, train_loss_epoch=5.050, valid_loss=5.350]\n","Epoch 307:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.320, train_loss_epoch=5.160, valid_loss=5.350]\n","Epoch 308:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.190, train_loss_epoch=5.160, valid_loss=5.350]\n","Epoch 309:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.880, train_loss_epoch=5.090, valid_loss=5.350]\n","Epoch 310:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.810, train_loss_epoch=5.540, valid_loss=5.350]\n","Epoch 311:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.560, train_loss_epoch=5.000, valid_loss=5.350]\n","Epoch 312:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.790, train_loss_epoch=5.310, valid_loss=5.350]\n","Epoch 313:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.010, train_loss_epoch=5.090, valid_loss=5.350]\n","Epoch 314:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.060, train_loss_epoch=5.130, valid_loss=5.350]\n","Epoch 315:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.510, train_loss_epoch=5.480, valid_loss=5.350]\n","Epoch 316:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.170, train_loss_epoch=5.160, valid_loss=5.350]\n","Epoch 317:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.140, train_loss_epoch=4.900, valid_loss=5.350]\n","Epoch 317: 100%|██████████| 4/4 [00:00\u003c00:00, 34.19it/s, v_num=0, train_loss_step=4.900, train_loss_epoch=5.080, valid_loss=5.350]\n","Epoch 318:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.900, train_loss_epoch=5.080, valid_loss=5.350]\n","Epoch 319:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.850, train_loss_epoch=5.090, valid_loss=5.350]\n","Epoch 320:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.480, train_loss_epoch=4.950, valid_loss=5.350]\n","Epoch 321:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.860, train_loss_epoch=5.070, valid_loss=5.350]\n","Epoch 322:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.750, train_loss_epoch=5.040, valid_loss=5.350]\n","Epoch 323:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.470, train_loss_epoch=4.950, valid_loss=5.350]\n","Epoch 324:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.710, train_loss_epoch=5.050, valid_loss=5.350]\n","Epoch 324: 100%|██████████| 4/4 [00:00\u003c00:00, 35.18it/s, v_num=0, train_loss_step=4.840, train_loss_epoch=5.050, valid_loss=5.350]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10171)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10171)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 27.29it/s]\u001b[A\n","Epoch 325:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.840, train_loss_epoch=5.050, valid_loss=5.330]\n","Epoch 326:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.400, train_loss_epoch=5.210, valid_loss=5.330]\n","Epoch 327:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.410, train_loss_epoch=5.200, valid_loss=5.330]\n","Epoch 328:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.350, train_loss_epoch=5.170, valid_loss=5.330]\n","Epoch 329:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.080, train_loss_epoch=5.110, valid_loss=5.330]\n","Epoch 330:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.400, train_loss_epoch=5.180, valid_loss=5.330]\n","Epoch 331:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.650, train_loss_epoch=5.290, valid_loss=5.330]\n","Epoch 332:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.190, train_loss_epoch=5.120, valid_loss=5.330]\n","Epoch 333:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.310, train_loss_epoch=4.890, valid_loss=5.330]\n","Epoch 334:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.290, train_loss_epoch=5.150, valid_loss=5.330]\n","Epoch 335:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.560, train_loss_epoch=4.980, valid_loss=5.330]\n","Epoch 336:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.820, train_loss_epoch=5.020, valid_loss=5.330]\n","Epoch 337:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.020, train_loss_epoch=5.100, valid_loss=5.330]\n","Epoch 338:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.470, train_loss_epoch=5.200, valid_loss=5.330]\n","Epoch 339:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.930, train_loss_epoch=5.060, valid_loss=5.330]\n","Epoch 340:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.820, train_loss_epoch=5.030, valid_loss=5.330]\n","Epoch 341:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.740, train_loss_epoch=5.000, valid_loss=5.330]\n","Epoch 342:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.080, train_loss_epoch=5.090, valid_loss=5.330]\n","Epoch 343:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.080, train_loss_epoch=5.130, valid_loss=5.330]\n","Epoch 344:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.030, train_loss_epoch=5.110, valid_loss=5.330]\n","Epoch 345:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.610, train_loss_epoch=4.970, valid_loss=5.330]\n","Epoch 346:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.360, train_loss_epoch=5.140, valid_loss=5.330]\n","Epoch 347:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.250, train_loss_epoch=4.870, valid_loss=5.330]\n","Epoch 348:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.330, train_loss_epoch=5.410, valid_loss=5.330]\n","Epoch 349:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.580, train_loss_epoch=4.950, valid_loss=5.330]\n","Epoch 349: 100%|██████████| 4/4 [00:00\u003c00:00, 32.03it/s, v_num=0, train_loss_step=4.600, train_loss_epoch=4.950, valid_loss=5.330]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10171)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 27.41it/s]\u001b[A\n","Epoch 350:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.600, train_loss_epoch=4.980, valid_loss=5.320]\n","Epoch 351:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.640, train_loss_epoch=5.000, valid_loss=5.320]\n","Epoch 352:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.010, train_loss_epoch=5.050, valid_loss=5.320]\n","Epoch 353:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.410, train_loss_epoch=5.170, valid_loss=5.320]\n","Epoch 354:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.650, train_loss_epoch=4.980, valid_loss=5.320]\n","Epoch 355:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.370, train_loss_epoch=5.170, valid_loss=5.320]\n","Epoch 356:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.950, train_loss_epoch=5.020, valid_loss=5.320]\n","Epoch 357:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.210, train_loss_epoch=5.120, valid_loss=5.320]\n","Epoch 358:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.910, train_loss_epoch=5.020, valid_loss=5.320]\n","Epoch 359:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.860, train_loss_epoch=5.030, valid_loss=5.320]\n","Epoch 360:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.730, train_loss_epoch=5.470, valid_loss=5.320]\n","Epoch 361:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.460, train_loss_epoch=5.420, valid_loss=5.320]\n","Epoch 362:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.500, train_loss_epoch=5.200, valid_loss=5.320]\n","Epoch 363:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.780, train_loss_epoch=5.030, valid_loss=5.320]\n","Epoch 364:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.420, train_loss_epoch=5.400, valid_loss=5.320]\n","Epoch 365:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.140, train_loss_epoch=5.080, valid_loss=5.320]\n","Epoch 366:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.910, train_loss_epoch=5.050, valid_loss=5.320]\n","Epoch 367:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.120, train_loss_epoch=5.070, valid_loss=5.320]\n","Epoch 368:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.960, train_loss_epoch=5.030, valid_loss=5.320]\n","Epoch 369:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.340, train_loss_epoch=5.380, valid_loss=5.320]\n","Epoch 370:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.220, train_loss_epoch=5.080, valid_loss=5.320]\n","Epoch 371:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.830, train_loss_epoch=5.010, valid_loss=5.320]\n","Epoch 372:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.220, train_loss_epoch=5.120, valid_loss=5.320]\n","Epoch 373:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.330, train_loss_epoch=4.880, valid_loss=5.320]\n","Epoch 374:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.550, train_loss_epoch=5.150, valid_loss=5.320]\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-18 13:17:25,485\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (1, 1, 1), 'n_pool_kernel_size': (1, 1, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=10171)\u001b[0m \rEpoch 374: 100%|██████████| 4/4 [00:00\u003c00:00, 35.51it/s, v_num=0, train_loss_step=5.550, train_loss_epoch=5.150, valid_loss=5.320]\rEpoch 374: 100%|██████████| 4/4 [00:00\u003c00:00, 35.18it/s, v_num=0, train_loss_step=5.300, train_loss_epoch=5.150, valid_loss=5.320]\n","\u001b[36m(_train_tune pid=10171)\u001b[0m \rValidation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10171)\u001b[0m \rValidation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10171)\u001b[0m \rValidation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=10171)\u001b[0m `Trainer.fit` stopped: `max_steps=1500.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=10171)\u001b[0m \n","\u001b[36m(_train_tune pid=10171)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 4/4 [00:00\u003c00:00, 28.17it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10171)\u001b[0m \r                                                                      \u001b[A\rEpoch 374: 100%|██████████| 4/4 [00:00\u003c00:00, 14.12it/s, v_num=0, train_loss_step=5.300, train_loss_epoch=5.150, valid_loss=5.280]\rEpoch 374: 100%|██████████| 4/4 [00:00\u003c00:00, 14.07it/s, v_num=0, train_loss_step=5.300, train_loss_epoch=5.110, valid_loss=5.280]\rEpoch 374: 100%|██████████| 4/4 [00:00\u003c00:00, 14.03it/s, v_num=0, train_loss_step=5.300, train_loss_epoch=5.110, valid_loss=5.280]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=10489)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=10489)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=10489)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=10489)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=10489)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=10489)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=10489)\u001b[0m 2024-11-18 13:17:32.768005: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=10489)\u001b[0m 2024-11-18 13:17:32.788632: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=10489)\u001b[0m 2024-11-18 13:17:32.814215: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=10489)\u001b[0m 2024-11-18 13:17:32.822055: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=10489)\u001b[0m 2024-11-18 13:17:32.840642: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=10489)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=10489)\u001b[0m 2024-11-18 13:17:34.003841: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=10489)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=10489)\u001b[0m \n","\u001b[36m(_train_tune pid=10489)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=10489)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=10489)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=10489)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=10489)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=10489)\u001b[0m 3 | blocks       | ModuleList       | 2.9 M  | train\n","\u001b[36m(_train_tune pid=10489)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=10489)\u001b[0m 2.9 M     Trainable params\n","\u001b[36m(_train_tune pid=10489)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=10489)\u001b[0m 2.9 M     Total params\n","\u001b[36m(_train_tune pid=10489)\u001b[0m 11.452    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=10489)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=10489)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=10489)\u001b[0m \rSanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","\u001b[36m(_train_tune pid=10489)\u001b[0m \rSanity Checking:   0%|          | 0/2 [00:00\u003c?, ?it/s]\rSanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-18 13:17:36,812\tERROR tune_controller.py:1331 -- Trial task failed for trial _train_tune_423843ea\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n","    result = ray.get(future)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2753, in get\n","    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 904, in get_objects\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(OutOfMemoryError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=10489, ip=172.28.0.12, actor_id=e09cf2faad8730a1d384f7c301000000, repr=_train_tune)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n","    raise skipped from exception_cause(skipped)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n","    self._ret = self._target(*self._args, **self._kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in \u003clambda\u003e\n","    training_func=lambda: self._trainable_func(self.config),\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n","    output = fn()\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/util.py\", line 130, in inner\n","    return trainable(config, **fn_kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_auto.py\", line 214, in _train_tune\n","    _ = self._fit_model(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_auto.py\", line 362, in _fit_model\n","    model = model.fit(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_windows.py\", line 661, in fit\n","    return self._fit(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_model.py\", line 356, in _fit\n","    trainer.fit(model, datamodule=datamodule)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 538, in fit\n","    call._call_and_handle_interrupt(\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 47, in _call_and_handle_interrupt\n","    return trainer_fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 574, in _fit_impl\n","    self._run(model, ckpt_path=ckpt_path)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 981, in _run\n","    results = self._run_stage()\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1023, in _run_stage\n","    self._run_sanity_check()\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1052, in _run_sanity_check\n","    val_loop.run()\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py\", line 178, in _decorator\n","    return loop_run(self, *args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 135, in run\n","    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 396, in _evaluation_step\n","    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 319, in _call_strategy_hook\n","    output = fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py\", line 411, in validation_step\n","    return self.lightning_module.validation_step(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_windows.py\", line 534, in validation_step\n","    valid_loss_batch = self._compute_valid_loss(\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/common/_base_windows.py\", line 461, in _compute_valid_loss\n","    _, sample_mean, quants = self.loss.sample(distr_args=distr_args)\n","  File \"/usr/local/lib/python3.10/dist-packages/neuralforecast/losses/pytorch.py\", line 2002, in sample\n","    samples = distr.sample(sample_shape=(num_samples,))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributions/distribution.py\", line 167, in sample\n","    return self.rsample(sample_shape)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributions/studentT.py\", line 90, in rsample\n","    Y = X * torch.rsqrt(Z / self.df)\n","torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.60 GiB. GPU 0 has a total capacity of 39.56 GiB of which 8.69 GiB is free. Process 52335 has 416.00 MiB memory in use. Process 148734 has 30.45 GiB memory in use. Of the allocated memory 29.55 GiB is allocated by PyTorch, and 425.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial _train_tune_423843ea errored after 0 iterations at 2024-11-18 13:17:36. Total running time: 13min 59s\n","Error file: /tmp/ray/session_2024-11-18_13-03-34_336407_4396/artifacts/2024-11-18_13-03-37/_train_tune_2024-11-18_13-03-34/driver_artifacts/_train_tune_423843ea_19_batch_size=256,h=20,hist_exog_list=DY_PTBV_P_PO_VO_PE,input_size=80,learning_rate=0.0000,loss=ref_ph_de895_2024-11-18_13-16-30/error.txt\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=10599)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=10599)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=10599)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=10599)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=10599)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=10599)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=10599)\u001b[0m 2024-11-18 13:17:44.772396: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=10599)\u001b[0m 2024-11-18 13:17:44.793578: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=10599)\u001b[0m 2024-11-18 13:17:44.820733: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=10599)\u001b[0m 2024-11-18 13:17:44.829126: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=10599)\u001b[0m 2024-11-18 13:17:44.848096: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=10599)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=10599)\u001b[0m 2024-11-18 13:17:46.015079: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=10599)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=10599)\u001b[0m \n","\u001b[36m(_train_tune pid=10599)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=10599)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=10599)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=10599)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=10599)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=10599)\u001b[0m 3 | blocks       | ModuleList       | 3.2 M  | train\n","\u001b[36m(_train_tune pid=10599)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=10599)\u001b[0m 3.2 M     Trainable params\n","\u001b[36m(_train_tune pid=10599)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=10599)\u001b[0m 3.2 M     Total params\n","\u001b[36m(_train_tune pid=10599)\u001b[0m 12.617    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=10599)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=10599)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/7 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.320, train_loss_epoch=37.80]\n","Epoch 2:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.140, train_loss_epoch=9.960]\n","Epoch 3:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.10, train_loss_epoch=10.20]\n","Epoch 4:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.240, train_loss_epoch=9.970]\n","Epoch 5:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.880, train_loss_epoch=10.20]\n","Epoch 6:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.10, train_loss_epoch=10.00]\n","Epoch 7:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.820, train_loss_epoch=9.960]\n","Epoch 8:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.520, train_loss_epoch=9.890]\n","Epoch 9:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.340, train_loss_epoch=9.790]\n","Epoch 10:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.870, train_loss_epoch=9.750]\n","Epoch 11:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.970, train_loss_epoch=9.640]\n","Epoch 12:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.620, train_loss_epoch=9.490]\n","Epoch 13:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.040, train_loss_epoch=9.280]\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.790, train_loss_epoch=9.250]\n","\u001b[36m(_train_tune pid=10599)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10599)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 39.51it/s]\u001b[A\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.550, train_loss_epoch=9.250, valid_loss=9.260]\n","Epoch 15:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.130, train_loss_epoch=9.430, valid_loss=9.260]\n","Epoch 16:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.050, train_loss_epoch=9.160, valid_loss=9.260]\n","Epoch 17:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.240, train_loss_epoch=9.150, valid_loss=9.260]\n","Epoch 18:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.310, train_loss_epoch=9.130, valid_loss=9.260]\n","Epoch 19:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.530, train_loss_epoch=9.130, valid_loss=9.260]\n","Epoch 20:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.740, train_loss_epoch=9.080, valid_loss=9.260]\n","Epoch 21:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=9.120, train_loss_epoch=8.980, valid_loss=9.260]\n","Epoch 22:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=10.30, train_loss_epoch=9.060, valid_loss=9.260]\n","Epoch 23:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.670, train_loss_epoch=8.890, valid_loss=9.260]\n","Epoch 24:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.420, train_loss_epoch=8.860, valid_loss=9.260]\n","Epoch 25:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.460, train_loss_epoch=8.640, valid_loss=9.260]\n","Epoch 26:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.300, train_loss_epoch=8.370, valid_loss=9.260]\n","Epoch 27:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.150, train_loss_epoch=8.140, valid_loss=9.260]\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=8.280, train_loss_epoch=7.940, valid_loss=9.260]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10599)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10599)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 40.02it/s]\u001b[A\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.840, train_loss_epoch=7.940, valid_loss=7.280]\n","Epoch 29:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.460, train_loss_epoch=7.580, valid_loss=7.280]\n","Epoch 30:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.660, train_loss_epoch=7.360, valid_loss=7.280]\n","Epoch 31:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.110, train_loss_epoch=7.410, valid_loss=7.280]\n","Epoch 32:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=7.400, train_loss_epoch=7.270, valid_loss=7.280]\n","Epoch 33:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.730, train_loss_epoch=7.060, valid_loss=7.280]\n","Epoch 34:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.970, train_loss_epoch=6.920, valid_loss=7.280]\n","Epoch 35:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.920, train_loss_epoch=6.510, valid_loss=7.280]\n","Epoch 36:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.000, train_loss_epoch=6.300, valid_loss=7.280]\n","Epoch 37:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.760, train_loss_epoch=6.230, valid_loss=7.280]\n","Epoch 38:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.140, train_loss_epoch=6.340, valid_loss=7.280]\n","Epoch 39:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.510, train_loss_epoch=6.210, valid_loss=7.280]\n","Epoch 40:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.870, train_loss_epoch=6.050, valid_loss=7.280]\n","Epoch 41:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=6.240, train_loss_epoch=6.010, valid_loss=7.280]\n","Epoch 42:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.290, train_loss_epoch=5.660, valid_loss=7.280]\n","\u001b[36m(_train_tune pid=10599)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10599)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 38.72it/s]\u001b[A\n","Epoch 42:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.090, train_loss_epoch=5.660, valid_loss=5.090]\n","Epoch 43:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=5.010, train_loss_epoch=5.330, valid_loss=5.090]\n","Epoch 44:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.610, train_loss_epoch=5.060, valid_loss=5.090]\n","Epoch 45:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=4.210, train_loss_epoch=4.570, valid_loss=5.090]\n","Epoch 46:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.820, train_loss_epoch=4.230, valid_loss=5.090]\n","Epoch 47:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=3.230, train_loss_epoch=3.590, valid_loss=5.090]\n","Epoch 48:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.350, train_loss_epoch=2.310, valid_loss=5.090]\n","Epoch 49:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=1.340, train_loss_epoch=1.470, valid_loss=5.090]\n","Epoch 50:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.333, train_loss_epoch=0.815, valid_loss=5.090]\n","Epoch 51:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=0.0157, train_loss_epoch=0.395, valid_loss=5.090]\n","Epoch 52:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.287, train_loss_epoch=0.0516, valid_loss=5.090]\n","Epoch 53:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.338, train_loss_epoch=-0.234, valid_loss=5.090]\n","Epoch 53: 100%|██████████| 7/7 [00:00\u003c00:00, 50.44it/s, v_num=0, train_loss_step=-0.683, train_loss_epoch=-0.234, valid_loss=5.090]\n","Epoch 54:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.683, train_loss_epoch=-0.464, valid_loss=5.090]\n","Epoch 55:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.876, train_loss_epoch=-0.683, valid_loss=5.090]\n","Epoch 56:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.924, train_loss_epoch=-0.767, valid_loss=5.090]\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.421, train_loss_epoch=-0.821, valid_loss=5.090]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10599)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 39.12it/s]\u001b[A\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.919, train_loss_epoch=-0.821, valid_loss=-1.03]\n","Epoch 58:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-0.946, valid_loss=-1.03]\n","Epoch 59:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.883, train_loss_epoch=-0.932, valid_loss=-1.03]\n","Epoch 60:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.988, train_loss_epoch=-0.937, valid_loss=-1.03]\n","Epoch 61:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.993, train_loss_epoch=-0.975, valid_loss=-1.03]\n","Epoch 62:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.00, valid_loss=-1.03]\n","Epoch 63:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.02, valid_loss=-1.03]\n","Epoch 64:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.05, train_loss_epoch=-1.01, valid_loss=-1.03]\n","Epoch 65:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.04, train_loss_epoch=-1.04, valid_loss=-1.03]\n","Epoch 66:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.01, train_loss_epoch=-1.02, valid_loss=-1.03]\n","Epoch 67:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.06, valid_loss=-1.03]\n","Epoch 68:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.06, valid_loss=-1.03]\n","Epoch 69:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.989, train_loss_epoch=-1.03, valid_loss=-1.03]\n","Epoch 70:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.05, train_loss_epoch=-1.05, valid_loss=-1.03]\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.459, train_loss_epoch=-0.949, valid_loss=-1.03]\n","\u001b[36m(_train_tune pid=10599)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10599)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 38.46it/s]\u001b[A\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.08, train_loss_epoch=-0.949, valid_loss=-1.13] \n","Epoch 72:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.07, valid_loss=-1.13]\n","Epoch 73:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.06, valid_loss=-1.13]\n","Epoch 74:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.09, valid_loss=-1.13]\n","Epoch 75:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.06, train_loss_epoch=-1.07, valid_loss=-1.13]\n","Epoch 76:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.07, valid_loss=-1.13]\n","Epoch 77:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.814, train_loss_epoch=-1.03, valid_loss=-1.13]\n","Epoch 78:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.09, valid_loss=-1.13]\n","Epoch 79:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.10, valid_loss=-1.13]\n","Epoch 80:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.11, valid_loss=-1.13]\n","Epoch 81:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.958, train_loss_epoch=-1.07, valid_loss=-1.13]\n","Epoch 82:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.11, valid_loss=-1.13]\n","Epoch 83:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.497, train_loss_epoch=-1.02, valid_loss=-1.13]\n","Epoch 84:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.07, train_loss_epoch=-1.11, valid_loss=-1.13]\n","Epoch 85:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.09, train_loss_epoch=-1.11, valid_loss=-1.13]\n","\u001b[36m(_train_tune pid=10599)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10599)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 39.90it/s]\u001b[A\n","Epoch 85:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.11, valid_loss=-1.16]\n","Epoch 86:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.11, valid_loss=-1.16]\n","Epoch 87:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.12, valid_loss=-1.16]\n","Epoch 88:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.14, valid_loss=-1.16]\n","Epoch 89:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.975, train_loss_epoch=-1.09, valid_loss=-1.16]\n","Epoch 90:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.10, valid_loss=-1.16]\n","Epoch 91:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.10, valid_loss=-1.16]\n","Epoch 92:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.11, valid_loss=-1.16]\n","Epoch 93:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.11, valid_loss=-1.16]\n","Epoch 94:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.05, train_loss_epoch=-1.11, valid_loss=-1.16]\n","Epoch 95:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.00, train_loss_epoch=-1.09, valid_loss=-1.16]\n","Epoch 96:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.14, valid_loss=-1.16]\n","Epoch 97:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.10, valid_loss=-1.16]\n","Epoch 98:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.12, valid_loss=-1.16]\n","Epoch 99:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.09, train_loss_epoch=-1.11, valid_loss=-1.16]\n","Epoch 99: 100%|██████████| 7/7 [00:00\u003c00:00, 47.99it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.11, valid_loss=-1.16]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10599)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 38.64it/s]\u001b[A\n","Epoch 100:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.14, valid_loss=-1.16]\n","Epoch 101:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.13, valid_loss=-1.16]\n","Epoch 102:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.07, train_loss_epoch=-1.10, valid_loss=-1.16]\n","Epoch 103:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.12, valid_loss=-1.16]\n","Epoch 104:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.14, valid_loss=-1.16]\n","Epoch 105:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.04, train_loss_epoch=-1.10, valid_loss=-1.16]\n","Epoch 106:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.76, train_loss_epoch=-1.06, valid_loss=-1.16]\n","Epoch 107:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.11, valid_loss=-1.16]\n","Epoch 108:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.14, valid_loss=-1.16]\n","Epoch 109:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.06, train_loss_epoch=-1.10, valid_loss=-1.16]\n","Epoch 110:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.735, train_loss_epoch=-1.06, valid_loss=-1.16]\n","Epoch 111:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.15, valid_loss=-1.16]\n","Epoch 112:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.13, valid_loss=-1.16]\n","Epoch 113:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.06, train_loss_epoch=-1.11, valid_loss=-1.16]\n","Epoch 114:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.15, valid_loss=-1.16]\n","\u001b[36m(_train_tune pid=10599)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10599)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 38.64it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10599)\u001b[0m \n","Epoch 115:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.14, valid_loss=-1.17]\n","Epoch 116:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.782, train_loss_epoch=-1.09, valid_loss=-1.17]\n","Epoch 117:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.14, valid_loss=-1.17]\n","Epoch 118:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.09, train_loss_epoch=-1.13, valid_loss=-1.17]\n","Epoch 119:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.15, valid_loss=-1.17]\n","Epoch 120:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.13, valid_loss=-1.17]\n","Epoch 121:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.13, valid_loss=-1.17]\n","Epoch 122:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.15, valid_loss=-1.17]\n","Epoch 123:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.14, valid_loss=-1.17]\n","Epoch 124:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.12, valid_loss=-1.17]\n","Epoch 125:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.15, valid_loss=-1.17]\n","Epoch 126:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.15, valid_loss=-1.17]\n","Epoch 127:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.77, train_loss_epoch=-1.07, valid_loss=-1.17]\n","Epoch 128:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.08, train_loss_epoch=-1.12, valid_loss=-1.17]\n","\u001b[36m(_train_tune pid=10599)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10599)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 38.52it/s]\u001b[A\n","                                                                      \u001b[A\n","Epoch 128:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.12, valid_loss=-1.17]\n","Epoch 129:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.15, valid_loss=-1.17]\n","Epoch 130:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.14, valid_loss=-1.17]\n","Epoch 131:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.14, valid_loss=-1.17]\n","Epoch 132:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.14, valid_loss=-1.17]\n","Epoch 133:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.06, train_loss_epoch=-1.13, valid_loss=-1.17]\n","Epoch 134:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.15, valid_loss=-1.17]\n","Epoch 135:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.00, train_loss_epoch=-1.13, valid_loss=-1.17]\n","Epoch 136:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.15, valid_loss=-1.17]\n","Epoch 137:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.08, train_loss_epoch=-1.13, valid_loss=-1.17]\n","Epoch 138:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.14, valid_loss=-1.17]\n","Epoch 139:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.16, valid_loss=-1.17]\n","Epoch 140:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.922, train_loss_epoch=-1.11, valid_loss=-1.17]\n","Epoch 141:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.14, valid_loss=-1.17]\n","Epoch 142:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.13, valid_loss=-1.17]\n","\u001b[36m(_train_tune pid=10599)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10599)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10599)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 39.37it/s]\u001b[A\n","Epoch 143:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.01, train_loss_epoch=-1.11, valid_loss=-1.17]\n","Epoch 144:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.15, valid_loss=-1.17]\n","Epoch 145:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.13, valid_loss=-1.17]\n","Epoch 146:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.13, valid_loss=-1.17]\n","Epoch 147:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.15, valid_loss=-1.17]\n","Epoch 148:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.15, valid_loss=-1.17]\n","Epoch 149:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.14, valid_loss=-1.17]\n","Epoch 150:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.08, train_loss_epoch=-1.12, valid_loss=-1.17]\n","Epoch 151:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.14, valid_loss=-1.17]\n","Epoch 152:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.15, valid_loss=-1.17]\n","Epoch 153:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.851, train_loss_epoch=-1.10, valid_loss=-1.17]\n","Epoch 154:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.04, train_loss_epoch=-1.12, valid_loss=-1.17]\n","Epoch 154: 100%|██████████| 7/7 [00:00\u003c00:00, 51.89it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.13, valid_loss=-1.17]\n","Epoch 155:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.13, valid_loss=-1.17]\n","Epoch 156:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.00, train_loss_epoch=-1.13, valid_loss=-1.17]\n","Epoch 157:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.14, valid_loss=-1.17]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10599)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 40.12it/s]\u001b[A\n","Epoch 157:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.14, valid_loss=-1.18]\n","Epoch 158:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.05, train_loss_epoch=-1.12, valid_loss=-1.18]\n","Epoch 159:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.16, valid_loss=-1.18]\n","Epoch 160:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.993, train_loss_epoch=-1.12, valid_loss=-1.18]\n","Epoch 160: 100%|██████████| 7/7 [00:00\u003c00:00, 49.37it/s, v_num=0, train_loss_step=-1.07, train_loss_epoch=-1.13, valid_loss=-1.18]\n","Epoch 161:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.07, train_loss_epoch=-1.13, valid_loss=-1.18]\n","Epoch 162:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.13, valid_loss=-1.18]\n","Epoch 163:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.16, valid_loss=-1.18]\n","Epoch 164:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.886, train_loss_epoch=-1.10, valid_loss=-1.18]\n","Epoch 165:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.931, train_loss_epoch=-1.12, valid_loss=-1.18]\n","Epoch 166:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.15, valid_loss=-1.18]\n","Epoch 167:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.07, train_loss_epoch=-1.13, valid_loss=-1.18]\n","Epoch 168:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.13, valid_loss=-1.18]\n","Epoch 169:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.01, train_loss_epoch=-1.11, valid_loss=-1.18]\n","Epoch 170:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.15, valid_loss=-1.18]\n","Epoch 171:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.01, train_loss_epoch=-1.11, valid_loss=-1.18]\n","\u001b[36m(_train_tune pid=10599)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-18 13:18:15,044\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (1, 1, 1), 'n_pool_kernel_size': (2, 2, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=10599)\u001b[0m `Trainer.fit` stopped: `max_steps=1200.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=10599)\u001b[0m \n","\u001b[36m(_train_tune pid=10599)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 7/7 [00:00\u003c00:00, 40.46it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10599)\u001b[0m \r                                                                      \u001b[A\rEpoch 171:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.09, train_loss_epoch=-1.11, valid_loss=-1.18]\rEpoch 171:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.09, train_loss_epoch=-1.14, valid_loss=-1.18]\rEpoch 171:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.09, train_loss_epoch=-1.14, valid_loss=-1.18]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=10819)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=10819)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=10819)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=10819)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=10819)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=10819)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=10819)\u001b[0m 2024-11-18 13:18:22.866769: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=10819)\u001b[0m 2024-11-18 13:18:22.887224: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=10819)\u001b[0m 2024-11-18 13:18:22.912115: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=10819)\u001b[0m 2024-11-18 13:18:22.919673: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=10819)\u001b[0m 2024-11-18 13:18:22.938059: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=10819)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=10819)\u001b[0m 2024-11-18 13:18:24.092180: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=10819)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=10819)\u001b[0m \n","\u001b[36m(_train_tune pid=10819)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=10819)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=10819)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=10819)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=10819)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=10819)\u001b[0m 3 | blocks       | ModuleList       | 2.7 M  | train\n","\u001b[36m(_train_tune pid=10819)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=10819)\u001b[0m 2.7 M     Trainable params\n","\u001b[36m(_train_tune pid=10819)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=10819)\u001b[0m 2.7 M     Total params\n","\u001b[36m(_train_tune pid=10819)\u001b[0m 10.938    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=10819)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=10819)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/7 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.27, train_loss_epoch=-1.36]\n","Epoch 2:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.43]\n","Epoch 3:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.37]\n","Epoch 4:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.38]\n","Epoch 5:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.44]\n","Epoch 6:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.23, train_loss_epoch=-1.37]\n","Epoch 7:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.37]\n","Epoch 8:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.39]\n","Epoch 9:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.46]\n","Epoch 10:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.12, train_loss_epoch=-1.36]\n","Epoch 11:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.50]\n","Epoch 12:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.39]\n","Epoch 13:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.41]\n","Epoch 14:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.21, train_loss_epoch=-1.40]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10819)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10819)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.42it/s]\u001b[A\n","Epoch 14:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.40, valid_loss=-1.42]\n","Epoch 15:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.03, train_loss_epoch=-1.41, valid_loss=-1.42]\n","Epoch 16:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.53, valid_loss=-1.42]\n","Epoch 17:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.47, valid_loss=-1.42]\n","Epoch 18:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.49, valid_loss=-1.42]\n","Epoch 19:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.56, valid_loss=-1.42]\n","Epoch 20:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.51, valid_loss=-1.42]\n","Epoch 21:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.54, valid_loss=-1.42]\n","Epoch 22:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.55, valid_loss=-1.42]\n","Epoch 23:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.52, valid_loss=-1.42]\n","Epoch 24:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.53, valid_loss=-1.42]\n","Epoch 25:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.60, valid_loss=-1.42]\n","Epoch 26:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.65, valid_loss=-1.42]\n","Epoch 27:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.58, valid_loss=-1.42]\n","Epoch 28:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.62, valid_loss=-1.42]\n","\u001b[36m(_train_tune pid=10819)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10819)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.42it/s]\u001b[A\n","Epoch 28:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.62, valid_loss=-1.59]\n","Epoch 29:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.806, train_loss_epoch=-1.49, valid_loss=-1.59]\n","Epoch 30:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.68, valid_loss=-1.59]\n","Epoch 31:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.36, train_loss_epoch=-1.58, valid_loss=-1.59]\n","Epoch 32:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.57, valid_loss=-1.59]\n","Epoch 33:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.64, valid_loss=-1.59]\n","Epoch 34:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.60, valid_loss=-1.59]\n","Epoch 35:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.66, valid_loss=-1.59]\n","Epoch 36:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.65, valid_loss=-1.59]\n","Epoch 37:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-1.73, valid_loss=-1.59]\n","Epoch 38:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.64, valid_loss=-1.59]\n","Epoch 39:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.62, valid_loss=-1.59]\n","Epoch 40:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.71, valid_loss=-1.59]\n","Epoch 41:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-1.75, valid_loss=-1.59]\n","Epoch 42:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.71, valid_loss=-1.59]\n","\u001b[36m(_train_tune pid=10819)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10819)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.43it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10819)\u001b[0m \n","Epoch 42:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.71, valid_loss=-1.67]\n","Epoch 43:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.67, valid_loss=-1.67]\n","Epoch 44:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.70, valid_loss=-1.67]\n","Epoch 45:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.71, valid_loss=-1.67]\n","Epoch 46:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.69, valid_loss=-1.67]\n","Epoch 47:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.76, valid_loss=-1.67]\n","Epoch 48:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.69, valid_loss=-1.67]\n","Epoch 49:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.75, valid_loss=-1.67]\n","Epoch 50:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.70, valid_loss=-1.67]\n","Epoch 51:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.77, valid_loss=-1.67]\n","Epoch 52:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.78, valid_loss=-1.67]\n","Epoch 53:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.65, valid_loss=-1.67]\n","Epoch 54:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.76, valid_loss=-1.67]\n","Epoch 55:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.82, valid_loss=-1.67]\n","Epoch 56:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.81, valid_loss=-1.67]\n","Epoch 57:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.69, valid_loss=-1.67]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10819)\u001b[0m \n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10819)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.45it/s]\u001b[A\n","Epoch 57:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.69, valid_loss=-1.77]\n","Epoch 58:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.84, valid_loss=-1.77]\n","Epoch 59:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.82, valid_loss=-1.77]\n","Epoch 60:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.83, valid_loss=-1.77]\n","Epoch 61:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.77, valid_loss=-1.77]\n","Epoch 62:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.85, valid_loss=-1.77]\n","Epoch 63:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.79, valid_loss=-1.77]\n","Epoch 64:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.80, valid_loss=-1.77]\n","Epoch 65:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.81, valid_loss=-1.77]\n","Epoch 66:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.79, valid_loss=-1.77]\n","Epoch 67:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.83, valid_loss=-1.77]\n","Epoch 68:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.86, valid_loss=-1.77]\n","Epoch 69:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.78, valid_loss=-1.77]\n","Epoch 70:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.80, valid_loss=-1.77]\n","Epoch 71:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.933, train_loss_epoch=-1.70, valid_loss=-1.77]\n","\u001b[36m(_train_tune pid=10819)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10819)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.45it/s]\u001b[A\n","Epoch 71:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.70, valid_loss=-1.81] \n","Epoch 72:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.81, valid_loss=-1.81]\n","Epoch 73:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.84, valid_loss=-1.81]\n","Epoch 74:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-1.86, valid_loss=-1.81]\n","Epoch 75:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.83, valid_loss=-1.81]\n","Epoch 76:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.85, valid_loss=-1.81]\n","Epoch 77:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.79, valid_loss=-1.81]\n","Epoch 78:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-1.88, valid_loss=-1.81]\n","Epoch 79:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.85, valid_loss=-1.81]\n","Epoch 80:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.87, valid_loss=-1.81]\n","Epoch 81:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.79, valid_loss=-1.81]\n","Epoch 82:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.82, valid_loss=-1.81]\n","Epoch 83:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.01, train_loss_epoch=-1.74, valid_loss=-1.81]\n","Epoch 84:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.83, valid_loss=-1.81]\n","Epoch 85:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.84, valid_loss=-1.81]\n","\u001b[36m(_train_tune pid=10819)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10819)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.45it/s]\u001b[A\n","Epoch 86:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.83, valid_loss=-1.83]\n","Epoch 87:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-1.94, valid_loss=-1.83]\n","Epoch 88:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.89, valid_loss=-1.83]\n","Epoch 89:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.84, valid_loss=-1.83]\n","Epoch 90:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.85, valid_loss=-1.83]\n","Epoch 91:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.86, valid_loss=-1.83]\n","Epoch 92:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.84, valid_loss=-1.83]\n","Epoch 93:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.88, valid_loss=-1.83]\n","Epoch 94:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.87, valid_loss=-1.83]\n","Epoch 95:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.83, valid_loss=-1.83]\n","Epoch 96:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-1.89, valid_loss=-1.83]\n","Epoch 97:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.87, valid_loss=-1.83]\n","Epoch 98:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.88, valid_loss=-1.83]\n","Epoch 99:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.83, valid_loss=-1.83]\n","Epoch 99: 100%|██████████| 7/7 [00:00\u003c00:00, 37.79it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.83, valid_loss=-1.83]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10819)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.44it/s]\u001b[A\n","Epoch 100:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.89, valid_loss=-1.84]\n","Epoch 101:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.85, valid_loss=-1.84]\n","Epoch 102:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.83, valid_loss=-1.84]\n","Epoch 103:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.86, valid_loss=-1.84]\n","Epoch 104:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-1.89, valid_loss=-1.84]\n","Epoch 105:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.82, valid_loss=-1.84]\n","Epoch 106:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.79, valid_loss=-1.84]\n","Epoch 107:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.87, valid_loss=-1.84]\n","Epoch 108:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-1.90, valid_loss=-1.84]\n","Epoch 109:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.84, valid_loss=-1.84]\n","Epoch 110:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.75, valid_loss=-1.84]\n","Epoch 111:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-1.91, valid_loss=-1.84]\n","Epoch 112:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.87, valid_loss=-1.84]\n","Epoch 113:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.86, valid_loss=-1.84]\n","Epoch 114:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.41, train_loss_epoch=-1.95, valid_loss=-1.84]\n","\u001b[36m(_train_tune pid=10819)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10819)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.44it/s]\u001b[A\n","Epoch 114:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.95, valid_loss=-1.85]\n","Epoch 115:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.86, valid_loss=-1.85]\n","Epoch 116:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.26, train_loss_epoch=-1.76, valid_loss=-1.85]\n","Epoch 117:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-1.92, valid_loss=-1.85]\n","Epoch 118:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.85, valid_loss=-1.85]\n","Epoch 119:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.90, valid_loss=-1.85]\n","Epoch 120:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.88, valid_loss=-1.85]\n","Epoch 121:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.85, valid_loss=-1.85]\n","Epoch 122:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.88, valid_loss=-1.85]\n","Epoch 123:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.87, valid_loss=-1.85]\n","Epoch 124:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.87, valid_loss=-1.85]\n","Epoch 125:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.88, valid_loss=-1.85]\n","Epoch 125: 100%|██████████| 7/7 [00:00\u003c00:00, 36.90it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.90, valid_loss=-1.85]\n","Epoch 126:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.90, valid_loss=-1.85]\n","Epoch 127:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.77, valid_loss=-1.85]\n","Epoch 128:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.83, valid_loss=-1.85]\n","\u001b[36m(_train_tune pid=10819)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10819)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.42it/s]\u001b[A\n","Epoch 128:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.83, valid_loss=-1.86]\n","Epoch 129:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.88, valid_loss=-1.86]\n","Epoch 130:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.85, valid_loss=-1.86]\n","Epoch 131:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.87, valid_loss=-1.86]\n","Epoch 132:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.84, valid_loss=-1.86]\n","Epoch 133:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.86, valid_loss=-1.86]\n","Epoch 134:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.42, train_loss_epoch=-1.96, valid_loss=-1.86]\n","Epoch 135:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.84, valid_loss=-1.86]\n","Epoch 136:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-1.93, valid_loss=-1.86]\n","Epoch 136: 100%|██████████| 7/7 [00:00\u003c00:00, 38.13it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.93, valid_loss=-1.86]\n","Epoch 137:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.89, valid_loss=-1.86]\n","Epoch 138:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.83, valid_loss=-1.86]\n","Epoch 139:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.89, valid_loss=-1.86]\n","Epoch 140:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.83, valid_loss=-1.86]\n","Epoch 141:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.91, valid_loss=-1.86]\n","Epoch 142:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.90, valid_loss=-1.86]\n","\u001b[36m(_train_tune pid=10819)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10819)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.43it/s]\u001b[A\n","Epoch 143:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.88, valid_loss=-1.87]\n","Epoch 144:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.91, valid_loss=-1.87]\n","Epoch 145:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.88, valid_loss=-1.87]\n","Epoch 146:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.88, valid_loss=-1.87]\n","Epoch 146: 100%|██████████| 7/7 [00:00\u003c00:00, 38.55it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.88, valid_loss=-1.87]\n","Epoch 147:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-1.94, valid_loss=-1.87]\n","Epoch 148:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-1.92, valid_loss=-1.87]\n","Epoch 149:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.85, valid_loss=-1.87]\n","Epoch 150:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.85, valid_loss=-1.87]\n","Epoch 151:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.85, valid_loss=-1.87]\n","Epoch 152:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.48, train_loss_epoch=-1.98, valid_loss=-1.87]\n","Epoch 153:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.30, train_loss_epoch=-1.80, valid_loss=-1.87]\n","Epoch 154:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.86, valid_loss=-1.87]\n","Epoch 155:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.88, valid_loss=-1.87]\n","Epoch 156:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.82, valid_loss=-1.87]\n","Epoch 157:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-1.93, valid_loss=-1.87]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10819)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.42it/s]\u001b[A\n","Epoch 157:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.93, valid_loss=-1.87]\n","Epoch 158:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.86, valid_loss=-1.87]\n","Epoch 159:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.86, valid_loss=-1.87]\n","Epoch 160:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.86, valid_loss=-1.87]\n","Epoch 161:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.83, valid_loss=-1.87]\n","Epoch 162:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.85, valid_loss=-1.87]\n","Epoch 163:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.50, train_loss_epoch=-1.98, valid_loss=-1.87]\n","Epoch 164:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.76, valid_loss=-1.87]\n","Epoch 165:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.86, valid_loss=-1.87]\n","Epoch 166:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.93, valid_loss=-1.87]\n","Epoch 167:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.85, valid_loss=-1.87]\n","Epoch 168:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.90, valid_loss=-1.87]\n","Epoch 169:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.86, valid_loss=-1.87]\n","Epoch 170:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-1.96, valid_loss=-1.87]\n","Epoch 171:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.88, valid_loss=-1.87]\n","\u001b[36m(_train_tune pid=10819)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10819)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.43it/s]\u001b[A\n","Epoch 171:   0%|          | 0/7 [00:02\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.88, valid_loss=-1.87]\n","Epoch 172:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.83, valid_loss=-1.87]\n","Epoch 173:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.86, valid_loss=-1.87]\n","Epoch 174:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.85, valid_loss=-1.87]\n","Epoch 175:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-1.93, valid_loss=-1.87]\n","Epoch 176:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.85, valid_loss=-1.87]\n","Epoch 177:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.89, valid_loss=-1.87]\n","Epoch 178:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.88, valid_loss=-1.87]\n","Epoch 179:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.82, valid_loss=-1.87]\n","Epoch 180:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-1.96, valid_loss=-1.87]\n","Epoch 181:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.79, valid_loss=-1.87]\n","Epoch 182:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.89, valid_loss=-1.87]\n","Epoch 183:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.87, valid_loss=-1.87]\n","Epoch 184:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-1.94, valid_loss=-1.87]\n","Epoch 185:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.88, valid_loss=-1.87]\n","\u001b[36m(_train_tune pid=10819)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10819)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.41it/s]\u001b[A\n","Epoch 186:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.89, valid_loss=-1.88]\n","Epoch 187:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.89, valid_loss=-1.88]\n","Epoch 188:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.08, train_loss_epoch=-1.77, valid_loss=-1.88]\n","Epoch 189:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.92, valid_loss=-1.88]\n","Epoch 190:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.86, valid_loss=-1.88]\n","Epoch 191:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.92, valid_loss=-1.88]\n","Epoch 192:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.90, valid_loss=-1.88]\n","Epoch 193:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.53, train_loss_epoch=-1.97, valid_loss=-1.88]\n","Epoch 194:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.89, valid_loss=-1.88]\n","Epoch 195:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.89, valid_loss=-1.88]\n","Epoch 196:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.84, valid_loss=-1.88]\n","Epoch 197:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.85, valid_loss=-1.88]\n","Epoch 198:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-1.94, valid_loss=-1.88]\n","Epoch 199:   0%|          | 0/7 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.94, valid_loss=-1.88]\n","Epoch 199: 100%|██████████| 7/7 [00:00\u003c00:00, 38.48it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.94, valid_loss=-1.88]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=10819)\u001b[0m `Trainer.fit` stopped: `max_steps=1400.0` reached.\n","2024-11-18 13:19:33,249\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (1, 1, 1), 'n_pool_kernel_size': (2, 2, 2), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=10819)\u001b[0m \n","\u001b[36m(_train_tune pid=10819)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 7/7 [00:02\u003c00:00,  3.43it/s]\u001b[A\n","\u001b[36m(_train_tune pid=10819)\u001b[0m \r                                                                      \u001b[A\rEpoch 199: 100%|██████████| 7/7 [00:02\u003c00:00,  3.12it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.94, valid_loss=-1.88]\rEpoch 199: 100%|██████████| 7/7 [00:02\u003c00:00,  3.12it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.85, valid_loss=-1.88]\rEpoch 199: 100%|██████████| 7/7 [00:02\u003c00:00,  3.12it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.85, valid_loss=-1.88]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=11207)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=11207)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=11207)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=11207)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=11207)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=11207)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=11207)\u001b[0m 2024-11-18 13:19:40.817861: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=11207)\u001b[0m 2024-11-18 13:19:40.838410: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=11207)\u001b[0m 2024-11-18 13:19:40.863545: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=11207)\u001b[0m 2024-11-18 13:19:40.871145: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=11207)\u001b[0m 2024-11-18 13:19:40.889030: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=11207)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=11207)\u001b[0m 2024-11-18 13:19:42.030773: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=11207)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=11207)\u001b[0m \n","\u001b[36m(_train_tune pid=11207)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=11207)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=11207)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=11207)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=11207)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=11207)\u001b[0m 3 | blocks       | ModuleList       | 2.5 M  | train\n","\u001b[36m(_train_tune pid=11207)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=11207)\u001b[0m 2.5 M     Trainable params\n","\u001b[36m(_train_tune pid=11207)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=11207)\u001b[0m 2.5 M     Total params\n","\u001b[36m(_train_tune pid=11207)\u001b[0m 10.079    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=11207)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=11207)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/4 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-0.753]\n","Epoch 2:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.36]\n","Epoch 3:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.47]\n","Epoch 4:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.36, train_loss_epoch=-1.46]\n","Epoch 5:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.58]\n","Epoch 6:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.48]\n","Epoch 7:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.63]\n","Epoch 8:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.51]\n","Epoch 9:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.68]\n","Epoch 10:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-1.53]\n","Epoch 11:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.79]\n","Epoch 12:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.63]\n","Epoch 13:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.59]\n","Epoch 14:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.59]\n","Epoch 15:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.55]\n","Epoch 16:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.59]\n","Epoch 17:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.78]\n","Epoch 18:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-1.80]\n","Epoch 19:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.74]\n","Epoch 20:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.64]\n","Epoch 21:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.79]\n","Epoch 22:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.66]\n","Epoch 23:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.73]\n","Epoch 24:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.64]\n","Epoch 24: 100%|██████████| 4/4 [00:00\u003c00:00, 26.48it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-1.64]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11207)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.16it/s]\u001b[A\n","Epoch 25:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-1.83, valid_loss=-1.69]\n","Epoch 26:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.79, valid_loss=-1.69]\n","Epoch 27:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.73, valid_loss=-1.69]\n","Epoch 28:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.71, valid_loss=-1.69]\n","Epoch 29:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.842, train_loss_epoch=-1.47, valid_loss=-1.69]\n","Epoch 30:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.72, valid_loss=-1.69]\n","Epoch 31:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.73, valid_loss=-1.69]\n","Epoch 32:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.65, valid_loss=-1.69]\n","Epoch 33:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.67, valid_loss=-1.69]\n","Epoch 34:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.63, valid_loss=-1.69]\n","Epoch 35:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.68, valid_loss=-1.69]\n","Epoch 36:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.78, valid_loss=-1.69]\n","Epoch 37:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.74, valid_loss=-1.69]\n","Epoch 38:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.79, valid_loss=-1.69]\n","Epoch 39:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.75, valid_loss=-1.69]\n","Epoch 40:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.64, valid_loss=-1.69]\n","Epoch 41:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.29, train_loss_epoch=-1.61, valid_loss=-1.69]\n","Epoch 42:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.55, valid_loss=-1.69]\n","Epoch 43:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.77, valid_loss=-1.69]\n","Epoch 44:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.72, valid_loss=-1.69]\n","Epoch 45:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.74, valid_loss=-1.69]\n","Epoch 46:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.70, valid_loss=-1.69]\n","Epoch 47:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.70, valid_loss=-1.69]\n","Epoch 48:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.75, valid_loss=-1.69]\n","Epoch 49:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.64, valid_loss=-1.69]\n","Epoch 49: 100%|██████████| 4/4 [00:00\u003c00:00, 26.44it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.64, valid_loss=-1.69]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11207)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.18it/s]\u001b[A\n","Epoch 50:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.75, valid_loss=-1.71]\n","Epoch 51:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.71, valid_loss=-1.71]\n","Epoch 52:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.77, valid_loss=-1.71]\n","Epoch 53:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.73, valid_loss=-1.71]\n","Epoch 54:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-1.86, valid_loss=-1.71]\n","Epoch 55:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.69, valid_loss=-1.71]\n","Epoch 56:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.74, valid_loss=-1.71]\n","Epoch 57:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-1.79, valid_loss=-1.71]\n","Epoch 58:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.65, valid_loss=-1.71]\n","Epoch 59:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-1.81, valid_loss=-1.71]\n","Epoch 60:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.75, valid_loss=-1.71]\n","Epoch 61:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.72, valid_loss=-1.71]\n","Epoch 62:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.66, valid_loss=-1.71]\n","Epoch 63:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.76, valid_loss=-1.71]\n","Epoch 64:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.74, valid_loss=-1.71]\n","Epoch 65:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.66, valid_loss=-1.71]\n","Epoch 66:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.67, valid_loss=-1.71]\n","Epoch 67:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.68, valid_loss=-1.71]\n","Epoch 68:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.79, valid_loss=-1.71]\n","Epoch 69:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.74, valid_loss=-1.71]\n","Epoch 70:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.72, valid_loss=-1.71]\n","Epoch 71:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.69, valid_loss=-1.71]\n","Epoch 72:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.771, train_loss_epoch=-1.48, valid_loss=-1.71]\n","Epoch 73:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.70, valid_loss=-1.71]\n","Epoch 74:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.72, valid_loss=-1.71]\n","Epoch 74: 100%|██████████| 4/4 [00:00\u003c00:00, 26.34it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.72, valid_loss=-1.71]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11207)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.15it/s]\u001b[A\n","Epoch 75:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.60, valid_loss=-1.73]\n","Epoch 76:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.73, valid_loss=-1.73]\n","Epoch 77:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.73, valid_loss=-1.73]\n","Epoch 78:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.64, valid_loss=-1.73]\n","Epoch 79:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.80, valid_loss=-1.73]\n","Epoch 80:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.70, valid_loss=-1.73]\n","Epoch 81:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.79, valid_loss=-1.73]\n","Epoch 82:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.70, valid_loss=-1.73]\n","Epoch 83:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.72, valid_loss=-1.73]\n","Epoch 84:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.681, train_loss_epoch=-1.44, valid_loss=-1.73]\n","Epoch 85:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.68, valid_loss=-1.73]\n","Epoch 86:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.73, valid_loss=-1.73]\n","Epoch 87:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.73, valid_loss=-1.73]\n","Epoch 88:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.82, valid_loss=-1.73]\n","Epoch 89:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.65, valid_loss=-1.73]\n","Epoch 90:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.20, train_loss_epoch=-1.60, valid_loss=-1.73]\n","Epoch 91:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.67, valid_loss=-1.73]\n","Epoch 92:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.83, valid_loss=-1.73]\n","Epoch 93:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.68, valid_loss=-1.73]\n","Epoch 94:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.83, valid_loss=-1.73]\n","Epoch 95:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.80, valid_loss=-1.73]\n","Epoch 96:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.67, valid_loss=-1.73]\n","Epoch 97:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.76, valid_loss=-1.73]\n","Epoch 98:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.82, valid_loss=-1.73]\n","Epoch 99:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.69, valid_loss=-1.73]\n","Epoch 99: 100%|██████████| 4/4 [00:00\u003c00:00, 27.04it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.69, valid_loss=-1.73]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11207)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.17it/s]\u001b[A\n","Epoch 100:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.72, valid_loss=-1.72]\n","Epoch 101:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-1.84, valid_loss=-1.72]\n","Epoch 102:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.71, valid_loss=-1.72]\n","Epoch 103:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.79, valid_loss=-1.72]\n","Epoch 104:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.68, valid_loss=-1.72]\n","Epoch 105:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.83, valid_loss=-1.72]\n","Epoch 106:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.81, valid_loss=-1.72]\n","Epoch 107:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.72, valid_loss=-1.72]\n","Epoch 108:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.75, valid_loss=-1.72]\n","Epoch 109:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.69, valid_loss=-1.72]\n","Epoch 110:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.79, valid_loss=-1.72]\n","Epoch 111:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.74, valid_loss=-1.72]\n","Epoch 112:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.69, valid_loss=-1.72]\n","Epoch 113:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-1.86, valid_loss=-1.72]\n","Epoch 114:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.83, valid_loss=-1.72]\n","Epoch 115:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.83, valid_loss=-1.72]\n","Epoch 116:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.70, valid_loss=-1.72]\n","Epoch 117:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.42, train_loss_epoch=-1.69, valid_loss=-1.72]\n","Epoch 118:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.04, train_loss_epoch=-1.61, valid_loss=-1.72]\n","Epoch 119:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-1.88, valid_loss=-1.72]\n","Epoch 120:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.64, valid_loss=-1.72]\n","Epoch 121:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.80, valid_loss=-1.72]\n","Epoch 122:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.75, valid_loss=-1.72]\n","Epoch 123:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.69, valid_loss=-1.72]\n","Epoch 124:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.75, valid_loss=-1.72]\n","Epoch 124: 100%|██████████| 4/4 [00:00\u003c00:00, 26.35it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.75, valid_loss=-1.72]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11207)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.18it/s]\u001b[A\n","Epoch 125:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.76, valid_loss=-1.75]\n","Epoch 126:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.81, valid_loss=-1.75]\n","Epoch 127:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.74, valid_loss=-1.75]\n","Epoch 128:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.81, valid_loss=-1.75]\n","Epoch 129:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.70, valid_loss=-1.75]\n","Epoch 130:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.72, valid_loss=-1.75]\n","Epoch 131:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.74, valid_loss=-1.75]\n","Epoch 132:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.74, valid_loss=-1.75]\n","Epoch 133:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.74, valid_loss=-1.75]\n","Epoch 134:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.72, valid_loss=-1.75]\n","Epoch 135:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.66, valid_loss=-1.75]\n","Epoch 136:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.35, train_loss_epoch=-1.88, valid_loss=-1.75]\n","Epoch 137:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.64, valid_loss=-1.75]\n","Epoch 138:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-1.85, valid_loss=-1.75]\n","Epoch 139:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.76, valid_loss=-1.75]\n","Epoch 140:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.71, valid_loss=-1.75]\n","Epoch 141:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.78, valid_loss=-1.75]\n","Epoch 142:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.57, valid_loss=-1.75]\n","Epoch 143:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.81, valid_loss=-1.75]\n","Epoch 144:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.80, valid_loss=-1.75]\n","Epoch 145:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.75, valid_loss=-1.75]\n","Epoch 146:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.76, valid_loss=-1.75]\n","Epoch 147:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.76, valid_loss=-1.75]\n","Epoch 148:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.80, valid_loss=-1.75]\n","Epoch 149:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.24, train_loss_epoch=-1.62, valid_loss=-1.75]\n","Epoch 149: 100%|██████████| 4/4 [00:00\u003c00:00, 27.18it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.62, valid_loss=-1.75]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11207)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11207)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.16it/s]\u001b[A\n","Epoch 150:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.75, valid_loss=-1.75]\n","Epoch 151:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.73, valid_loss=-1.75]\n","Epoch 152:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.67, valid_loss=-1.75]\n","Epoch 153:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.71, valid_loss=-1.75]\n","Epoch 154:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-1.91, valid_loss=-1.75]\n","Epoch 155:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.61, valid_loss=-1.75]\n","Epoch 156:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.67, valid_loss=-1.75]\n","Epoch 156: 100%|██████████| 4/4 [00:00\u003c00:00, 28.60it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.78, valid_loss=-1.75]\n","Epoch 157:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.78, valid_loss=-1.75]\n","Epoch 158:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.66, valid_loss=-1.75]\n","Epoch 159:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-1.85, valid_loss=-1.75]\n","Epoch 160:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.69, valid_loss=-1.75]\n","Epoch 161:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-1.91, valid_loss=-1.75]\n","Epoch 162:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-1.91, valid_loss=-1.75]\n","Epoch 163:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.81, valid_loss=-1.75]\n","Epoch 164:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.80, valid_loss=-1.75]\n","Epoch 165:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.17, train_loss_epoch=-1.62, valid_loss=-1.75]\n","Epoch 166:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.70, valid_loss=-1.75]\n","Epoch 167:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.76, valid_loss=-1.75]\n","Epoch 168:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.74, valid_loss=-1.75]\n","Epoch 169:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-1.89, valid_loss=-1.75]\n","Epoch 170:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.70, valid_loss=-1.75]\n","Epoch 171:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.82, valid_loss=-1.75]\n","Epoch 172:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.87, valid_loss=-1.75]\n","Epoch 173:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.65, valid_loss=-1.75]\n","Epoch 174:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.80, valid_loss=-1.75]\n","Epoch 174: 100%|██████████| 4/4 [00:00\u003c00:00, 26.79it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.80, valid_loss=-1.75]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11207)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.19it/s]\u001b[A\n","Epoch 175:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.81, valid_loss=-1.75]\n","Epoch 176:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.78, valid_loss=-1.75]\n","Epoch 177:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.75, valid_loss=-1.75]\n","Epoch 178:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.75, valid_loss=-1.75]\n","Epoch 179:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.73, valid_loss=-1.75]\n","Epoch 180:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.73, valid_loss=-1.75]\n","Epoch 181:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.86, valid_loss=-1.75]\n","Epoch 182:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.82, valid_loss=-1.75]\n","Epoch 183:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.81, valid_loss=-1.75]\n","Epoch 184:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.66, valid_loss=-1.75]\n","Epoch 185:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.36, train_loss_epoch=-1.66, valid_loss=-1.75]\n","Epoch 186:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.85, valid_loss=-1.75]\n","Epoch 187:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.66, valid_loss=-1.75]\n","Epoch 188:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.74, valid_loss=-1.75]\n","Epoch 189:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.922, train_loss_epoch=-1.57, valid_loss=-1.75]\n","Epoch 190:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.73, valid_loss=-1.75]\n","Epoch 191:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.03, train_loss_epoch=-1.57, valid_loss=-1.75]\n","Epoch 192:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.80, valid_loss=-1.75]\n","Epoch 193:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.70, valid_loss=-1.75]\n","Epoch 194:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.78, valid_loss=-1.75]\n","Epoch 195:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.79, valid_loss=-1.75]\n","Epoch 196:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.36, train_loss_epoch=-1.92, valid_loss=-1.75]\n","Epoch 197:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.74, valid_loss=-1.75]\n","Epoch 198:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.75, valid_loss=-1.75]\n","Epoch 199:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.72, valid_loss=-1.75]\n","Epoch 199: 100%|██████████| 4/4 [00:00\u003c00:00, 26.13it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.72, valid_loss=-1.75]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11207)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.19it/s]\u001b[A\n","Epoch 200:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.74, valid_loss=-1.75]\n","Epoch 201:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.73, valid_loss=-1.75]\n","Epoch 202:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.73, valid_loss=-1.75]\n","Epoch 203:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.72, valid_loss=-1.75]\n","Epoch 204:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.68, valid_loss=-1.75]\n","Epoch 205:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-1.87, valid_loss=-1.75]\n","Epoch 206:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-1.86, valid_loss=-1.75]\n","Epoch 207:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-1.88, valid_loss=-1.75]\n","Epoch 208:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.75, valid_loss=-1.75]\n","Epoch 209:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.04, train_loss_epoch=-1.57, valid_loss=-1.75]\n","Epoch 210:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.76, valid_loss=-1.75]\n","Epoch 211:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.75, valid_loss=-1.75]\n","Epoch 212:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.70, valid_loss=-1.75]\n","Epoch 213:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.67, valid_loss=-1.75]\n","Epoch 214:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.82, valid_loss=-1.75]\n","Epoch 215:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.81, valid_loss=-1.75]\n","Epoch 216:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.73, valid_loss=-1.75]\n","Epoch 217:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-1.90, valid_loss=-1.75]\n","Epoch 218:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.65, valid_loss=-1.75]\n","Epoch 219:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.71, valid_loss=-1.75]\n","Epoch 220:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.03, train_loss_epoch=-1.57, valid_loss=-1.75]\n","Epoch 221:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.77, valid_loss=-1.75]\n","Epoch 222:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.72, valid_loss=-1.75]\n","Epoch 222: 100%|██████████| 4/4 [00:00\u003c00:00, 27.31it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.72, valid_loss=-1.75]\n","Epoch 222:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-1.87, valid_loss=-1.75]        \n","Epoch 223:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-1.87, valid_loss=-1.75]\n","Epoch 224:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.79, valid_loss=-1.75]\n","Epoch 224: 100%|██████████| 4/4 [00:00\u003c00:00, 26.77it/s, v_num=0, train_loss_step=-0.861, train_loss_epoch=-1.79, valid_loss=-1.75]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11207)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11207)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.17it/s]\u001b[A\n","Epoch 225:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.861, train_loss_epoch=-1.56, valid_loss=-1.76]\n","Epoch 226:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.73, valid_loss=-1.76]\n","Epoch 227:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.81, valid_loss=-1.76]\n","Epoch 228:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.76, valid_loss=-1.76]\n","Epoch 229:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.60, valid_loss=-1.76]\n","Epoch 230:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.67, valid_loss=-1.76]\n","Epoch 231:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.13, train_loss_epoch=-1.61, valid_loss=-1.76]\n","Epoch 232:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.62, valid_loss=-1.76]\n","Epoch 233:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.75, valid_loss=-1.76]\n","Epoch 234:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.76, valid_loss=-1.76]\n","Epoch 235:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.86, valid_loss=-1.76]\n","Epoch 236:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.72, valid_loss=-1.76]\n","Epoch 237:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.15, train_loss_epoch=-1.62, valid_loss=-1.76]\n","Epoch 238:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.77, valid_loss=-1.76]\n","Epoch 239:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.63, valid_loss=-1.76]\n","Epoch 240:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.74, valid_loss=-1.76]\n","Epoch 241:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.83, valid_loss=-1.76]\n","Epoch 242:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.76, valid_loss=-1.76]\n","Epoch 242: 100%|██████████| 4/4 [00:00\u003c00:00, 26.66it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.76, valid_loss=-1.76]\n","Epoch 243:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.71, valid_loss=-1.76]\n","Epoch 244:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.80, valid_loss=-1.76]\n","Epoch 245:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.36, train_loss_epoch=-1.69, valid_loss=-1.76]\n","Epoch 246:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.75, valid_loss=-1.76]\n","Epoch 247:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.67, valid_loss=-1.76]\n","Epoch 248:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.75, valid_loss=-1.76]\n","Epoch 249:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-1.91, valid_loss=-1.76]\n","Epoch 249: 100%|██████████| 4/4 [00:00\u003c00:00, 26.92it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.91, valid_loss=-1.76]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11207)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.15it/s]\u001b[A\n","Epoch 250:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.81, valid_loss=-1.75]\n","Epoch 251:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.59, valid_loss=-1.75]\n","Epoch 252:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.31, train_loss_epoch=-1.67, valid_loss=-1.75]\n","Epoch 253:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.77, valid_loss=-1.75]\n","Epoch 254:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.60, valid_loss=-1.75]\n","Epoch 255:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.76, valid_loss=-1.75]\n","Epoch 256:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.84, valid_loss=-1.75]\n","Epoch 257:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.78, valid_loss=-1.75]\n","Epoch 258:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-1.86, valid_loss=-1.75]\n","Epoch 259:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.902, train_loss_epoch=-1.55, valid_loss=-1.75]\n","Epoch 260:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.79, valid_loss=-1.75]\n","Epoch 261:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.72, valid_loss=-1.75]\n","Epoch 262:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.79, valid_loss=-1.75]\n","Epoch 263:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.751, train_loss_epoch=-1.52, valid_loss=-1.75]\n","Epoch 264:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.77, valid_loss=-1.75]\n","Epoch 265:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-1.90, valid_loss=-1.75]\n","Epoch 266:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.85, valid_loss=-1.75]\n","Epoch 267:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.52, train_loss_epoch=-1.71, valid_loss=-1.75]\n","Epoch 268:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.61, valid_loss=-1.75]\n","Epoch 269:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-1.89, valid_loss=-1.75]\n","Epoch 270:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.71, valid_loss=-1.75]\n","Epoch 271:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.73, valid_loss=-1.75]\n","Epoch 272:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.71, valid_loss=-1.75]\n","Epoch 273:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.66, valid_loss=-1.75]\n","Epoch 274:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.36, train_loss_epoch=-1.67, valid_loss=-1.75]\n","Epoch 274: 100%|██████████| 4/4 [00:00\u003c00:00, 26.10it/s, v_num=0, train_loss_step=-1.04, train_loss_epoch=-1.67, valid_loss=-1.75]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11207)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11207)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.15it/s]\u001b[A\n","Epoch 275:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.04, train_loss_epoch=-1.58, valid_loss=-1.76]\n","Epoch 276:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.79, valid_loss=-1.76]\n","Epoch 277:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.78, valid_loss=-1.76]\n","Epoch 278:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.74, valid_loss=-1.76]\n","Epoch 279:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.46, train_loss_epoch=-1.71, valid_loss=-1.76]\n","Epoch 280:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.83, valid_loss=-1.76]\n","Epoch 281:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.82, valid_loss=-1.76]\n","Epoch 282:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.83, valid_loss=-1.76]\n","Epoch 283:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.81, valid_loss=-1.76]\n","Epoch 284:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.78, valid_loss=-1.76]\n","Epoch 285:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.72, valid_loss=-1.76]\n","Epoch 286:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.72, valid_loss=-1.76]\n","Epoch 287:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.83, valid_loss=-1.76]\n","Epoch 288:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.79, valid_loss=-1.76]\n","Epoch 289:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.87, valid_loss=-1.76]\n","Epoch 290:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.79, valid_loss=-1.76]\n","Epoch 291:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.79, valid_loss=-1.76]\n","Epoch 292:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.34, train_loss_epoch=-1.65, valid_loss=-1.76]\n","Epoch 293:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.76, valid_loss=-1.76]\n","Epoch 294:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.85, valid_loss=-1.76]\n","Epoch 295:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-1.90, valid_loss=-1.76]\n","Epoch 296:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.83, valid_loss=-1.76]\n","Epoch 297:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.79, valid_loss=-1.76]\n","Epoch 298:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.75, valid_loss=-1.76]\n","Epoch 299:   0%|          | 0/4 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.74, valid_loss=-1.76]\n","Epoch 299: 100%|██████████| 4/4 [00:00\u003c00:00, 27.15it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.74, valid_loss=-1.76]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11207)\u001b[0m \n","Validation:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/4 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-18 13:20:53,011\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (24, 12, 1), 'n_pool_kernel_size': (16, 8, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=11207)\u001b[0m `Trainer.fit` stopped: `max_steps=1200.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=11207)\u001b[0m \n","\u001b[36m(_train_tune pid=11207)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 4/4 [00:01\u003c00:00,  2.14it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11207)\u001b[0m \r                                                                      \u001b[A\rEpoch 299: 100%|██████████| 4/4 [00:02\u003c00:00,  1.96it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.74, valid_loss=-1.76]\rEpoch 299: 100%|██████████| 4/4 [00:02\u003c00:00,  1.96it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.80, valid_loss=-1.76]\rEpoch 299: 100%|██████████| 4/4 [00:02\u003c00:00,  1.96it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.80, valid_loss=-1.76]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=11597)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=11597)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=11597)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=11597)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=11597)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=11597)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=11597)\u001b[0m 2024-11-18 13:21:00.835196: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=11597)\u001b[0m 2024-11-18 13:21:00.856702: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=11597)\u001b[0m 2024-11-18 13:21:00.883761: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=11597)\u001b[0m 2024-11-18 13:21:00.892706: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=11597)\u001b[0m 2024-11-18 13:21:00.912164: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=11597)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=11597)\u001b[0m 2024-11-18 13:21:02.080070: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=11597)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=11597)\u001b[0m \n","\u001b[36m(_train_tune pid=11597)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=11597)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=11597)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=11597)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=11597)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=11597)\u001b[0m 3 | blocks       | ModuleList       | 3.0 M  | train\n","\u001b[36m(_train_tune pid=11597)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=11597)\u001b[0m 3.0 M     Trainable params\n","\u001b[36m(_train_tune pid=11597)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=11597)\u001b[0m 3.0 M     Total params\n","\u001b[36m(_train_tune pid=11597)\u001b[0m 12.197    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=11597)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=11597)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/13 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=4.020]\n","Epoch 2:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-1.59]\n","Epoch 3:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.66]\n","Epoch 4:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.69]\n","Epoch 5:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.75]\n","Epoch 6:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.75]\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.78]\n","\u001b[36m(_train_tune pid=11597)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11597)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 65.90it/s]\u001b[A\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.78, valid_loss=-1.80]\n","Epoch 8:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.77, valid_loss=-1.80]\n","Epoch 9:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.76, valid_loss=-1.80]\n","Epoch 10:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.80, valid_loss=-1.80]\n","Epoch 11:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-1.84, valid_loss=-1.80]\n","Epoch 12:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.79, valid_loss=-1.80]\n","Epoch 13:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.84, valid_loss=-1.80]\n","Epoch 14:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.39, train_loss_epoch=-1.76, valid_loss=-1.80]\n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.78, valid_loss=-1.80]\n","\u001b[36m(_train_tune pid=11597)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11597)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 63.12it/s]\u001b[A\n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.78, valid_loss=-1.77]\n","Epoch 16:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-1.85, valid_loss=-1.77]\n","Epoch 17:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-1.84, valid_loss=-1.77]\n","Epoch 18:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.82, valid_loss=-1.77]\n","Epoch 19:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.80, valid_loss=-1.77]\n","Epoch 20:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-1.83, valid_loss=-1.77]\n","Epoch 21:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.82, valid_loss=-1.77]\n","Epoch 22:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.81, valid_loss=-1.77]\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.80, valid_loss=-1.77]\n","\u001b[36m(_train_tune pid=11597)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11597)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 65.39it/s]\u001b[A\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.80, valid_loss=-1.81]\n","Epoch 24:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-1.84, valid_loss=-1.81]\n","Epoch 25:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-1.85, valid_loss=-1.81]\n","Epoch 26:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.83, valid_loss=-1.81]\n","Epoch 27:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.83, valid_loss=-1.81]\n","Epoch 28:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.04, train_loss_epoch=-1.77, valid_loss=-1.81]\n","Epoch 29:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-1.85, valid_loss=-1.81]\n","Epoch 30:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.84, valid_loss=-1.81]\n","\u001b[36m(_train_tune pid=11597)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11597)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 64.43it/s]\u001b[A\n","Epoch 31:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.80, valid_loss=-1.81]\n","Epoch 32:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.83, valid_loss=-1.81]\n","Epoch 33:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.81, valid_loss=-1.81]\n","Epoch 34:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.86, valid_loss=-1.81]\n","Epoch 35:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.84, valid_loss=-1.81]\n","Epoch 36:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.42, train_loss_epoch=-1.89, valid_loss=-1.81]\n","Epoch 37:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.85, valid_loss=-1.81]\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.80, valid_loss=-1.81]\n","\u001b[36m(_train_tune pid=11597)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11597)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 66.22it/s]\u001b[A\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.80, valid_loss=-1.80]\n","Epoch 39:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.83, valid_loss=-1.80]\n","Epoch 40:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.79, valid_loss=-1.80]\n","Epoch 41:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.84, valid_loss=-1.80]\n","Epoch 42:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.83, valid_loss=-1.80]\n","Epoch 43:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.84, valid_loss=-1.80]\n","Epoch 44:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.81, valid_loss=-1.80]\n","Epoch 45:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.83, valid_loss=-1.80]\n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.87, valid_loss=-1.80]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11597)\u001b[0m \n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11597)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 66.18it/s]\u001b[A\n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.87, valid_loss=-1.82]\n","Epoch 47:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.82, valid_loss=-1.82]\n","Epoch 48:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-1.87, valid_loss=-1.82]\n","Epoch 49:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.84, valid_loss=-1.82]\n","Epoch 50:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.85, valid_loss=-1.82]\n","Epoch 51:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.84, valid_loss=-1.82]\n","Epoch 52:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.44, train_loss_epoch=-1.90, valid_loss=-1.82]\n","Epoch 53:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.83, valid_loss=-1.82]\n","\u001b[36m(_train_tune pid=11597)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11597)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 66.97it/s]\u001b[A\n","Epoch 53:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.83, valid_loss=-1.78]\n","Epoch 54:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.81, valid_loss=-1.78]\n","Epoch 55:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.18, train_loss_epoch=-1.78, valid_loss=-1.78]\n","Epoch 56:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.86, valid_loss=-1.78]\n","Epoch 57:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.82, valid_loss=-1.78]\n","Epoch 58:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-1.87, valid_loss=-1.78]\n","Epoch 59:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.83, valid_loss=-1.78]\n","Epoch 60:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.82, valid_loss=-1.78]\n","Epoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.85, valid_loss=-1.78]\n","\u001b[36m(_train_tune pid=11597)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11597)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 64.34it/s]\u001b[A\n","Epoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.85, valid_loss=-1.83]\n","Epoch 62:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.83, valid_loss=-1.83]\n","Epoch 63:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.84, valid_loss=-1.83]\n","Epoch 63: 100%|██████████| 13/13 [00:00\u003c00:00, 60.52it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.83, valid_loss=-1.83]\n","Epoch 64:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.83, valid_loss=-1.83]\n","Epoch 65:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.87, valid_loss=-1.83]\n","Epoch 66:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.84, valid_loss=-1.83]\n","Epoch 67:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.81, valid_loss=-1.83]\n","Epoch 68:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.82, valid_loss=-1.83]\n","Epoch 69:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.988, train_loss_epoch=-1.79, valid_loss=-1.83]\n","\u001b[36m(_train_tune pid=11597)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11597)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 61.22it/s]\u001b[A\n","Epoch 69:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.79, valid_loss=-1.82] \n","Epoch 70:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.82, valid_loss=-1.82]\n","Epoch 71:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.86, valid_loss=-1.82]\n","Epoch 72:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.88, valid_loss=-1.82]\n","Epoch 73:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.85, valid_loss=-1.82]\n","Epoch 74:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.84, valid_loss=-1.82]\n","Epoch 75:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.82, valid_loss=-1.82]\n","Epoch 76:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-1.85, valid_loss=-1.82]\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-18 13:21:23,201\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (60, 8, 1), 'n_pool_kernel_size': (8, 4, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=11597)\u001b[0m \n","\u001b[36m(_train_tune pid=11597)\u001b[0m \rValidation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11597)\u001b[0m \rValidation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11597)\u001b[0m \rValidation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=11597)\u001b[0m `Trainer.fit` stopped: `max_steps=1000.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=11597)\u001b[0m \n","\u001b[36m(_train_tune pid=11597)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 66.99it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11597)\u001b[0m \r                                                                        \u001b[A\rEpoch 76:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.85, valid_loss=-1.82]\rEpoch 76:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.84, valid_loss=-1.82]\rEpoch 76:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.84, valid_loss=-1.82]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=11782)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=11782)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=11782)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=11782)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=11782)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=11782)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=11782)\u001b[0m 2024-11-18 13:21:30.829160: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=11782)\u001b[0m 2024-11-18 13:21:30.849745: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=11782)\u001b[0m 2024-11-18 13:21:30.874848: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=11782)\u001b[0m 2024-11-18 13:21:30.882446: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=11782)\u001b[0m 2024-11-18 13:21:30.900532: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=11782)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=11782)\u001b[0m 2024-11-18 13:21:32.063498: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=11782)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=11782)\u001b[0m \n","\u001b[36m(_train_tune pid=11782)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=11782)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=11782)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=11782)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=11782)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=11782)\u001b[0m 3 | blocks       | ModuleList       | 2.6 M  | train\n","\u001b[36m(_train_tune pid=11782)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=11782)\u001b[0m 2.6 M     Trainable params\n","\u001b[36m(_train_tune pid=11782)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=11782)\u001b[0m 2.6 M     Total params\n","\u001b[36m(_train_tune pid=11782)\u001b[0m 10.280    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=11782)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=11782)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","                                                                           \n","Epoch 0:   0%|          | 0/13 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.663, train_loss_epoch=0.338]\n","Epoch 2:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.28]\n","Epoch 3:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.53]\n","Epoch 4:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.68]\n","Epoch 5:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.74]\n","Epoch 6:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.72]\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.81]\n","\u001b[36m(_train_tune pid=11782)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11782)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 64.91it/s]\u001b[A\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.81, valid_loss=-1.83]\n","Epoch 8:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.81, valid_loss=-1.83]\n","Epoch 9:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.83, valid_loss=-1.83]\n","Epoch 10:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.86, valid_loss=-1.83]\n","Epoch 11:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-1.91, valid_loss=-1.83]\n","Epoch 12:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.88, valid_loss=-1.83]\n","Epoch 13:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.89, valid_loss=-1.83]\n","Epoch 14:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.86, valid_loss=-1.83]\n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.90, valid_loss=-1.83]\n","\u001b[36m(_train_tune pid=11782)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11782)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 66.79it/s]\u001b[A\n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.90, valid_loss=-1.88]\n","Epoch 16:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-1.98, valid_loss=-1.88]\n","Epoch 17:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.44, train_loss_epoch=-1.95, valid_loss=-1.88]\n","Epoch 18:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-1.94, valid_loss=-1.88]\n","Epoch 18: 100%|██████████| 13/13 [00:00\u003c00:00, 59.77it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.89, valid_loss=-1.88]\n","Epoch 19:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.89, valid_loss=-1.88]\n","Epoch 20:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-1.94, valid_loss=-1.88]\n","Epoch 21:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.92, valid_loss=-1.88]\n","Epoch 22:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.94, valid_loss=-1.88]\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.94, valid_loss=-1.88]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11782)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 64.38it/s]\u001b[A\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.94, valid_loss=-1.88]\n","Epoch 24:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.49, train_loss_epoch=-1.99, valid_loss=-1.88]\n","Epoch 25:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-1.96, valid_loss=-1.88]\n","Epoch 26:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.94, valid_loss=-1.88]\n","Epoch 27:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.96, valid_loss=-1.88]\n","Epoch 28:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.90, valid_loss=-1.88]\n","Epoch 29:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-1.97, valid_loss=-1.88]\n","Epoch 30:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.97, valid_loss=-1.88]\n","\u001b[36m(_train_tune pid=11782)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11782)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 64.48it/s]\u001b[A\n","Epoch 30:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.97, valid_loss=-1.92]\n","Epoch 31:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.92, valid_loss=-1.92]\n","Epoch 32:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.00, valid_loss=-1.92]\n","Epoch 33:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.94, valid_loss=-1.92]\n","Epoch 34:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.99, valid_loss=-1.92]\n","Epoch 35:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.96, valid_loss=-1.92]\n","Epoch 36:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-2.01, valid_loss=-1.92]\n","Epoch 36: 100%|██████████| 13/13 [00:00\u003c00:00, 60.14it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.95, valid_loss=-1.92]\n","Epoch 37:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.95, valid_loss=-1.92]\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.96, valid_loss=-1.92]\n","\u001b[36m(_train_tune pid=11782)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11782)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 64.21it/s]\u001b[A\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.96, valid_loss=-1.89]\n","Epoch 39:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.01, valid_loss=-1.89]\n","Epoch 40:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.95, valid_loss=-1.89]\n","Epoch 41:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-1.99, valid_loss=-1.89]\n","Epoch 42:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-2.00, valid_loss=-1.89]\n","Epoch 43:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.99, valid_loss=-1.89]\n","Epoch 44:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.00, valid_loss=-1.89]\n","Epoch 45:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.01, valid_loss=-1.89]\n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.03, valid_loss=-1.89]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11782)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 65.65it/s]\u001b[A\n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.03, valid_loss=-1.93]\n","Epoch 47:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-2.00, valid_loss=-1.93]\n","Epoch 48:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.02, valid_loss=-1.93]\n","Epoch 49:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.02, valid_loss=-1.93]\n","Epoch 50:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-2.05, valid_loss=-1.93]\n","Epoch 51:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.02, valid_loss=-1.93]\n","Epoch 52:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.56, train_loss_epoch=-2.05, valid_loss=-1.93]\n","Epoch 53:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.03, valid_loss=-1.93]\n","\u001b[36m(_train_tune pid=11782)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11782)\u001b[0m \n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11782)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 64.19it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11782)\u001b[0m \n","Epoch 54:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.02, valid_loss=-1.93]\n","Epoch 55:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.99, valid_loss=-1.93]\n","Epoch 56:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.40, train_loss_epoch=-2.06, valid_loss=-1.93]\n","Epoch 57:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.00, valid_loss=-1.93]\n","Epoch 58:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.36, train_loss_epoch=-2.05, valid_loss=-1.93]\n","Epoch 59:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.99, valid_loss=-1.93]\n","Epoch 60:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-2.02, valid_loss=-1.93]\n","Epoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-2.01, valid_loss=-1.93]\n","\u001b[36m(_train_tune pid=11782)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-18 13:21:49,511\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (60, 8, 1), 'n_pool_kernel_size': (4, 4, 4), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=11782)\u001b[0m `Trainer.fit` stopped: `max_steps=800.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=11782)\u001b[0m \n","\u001b[36m(_train_tune pid=11782)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 65.98it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11782)\u001b[0m \r                                                                        \u001b[A\rEpoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.01, valid_loss=-1.92]\rEpoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.01, valid_loss=-1.92]\rEpoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.01, valid_loss=-1.92]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=11951)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=11951)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=11951)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=11951)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=11951)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=11951)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=11951)\u001b[0m 2024-11-18 13:21:56.858499: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=11951)\u001b[0m 2024-11-18 13:21:56.878829: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=11951)\u001b[0m 2024-11-18 13:21:56.905784: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=11951)\u001b[0m 2024-11-18 13:21:56.914660: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=11951)\u001b[0m 2024-11-18 13:21:56.943646: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=11951)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=11951)\u001b[0m 2024-11-18 13:21:58.101540: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=11951)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=11951)\u001b[0m \n","\u001b[36m(_train_tune pid=11951)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=11951)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=11951)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=11951)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=11951)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=11951)\u001b[0m 3 | blocks       | ModuleList       | 3.0 M  | train\n","\u001b[36m(_train_tune pid=11951)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=11951)\u001b[0m 3.0 M     Trainable params\n","\u001b[36m(_train_tune pid=11951)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=11951)\u001b[0m 3.0 M     Total params\n","\u001b[36m(_train_tune pid=11951)\u001b[0m 12.197    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=11951)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=11951)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/13 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=2.500]\n","Epoch 2:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.62]\n","Epoch 3:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.63]\n","Epoch 4:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.64]\n","Epoch 5:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.68]\n","Epoch 6:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.66]\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.68]\n","\u001b[36m(_train_tune pid=11951)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11951)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 64.87it/s]\u001b[A\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.68, valid_loss=-1.69]\n","Epoch 8:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.70, valid_loss=-1.69]\n","Epoch 9:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.65, valid_loss=-1.69]\n","Epoch 10:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.69, valid_loss=-1.69]\n","Epoch 11:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-1.80, valid_loss=-1.69]\n","Epoch 12:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.73, valid_loss=-1.69]\n","Epoch 13:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.78, valid_loss=-1.69]\n","Epoch 14:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.47, train_loss_epoch=-1.72, valid_loss=-1.69]\n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.33, train_loss_epoch=-1.74, valid_loss=-1.69]\n","\u001b[36m(_train_tune pid=11951)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11951)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 64.33it/s]\u001b[A\n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.74, valid_loss=-1.76]\n","Epoch 16:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-1.77, valid_loss=-1.76]\n","Epoch 17:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-1.83, valid_loss=-1.76]\n","Epoch 18:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.80, valid_loss=-1.76]\n","Epoch 19:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.76, valid_loss=-1.76]\n","Epoch 20:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-1.82, valid_loss=-1.76]\n","Epoch 21:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.80, valid_loss=-1.76]\n","Epoch 22:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.79, valid_loss=-1.76]\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.48, train_loss_epoch=-1.78, valid_loss=-1.76]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11951)\u001b[0m \n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11951)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 69.29it/s]\u001b[A\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.78, valid_loss=-1.79]\n","Epoch 24:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-1.79, valid_loss=-1.79]\n","Epoch 25:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.78, valid_loss=-1.79]\n","Epoch 26:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.77, valid_loss=-1.79]\n","Epoch 27:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.80, valid_loss=-1.79]\n","Epoch 28:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.02, train_loss_epoch=-1.76, valid_loss=-1.79]\n","Epoch 29:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.81, valid_loss=-1.79]\n","Epoch 30:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.82, valid_loss=-1.79]\n","\u001b[36m(_train_tune pid=11951)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11951)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 67.53it/s]\u001b[A\n","Epoch 30:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.82, valid_loss=-1.80]\n","Epoch 31:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.81, valid_loss=-1.80]\n","Epoch 32:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.84, valid_loss=-1.80]\n","Epoch 33:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.78, valid_loss=-1.80]\n","Epoch 34:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.83, valid_loss=-1.80]\n","Epoch 35:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.76, valid_loss=-1.80]\n","Epoch 36:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-1.84, valid_loss=-1.80]\n","Epoch 37:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.81, valid_loss=-1.80]\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.81, valid_loss=-1.80]\n","\u001b[36m(_train_tune pid=11951)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11951)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 64.71it/s]\u001b[A\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.81, valid_loss=-1.80]\n","Epoch 39:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.84, valid_loss=-1.80]\n","Epoch 40:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.28, train_loss_epoch=-1.75, valid_loss=-1.80]\n","Epoch 41:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.82, valid_loss=-1.80]\n","Epoch 42:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.81, valid_loss=-1.80]\n","Epoch 43:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.80, valid_loss=-1.80]\n","Epoch 44:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.81, valid_loss=-1.80]\n","Epoch 45:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.81, valid_loss=-1.80]\n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.85, valid_loss=-1.80]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11951)\u001b[0m \n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11951)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 63.89it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11951)\u001b[0m \n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.85, valid_loss=-1.79]\n","Epoch 47:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.82, valid_loss=-1.79]\n","Epoch 48:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.84, valid_loss=-1.79]\n","Epoch 49:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.83, valid_loss=-1.79]\n","Epoch 50:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.84, valid_loss=-1.79]\n","Epoch 51:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.84, valid_loss=-1.79]\n","Epoch 52:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.33, train_loss_epoch=-1.87, valid_loss=-1.79]\n","Epoch 53:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.83, valid_loss=-1.79]\n","\u001b[36m(_train_tune pid=11951)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11951)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 64.95it/s]\u001b[A\n","Epoch 53:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.83, valid_loss=-1.80]\n","Epoch 54:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.79, valid_loss=-1.80]\n","Epoch 55:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.19, train_loss_epoch=-1.79, valid_loss=-1.80]\n","Epoch 56:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.84, valid_loss=-1.80]\n","Epoch 57:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.83, valid_loss=-1.80]\n","Epoch 58:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.85, valid_loss=-1.80]\n","Epoch 59:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.82, valid_loss=-1.80]\n","Epoch 60:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.79, valid_loss=-1.80]\n","Epoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.83, valid_loss=-1.80]\n","\u001b[36m(_train_tune pid=11951)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11951)\u001b[0m \n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11951)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 65.65it/s]\u001b[A\n","Epoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.83, valid_loss=-1.81]\n","Epoch 62:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.53, train_loss_epoch=-1.83, valid_loss=-1.81]\n","Epoch 63:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.80, valid_loss=-1.81]\n","Epoch 64:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.45, train_loss_epoch=-1.79, valid_loss=-1.81]\n","Epoch 64: 100%|██████████| 13/13 [00:00\u003c00:00, 61.87it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.79, valid_loss=-1.81]\n","Epoch 65:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.84, valid_loss=-1.81]\n","Epoch 65: 100%|██████████| 13/13 [00:00\u003c00:00, 62.64it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.84, valid_loss=-1.81]\n","Epoch 66:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.82, valid_loss=-1.81]\n","Epoch 66: 100%|██████████| 13/13 [00:00\u003c00:00, 62.82it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.80, valid_loss=-1.81]\n","Epoch 67:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.80, valid_loss=-1.81]\n","Epoch 68:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.81, valid_loss=-1.81]\n","Epoch 69:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.995, train_loss_epoch=-1.79, valid_loss=-1.81]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11951)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 67.06it/s]\u001b[A\n","Epoch 69:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.79, valid_loss=-1.82] \n","Epoch 70:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.84, valid_loss=-1.82]\n","Epoch 71:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.87, valid_loss=-1.82]\n","Epoch 72:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-1.89, valid_loss=-1.82]\n","Epoch 73:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.83, valid_loss=-1.82]\n","Epoch 74:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.86, valid_loss=-1.82]\n","Epoch 75:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.81, valid_loss=-1.82]\n","Epoch 76:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-1.88, valid_loss=-1.82]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=11951)\u001b[0m `Trainer.fit` stopped: `max_steps=1000.0` reached.\n","2024-11-18 13:22:18,863\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (60, 8, 1), 'n_pool_kernel_size': (8, 4, 1), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=11951)\u001b[0m \n","\u001b[36m(_train_tune pid=11951)\u001b[0m \rValidation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11951)\u001b[0m \rValidation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11951)\u001b[0m \rValidation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11951)\u001b[0m \n","\u001b[36m(_train_tune pid=11951)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 67.19it/s]\u001b[A\n","\u001b[36m(_train_tune pid=11951)\u001b[0m \n","\u001b[36m(_train_tune pid=11951)\u001b[0m \r                                                                        \u001b[A\rEpoch 76:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.88, valid_loss=-1.82]\rEpoch 76:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.83, valid_loss=-1.82]\n","\u001b[36m(_train_tune pid=11951)\u001b[0m \rEpoch 76:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.83, valid_loss=-1.82]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=12136)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=12136)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=12136)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=12136)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=12136)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=12136)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=12136)\u001b[0m 2024-11-18 13:22:26.952896: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=12136)\u001b[0m 2024-11-18 13:22:26.974216: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=12136)\u001b[0m 2024-11-18 13:22:27.001966: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=12136)\u001b[0m 2024-11-18 13:22:27.009662: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=12136)\u001b[0m 2024-11-18 13:22:27.032644: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=12136)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=12136)\u001b[0m 2024-11-18 13:22:28.208796: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=12136)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=12136)\u001b[0m \n","\u001b[36m(_train_tune pid=12136)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=12136)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=12136)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=12136)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=12136)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=12136)\u001b[0m 3 | blocks       | ModuleList       | 2.6 M  | train\n","\u001b[36m(_train_tune pid=12136)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=12136)\u001b[0m 2.6 M     Trainable params\n","\u001b[36m(_train_tune pid=12136)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=12136)\u001b[0m 2.6 M     Total params\n","\u001b[36m(_train_tune pid=12136)\u001b[0m 10.274    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=12136)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=12136)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/2 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=2.310, train_loss_epoch=0.496]\n","Epoch 2:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-0.557]\n","Epoch 3:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.10, train_loss_epoch=-1.20]\n","Epoch 4:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.43, train_loss_epoch=-1.35]\n","Epoch 5:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.40, train_loss_epoch=-1.34]\n","Epoch 6:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.46]\n","Epoch 7:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.52]\n","Epoch 8:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.56]\n","Epoch 9:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.55]\n","Epoch 10:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.62]\n","Epoch 11:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.67, train_loss_epoch=-1.67]\n","Epoch 12:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.77]\n","Epoch 13:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.74]\n","Epoch 14:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.72]\n","Epoch 15:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.75]\n","Epoch 16:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.58, train_loss_epoch=-1.71]\n","Epoch 17:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.82]\n","Epoch 18:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.75]\n","Epoch 19:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.81]\n","Epoch 20:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.80]\n","Epoch 21:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.83]\n","Epoch 22:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.72]\n","Epoch 22: 100%|██████████| 2/2 [00:00\u003c00:00, 18.44it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.69]\n","Epoch 23:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.69]\n","Epoch 24:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.80]\n","Epoch 25:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.79]\n","Epoch 26:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.82]\n","Epoch 27:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.80]\n","Epoch 28:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.79]\n","Epoch 29:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.86]\n","Epoch 30:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.81]\n","Epoch 31:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.86]\n","Epoch 32:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.82]\n","Epoch 33:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.83]\n","Epoch 34:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.83]\n","Epoch 35:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.87]\n","Epoch 36:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.81]\n","Epoch 37:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.94]\n","Epoch 38:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.82]\n","Epoch 39:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.80]\n","Epoch 40:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.75]\n","Epoch 41:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.83]\n","Epoch 42:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.75]\n","Epoch 43:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.86]\n","Epoch 44:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.76]\n","Epoch 45:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.87]\n","Epoch 46:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.92]\n","Epoch 47:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.81]\n","Epoch 48:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.87]\n","Epoch 49:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.80]\n","Epoch 49: 100%|██████████| 2/2 [00:00\u003c00:00, 19.59it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.80]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12136)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12136)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 16.21it/s]\u001b[A\n","Epoch 50:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.83, valid_loss=-1.80]\n","Epoch 51:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.82, valid_loss=-1.80]\n","Epoch 52:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.84, valid_loss=-1.80]\n","Epoch 53:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.90, valid_loss=-1.80]\n","Epoch 54:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.83, valid_loss=-1.80]\n","Epoch 55:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.86, valid_loss=-1.80]\n","Epoch 56:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.87, valid_loss=-1.80]\n","Epoch 57:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.92, valid_loss=-1.80]\n","Epoch 58:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.86, valid_loss=-1.80]\n","Epoch 59:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.89, valid_loss=-1.80]\n","Epoch 60:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.81, valid_loss=-1.80]\n","Epoch 61:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.88, valid_loss=-1.80]\n","Epoch 62:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.95, valid_loss=-1.80]\n","Epoch 63:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.94, valid_loss=-1.80]\n","Epoch 64:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.87, valid_loss=-1.80]\n","Epoch 65:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.86, valid_loss=-1.80]\n","Epoch 66:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.85, valid_loss=-1.80]\n","Epoch 67:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.90, valid_loss=-1.80]\n","Epoch 68:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.79, valid_loss=-1.80]\n","Epoch 69:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.87, valid_loss=-1.80]\n","Epoch 70:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.86, valid_loss=-1.80]\n","Epoch 71:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.89, valid_loss=-1.80]\n","Epoch 72:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.85, valid_loss=-1.80]\n","Epoch 73:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.85, valid_loss=-1.80]\n","Epoch 74:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.91, valid_loss=-1.80]\n","Epoch 75:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.85, valid_loss=-1.80]\n","Epoch 76:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.88, valid_loss=-1.80]\n","Epoch 77:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.90, valid_loss=-1.80]\n","Epoch 78:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.87, valid_loss=-1.80]\n","Epoch 79:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.81, valid_loss=-1.80]\n","Epoch 80:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.85, valid_loss=-1.80]\n","Epoch 81:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.84, valid_loss=-1.80]\n","Epoch 82:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.80, valid_loss=-1.80]\n","Epoch 83:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.86, valid_loss=-1.80]\n","Epoch 84:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.92, valid_loss=-1.80]\n","Epoch 85:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.83, valid_loss=-1.80]\n","Epoch 86:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.91, valid_loss=-1.80]\n","Epoch 87:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.95, valid_loss=-1.80]\n","Epoch 88:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.90, valid_loss=-1.80]\n","Epoch 89:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.88, valid_loss=-1.80]\n","Epoch 90:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.86, valid_loss=-1.80]\n","Epoch 91:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.85, valid_loss=-1.80]\n","Epoch 92:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.78, valid_loss=-1.80]\n","Epoch 93:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.85, valid_loss=-1.80]\n","Epoch 94:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.90, valid_loss=-1.80]\n","Epoch 95:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.88, valid_loss=-1.80]\n","Epoch 96:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.83, valid_loss=-1.80]\n","Epoch 97:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.87, valid_loss=-1.80]\n","Epoch 98:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.89, valid_loss=-1.80]\n","Epoch 99:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.93, valid_loss=-1.80]\n","Epoch 99: 100%|██████████| 2/2 [00:00\u003c00:00, 19.17it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.93, valid_loss=-1.80]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12136)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12136)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 16.03it/s]\u001b[A\n","Epoch 100:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.79, valid_loss=-1.82]\n","Epoch 101:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.89, valid_loss=-1.82]\n","Epoch 102:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.87, valid_loss=-1.82]\n","Epoch 103:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.83, valid_loss=-1.82]\n","Epoch 104:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.94, valid_loss=-1.82]\n","Epoch 105:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.87, valid_loss=-1.82]\n","Epoch 106:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.86, valid_loss=-1.82]\n","Epoch 107:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.85, valid_loss=-1.82]\n","Epoch 108:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.86, valid_loss=-1.82]\n","Epoch 109:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.95, valid_loss=-1.82]\n","Epoch 110:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.97, valid_loss=-1.82]\n","Epoch 111:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.92, valid_loss=-1.82]\n","Epoch 112:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.83, valid_loss=-1.82]\n","Epoch 113:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.86, valid_loss=-1.82]\n","Epoch 114:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.89, valid_loss=-1.82]\n","Epoch 115:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.91, valid_loss=-1.82]\n","Epoch 116:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.89, valid_loss=-1.82]\n","Epoch 117:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.89, valid_loss=-1.82]\n","Epoch 118:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.92, valid_loss=-1.82]\n","Epoch 119:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.95, valid_loss=-1.82]\n","Epoch 120:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.90, valid_loss=-1.82]\n","Epoch 121:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.94, valid_loss=-1.82]\n","Epoch 122:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.97, valid_loss=-1.82]\n","Epoch 123:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.93, valid_loss=-1.82]\n","Epoch 124:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.87, valid_loss=-1.82]\n","Epoch 125:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.93, valid_loss=-1.82]\n","Epoch 126:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.01, valid_loss=-1.82]\n","Epoch 127:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.90, valid_loss=-1.82]\n","Epoch 128:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.92, valid_loss=-1.82]\n","Epoch 129:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.88, valid_loss=-1.82]\n","Epoch 130:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.94, valid_loss=-1.82]\n","Epoch 131:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.93, valid_loss=-1.82]\n","Epoch 132:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.92, valid_loss=-1.82]\n","Epoch 133:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.83, valid_loss=-1.82]\n","Epoch 134:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.88, valid_loss=-1.82]\n","Epoch 135:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.80, valid_loss=-1.82]\n","Epoch 136:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.84, valid_loss=-1.82]\n","Epoch 137:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.87, valid_loss=-1.82]\n","Epoch 138:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.84, valid_loss=-1.82]\n","Epoch 139:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.82, valid_loss=-1.82]\n","Epoch 140:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.89, valid_loss=-1.82]\n","Epoch 141:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.78, valid_loss=-1.82]\n","Epoch 142:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.92, valid_loss=-1.82]\n","Epoch 143:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.95, valid_loss=-1.82]\n","Epoch 144:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.88, valid_loss=-1.82]\n","Epoch 145:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.90, valid_loss=-1.82]\n","Epoch 146:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.82, valid_loss=-1.82]\n","Epoch 147:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.89, valid_loss=-1.82]\n","Epoch 148:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.90, valid_loss=-1.82]\n","Epoch 148: 100%|██████████| 2/2 [00:00\u003c00:00, 18.98it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.91, valid_loss=-1.82]\n","Epoch 149:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.91, valid_loss=-1.82]\n","Epoch 149: 100%|██████████| 2/2 [00:00\u003c00:00, 19.34it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.91, valid_loss=-1.82]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12136)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12136)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 16.15it/s]\u001b[A\n","Epoch 150:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.95, valid_loss=-1.84]\n","Epoch 151:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.92, valid_loss=-1.84]\n","Epoch 152:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.84, valid_loss=-1.84]\n","Epoch 153:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.92, valid_loss=-1.84]\n","Epoch 154:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.92, valid_loss=-1.84]\n","Epoch 155:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.00, valid_loss=-1.84]\n","Epoch 156:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.97, valid_loss=-1.84]\n","Epoch 157:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.93, valid_loss=-1.84]\n","Epoch 158:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.02, valid_loss=-1.84]\n","Epoch 159:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.88, valid_loss=-1.84]\n","Epoch 160:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.96, valid_loss=-1.84]\n","Epoch 161:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.97, valid_loss=-1.84]\n","Epoch 162:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.92, valid_loss=-1.84]\n","Epoch 163:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.91, valid_loss=-1.84]\n","Epoch 164:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-2.00, valid_loss=-1.84]\n","Epoch 165:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.93, valid_loss=-1.84]\n","Epoch 166:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.95, valid_loss=-1.84]\n","Epoch 167:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.96, valid_loss=-1.84]\n","Epoch 168:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.87, valid_loss=-1.84]\n","Epoch 169:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.93, valid_loss=-1.84]\n","Epoch 170:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.95, valid_loss=-1.84]\n","Epoch 171:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.03, valid_loss=-1.84]\n","Epoch 172:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.90, valid_loss=-1.84]\n","Epoch 173:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.88, valid_loss=-1.84]\n","Epoch 174:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.96, valid_loss=-1.84]\n","Epoch 175:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.93, valid_loss=-1.84]\n","Epoch 176:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.92, valid_loss=-1.84]\n","Epoch 177:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.89, valid_loss=-1.84]\n","Epoch 178:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.95, valid_loss=-1.84]\n","Epoch 179:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.93, valid_loss=-1.84]\n","Epoch 180:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.98, valid_loss=-1.84]\n","Epoch 181:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.97, valid_loss=-1.84]\n","Epoch 182:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.94, valid_loss=-1.84]\n","Epoch 183:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.93, valid_loss=-1.84]\n","Epoch 184:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.98, valid_loss=-1.84]\n","Epoch 185:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.95, valid_loss=-1.84]\n","Epoch 186:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.99, valid_loss=-1.84]\n","Epoch 187:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.96, valid_loss=-1.84]\n","Epoch 188:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.92, valid_loss=-1.84]\n","Epoch 189:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.00, valid_loss=-1.84]\n","Epoch 190:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.95, valid_loss=-1.84]\n","Epoch 191:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.81, valid_loss=-1.84]\n","Epoch 192:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.97, valid_loss=-1.84]\n","Epoch 193:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.92, valid_loss=-1.84]\n","Epoch 194:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.04, valid_loss=-1.84]\n","Epoch 195:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.95, valid_loss=-1.84]\n","Epoch 196:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.00, valid_loss=-1.84]\n","Epoch 197:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.96, valid_loss=-1.84]\n","Epoch 198:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.98, valid_loss=-1.84]\n","Epoch 199:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.00, valid_loss=-1.84]\n","Epoch 199: 100%|██████████| 2/2 [00:00\u003c00:00, 19.02it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.00, valid_loss=-1.84]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12136)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12136)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 15.71it/s]\u001b[A\n","Epoch 200:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.05, valid_loss=-1.88]\n","Epoch 201:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.97, valid_loss=-1.88]\n","Epoch 202:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.93, valid_loss=-1.88]\n","Epoch 203:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.01, valid_loss=-1.88]\n","Epoch 204:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.90, valid_loss=-1.88]\n","Epoch 205:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.94, valid_loss=-1.88]\n","Epoch 206:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.81, valid_loss=-1.88]\n","Epoch 207:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.98, valid_loss=-1.88]\n","Epoch 208:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.02, valid_loss=-1.88]\n","Epoch 209:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.98, valid_loss=-1.88]\n","Epoch 210:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.98, valid_loss=-1.88]\n","Epoch 211:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.92, valid_loss=-1.88]\n","Epoch 212:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.93, valid_loss=-1.88]\n","Epoch 213:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.01, valid_loss=-1.88]\n","Epoch 214:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.06, valid_loss=-1.88]\n","Epoch 215:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.97, valid_loss=-1.88]\n","Epoch 216:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.89, valid_loss=-1.88]\n","Epoch 217:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.93, valid_loss=-1.88]\n","Epoch 218:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.98, valid_loss=-1.88]\n","Epoch 219:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.94, valid_loss=-1.88]\n","Epoch 220:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.91, valid_loss=-1.88]\n","Epoch 221:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.96, valid_loss=-1.88]\n","Epoch 222:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.93, valid_loss=-1.88]\n","Epoch 223:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.98, valid_loss=-1.88]\n","Epoch 224:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.92, valid_loss=-1.88]\n","Epoch 225:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.96, valid_loss=-1.88]\n","Epoch 226:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.97, valid_loss=-1.88]\n","Epoch 227:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.94, valid_loss=-1.88]\n","Epoch 228:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.91, valid_loss=-1.88]\n","Epoch 229:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.89, valid_loss=-1.88]\n","Epoch 230:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.97, valid_loss=-1.88]\n","Epoch 231:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.97, valid_loss=-1.88]\n","Epoch 232:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.98, valid_loss=-1.88]\n","Epoch 233:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.97, valid_loss=-1.88]\n","Epoch 234:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.88, valid_loss=-1.88]\n","Epoch 235:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.93, valid_loss=-1.88]\n","Epoch 236:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.94, valid_loss=-1.88]\n","Epoch 237:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.93, valid_loss=-1.88]\n","Epoch 238:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.01, valid_loss=-1.88]\n","Epoch 239:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.98, valid_loss=-1.88]\n","Epoch 240:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.01, valid_loss=-1.88]\n","Epoch 241:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.92, valid_loss=-1.88]\n","Epoch 242:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.00, valid_loss=-1.88]\n","Epoch 243:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.89, valid_loss=-1.88]\n","Epoch 244:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.99, valid_loss=-1.88]\n","Epoch 245:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.94, valid_loss=-1.88]\n","Epoch 246:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.89, valid_loss=-1.88]\n","Epoch 247:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.94, valid_loss=-1.88]\n","Epoch 248:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.90, valid_loss=-1.88]\n","Epoch 249:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.91, valid_loss=-1.88]\n","Epoch 249: 100%|██████████| 2/2 [00:00\u003c00:00, 18.39it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.91, valid_loss=-1.88]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12136)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12136)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 16.01it/s]\u001b[A\n","Epoch 250:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.99, valid_loss=-1.88]\n","Epoch 251:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.90, valid_loss=-1.88]\n","Epoch 252:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.89, valid_loss=-1.88]\n","Epoch 253:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.98, valid_loss=-1.88]\n","Epoch 254:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.93, valid_loss=-1.88]\n","Epoch 255:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.98, valid_loss=-1.88]\n","Epoch 256:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.96, valid_loss=-1.88]\n","Epoch 257:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.95, valid_loss=-1.88]\n","Epoch 258:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.96, valid_loss=-1.88]\n","Epoch 259:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.96, valid_loss=-1.88]\n","Epoch 260:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.97, valid_loss=-1.88]\n","Epoch 261:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.95, valid_loss=-1.88]\n","Epoch 262:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.00, valid_loss=-1.88]\n","Epoch 263:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.95, valid_loss=-1.88]\n","Epoch 264:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.97, valid_loss=-1.88]\n","Epoch 265:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.00, valid_loss=-1.88]\n","Epoch 266:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.86, valid_loss=-1.88]\n","Epoch 267:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.06, valid_loss=-1.88]\n","Epoch 268:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.95, valid_loss=-1.88]\n","Epoch 269:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.00, valid_loss=-1.88]\n","Epoch 270:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.95, valid_loss=-1.88]\n","Epoch 271:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.91, valid_loss=-1.88]\n","Epoch 272:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.97, valid_loss=-1.88]\n","Epoch 273:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.02, valid_loss=-1.88]\n","Epoch 274:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.99, valid_loss=-1.88]\n","Epoch 275:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.93, valid_loss=-1.88]\n","Epoch 276:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.01, valid_loss=-1.88]\n","Epoch 277:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.12, valid_loss=-1.88]\n","Epoch 278:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.00, valid_loss=-1.88]\n","Epoch 279:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.01, valid_loss=-1.88]\n","Epoch 280:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.03, valid_loss=-1.88]\n","Epoch 281:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.94, valid_loss=-1.88]\n","Epoch 282:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.89, valid_loss=-1.88]\n","Epoch 283:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.95, valid_loss=-1.88]\n","Epoch 284:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.01, valid_loss=-1.88]\n","Epoch 285:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.94, valid_loss=-1.88]\n","Epoch 286:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.03, valid_loss=-1.88]\n","Epoch 287:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.95, valid_loss=-1.88]\n","Epoch 288:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.89, valid_loss=-1.88]\n","Epoch 289:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.98, valid_loss=-1.88]\n","Epoch 290:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.94, valid_loss=-1.88]\n","Epoch 291:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.96, valid_loss=-1.88]\n","Epoch 292:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.94, valid_loss=-1.88]\n","Epoch 293:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.92, valid_loss=-1.88]\n","Epoch 294:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.97, valid_loss=-1.88]\n","Epoch 295:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.95, valid_loss=-1.88]\n","Epoch 296:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.95, valid_loss=-1.88]\n","Epoch 297:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.92, valid_loss=-1.88]\n","Epoch 298:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.92, valid_loss=-1.88]\n","Epoch 299:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.03, valid_loss=-1.88]\n","Epoch 299: 100%|██████████| 2/2 [00:00\u003c00:00, 19.09it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.03, valid_loss=-1.88]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12136)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12136)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 15.94it/s]\u001b[A\n","Epoch 300:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.94, valid_loss=-1.89]\n","Epoch 301:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.04, valid_loss=-1.89]\n","Epoch 302:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.97, valid_loss=-1.89]\n","Epoch 303:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.92, valid_loss=-1.89]\n","Epoch 304:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.92, valid_loss=-1.89]\n","Epoch 305:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.93, valid_loss=-1.89]\n","Epoch 306:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.97, valid_loss=-1.89]\n","Epoch 307:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.05, valid_loss=-1.89]\n","Epoch 308:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.99, valid_loss=-1.89]\n","Epoch 309:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.90, valid_loss=-1.89]\n","Epoch 310:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.98, valid_loss=-1.89]\n","Epoch 311:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.10, valid_loss=-1.89]\n","Epoch 312:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.98, valid_loss=-1.89]\n","Epoch 313:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.93, valid_loss=-1.89]\n","Epoch 314:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.03, valid_loss=-1.89]\n","Epoch 315:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.07, valid_loss=-1.89]\n","Epoch 316:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.91, valid_loss=-1.89]\n","Epoch 317:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.92, valid_loss=-1.89]\n","Epoch 318:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.93, valid_loss=-1.89]\n","Epoch 319:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.88, valid_loss=-1.89]\n","Epoch 320:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.00, valid_loss=-1.89]\n","Epoch 321:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.90, valid_loss=-1.89]\n","Epoch 322:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.88, valid_loss=-1.89]\n","Epoch 323:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.91, valid_loss=-1.89]\n","Epoch 324:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.92, valid_loss=-1.89]\n","Epoch 325:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.96, valid_loss=-1.89]\n","Epoch 326:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-2.01, valid_loss=-1.89]\n","Epoch 327:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.98, valid_loss=-1.89]\n","Epoch 328:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.05, valid_loss=-1.89]\n","Epoch 329:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.97, valid_loss=-1.89]\n","Epoch 330:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.98, valid_loss=-1.89]\n","Epoch 331:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.95, valid_loss=-1.89]\n","Epoch 332:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.99, valid_loss=-1.89]\n","Epoch 333:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.00, valid_loss=-1.89]\n","Epoch 334:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.03, valid_loss=-1.89]\n","Epoch 335:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.96, valid_loss=-1.89]\n","Epoch 336:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.05, valid_loss=-1.89]\n","Epoch 337:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.01, valid_loss=-1.89]\n","Epoch 338:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.01, valid_loss=-1.89]\n","Epoch 339:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.97, valid_loss=-1.89]\n","Epoch 340:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.92, valid_loss=-1.89]\n","Epoch 341:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.95, valid_loss=-1.89]\n","Epoch 342:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.04, valid_loss=-1.89]\n","Epoch 343:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.97, valid_loss=-1.89]\n","Epoch 344:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.89, valid_loss=-1.89]\n","Epoch 345:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.94, valid_loss=-1.89]\n","Epoch 346:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.00, valid_loss=-1.89]\n","Epoch 347:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.99, valid_loss=-1.89]\n","Epoch 348:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.91, valid_loss=-1.89]\n","Epoch 349:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.98, valid_loss=-1.89]\n","Epoch 349: 100%|██████████| 2/2 [00:00\u003c00:00, 19.15it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.98, valid_loss=-1.89]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12136)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12136)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 16.13it/s]\u001b[A\n","Epoch 350:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.94, valid_loss=-1.90]\n","Epoch 351:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.07, valid_loss=-1.90]\n","Epoch 352:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.92, valid_loss=-1.90]\n","Epoch 353:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.99, valid_loss=-1.90]\n","Epoch 354:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.94, valid_loss=-1.90]\n","Epoch 355:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.90, valid_loss=-1.90]\n","Epoch 356:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.97, valid_loss=-1.90]\n","Epoch 357:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.97, valid_loss=-1.90]\n","Epoch 358:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.02, valid_loss=-1.90]\n","Epoch 359:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.94, valid_loss=-1.90]\n","Epoch 360:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.01, valid_loss=-1.90]\n","Epoch 361:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.98, valid_loss=-1.90]\n","Epoch 362:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.04, valid_loss=-1.90]\n","Epoch 363:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.03, valid_loss=-1.90]\n","Epoch 364:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.99, valid_loss=-1.90]\n","Epoch 365:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.90, valid_loss=-1.90]\n","Epoch 366:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.99, valid_loss=-1.90]\n","Epoch 367:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.03, valid_loss=-1.90]\n","Epoch 368:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.04, valid_loss=-1.90]\n","Epoch 369:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.00, valid_loss=-1.90]\n","Epoch 370:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.99, valid_loss=-1.90]\n","Epoch 371:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.01, valid_loss=-1.90]\n","Epoch 372:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.04, valid_loss=-1.90]\n","Epoch 373:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.00, valid_loss=-1.90]\n","Epoch 374:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.95, valid_loss=-1.90]\n","Epoch 375:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.02, valid_loss=-1.90]\n","Epoch 376:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.98, valid_loss=-1.90]\n","Epoch 377:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.01, valid_loss=-1.90]\n","Epoch 378:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.03, valid_loss=-1.90]\n","Epoch 379:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.04, valid_loss=-1.90]\n","Epoch 380:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.04, valid_loss=-1.90]\n","Epoch 381:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.91, valid_loss=-1.90]\n","Epoch 382:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.00, valid_loss=-1.90]\n","Epoch 383:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.03, valid_loss=-1.90]\n","Epoch 384:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.95, valid_loss=-1.90]\n","Epoch 385:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.98, valid_loss=-1.90]\n","Epoch 386:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.04, valid_loss=-1.90]\n","Epoch 387:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.98, valid_loss=-1.90]\n","Epoch 388:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.99, valid_loss=-1.90]\n","Epoch 389:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.94, valid_loss=-1.90]\n","Epoch 390:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.94, valid_loss=-1.90]\n","Epoch 391:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.91, valid_loss=-1.90]\n","Epoch 392:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.01, valid_loss=-1.90]\n","Epoch 393:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.00, valid_loss=-1.90]\n","Epoch 394:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.92, valid_loss=-1.90]\n","Epoch 395:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.01, valid_loss=-1.90]\n","Epoch 396:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.97, valid_loss=-1.90]\n","Epoch 397:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.99, valid_loss=-1.90]\n","Epoch 398:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.06, valid_loss=-1.90]\n","Epoch 399:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.99, valid_loss=-1.90]\n","Epoch 399: 100%|██████████| 2/2 [00:00\u003c00:00, 19.03it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.99, valid_loss=-1.90]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12136)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 16.07it/s]\u001b[A\n","Epoch 400:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.00, valid_loss=-1.89]\n","Epoch 401:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.96, valid_loss=-1.89]\n","Epoch 402:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.99, valid_loss=-1.89]\n","Epoch 403:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.96, valid_loss=-1.89]\n","Epoch 404:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.96, valid_loss=-1.89]\n","Epoch 405:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.97, valid_loss=-1.89]\n","Epoch 405: 100%|██████████| 2/2 [00:00\u003c00:00, 18.83it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.01, valid_loss=-1.89]\n","Epoch 406:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.01, valid_loss=-1.89]\n","Epoch 406: 100%|██████████| 2/2 [00:00\u003c00:00, 18.88it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.01, valid_loss=-1.89]\n","Epoch 407:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.96, valid_loss=-1.89]\n","Epoch 407: 100%|██████████| 2/2 [00:00\u003c00:00, 19.02it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.96, valid_loss=-1.89]\n","Epoch 408:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.93, valid_loss=-1.89]\n","Epoch 408: 100%|██████████| 2/2 [00:00\u003c00:00, 19.03it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.91, valid_loss=-1.89]\n","Epoch 409:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.91, valid_loss=-1.89]\n","Epoch 410:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.00, valid_loss=-1.89]\n","Epoch 411:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.96, valid_loss=-1.89]\n","Epoch 412:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.90, valid_loss=-1.89]\n","Epoch 413:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.99, valid_loss=-1.89]\n","Epoch 414:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.00, valid_loss=-1.89]\n","Epoch 415:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.02, valid_loss=-1.89]\n","Epoch 416:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.95, valid_loss=-1.89]\n","Epoch 417:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.02, valid_loss=-1.89]\n","Epoch 418:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.94, valid_loss=-1.89]\n","Epoch 419:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.07, valid_loss=-1.89]\n","Epoch 420:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.06, valid_loss=-1.89]\n","Epoch 421:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.09, valid_loss=-1.89]\n","Epoch 422:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.90, valid_loss=-1.89]\n","Epoch 423:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.99, valid_loss=-1.89]\n","Epoch 424:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.03, valid_loss=-1.89]\n","Epoch 425:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.00, valid_loss=-1.89]\n","Epoch 426:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.00, valid_loss=-1.89]\n","Epoch 427:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.98, valid_loss=-1.89]\n","Epoch 428:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.99, valid_loss=-1.89]\n","Epoch 429:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.02, valid_loss=-1.89]\n","Epoch 430:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.01, valid_loss=-1.89]\n","Epoch 431:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.98, valid_loss=-1.89]\n","Epoch 432:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.97, valid_loss=-1.89]\n","Epoch 433:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.99, valid_loss=-1.89]\n","Epoch 434:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.96, valid_loss=-1.89]\n","Epoch 435:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.99, valid_loss=-1.89]\n","Epoch 436:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.02, valid_loss=-1.89]\n","Epoch 437:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.97, valid_loss=-1.89]\n","Epoch 438:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.01, valid_loss=-1.89]\n","Epoch 439:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.03, valid_loss=-1.89]\n","Epoch 440:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.02, valid_loss=-1.89]\n","Epoch 440: 100%|██████████| 2/2 [00:00\u003c00:00, 18.75it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.02, valid_loss=-1.89]\n","Epoch 441:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.07, valid_loss=-1.89]\n","Epoch 442:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.98, valid_loss=-1.89]\n","Epoch 443:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.97, valid_loss=-1.89]\n","Epoch 444:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.03, valid_loss=-1.89]\n","Epoch 445:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.98, valid_loss=-1.89]\n","Epoch 446:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.00, valid_loss=-1.89]\n","Epoch 447:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.92, valid_loss=-1.89]\n","Epoch 448:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.03, valid_loss=-1.89]\n","Epoch 449:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.04, valid_loss=-1.89]\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-18 13:23:20,719\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (180, 60, 1), 'n_pool_kernel_size': (4, 4, 4), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=12136)\u001b[0m \rEpoch 449: 100%|██████████| 2/2 [00:00\u003c00:00, 19.09it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.04, valid_loss=-1.89]\rEpoch 449: 100%|██████████| 2/2 [00:00\u003c00:00, 19.01it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.04, valid_loss=-1.89]\n","\u001b[36m(_train_tune pid=12136)\u001b[0m \rValidation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12136)\u001b[0m \rValidation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12136)\u001b[0m \rValidation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12136)\u001b[0m \n","\u001b[36m(_train_tune pid=12136)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 15.89it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=12136)\u001b[0m `Trainer.fit` stopped: `max_steps=900.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=12136)\u001b[0m \n","\u001b[36m(_train_tune pid=12136)\u001b[0m \r                                                                      \u001b[A\rEpoch 449: 100%|██████████| 2/2 [00:00\u003c00:00,  6.77it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.04, valid_loss=-1.90]\rEpoch 449: 100%|██████████| 2/2 [00:00\u003c00:00,  6.74it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.00, valid_loss=-1.90]\rEpoch 449: 100%|██████████| 2/2 [00:00\u003c00:00,  6.72it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.00, valid_loss=-1.90]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=12455)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=12455)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=12455)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=12455)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=12455)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=12455)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=12455)\u001b[0m 2024-11-18 13:23:28.979889: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=12455)\u001b[0m 2024-11-18 13:23:29.002003: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=12455)\u001b[0m 2024-11-18 13:23:29.029689: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=12455)\u001b[0m 2024-11-18 13:23:29.038047: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=12455)\u001b[0m 2024-11-18 13:23:29.057797: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=12455)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=12455)\u001b[0m 2024-11-18 13:23:30.203171: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=12455)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=12455)\u001b[0m \n","\u001b[36m(_train_tune pid=12455)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=12455)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=12455)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=12455)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=12455)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=12455)\u001b[0m 3 | blocks       | ModuleList       | 2.6 M  | train\n","\u001b[36m(_train_tune pid=12455)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=12455)\u001b[0m 2.6 M     Trainable params\n","\u001b[36m(_train_tune pid=12455)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=12455)\u001b[0m 2.6 M     Total params\n","\u001b[36m(_train_tune pid=12455)\u001b[0m 10.280    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=12455)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=12455)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/13 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.44, train_loss_epoch=-0.979]\n","Epoch 2:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-1.64]\n","Epoch 3:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.69]\n","Epoch 4:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.68]\n","Epoch 5:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-1.79]\n","Epoch 6:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.82]\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.80]\n","\u001b[36m(_train_tune pid=12455)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12455)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 61.70it/s]\u001b[A\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.80, valid_loss=-1.78]\n","Epoch 8:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.80, valid_loss=-1.78]\n","Epoch 9:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.78, valid_loss=-1.78]\n","Epoch 10:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.82, valid_loss=-1.78]\n","Epoch 11:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-1.88, valid_loss=-1.78]\n","Epoch 12:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.80, valid_loss=-1.78]\n","Epoch 13:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.90, valid_loss=-1.78]\n","Epoch 14:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.82, valid_loss=-1.78]\n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-1.83, valid_loss=-1.78]\n","\u001b[36m(_train_tune pid=12455)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12455)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 62.78it/s]\u001b[A\n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.83, valid_loss=-1.83]\n","Epoch 16:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.88, valid_loss=-1.83]\n","Epoch 17:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-1.87, valid_loss=-1.83]\n","Epoch 18:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-1.86, valid_loss=-1.83]\n","Epoch 19:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.83, valid_loss=-1.83]\n","Epoch 20:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.88, valid_loss=-1.83]\n","Epoch 21:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.90, valid_loss=-1.83]\n","Epoch 22:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.87, valid_loss=-1.83]\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.83, valid_loss=-1.83]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12455)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 65.86it/s]\u001b[A\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.83, valid_loss=-1.83]\n","Epoch 24:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.41, train_loss_epoch=-1.91, valid_loss=-1.83]\n","Epoch 25:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.90, valid_loss=-1.83]\n","Epoch 26:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.87, valid_loss=-1.83]\n","Epoch 27:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.88, valid_loss=-1.83]\n","Epoch 28:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.16, train_loss_epoch=-1.80, valid_loss=-1.83]\n","Epoch 29:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.90, valid_loss=-1.83]\n","Epoch 30:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.89, valid_loss=-1.83]\n","\u001b[36m(_train_tune pid=12455)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12455)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 65.45it/s]\u001b[A\n","Epoch 31:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.62, train_loss_epoch=-1.90, valid_loss=-1.87]\n","Epoch 32:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.88, valid_loss=-1.87]\n","Epoch 33:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.73, train_loss_epoch=-1.90, valid_loss=-1.87]\n","Epoch 34:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.89, valid_loss=-1.87]\n","Epoch 34:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.88, valid_loss=-1.87]         \n","Epoch 35:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.88, valid_loss=-1.87]\n","Epoch 36:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-1.98, valid_loss=-1.87]\n","Epoch 37:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.83, valid_loss=-1.87]\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.92, valid_loss=-1.87]\n","\u001b[36m(_train_tune pid=12455)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12455)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 64.32it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12455)\u001b[0m \n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.92, valid_loss=-1.87]\n","Epoch 39:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.96, valid_loss=-1.87]\n","Epoch 40:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.41, train_loss_epoch=-1.86, valid_loss=-1.87]\n","Epoch 41:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-1.96, valid_loss=-1.87]\n","Epoch 42:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.92, valid_loss=-1.87]\n","Epoch 43:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.90, valid_loss=-1.87]\n","Epoch 44:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.90, valid_loss=-1.87]\n","Epoch 45:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.91, valid_loss=-1.87]\n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-1.94, valid_loss=-1.87]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12455)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 63.31it/s]\u001b[A\n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.94, valid_loss=-1.88]\n","Epoch 47:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.85, valid_loss=-1.88]\n","Epoch 48:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-1.92, valid_loss=-1.88]\n","Epoch 49:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.93, valid_loss=-1.88]\n","Epoch 50:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-1.89, valid_loss=-1.88]\n","Epoch 51:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.92, valid_loss=-1.88]\n","Epoch 52:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.44, train_loss_epoch=-1.92, valid_loss=-1.88]\n","Epoch 53:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.93, valid_loss=-1.88]\n","\u001b[36m(_train_tune pid=12455)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12455)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 64.13it/s]\u001b[A\n","Epoch 54:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.90, valid_loss=-1.90]\n","Epoch 55:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.14, train_loss_epoch=-1.79, valid_loss=-1.90]\n","Epoch 56:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-1.93, valid_loss=-1.90]\n","Epoch 57:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.93, valid_loss=-1.90]\n","Epoch 58:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.34, train_loss_epoch=-1.99, valid_loss=-1.90]\n","Epoch 59:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.87, valid_loss=-1.90]\n","Epoch 60:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.95, valid_loss=-1.90]\n","Epoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.97, valid_loss=-1.90]\n","\u001b[36m(_train_tune pid=12455)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12455)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 62.34it/s]\u001b[A\n","Epoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.97, valid_loss=-1.91]\n","Epoch 62:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.90, valid_loss=-1.91]\n","Epoch 63:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.95, valid_loss=-1.91]\n","Epoch 64:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.92, valid_loss=-1.91]\n","Epoch 65:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-1.94, valid_loss=-1.91]\n","Epoch 66:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.96, valid_loss=-1.91]\n","Epoch 67:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.94, valid_loss=-1.91]\n","Epoch 68:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.92, valid_loss=-1.91]\n","Epoch 69:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.11, train_loss_epoch=-1.90, valid_loss=-1.91]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12455)\u001b[0m \n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12455)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 67.78it/s]\u001b[A\n","Epoch 69:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.90, valid_loss=-1.92]\n","Epoch 70:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.88, valid_loss=-1.92]\n","Epoch 71:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-1.99, valid_loss=-1.92]\n","Epoch 72:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-2.00, valid_loss=-1.92]\n","Epoch 73:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.96, valid_loss=-1.92]\n","Epoch 74:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.92, valid_loss=-1.92]\n","Epoch 75:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.61, train_loss_epoch=-1.92, valid_loss=-1.92]\n","Epoch 76:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-1.91, valid_loss=-1.92]\n","\u001b[36m(_train_tune pid=12455)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12455)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 64.59it/s]\u001b[A\n","Epoch 77:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.91, valid_loss=-1.91]\n","Epoch 78:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.00, valid_loss=-1.91]\n","Epoch 79:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.74, train_loss_epoch=-1.88, valid_loss=-1.91]\n","Epoch 80:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.94, valid_loss=-1.91]\n","Epoch 81:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-2.02, valid_loss=-1.91]\n","Epoch 82:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-1.97, valid_loss=-1.91]\n","Epoch 83:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.93, valid_loss=-1.91]\n","Epoch 84:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.96, valid_loss=-1.91]\n","\u001b[36m(_train_tune pid=12455)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-18 13:23:52,824\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (60, 8, 1), 'n_pool_kernel_size': (4, 4, 4), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=12455)\u001b[0m `Trainer.fit` stopped: `max_steps=1100.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=12455)\u001b[0m \n","\u001b[36m(_train_tune pid=12455)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 66.34it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12455)\u001b[0m \r                                                                        \u001b[A\rEpoch 84:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.96, valid_loss=-1.93]\rEpoch 84:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.95, valid_loss=-1.93]\rEpoch 84:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.95, valid_loss=-1.93]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=12648)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=12648)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=12648)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=12648)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=12648)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=12648)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=12648)\u001b[0m 2024-11-18 13:24:00.908263: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=12648)\u001b[0m 2024-11-18 13:24:00.927851: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=12648)\u001b[0m 2024-11-18 13:24:00.952283: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=12648)\u001b[0m 2024-11-18 13:24:00.959844: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=12648)\u001b[0m 2024-11-18 13:24:00.977039: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=12648)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=12648)\u001b[0m 2024-11-18 13:24:02.136175: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=12648)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=12648)\u001b[0m \n","\u001b[36m(_train_tune pid=12648)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=12648)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=12648)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=12648)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=12648)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=12648)\u001b[0m 3 | blocks       | ModuleList       | 2.6 M  | train\n","\u001b[36m(_train_tune pid=12648)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=12648)\u001b[0m 2.6 M     Trainable params\n","\u001b[36m(_train_tune pid=12648)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=12648)\u001b[0m 2.6 M     Total params\n","\u001b[36m(_train_tune pid=12648)\u001b[0m 10.280    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=12648)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=12648)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/13 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-0.663, train_loss_epoch=0.338]\n","Epoch 2:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.28]\n","Epoch 3:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.53]\n","Epoch 4:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.68]\n","Epoch 5:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.74]\n","Epoch 6:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.72]\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.81]\n","\u001b[36m(_train_tune pid=12648)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12648)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 64.70it/s]\u001b[A\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.81, valid_loss=-1.83]\n","Epoch 8:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.81, valid_loss=-1.83]\n","Epoch 9:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.49, train_loss_epoch=-1.83, valid_loss=-1.83]\n","Epoch 10:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.86, valid_loss=-1.83]\n","Epoch 11:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.30, train_loss_epoch=-1.91, valid_loss=-1.83]\n","Epoch 12:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.88, valid_loss=-1.83]\n","Epoch 13:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.89, valid_loss=-1.83]\n","Epoch 14:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.86, valid_loss=-1.83]\n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.90, valid_loss=-1.83]\n","\u001b[36m(_train_tune pid=12648)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12648)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 65.64it/s]\u001b[A\n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.90, valid_loss=-1.88]\n","Epoch 16:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-1.98, valid_loss=-1.88]\n","Epoch 17:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.44, train_loss_epoch=-1.95, valid_loss=-1.88]\n","Epoch 18:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-1.94, valid_loss=-1.88]\n","Epoch 19:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.70, train_loss_epoch=-1.89, valid_loss=-1.88]\n","Epoch 20:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-1.94, valid_loss=-1.88]\n","Epoch 21:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.92, valid_loss=-1.88]\n","Epoch 22:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.94, valid_loss=-1.88]\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.94, valid_loss=-1.88]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12648)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 65.55it/s]\u001b[A\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.94, valid_loss=-1.88]\n","Epoch 24:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.49, train_loss_epoch=-1.99, valid_loss=-1.88]\n","Epoch 25:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-1.96, valid_loss=-1.88]\n","Epoch 26:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.94, valid_loss=-1.88]\n","Epoch 27:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.96, valid_loss=-1.88]\n","Epoch 27: 100%|██████████| 13/13 [00:00\u003c00:00, 59.32it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.90, valid_loss=-1.88]\n","Epoch 28:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.25, train_loss_epoch=-1.90, valid_loss=-1.88]\n","Epoch 29:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-1.97, valid_loss=-1.88]\n","Epoch 30:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.97, valid_loss=-1.88]\n","\u001b[36m(_train_tune pid=12648)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12648)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 60.63it/s]\u001b[A\n","Epoch 30:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.97, valid_loss=-1.92]\n","Epoch 31:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.92, valid_loss=-1.92]\n","Epoch 32:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.00, valid_loss=-1.92]\n","Epoch 33:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.94, valid_loss=-1.92]\n","Epoch 34:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-1.99, valid_loss=-1.92]\n","Epoch 35:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.96, valid_loss=-1.92]\n","Epoch 36:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.38, train_loss_epoch=-2.01, valid_loss=-1.92]\n","Epoch 37:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.95, valid_loss=-1.92]\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.96, valid_loss=-1.92]\n","\u001b[36m(_train_tune pid=12648)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12648)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 63.70it/s]\u001b[A\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.96, valid_loss=-1.89]\n","Epoch 39:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.01, valid_loss=-1.89]\n","Epoch 40:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.50, train_loss_epoch=-1.95, valid_loss=-1.89]\n","Epoch 41:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-1.99, valid_loss=-1.89]\n","Epoch 42:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-2.00, valid_loss=-1.89]\n","Epoch 43:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.99, valid_loss=-1.89]\n","Epoch 44:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.00, valid_loss=-1.89]\n","Epoch 45:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.01, valid_loss=-1.89]\n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.03, valid_loss=-1.89]\n","\u001b[36m(_train_tune pid=12648)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12648)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 61.92it/s]\u001b[A\n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.03, valid_loss=-1.93]\n","Epoch 47:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-2.00, valid_loss=-1.93]\n","Epoch 48:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.02, valid_loss=-1.93]\n","Epoch 49:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.02, valid_loss=-1.93]\n","Epoch 50:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-2.05, valid_loss=-1.93]\n","Epoch 51:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.02, valid_loss=-1.93]\n","Epoch 51: 100%|██████████| 13/13 [00:00\u003c00:00, 61.28it/s, v_num=0, train_loss_step=-2.56, train_loss_epoch=-2.05, valid_loss=-1.93]\n","Epoch 52:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.56, train_loss_epoch=-2.05, valid_loss=-1.93]\n","Epoch 53:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.03, valid_loss=-1.93]\n","\u001b[36m(_train_tune pid=12648)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12648)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 65.53it/s]\u001b[A\n","Epoch 53:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.03, valid_loss=-1.93]\n","Epoch 53: 100%|██████████| 13/13 [00:00\u003c00:00, 30.80it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.03, valid_loss=-1.93]\n","Epoch 54:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.02, valid_loss=-1.93]\n","Epoch 55:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.99, valid_loss=-1.93]\n","Epoch 56:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.40, train_loss_epoch=-2.06, valid_loss=-1.93]\n","Epoch 57:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.00, valid_loss=-1.93]\n","Epoch 58:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.36, train_loss_epoch=-2.05, valid_loss=-1.93]\n","Epoch 59:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.99, valid_loss=-1.93]\n","Epoch 60:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-2.02, valid_loss=-1.93]\n","Epoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-2.01, valid_loss=-1.93]\n","\u001b[36m(_train_tune pid=12648)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-18 13:24:19,278\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (60, 8, 1), 'n_pool_kernel_size': (4, 4, 4), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","\u001b[36m(_train_tune pid=12648)\u001b[0m `Trainer.fit` stopped: `max_steps=800.0` reached.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=12648)\u001b[0m \n","\u001b[36m(_train_tune pid=12648)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 64.95it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12648)\u001b[0m \r                                                                        \u001b[A\rEpoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.01, valid_loss=-1.92]\rEpoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.01, valid_loss=-1.92]\rEpoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.01, valid_loss=-1.92]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=12817)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=12817)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=12817)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=12817)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=12817)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=12817)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=12817)\u001b[0m 2024-11-18 13:24:26.907458: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=12817)\u001b[0m 2024-11-18 13:24:26.927190: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=12817)\u001b[0m 2024-11-18 13:24:26.951998: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=12817)\u001b[0m 2024-11-18 13:24:26.959644: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=12817)\u001b[0m 2024-11-18 13:24:26.977406: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=12817)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=12817)\u001b[0m 2024-11-18 13:24:28.242339: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=12817)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=12817)\u001b[0m \n","\u001b[36m(_train_tune pid=12817)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=12817)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=12817)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=12817)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=12817)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=12817)\u001b[0m 3 | blocks       | ModuleList       | 2.6 M  | train\n","\u001b[36m(_train_tune pid=12817)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=12817)\u001b[0m 2.6 M     Trainable params\n","\u001b[36m(_train_tune pid=12817)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=12817)\u001b[0m 2.6 M     Total params\n","\u001b[36m(_train_tune pid=12817)\u001b[0m 10.274    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=12817)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=12817)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]\n","Sanity Checking:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/2 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.32, train_loss_epoch=-1.31]\n","Epoch 2:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.59, train_loss_epoch=-1.53]\n","Epoch 3:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.55, train_loss_epoch=-1.48]\n","Epoch 4:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.51, train_loss_epoch=-1.53]\n","Epoch 5:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.63, train_loss_epoch=-1.60]\n","Epoch 6:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.68, train_loss_epoch=-1.67]\n","Epoch 7:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.71]\n","Epoch 8:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.68]\n","Epoch 9:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.78]\n","Epoch 10:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.73]\n","Epoch 11:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.69, train_loss_epoch=-1.71]\n","Epoch 12:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.94]\n","Epoch 13:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.79]\n","Epoch 14:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.82]\n","Epoch 15:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.82]\n","Epoch 16:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.54, train_loss_epoch=-1.67]\n","Epoch 17:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.82]\n","Epoch 18:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.79]\n","Epoch 19:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.85]\n","Epoch 20:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.83]\n","Epoch 21:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.83]\n","Epoch 22:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.84]\n","Epoch 23:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.71, train_loss_epoch=-1.63]\n","Epoch 24:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.94]\n","Epoch 25:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.78, train_loss_epoch=-1.79]\n","Epoch 26:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.83]\n","Epoch 27:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.84]\n","Epoch 28:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.78]\n","Epoch 29:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.76, train_loss_epoch=-1.86]\n","Epoch 30:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.84]\n","Epoch 31:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.93]\n","Epoch 32:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.93]\n","Epoch 33:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.88]\n","Epoch 34:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.96]\n","Epoch 35:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.84]\n","Epoch 36:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.89]\n","Epoch 37:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.00]\n","Epoch 38:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.87]\n","Epoch 39:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.83]\n","Epoch 40:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.84]\n","Epoch 41:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.90]\n","Epoch 42:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.79]\n","Epoch 43:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.89]\n","Epoch 44:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.88]\n","Epoch 45:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.00]\n","Epoch 46:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.01]\n","Epoch 47:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.84]\n","Epoch 48:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.00]\n","Epoch 49:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.82]\n","Epoch 49: 100%|██████████| 2/2 [00:00\u003c00:00, 16.64it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.82]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12817)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12817)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 16.02it/s]\u001b[A\n","Epoch 50:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.82, valid_loss=-1.85]\n","Epoch 51:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.95, valid_loss=-1.85]\n","Epoch 52:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.90, valid_loss=-1.85]\n","Epoch 53:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.91, valid_loss=-1.85]\n","Epoch 54:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.85, valid_loss=-1.85]\n","Epoch 55:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.90, valid_loss=-1.85]\n","Epoch 56:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.87, valid_loss=-1.85]\n","Epoch 57:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.98, valid_loss=-1.85]\n","Epoch 58:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.86, valid_loss=-1.85]\n","Epoch 59:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.94, valid_loss=-1.85]\n","Epoch 60:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.83, valid_loss=-1.85]\n","Epoch 61:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.94, valid_loss=-1.85]\n","Epoch 62:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.95, valid_loss=-1.85]\n","Epoch 63:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.97, valid_loss=-1.85]\n","Epoch 64:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.89, valid_loss=-1.85]\n","Epoch 65:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.88, valid_loss=-1.85]\n","Epoch 66:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.86, valid_loss=-1.85]\n","Epoch 67:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.92, valid_loss=-1.85]\n","Epoch 68:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.87, valid_loss=-1.85]\n","Epoch 69:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.89, valid_loss=-1.85]\n","Epoch 70:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.86, valid_loss=-1.85]\n","Epoch 71:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.82, valid_loss=-1.85]\n","Epoch 72:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.93, valid_loss=-1.85]\n","Epoch 73:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.88, valid_loss=-1.85]\n","Epoch 74:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.93, valid_loss=-1.85]\n","Epoch 75:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.01, valid_loss=-1.85]\n","Epoch 76:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-1.96, valid_loss=-1.85]\n","Epoch 77:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.98, valid_loss=-1.85]\n","Epoch 78:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.96, valid_loss=-1.85]\n","Epoch 79:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.94, valid_loss=-1.85]\n","Epoch 79: 100%|██████████| 2/2 [00:00\u003c00:00, 17.83it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.95, valid_loss=-1.85]\n","Epoch 80:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.95, valid_loss=-1.85]\n","Epoch 80: 100%|██████████| 2/2 [00:00\u003c00:00, 18.75it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.95, valid_loss=-1.85]\n","Epoch 81:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.88, valid_loss=-1.85]\n","Epoch 82:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.84, valid_loss=-1.85]\n","Epoch 83:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.97, valid_loss=-1.85]\n","Epoch 84:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.91, valid_loss=-1.85]\n","Epoch 85:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.85, valid_loss=-1.85]\n","Epoch 86:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.98, valid_loss=-1.85]\n","Epoch 87:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.01, valid_loss=-1.85]\n","Epoch 88:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.06, valid_loss=-1.85]\n","Epoch 89:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.83, valid_loss=-1.85]\n","Epoch 90:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.88, valid_loss=-1.85]\n","Epoch 91:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.90, valid_loss=-1.85]\n","Epoch 92:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.94, valid_loss=-1.85]\n","Epoch 93:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.97, valid_loss=-1.85]\n","Epoch 94:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.97, valid_loss=-1.85]\n","Epoch 95:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.03, valid_loss=-1.85]\n","Epoch 96:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.92, valid_loss=-1.85]\n","Epoch 97:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.86, valid_loss=-1.85]\n","Epoch 98:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.97, valid_loss=-1.85]\n","Epoch 99:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.07, valid_loss=-1.85]\n","Epoch 99: 100%|██████████| 2/2 [00:00\u003c00:00, 19.28it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-2.07, valid_loss=-1.85]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12817)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 16.21it/s]\u001b[A\n","Epoch 100:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.87, valid_loss=-1.85]\n","Epoch 101:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.89, valid_loss=-1.85]\n","Epoch 102:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-1.97, valid_loss=-1.85]\n","Epoch 103:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.91, valid_loss=-1.85]\n","Epoch 104:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.99, valid_loss=-1.85]\n","Epoch 105:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.89, valid_loss=-1.85]\n","Epoch 106:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.89, valid_loss=-1.85]\n","Epoch 107:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.72, train_loss_epoch=-1.94, valid_loss=-1.85]\n","Epoch 108:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.93, valid_loss=-1.85]\n","Epoch 109:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.99, valid_loss=-1.85]\n","Epoch 110:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.03, valid_loss=-1.85]\n","Epoch 111:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.93, valid_loss=-1.85]\n","Epoch 112:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.85, valid_loss=-1.85]\n","Epoch 113:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.05, valid_loss=-1.85]\n","Epoch 114:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.05, valid_loss=-1.85]\n","Epoch 115:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.02, valid_loss=-1.85]\n","Epoch 116:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.05, valid_loss=-1.85]\n","Epoch 117:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.93, valid_loss=-1.85]\n","Epoch 118:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.06, valid_loss=-1.85]\n","Epoch 119:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.04, valid_loss=-1.85]\n","Epoch 120:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.00, valid_loss=-1.85]\n","Epoch 121:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.92, valid_loss=-1.85]\n","Epoch 122:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.02, valid_loss=-1.85]\n","Epoch 123:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.01, valid_loss=-1.85]\n","Epoch 124:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.00, valid_loss=-1.85]\n","Epoch 125:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.09, valid_loss=-1.85]\n","Epoch 126:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.08, valid_loss=-1.85]\n","Epoch 127:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.96, valid_loss=-1.85]\n","Epoch 128:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.96, valid_loss=-1.85]\n","Epoch 129:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.01, valid_loss=-1.85]\n","Epoch 130:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.96, valid_loss=-1.85]\n","Epoch 131:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.96, valid_loss=-1.85]\n","Epoch 132:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.93, valid_loss=-1.85]\n","Epoch 133:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.00, valid_loss=-1.85]\n","Epoch 134:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.00, valid_loss=-1.85]\n","Epoch 135:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.84, valid_loss=-1.85]\n","Epoch 136:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.00, valid_loss=-1.85]\n","Epoch 137:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.91, valid_loss=-1.85]\n","Epoch 138:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.92, valid_loss=-1.85]\n","Epoch 139:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.88, valid_loss=-1.85]\n","Epoch 140:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.91, valid_loss=-1.85]\n","Epoch 141:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.77, valid_loss=-1.85]\n","Epoch 142:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.98, valid_loss=-1.85]\n","Epoch 143:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.98, valid_loss=-1.85]\n","Epoch 144:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.87, valid_loss=-1.85]\n","Epoch 145:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.94, valid_loss=-1.85]\n","Epoch 146:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-1.89, valid_loss=-1.85]\n","Epoch 147:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.89, valid_loss=-1.85]\n","Epoch 148:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.99, valid_loss=-1.85]\n","Epoch 149:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.00, valid_loss=-1.85]\n","Epoch 149: 100%|██████████| 2/2 [00:00\u003c00:00, 19.20it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.00, valid_loss=-1.85]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12817)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12817)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 16.15it/s]\u001b[A\n","Epoch 150:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.02, valid_loss=-1.88]\n","Epoch 151:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.91, valid_loss=-1.88]\n","Epoch 152:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.83, train_loss_epoch=-1.89, valid_loss=-1.88]\n","Epoch 153:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.94, valid_loss=-1.88]\n","Epoch 154:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.04, valid_loss=-1.88]\n","Epoch 155:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.08, valid_loss=-1.88]\n","Epoch 156:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.10, valid_loss=-1.88]\n","Epoch 157:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.03, valid_loss=-1.88]\n","Epoch 158:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.12, valid_loss=-1.88]\n","Epoch 159:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.92, valid_loss=-1.88]\n","Epoch 160:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.02, valid_loss=-1.88]\n","Epoch 161:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.03, valid_loss=-1.88]\n","Epoch 162:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.97, valid_loss=-1.88]\n","Epoch 163:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.99, valid_loss=-1.88]\n","Epoch 164:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.99, valid_loss=-1.88]\n","Epoch 165:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.96, valid_loss=-1.88]\n","Epoch 166:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.01, valid_loss=-1.88]\n","Epoch 167:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-2.00, valid_loss=-1.88]\n","Epoch 168:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.90, valid_loss=-1.88]\n","Epoch 169:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.00, valid_loss=-1.88]\n","Epoch 170:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.98, valid_loss=-1.88]\n","Epoch 171:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.10, valid_loss=-1.88]\n","Epoch 172:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.91, valid_loss=-1.88]\n","Epoch 173:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.92, valid_loss=-1.88]\n","Epoch 174:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.96, valid_loss=-1.88]\n","Epoch 175:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.92, valid_loss=-1.88]\n","Epoch 176:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.99, valid_loss=-1.88]\n","Epoch 177:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.89, valid_loss=-1.88]\n","Epoch 178:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.98, valid_loss=-1.88]\n","Epoch 179:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.96, valid_loss=-1.88]\n","Epoch 180:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.99, valid_loss=-1.88]\n","Epoch 181:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.04, valid_loss=-1.88]\n","Epoch 182:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.00, valid_loss=-1.88]\n","Epoch 183:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.01, valid_loss=-1.88]\n","Epoch 184:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.94, valid_loss=-1.88]\n","Epoch 185:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.91, valid_loss=-1.88]\n","Epoch 186:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.99, valid_loss=-1.88]\n","Epoch 187:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.96, valid_loss=-1.88]\n","Epoch 188:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.96, valid_loss=-1.88]\n","Epoch 189:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-2.02, valid_loss=-1.88]\n","Epoch 190:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-2.05, valid_loss=-1.88]\n","Epoch 191:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.79, train_loss_epoch=-1.86, valid_loss=-1.88]\n","Epoch 192:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.01, valid_loss=-1.88]\n","Epoch 193:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.97, valid_loss=-1.88]\n","Epoch 194:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.06, valid_loss=-1.88]\n","Epoch 195:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.06, valid_loss=-1.88]\n","Epoch 196:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.08, valid_loss=-1.88]\n","Epoch 197:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.99, valid_loss=-1.88]\n","Epoch 198:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.04, valid_loss=-1.88]\n","Epoch 199:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.04, valid_loss=-1.88]\n","Epoch 199: 100%|██████████| 2/2 [00:00\u003c00:00, 18.93it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.04, valid_loss=-1.88]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12817)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 15.91it/s]\u001b[A\n","Epoch 200:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.11, valid_loss=-1.88]\n","Epoch 201:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.00, valid_loss=-1.88]\n","Epoch 201: 100%|██████████| 2/2 [00:00\u003c00:00, 18.81it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.93, valid_loss=-1.88]\n","Epoch 202:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.93, valid_loss=-1.88]\n","Epoch 203:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.06, valid_loss=-1.88]\n","Epoch 204:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.98, valid_loss=-1.88]\n","Epoch 205:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.02, valid_loss=-1.88]\n","Epoch 206:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-1.80, valid_loss=-1.88]\n","Epoch 207:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.08, valid_loss=-1.88]\n","Epoch 208:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.08, valid_loss=-1.88]\n","Epoch 209:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-2.01, valid_loss=-1.88]\n","Epoch 210:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.01, valid_loss=-1.88]\n","Epoch 211:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.94, valid_loss=-1.88]\n","Epoch 212:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-1.94, valid_loss=-1.88]\n","Epoch 213:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.05, valid_loss=-1.88]\n","Epoch 214:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.14, valid_loss=-1.88]\n","Epoch 215:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.07, valid_loss=-1.88]\n","Epoch 216:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.10, valid_loss=-1.88]\n","Epoch 217:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.86, train_loss_epoch=-1.94, valid_loss=-1.88]\n","Epoch 218:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.03, valid_loss=-1.88]\n","Epoch 219:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.03, valid_loss=-1.88]\n","Epoch 220:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.98, valid_loss=-1.88]\n","Epoch 221:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.02, valid_loss=-1.88]\n","Epoch 222:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-2.03, valid_loss=-1.88]\n","Epoch 223:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.02, valid_loss=-1.88]\n","Epoch 224:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.97, valid_loss=-1.88]\n","Epoch 225:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.08, valid_loss=-1.88]\n","Epoch 226:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.08, valid_loss=-1.88]\n","Epoch 227:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.06, valid_loss=-1.88]\n","Epoch 228:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.99, valid_loss=-1.88]\n","Epoch 229:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.01, valid_loss=-1.88]\n","Epoch 230:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.07, valid_loss=-1.88]\n","Epoch 231:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.03, valid_loss=-1.88]\n","Epoch 232:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.04, valid_loss=-1.88]\n","Epoch 233:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.14, valid_loss=-1.88]\n","Epoch 234:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.02, valid_loss=-1.88]\n","Epoch 235:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.95, valid_loss=-1.88]\n","Epoch 236:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.06, valid_loss=-1.88]\n","Epoch 237:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.02, valid_loss=-1.88]\n","Epoch 238:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.10, valid_loss=-1.88]\n","Epoch 239:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.99, valid_loss=-1.88]\n","Epoch 240:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.14, valid_loss=-1.88]\n","Epoch 241:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-1.99, valid_loss=-1.88]\n","Epoch 242:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.10, valid_loss=-1.88]\n","Epoch 243:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-1.95, valid_loss=-1.88]\n","Epoch 244:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.00, valid_loss=-1.88]\n","Epoch 245:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.05, valid_loss=-1.88]\n","Epoch 246:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.97, valid_loss=-1.88]\n","Epoch 247:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.01, valid_loss=-1.88]\n","Epoch 248:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.01, valid_loss=-1.88]\n","Epoch 249:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.98, valid_loss=-1.88]\n","Epoch 249: 100%|██████████| 2/2 [00:00\u003c00:00, 18.83it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-1.98, valid_loss=-1.88]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12817)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12817)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 15.79it/s]\u001b[A\n","Epoch 250:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.12, valid_loss=-1.90]\n","Epoch 251:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.91, valid_loss=-1.90]\n","Epoch 252:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.01, valid_loss=-1.90]\n","Epoch 253:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.07, valid_loss=-1.90]\n","Epoch 254:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-1.97, valid_loss=-1.90]\n","Epoch 255:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.97, valid_loss=-1.90]\n","Epoch 256:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.06, valid_loss=-1.90]\n","Epoch 257:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-2.01, valid_loss=-1.90]\n","Epoch 258:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.99, valid_loss=-1.90]\n","Epoch 258: 100%|██████████| 2/2 [00:00\u003c00:00, 19.04it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.96, valid_loss=-1.90]\n","Epoch 259:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.96, valid_loss=-1.90]\n","Epoch 260:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.06, valid_loss=-1.90]\n","Epoch 261:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.02, valid_loss=-1.90]\n","Epoch 262:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.03, valid_loss=-1.90]\n","Epoch 263:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.06, valid_loss=-1.90]\n","Epoch 264:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.03, valid_loss=-1.90]\n","Epoch 265:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.09, valid_loss=-1.90]\n","Epoch 266:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.93, valid_loss=-1.90]\n","Epoch 267:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.15, valid_loss=-1.90]\n","Epoch 268:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.06, valid_loss=-1.90]\n","Epoch 269:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.08, valid_loss=-1.90]\n","Epoch 270:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.93, valid_loss=-1.90]\n","Epoch 271:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.07, valid_loss=-1.90]\n","Epoch 272:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.09, valid_loss=-1.90]\n","Epoch 273:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.13, valid_loss=-1.90]\n","Epoch 274:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.03, valid_loss=-1.90]\n","Epoch 275:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.96, valid_loss=-1.90]\n","Epoch 276:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.10, valid_loss=-1.90]\n","Epoch 277:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.19, valid_loss=-1.90]\n","Epoch 278:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.04, valid_loss=-1.90]\n","Epoch 279:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.14, valid_loss=-1.90]\n","Epoch 280:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.07, valid_loss=-1.90]\n","Epoch 281:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.09, valid_loss=-1.90]\n","Epoch 282:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.97, valid_loss=-1.90]\n","Epoch 283:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.03, valid_loss=-1.90]\n","Epoch 284:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.09, valid_loss=-1.90]\n","Epoch 285:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.07, valid_loss=-1.90]\n","Epoch 286:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.04, valid_loss=-1.90]\n","Epoch 287:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.04, valid_loss=-1.90]\n","Epoch 288:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.96, valid_loss=-1.90]\n","Epoch 289:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.10, valid_loss=-1.90]\n","Epoch 290:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.02, valid_loss=-1.90]\n","Epoch 291:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.04, valid_loss=-1.90]\n","Epoch 292:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.02, valid_loss=-1.90]\n","Epoch 293:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.04, valid_loss=-1.90]\n","Epoch 294:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.05, valid_loss=-1.90]\n","Epoch 294: 100%|██████████| 2/2 [00:00\u003c00:00, 19.00it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.10, valid_loss=-1.90]\n","Epoch 295:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.10, valid_loss=-1.90]\n","Epoch 296:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.99, valid_loss=-1.90]\n","Epoch 297:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.05, valid_loss=-1.90]\n","Epoch 298:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.99, valid_loss=-1.90]\n","Epoch 299:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.05, valid_loss=-1.90]\n","Epoch 299: 100%|██████████| 2/2 [00:00\u003c00:00, 19.03it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.05, valid_loss=-1.90]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12817)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 15.96it/s]\u001b[A\n","Epoch 300:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-1.99, valid_loss=-1.92]\n","Epoch 301:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.14, valid_loss=-1.92]\n","Epoch 302:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.13, valid_loss=-1.92]\n","Epoch 303:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.03, valid_loss=-1.92]\n","Epoch 304:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.95, valid_loss=-1.92]\n","Epoch 305:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-2.02, valid_loss=-1.92]\n","Epoch 306:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.01, valid_loss=-1.92]\n","Epoch 307:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.13, valid_loss=-1.92]\n","Epoch 308:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.13, valid_loss=-1.92]\n","Epoch 309:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.03, valid_loss=-1.92]\n","Epoch 310:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.09, valid_loss=-1.92]\n","Epoch 311:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-2.20, valid_loss=-1.92]\n","Epoch 312:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.06, valid_loss=-1.92]\n","Epoch 313:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.08, valid_loss=-1.92]\n","Epoch 314:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.12, valid_loss=-1.92]\n","Epoch 315:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.19, valid_loss=-1.92]\n","Epoch 316:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.96, valid_loss=-1.92]\n","Epoch 317:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-1.99, valid_loss=-1.92]\n","Epoch 318:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.09, valid_loss=-1.92]\n","Epoch 319:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.95, valid_loss=-1.92]\n","Epoch 320:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.02, valid_loss=-1.92]\n","Epoch 321:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.97, valid_loss=-1.92]\n","Epoch 322:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.00, valid_loss=-1.92]\n","Epoch 323:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.06, valid_loss=-1.92]\n","Epoch 324:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.06, valid_loss=-1.92]\n","Epoch 325:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.97, valid_loss=-1.92]\n","Epoch 326:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.05, valid_loss=-1.92]\n","Epoch 327:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.09, valid_loss=-1.92]\n","Epoch 328:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.13, valid_loss=-1.92]\n","Epoch 329:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.01, valid_loss=-1.92]\n","Epoch 330:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.01, valid_loss=-1.92]\n","Epoch 331:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.94, valid_loss=-1.92]\n","Epoch 332:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.03, valid_loss=-1.92]\n","Epoch 333:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.12, valid_loss=-1.92]\n","Epoch 334:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.11, valid_loss=-1.92]\n","Epoch 334: 100%|██████████| 2/2 [00:00\u003c00:00, 19.05it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.11, valid_loss=-1.92]\n","Epoch 335:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.04, valid_loss=-1.92]\n","Epoch 335: 100%|██████████| 2/2 [00:00\u003c00:00, 18.94it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.04, valid_loss=-1.92]\n","Epoch 336:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.20, valid_loss=-1.92]\n","Epoch 336: 100%|██████████| 2/2 [00:00\u003c00:00, 19.02it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.10, valid_loss=-1.92]\n","Epoch 337:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.10, valid_loss=-1.92]\n","Epoch 338:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.10, valid_loss=-1.92]\n","Epoch 338: 100%|██████████| 2/2 [00:00\u003c00:00, 19.05it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.10, valid_loss=-1.92]\n","Epoch 339:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.97, valid_loss=-1.92]\n","Epoch 339: 100%|██████████| 2/2 [00:00\u003c00:00, 19.00it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.01, valid_loss=-1.92]\n","Epoch 340:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.01, valid_loss=-1.92]\n","Epoch 341:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-1.97, valid_loss=-1.92]\n","Epoch 342:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.16, valid_loss=-1.92]\n","Epoch 343:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.12, valid_loss=-1.92]\n","Epoch 343: 100%|██████████| 2/2 [00:00\u003c00:00, 18.96it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-2.12, valid_loss=-1.92]\n","Epoch 344:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-1.96, valid_loss=-1.92]\n","Epoch 345:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.04, valid_loss=-1.92]\n","Epoch 346:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.17, valid_loss=-1.92]\n","Epoch 347:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.07, valid_loss=-1.92]\n","Epoch 348:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.09, valid_loss=-1.92]\n","Epoch 349:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.03, valid_loss=-1.92]\n","Epoch 349: 100%|██████████| 2/2 [00:00\u003c00:00, 18.73it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.03, valid_loss=-1.92]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12817)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 16.10it/s]\u001b[A\n","Epoch 350:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.01, valid_loss=-1.90]\n","Epoch 351:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.09, valid_loss=-1.90]\n","Epoch 351: 100%|██████████| 2/2 [00:00\u003c00:00, 18.93it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-2.09, valid_loss=-1.90]\n","Epoch 352:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-1.95, valid_loss=-1.90]\n","Epoch 353:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.02, valid_loss=-1.90]\n","Epoch 354:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.03, valid_loss=-1.90]\n","Epoch 355:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.96, valid_loss=-1.90]\n","Epoch 356:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.98, valid_loss=-1.90]\n","Epoch 357:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.02, valid_loss=-1.90]\n","Epoch 358:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.11, valid_loss=-1.90]\n","Epoch 359:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.00, valid_loss=-1.90]\n","Epoch 360:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.10, valid_loss=-1.90]\n","Epoch 361:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.02, valid_loss=-1.90]\n","Epoch 362:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.08, valid_loss=-1.90]\n","Epoch 363:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.07, valid_loss=-1.90]\n","Epoch 364:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.06, valid_loss=-1.90]\n","Epoch 365:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-1.94, valid_loss=-1.90]\n","Epoch 366:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.13, valid_loss=-1.90]\n","Epoch 367:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.08, valid_loss=-1.90]\n","Epoch 368:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.11, valid_loss=-1.90]\n","Epoch 369:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.07, valid_loss=-1.90]\n","Epoch 370:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.14, valid_loss=-1.90]\n","Epoch 371:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.09, valid_loss=-1.90]\n","Epoch 372:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.11, valid_loss=-1.90]\n","Epoch 373:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.12, valid_loss=-1.90]\n","Epoch 374:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-1.99, valid_loss=-1.90]\n","Epoch 375:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.08, valid_loss=-1.90]\n","Epoch 376:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.10, valid_loss=-1.90]\n","Epoch 377:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.05, valid_loss=-1.90]\n","Epoch 378:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.08, valid_loss=-1.90]\n","Epoch 379:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.09, valid_loss=-1.90]\n","Epoch 380:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.16, valid_loss=-1.90]\n","Epoch 381:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.05, valid_loss=-1.90]\n","Epoch 382:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.02, valid_loss=-1.90]\n","Epoch 383:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.09, valid_loss=-1.90]\n","Epoch 384:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.04, valid_loss=-1.90]\n","Epoch 385:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.10, valid_loss=-1.90]\n","Epoch 386:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.17, valid_loss=-1.90]\n","Epoch 387:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.99, valid_loss=-1.90]\n","Epoch 388:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.17, valid_loss=-1.90]\n","Epoch 389:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-1.97, valid_loss=-1.90]\n","Epoch 390:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.02, valid_loss=-1.90]\n","Epoch 390: 100%|██████████| 2/2 [00:00\u003c00:00, 18.54it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.01, valid_loss=-1.90]\n","Epoch 391:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.01, valid_loss=-1.90]\n","Epoch 391: 100%|██████████| 2/2 [00:00\u003c00:00, 18.93it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.12, valid_loss=-1.90]\n","Epoch 392:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.12, valid_loss=-1.90]\n","Epoch 392:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.16, valid_loss=-1.90]        \n","Epoch 393:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.16, valid_loss=-1.90]\n","Epoch 394:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.88, valid_loss=-1.90]\n","Epoch 394: 100%|██████████| 2/2 [00:00\u003c00:00, 19.05it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.05, valid_loss=-1.90]\n","Epoch 395:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.05, valid_loss=-1.90]\n","Epoch 396:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.09, valid_loss=-1.90]\n","Epoch 396: 100%|██████████| 2/2 [00:00\u003c00:00, 19.06it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.09, valid_loss=-1.90]\n","Epoch 397:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.06, valid_loss=-1.90]\n","Epoch 398:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.22, valid_loss=-1.90]\n","Epoch 398: 100%|██████████| 2/2 [00:00\u003c00:00, 19.00it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.22, valid_loss=-1.90]\n","Epoch 399:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.01, valid_loss=-1.90]\n","Epoch 399: 100%|██████████| 2/2 [00:00\u003c00:00, 19.19it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.01, valid_loss=-1.90]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\n","\u001b[36m(_train_tune pid=12817)\u001b[0m \u001b[A\n","\u001b[36m(_train_tune pid=12817)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12817)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 16.16it/s]\u001b[A\n","Epoch 400:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.12, valid_loss=-1.93]\n","Epoch 401:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.06, valid_loss=-1.93]\n","Epoch 402:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.14, valid_loss=-1.93]\n","Epoch 403:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.03, valid_loss=-1.93]\n","Epoch 404:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.09, valid_loss=-1.93]\n","Epoch 405:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.06, valid_loss=-1.93]\n","Epoch 406:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.06, valid_loss=-1.93]\n","Epoch 407:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.06, valid_loss=-1.93]\n","Epoch 408:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.03, valid_loss=-1.93]\n","Epoch 409:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-1.96, valid_loss=-1.93]\n","Epoch 410:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.05, valid_loss=-1.93]\n","Epoch 411:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.08, valid_loss=-1.93]\n","Epoch 412:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.91, valid_loss=-1.93]\n","Epoch 413:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.03, valid_loss=-1.93]\n","Epoch 414:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.01, valid_loss=-1.93]\n","Epoch 415:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.12, valid_loss=-1.93]\n","Epoch 416:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.01, valid_loss=-1.93]\n","Epoch 417:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.15, valid_loss=-1.93]\n","Epoch 418:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.94, train_loss_epoch=-1.95, valid_loss=-1.93]\n","Epoch 419:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.15, valid_loss=-1.93]\n","Epoch 420:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.12, valid_loss=-1.93]\n","Epoch 421:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-2.22, valid_loss=-1.93]\n","Epoch 422:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-1.95, valid_loss=-1.93]\n","Epoch 423:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.02, valid_loss=-1.93]\n","Epoch 423: 100%|██████████| 2/2 [00:00\u003c00:00, 18.88it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.11, valid_loss=-1.93]\n","Epoch 424:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.11, valid_loss=-1.93]\n","Epoch 424: 100%|██████████| 2/2 [00:00\u003c00:00, 18.99it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.11, valid_loss=-1.93]\n","Epoch 425:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.02, valid_loss=-1.93]\n","Epoch 426:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.06, valid_loss=-1.93]\n","Epoch 427:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.05, valid_loss=-1.93]\n","Epoch 428:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.13, valid_loss=-1.93]\n","Epoch 429:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.20, valid_loss=-1.93]\n","Epoch 430:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.12, valid_loss=-1.93]\n","Epoch 431:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.09, valid_loss=-1.93]\n","Epoch 432:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-1.98, valid_loss=-1.93]\n","Epoch 433:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.01, valid_loss=-1.93]\n","Epoch 434:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.08, valid_loss=-1.93]\n","Epoch 435:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.14, valid_loss=-1.93]\n","Epoch 436:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.10, valid_loss=-1.93]\n","Epoch 437:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.11, valid_loss=-1.93]\n","Epoch 438:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.08, valid_loss=-1.93]\n","Epoch 439:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.08, valid_loss=-1.93]\n","Epoch 440:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.85, train_loss_epoch=-2.03, valid_loss=-1.93]\n","Epoch 441:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.09, valid_loss=-1.93]\n","Epoch 442:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.05, valid_loss=-1.93]\n","Epoch 443:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-2.07, valid_loss=-1.93]\n","Epoch 444:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.16, valid_loss=-1.93]\n","Epoch 445:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.12, valid_loss=-1.93]\n","Epoch 446:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.12, valid_loss=-1.93]\n","Epoch 446: 100%|██████████| 2/2 [00:00\u003c00:00, 18.42it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.01, valid_loss=-1.93]\n","Epoch 447:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.01, valid_loss=-1.93]\n","Epoch 448:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.10, valid_loss=-1.93]\n","Epoch 449:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.12, valid_loss=-1.93]\n","Epoch 449: 100%|██████████| 2/2 [00:00\u003c00:00, 18.82it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.12, valid_loss=-1.93]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12817)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 15.98it/s]\u001b[A\n","Epoch 450:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.08, valid_loss=-1.93]\n","Epoch 451:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.11, valid_loss=-1.93]\n","Epoch 452:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.17, valid_loss=-1.93]\n","Epoch 452: 100%|██████████| 2/2 [00:00\u003c00:00, 18.60it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.17, valid_loss=-1.93]\n","Epoch 452: 100%|██████████| 2/2 [00:00\u003c00:00, 18.51it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.17, valid_loss=-1.93]\n","Epoch 453:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.02, valid_loss=-1.93]\n","Epoch 454:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-1.89, valid_loss=-1.93]\n","Epoch 455:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.14, valid_loss=-1.93]\n","Epoch 456:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.03, valid_loss=-1.93]\n","Epoch 457:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.18, valid_loss=-1.93]\n","Epoch 458:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.01, valid_loss=-1.93]\n","Epoch 459:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.14, valid_loss=-1.93]\n","Epoch 460:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.13, valid_loss=-1.93]\n","Epoch 461:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.09, valid_loss=-1.93]\n","Epoch 462:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.03, valid_loss=-1.93]\n","Epoch 463:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.05, valid_loss=-1.93]\n","Epoch 464:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.13, valid_loss=-1.93]\n","Epoch 465:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.12, valid_loss=-1.93]\n","Epoch 466:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.17, valid_loss=-1.93]\n","Epoch 467:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.95, valid_loss=-1.93]\n","Epoch 468:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.07, valid_loss=-1.93]\n","Epoch 469:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.96, train_loss_epoch=-2.05, valid_loss=-1.93]\n","Epoch 470:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-2.14, valid_loss=-1.93]\n","Epoch 471:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.10, valid_loss=-1.93]\n","Epoch 472:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.03, valid_loss=-1.93]\n","Epoch 473:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.07, valid_loss=-1.93]\n","Epoch 474:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.14, valid_loss=-1.93]\n","Epoch 475:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.15, valid_loss=-1.93]\n","Epoch 476:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.17, valid_loss=-1.93]\n","Epoch 477:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.05, valid_loss=-1.93]\n","Epoch 478:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.14, valid_loss=-1.93]\n","Epoch 479:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.06, valid_loss=-1.93]\n","Epoch 480:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.18, valid_loss=-1.93]\n","Epoch 481:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.98, train_loss_epoch=-2.10, valid_loss=-1.93]\n","Epoch 482:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.23, valid_loss=-1.93]\n","Epoch 483:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.23, valid_loss=-1.93]\n","Epoch 484:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.12, valid_loss=-1.93]\n","Epoch 485:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.14, valid_loss=-1.93]\n","Epoch 486:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.09, valid_loss=-1.93]\n","Epoch 487:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.13, valid_loss=-1.93]\n","Epoch 488:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.13, valid_loss=-1.93]\n","Epoch 489:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.13, valid_loss=-1.93]\n","Epoch 490:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.00, valid_loss=-1.93]\n","Epoch 491:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.18, valid_loss=-1.93]\n","Epoch 492:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.12, valid_loss=-1.93]\n","Epoch 493:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.07, valid_loss=-1.93]\n","Epoch 494:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.12, valid_loss=-1.93]\n","Epoch 495:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.13, valid_loss=-1.93]\n","Epoch 496:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.04, valid_loss=-1.93]\n","Epoch 497:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.11, valid_loss=-1.93]\n","Epoch 498:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.07, valid_loss=-1.93]\n","Epoch 499:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.15, valid_loss=-1.93]\n","Epoch 499: 100%|██████████| 2/2 [00:00\u003c00:00, 19.10it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.15, valid_loss=-1.93]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12817)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","\u001b[36m(_train_tune pid=12817)\u001b[0m \u001b[A\n","\u001b[36m(_train_tune pid=12817)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 16.10it/s]\u001b[A\n","Epoch 500:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.22, valid_loss=-1.94]\n","Epoch 501:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.15, valid_loss=-1.94]\n","Epoch 502:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.08, valid_loss=-1.94]\n","Epoch 503:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.11, valid_loss=-1.94]\n","Epoch 504:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.09, valid_loss=-1.94]\n","Epoch 505:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.19, valid_loss=-1.94]\n","Epoch 506:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.15, valid_loss=-1.94]\n","Epoch 507:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.12, valid_loss=-1.94]\n","Epoch 508:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.10, valid_loss=-1.94]\n","Epoch 509:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.19, valid_loss=-1.94]\n","Epoch 510:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.09, valid_loss=-1.94]\n","Epoch 511:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.20, valid_loss=-1.94]\n","Epoch 512:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.17, valid_loss=-1.94]\n","Epoch 513:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.04, valid_loss=-1.94]\n","Epoch 514:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.08, valid_loss=-1.94]\n","Epoch 515:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.15, valid_loss=-1.94]\n","Epoch 516:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.11, valid_loss=-1.94]\n","Epoch 517:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.03, valid_loss=-1.94]\n","Epoch 518:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.16, valid_loss=-1.94]\n","Epoch 519:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.16, valid_loss=-1.94]\n","Epoch 520:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.12, valid_loss=-1.94]\n","Epoch 521:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.20, valid_loss=-1.94]\n","Epoch 522:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.17, valid_loss=-1.94]\n","Epoch 523:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.08, valid_loss=-1.94]\n","Epoch 524:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.06, valid_loss=-1.94]\n","Epoch 525:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.05, valid_loss=-1.94]\n","Epoch 526:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-2.10, valid_loss=-1.94]\n","Epoch 527:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.16, valid_loss=-1.94]\n","Epoch 528:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-2.00, valid_loss=-1.94]\n","Epoch 529:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.08, valid_loss=-1.94]\n","Epoch 530:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.14, valid_loss=-1.94]\n","Epoch 531:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.05, valid_loss=-1.94]\n","Epoch 532:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-2.22, valid_loss=-1.94]\n","Epoch 533:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.10, valid_loss=-1.94]\n","Epoch 534:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.21, valid_loss=-1.94]\n","Epoch 535:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.03, valid_loss=-1.94]\n","Epoch 536:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.11, valid_loss=-1.94]\n","Epoch 537:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.03, valid_loss=-1.94]\n","Epoch 538:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.15, valid_loss=-1.94]\n","Epoch 539:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.10, valid_loss=-1.94]\n","Epoch 540:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.07, valid_loss=-1.94]\n","Epoch 541:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.14, valid_loss=-1.94]\n","Epoch 542:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.07, valid_loss=-1.94]\n","Epoch 543:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.14, valid_loss=-1.94]\n","Epoch 544:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.14, valid_loss=-1.94]\n","Epoch 545:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.16, valid_loss=-1.94]\n","Epoch 546:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.20, valid_loss=-1.94]\n","Epoch 547:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.08, valid_loss=-1.94]\n","Epoch 548:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.08, valid_loss=-1.94]\n","Epoch 549:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.18, valid_loss=-1.94]\n","Epoch 549: 100%|██████████| 2/2 [00:00\u003c00:00, 17.93it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.18, valid_loss=-1.94]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12817)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12817)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 15.50it/s]\u001b[A\n","Epoch 550:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.21, valid_loss=-1.94]\n","Epoch 551:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.12, valid_loss=-1.94]\n","Epoch 552:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.08, valid_loss=-1.94]\n","Epoch 553:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.15, valid_loss=-1.94]\n","Epoch 554:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.18, valid_loss=-1.94]\n","Epoch 555:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.10, valid_loss=-1.94]\n","Epoch 556:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.10, valid_loss=-1.94]\n","Epoch 557:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.07, valid_loss=-1.94]\n","Epoch 558:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.13, valid_loss=-1.94]\n","Epoch 559:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.12, valid_loss=-1.94]\n","Epoch 560:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.15, valid_loss=-1.94]\n","Epoch 561:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.29, train_loss_epoch=-2.23, valid_loss=-1.94]\n","Epoch 562:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.13, valid_loss=-1.94]\n","Epoch 563:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.13, valid_loss=-1.94]\n","Epoch 564:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.04, valid_loss=-1.94]\n","Epoch 565:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.15, valid_loss=-1.94]\n","Epoch 566:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.09, valid_loss=-1.94]\n","Epoch 567:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.18, valid_loss=-1.94]\n","Epoch 567: 100%|██████████| 2/2 [00:00\u003c00:00, 18.99it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.97, valid_loss=-1.94]\n","Epoch 568:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.97, valid_loss=-1.94]\n","Epoch 568: 100%|██████████| 2/2 [00:00\u003c00:00, 18.75it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.03, valid_loss=-1.94]\n","Epoch 569:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.03, valid_loss=-1.94]\n","Epoch 569: 100%|██████████| 2/2 [00:00\u003c00:00, 18.90it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.03, valid_loss=-1.94]\n","Epoch 570:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.08, valid_loss=-1.94]\n","Epoch 570:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.21, valid_loss=-1.94]        \n","Epoch 571:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.21, valid_loss=-1.94]\n","Epoch 572:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-2.10, valid_loss=-1.94]\n","Epoch 572: 100%|██████████| 2/2 [00:00\u003c00:00, 18.92it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-2.10, valid_loss=-1.94]\n","Epoch 573:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.13, valid_loss=-1.94]\n","Epoch 574:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.15, valid_loss=-1.94]\n","Epoch 575:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.10, valid_loss=-1.94]\n","Epoch 576:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.17, valid_loss=-1.94]\n","Epoch 577:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.27, train_loss_epoch=-2.23, valid_loss=-1.94]\n","Epoch 578:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.03, valid_loss=-1.94]\n","Epoch 579:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.18, valid_loss=-1.94]\n","Epoch 580:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.01, valid_loss=-1.94]\n","Epoch 581:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.17, valid_loss=-1.94]\n","Epoch 582:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-2.22, valid_loss=-1.94]\n","Epoch 583:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.16, valid_loss=-1.94]\n","Epoch 584:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.05, valid_loss=-1.94]\n","Epoch 585:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.16, valid_loss=-1.94]\n","Epoch 586:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.12, valid_loss=-1.94]\n","Epoch 587:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.06, valid_loss=-1.94]\n","Epoch 588:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.00, valid_loss=-1.94]\n","Epoch 589:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.14, valid_loss=-1.94]\n","Epoch 590:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.16, valid_loss=-1.94]\n","Epoch 591:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.08, valid_loss=-1.94]\n","Epoch 592:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.13, valid_loss=-1.94]\n","Epoch 593:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.11, valid_loss=-1.94]\n","Epoch 594:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.17, valid_loss=-1.94]\n","Epoch 595:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.22, valid_loss=-1.94]\n","Epoch 596:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.08, valid_loss=-1.94]\n","Epoch 597:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.12, valid_loss=-1.94]\n","Epoch 598:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.25, valid_loss=-1.94]\n","Epoch 599:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.04, valid_loss=-1.94]\n","Epoch 599: 100%|██████████| 2/2 [00:00\u003c00:00, 18.84it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.04, valid_loss=-1.94]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12817)\u001b[0m \n","Validation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12817)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 16.06it/s]\u001b[A\n","Epoch 600:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.07, valid_loss=-1.92]\n","Epoch 601:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.13, valid_loss=-1.92]\n","Epoch 602:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.05, valid_loss=-1.92]\n","Epoch 603:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.08, valid_loss=-1.92]\n","Epoch 604:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.19, train_loss_epoch=-2.09, valid_loss=-1.92]\n","Epoch 605:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.08, valid_loss=-1.92]\n","Epoch 606:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.16, valid_loss=-1.92]\n","Epoch 607:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.14, valid_loss=-1.92]\n","Epoch 608:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.20, valid_loss=-1.92]\n","Epoch 608: 100%|██████████| 2/2 [00:00\u003c00:00, 18.87it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.15, valid_loss=-1.92]\n","Epoch 609:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.15, valid_loss=-1.92]\n","Epoch 610:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.06, valid_loss=-1.92]\n","Epoch 611:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.11, valid_loss=-1.92]\n","Epoch 612:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-2.22, valid_loss=-1.92]\n","Epoch 613:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.17, train_loss_epoch=-2.09, valid_loss=-1.92]\n","Epoch 614:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.13, valid_loss=-1.92]\n","Epoch 615:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.10, valid_loss=-1.92]\n","Epoch 616:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.16, valid_loss=-1.92]\n","Epoch 617:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.11, valid_loss=-1.92]\n","Epoch 618:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.16, valid_loss=-1.92]\n","Epoch 619:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.23, valid_loss=-1.92]\n","Epoch 620:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.22, valid_loss=-1.92]\n","Epoch 621:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.15, valid_loss=-1.92]\n","Epoch 622:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.26, train_loss_epoch=-2.21, valid_loss=-1.92]\n","Epoch 623:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.22, valid_loss=-1.92]\n","Epoch 624:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.21, valid_loss=-1.92]\n","Epoch 625:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.07, valid_loss=-1.92]\n","Epoch 626:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.17, valid_loss=-1.92]\n","Epoch 627:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.09, valid_loss=-1.92]\n","Epoch 628:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.11, valid_loss=-1.92]\n","Epoch 629:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.02, valid_loss=-1.92]\n","Epoch 630:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.07, valid_loss=-1.92]\n","Epoch 631:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.12, valid_loss=-1.92]\n","Epoch 632:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.15, valid_loss=-1.92]\n","Epoch 633:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.13, valid_loss=-1.92]\n","Epoch 634:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.17, valid_loss=-1.92]\n","Epoch 635:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.04, valid_loss=-1.92]\n","Epoch 636:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.19, valid_loss=-1.92]\n","Epoch 637:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.16, valid_loss=-1.92]\n","Epoch 638:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.16, valid_loss=-1.92]\n","Epoch 639:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.05, valid_loss=-1.92]\n","Epoch 640:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.18, valid_loss=-1.92]\n","Epoch 641:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.22, valid_loss=-1.92]\n","Epoch 642:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.24, valid_loss=-1.92]\n","Epoch 643:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.16, valid_loss=-1.92]\n","Epoch 644:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.06, valid_loss=-1.92]\n","Epoch 645:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.13, train_loss_epoch=-2.15, valid_loss=-1.92]\n","Epoch 646:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.12, valid_loss=-1.92]\n","Epoch 647:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.07, train_loss_epoch=-2.11, valid_loss=-1.92]\n","Epoch 648:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.12, valid_loss=-1.92]\n","Epoch 649:   0%|          | 0/2 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.22, valid_loss=-1.92]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=12817)\u001b[0m `Trainer.fit` stopped: `max_steps=1300.0` reached.\n","2024-11-18 13:25:43,875\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (180, 60, 1), 'n_pool_kernel_size': (4, 4, 4), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=12817)\u001b[0m \rEpoch 649: 100%|██████████| 2/2 [00:00\u003c00:00, 18.90it/s, v_num=0, train_loss_step=-2.22, train_loss_epoch=-2.22, valid_loss=-1.92]\rEpoch 649: 100%|██████████| 2/2 [00:00\u003c00:00, 18.83it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.22, valid_loss=-1.92]\n","\u001b[36m(_train_tune pid=12817)\u001b[0m \rValidation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12817)\u001b[0m \rValidation:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12817)\u001b[0m \rValidation DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12817)\u001b[0m \n","\u001b[36m(_train_tune pid=12817)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 2/2 [00:00\u003c00:00, 16.02it/s]\u001b[A\n","\u001b[36m(_train_tune pid=12817)\u001b[0m \r                                                                      \u001b[A\rEpoch 649: 100%|██████████| 2/2 [00:00\u003c00:00,  6.78it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.22, valid_loss=-1.93]\rEpoch 649: 100%|██████████| 2/2 [00:00\u003c00:00,  6.76it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.14, valid_loss=-1.93]\rEpoch 649: 100%|██████████| 2/2 [00:00\u003c00:00,  6.73it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.14, valid_loss=-1.93]\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=13227)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n","\u001b[36m(_train_tune pid=13227)\u001b[0m Seed set to 42\n","\u001b[36m(_train_tune pid=13227)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(_train_tune pid=13227)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(_train_tune pid=13227)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(_train_tune pid=13227)\u001b[0m You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(_train_tune pid=13227)\u001b[0m 2024-11-18 13:25:51.941613: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[36m(_train_tune pid=13227)\u001b[0m 2024-11-18 13:25:51.962230: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(_train_tune pid=13227)\u001b[0m 2024-11-18 13:25:51.987031: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(_train_tune pid=13227)\u001b[0m 2024-11-18 13:25:51.994563: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(_train_tune pid=13227)\u001b[0m 2024-11-18 13:25:52.012446: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","\u001b[36m(_train_tune pid=13227)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[36m(_train_tune pid=13227)\u001b[0m 2024-11-18 13:25:53.176063: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(_train_tune pid=13227)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(_train_tune pid=13227)\u001b[0m \n","\u001b[36m(_train_tune pid=13227)\u001b[0m   | Name         | Type             | Params | Mode \n","\u001b[36m(_train_tune pid=13227)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=13227)\u001b[0m 0 | loss         | DistributionLoss | 3      | train\n","\u001b[36m(_train_tune pid=13227)\u001b[0m 1 | padder_train | ConstantPad1d    | 0      | train\n","\u001b[36m(_train_tune pid=13227)\u001b[0m 2 | scaler       | TemporalNorm     | 0      | train\n","\u001b[36m(_train_tune pid=13227)\u001b[0m 3 | blocks       | ModuleList       | 2.7 M  | train\n","\u001b[36m(_train_tune pid=13227)\u001b[0m ----------------------------------------------------------\n","\u001b[36m(_train_tune pid=13227)\u001b[0m 2.7 M     Trainable params\n","\u001b[36m(_train_tune pid=13227)\u001b[0m 3         Non-trainable params\n","\u001b[36m(_train_tune pid=13227)\u001b[0m 2.7 M     Total params\n","\u001b[36m(_train_tune pid=13227)\u001b[0m 10.956    Total estimated model params size (MB)\n","\u001b[36m(_train_tune pid=13227)\u001b[0m 34        Modules in train mode\n","\u001b[36m(_train_tune pid=13227)\u001b[0m 0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00\u003c?, ?it/s]\n","Epoch 0:   0%|          | 0/13 [00:00\u003c?, ?it/s] \n","Epoch 1:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.60, train_loss_epoch=-1.58]\n","Epoch 2:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.37, train_loss_epoch=-1.75]\n","Epoch 3:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-1.79]\n","Epoch 4:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.66, train_loss_epoch=-1.80]\n","Epoch 5:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-1.87]\n","Epoch 6:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.81, train_loss_epoch=-1.83]\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.85]\n","\u001b[36m(_train_tune pid=13227)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13227)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 62.43it/s]\u001b[A\n","Epoch 7:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-1.85, valid_loss=-1.85]\n","Epoch 8:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.84, valid_loss=-1.85]\n","Epoch 9:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.57, train_loss_epoch=-1.91, valid_loss=-1.85]\n","Epoch 10:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.77, train_loss_epoch=-1.89, valid_loss=-1.85]\n","Epoch 11:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.39, train_loss_epoch=-2.00, valid_loss=-1.85]\n","Epoch 12:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.94, valid_loss=-1.85]\n","Epoch 13:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-1.92, valid_loss=-1.85]\n","Epoch 14:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.64, train_loss_epoch=-1.89, valid_loss=-1.85]\n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.65, train_loss_epoch=-1.94, valid_loss=-1.85]\n","\u001b[36m(_train_tune pid=13227)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13227)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 61.48it/s]\u001b[A\n","Epoch 15:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-1.94, valid_loss=-1.90]\n","Epoch 16:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-1.96, valid_loss=-1.90]\n","Epoch 17:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.40, train_loss_epoch=-1.99, valid_loss=-1.90]\n","Epoch 18:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-1.96, valid_loss=-1.90]\n","Epoch 19:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-1.94, valid_loss=-1.90]\n","Epoch 20:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.45, train_loss_epoch=-2.02, valid_loss=-1.90]\n","Epoch 21:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-1.97, valid_loss=-1.90]\n","Epoch 22:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-1.96, valid_loss=-1.90]\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.75, train_loss_epoch=-1.96, valid_loss=-1.90]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13227)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 63.70it/s]\u001b[A\n","Epoch 23:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-1.96, valid_loss=-1.91]\n","Epoch 24:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.53, train_loss_epoch=-2.00, valid_loss=-1.91]\n","Epoch 25:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.31, train_loss_epoch=-1.99, valid_loss=-1.91]\n","Epoch 26:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.02, valid_loss=-1.91]\n","Epoch 27:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.87, train_loss_epoch=-1.99, valid_loss=-1.91]\n","Epoch 28:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.35, train_loss_epoch=-1.97, valid_loss=-1.91]\n","Epoch 29:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.05, valid_loss=-1.91]\n","Epoch 30:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.14, train_loss_epoch=-2.02, valid_loss=-1.91]\n","\u001b[36m(_train_tune pid=13227)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13227)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 65.49it/s]\u001b[A\n","Epoch 30:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.02, valid_loss=-1.93]\n","Epoch 31:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.82, train_loss_epoch=-1.98, valid_loss=-1.93]\n","Epoch 32:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.16, train_loss_epoch=-2.02, valid_loss=-1.93]\n","Epoch 33:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.84, train_loss_epoch=-2.01, valid_loss=-1.93]\n","Epoch 34:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.06, valid_loss=-1.93]\n","Epoch 35:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.03, valid_loss=-1.93]\n","Epoch 36:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.67, train_loss_epoch=-2.09, valid_loss=-1.93]\n","Epoch 37:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.11, train_loss_epoch=-2.03, valid_loss=-1.93]\n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.93, train_loss_epoch=-2.02, valid_loss=-1.93]\n","\u001b[36m(_train_tune pid=13227)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13227)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 62.27it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13227)\u001b[0m \n","Epoch 38:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.03, train_loss_epoch=-2.02, valid_loss=-1.90]\n","Epoch 39:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.11, valid_loss=-1.90]\n","Epoch 40:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.56, train_loss_epoch=-2.03, valid_loss=-1.90]\n","Epoch 41:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.07, valid_loss=-1.90]\n","Epoch 42:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.03, valid_loss=-1.90]\n","Epoch 43:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.04, train_loss_epoch=-2.08, valid_loss=-1.90]\n","Epoch 44:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.06, valid_loss=-1.90]\n","Epoch 45:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.10, train_loss_epoch=-2.09, valid_loss=-1.90]\n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.48, train_loss_epoch=-2.12, valid_loss=-1.90]\n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13227)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 67.01it/s]\u001b[A\n","Epoch 46:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.08, train_loss_epoch=-2.12, valid_loss=-1.94]\n","Epoch 47:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-2.06, valid_loss=-1.94]\n","Epoch 48:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.36, train_loss_epoch=-2.12, valid_loss=-1.94]\n","Epoch 49:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.15, train_loss_epoch=-2.13, valid_loss=-1.94]\n","Epoch 50:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.28, train_loss_epoch=-2.11, valid_loss=-1.94]\n","Epoch 51:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.20, train_loss_epoch=-2.10, valid_loss=-1.94]\n","Epoch 52:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.60, train_loss_epoch=-2.14, valid_loss=-1.94]\n","Epoch 53:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.88, train_loss_epoch=-2.08, valid_loss=-1.94]\n","\u001b[36m(_train_tune pid=13227)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13227)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 65.40it/s]\u001b[A\n","Epoch 53:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.08, valid_loss=-1.90]\n","Epoch 54:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.01, train_loss_epoch=-2.10, valid_loss=-1.90]\n","Epoch 55:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.38, train_loss_epoch=-1.98, valid_loss=-1.90]\n","Epoch 56:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.46, train_loss_epoch=-2.13, valid_loss=-1.90]\n","Epoch 57:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.09, valid_loss=-1.90]\n","Epoch 58:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-2.16, valid_loss=-1.90]\n","Epoch 59:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-2.11, valid_loss=-1.90]\n","Epoch 60:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.91, train_loss_epoch=-2.12, valid_loss=-1.90]\n","Epoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.13, valid_loss=-1.90]\n","\u001b[36m(_train_tune pid=13227)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13227)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 64.18it/s]\u001b[A\n","Epoch 61:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.99, train_loss_epoch=-2.13, valid_loss=-1.94]\n","Epoch 62:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.95, train_loss_epoch=-2.11, valid_loss=-1.94]\n","Epoch 63:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.02, train_loss_epoch=-2.16, valid_loss=-1.94]\n","Epoch 64:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.15, valid_loss=-1.94]\n","Epoch 65:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.25, train_loss_epoch=-2.11, valid_loss=-1.94]\n","Epoch 66:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.21, train_loss_epoch=-2.15, valid_loss=-1.94]\n","Epoch 67:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.90, train_loss_epoch=-2.10, valid_loss=-1.94]\n","Epoch 68:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.80, train_loss_epoch=-2.10, valid_loss=-1.94]\n","Epoch 69:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.22, train_loss_epoch=-2.08, valid_loss=-1.94]\n","\u001b[36m(_train_tune pid=13227)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13227)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 66.68it/s]\u001b[A\n","Epoch 69:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.12, train_loss_epoch=-2.08, valid_loss=-1.94]\n","Epoch 70:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.06, train_loss_epoch=-2.12, valid_loss=-1.94]\n","Epoch 71:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-2.20, valid_loss=-1.94]\n","Epoch 72:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.49, train_loss_epoch=-2.19, valid_loss=-1.94]\n","Epoch 73:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.09, train_loss_epoch=-2.13, valid_loss=-1.94]\n","Epoch 74:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.18, train_loss_epoch=-2.08, valid_loss=-1.94]\n","Epoch 75:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.97, train_loss_epoch=-2.12, valid_loss=-1.94]\n","Epoch 76:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.43, train_loss_epoch=-2.14, valid_loss=-1.94]\n","\u001b[36m(_train_tune pid=13227)\u001b[0m \n","Validation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13227)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 64.00it/s]\u001b[A\n","Epoch 77:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.23, train_loss_epoch=-2.19, valid_loss=-1.92]\n","Epoch 78:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.32, train_loss_epoch=-2.17, valid_loss=-1.92]\n","Epoch 79:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.89, train_loss_epoch=-2.14, valid_loss=-1.92]\n","Epoch 80:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.00, train_loss_epoch=-2.15, valid_loss=-1.92]\n","Epoch 80: 100%|██████████| 13/13 [00:00\u003c00:00, 61.19it/s, v_num=0, train_loss_step=-2.51, train_loss_epoch=-2.17, valid_loss=-1.92]\n","Epoch 81:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.51, train_loss_epoch=-2.17, valid_loss=-1.92]\n","Epoch 82:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.50, train_loss_epoch=-2.16, valid_loss=-1.92]\n","Epoch 83:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.05, train_loss_epoch=-2.19, valid_loss=-1.92]\n","Epoch 84:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-2.24, train_loss_epoch=-2.16, valid_loss=-1.92]\n"]},{"name":"stderr","output_type":"stream","text":["2024-11-18 13:26:15,947\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'hist_exog_list': ('DY', 'PTBV', 'P', 'PO', 'VO', 'PE'), 'loss': ('__ref_ph', 'de895953'), 'n_freq_downsample': (60, 8, 1), 'n_pool_kernel_size': (4, 4, 4), 'valid_loss': ('__ref_ph', '004b9a7a')}\n","2024-11-18 13:26:15,970\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/root/ray_results/_train_tune_2024-11-18_13-03-34' in 0.0192s.\n","2024-11-18 13:26:15,973\tERROR tune.py:1037 -- Trials did not complete: [_train_tune_a76ac303, _train_tune_423843ea]\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=13227)\u001b[0m \n","\u001b[36m(_train_tune pid=13227)\u001b[0m \rValidation: |          | 0/? [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13227)\u001b[0m \rValidation:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13227)\u001b[0m \rValidation DataLoader 0:   0%|          | 0/13 [00:00\u003c?, ?it/s]\u001b[A\n","\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_train_tune pid=13227)\u001b[0m `Trainer.fit` stopped: `max_steps=1100.0` reached.\n","INFO:lightning_fabric.utilities.seed:Seed set to 42\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n","INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_train_tune pid=13227)\u001b[0m \n","\u001b[36m(_train_tune pid=13227)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 13/13 [00:00\u003c00:00, 69.23it/s]\u001b[A\n","\u001b[36m(_train_tune pid=13227)\u001b[0m \r                                                                        \u001b[A\rEpoch 84:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-2.16, valid_loss=-1.92]\rEpoch 84:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-2.15, valid_loss=-1.92]\rEpoch 84:   0%|          | 0/13 [00:00\u003c?, ?it/s, v_num=0, train_loss_step=-1.92, train_loss_epoch=-2.15, valid_loss=-1.92]\n"]},{"name":"stderr","output_type":"stream","text":["INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","INFO:pytorch_lightning.callbacks.model_summary:\n","  | Name         | Type             | Params | Mode \n","----------------------------------------------------------\n","0 | loss         | DistributionLoss | 3      | eval \n","1 | padder_train | ConstantPad1d    | 0      | train\n","2 | scaler       | TemporalNorm     | 0      | train\n","3 | blocks       | ModuleList       | 2.8 M  | train\n","----------------------------------------------------------\n","2.8 M     Trainable params\n","3         Non-trainable params\n","2.8 M     Total params\n","11.288    Total estimated model params size (MB)\n","33        Modules in train mode\n","1         Modules in eval mode\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6fe4793ea2cb4b44a0d414c27f07956a","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6fce464b78a04fb3996f498909078686","version_major":2,"version_minor":0},"text/plain":["Training: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d6be4531ba934e5c980ded47b4e24796","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8d44ef5554a64135a7a65a0db60cd5e2","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3dfc670def694eb597ef5c9af8939535","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b087081446e44d42808fac11f3a77027","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a9cdb0589f1f43508f2e52f20233fd6a","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fce331a295b24360b125770b0952bbca","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dca3665bd85445a78f0237c44e66dc17","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1e7cfb10131546be995ac1acbf9848b8","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b0d67535172746deb8b0fde69d4d5606","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2e6b2f2a05394ffc838ef808de6a52ff","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d69d0c6a186746b59c7020cc5c1bf401","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"99bf472cd9744ad186cb2536b3de5821","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_steps=1200.0` reached.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n","INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"908ccfe57d454693a013862783bf5959","version_major":2,"version_minor":0},"text/plain":["Predicting: |          | 0/? [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Horizon 20 CV Minutes: 24.180792804559072\n"]}],"source":["# Initialize and run the trainer\n","trainer = AutoNHITSTrainer(horizons, levels, exog_list, df, val_size, test_size)\n","trainer.run_training()"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPwTn8NE6o98wrpubbgK1UZ","gpuType":"A100","machine_shape":"hm","name":"","toc_visible":true,"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}